{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing before main analysis\n",
    "\n",
    "- Note that there are fluctuations in the illumination intensity which may be resulting in pathological behavior from the reporter\n",
    "\n",
    "- This has been normalized out in the upstream processing, but try to fix long term\n",
    "\n",
    "- Also consider a flat field correction for the final experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask\n",
    "import warnings\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import (\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    "    cosine_distances,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "\n",
    "\n",
    "import pylab\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgrnadf_from_scoredf(\n",
    "    scoredf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    scoredf_groupby = scoredf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        scoredf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": score\"] = scoredf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label + \": score\"].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = scoredf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = scoredf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = scoredf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = scoredf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = scoredf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = scoredf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "def normalize_timeseries(feature_vector_series, lmbda=0.5):\n",
    "    timeseries_arr = np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    sigma = np.std(timeseries_arr, axis=1)\n",
    "    if lmbda > 0.0:\n",
    "        sigma_prime = ((sigma + 1) ** lmbda - 1) / lmbda  ##yeo-johnson\n",
    "    elif lmbda == 0.0:\n",
    "        sigma_prime = np.log(sigma + 1)\n",
    "    else:\n",
    "        raise ValueError(\"lmbda cannot be negative\")\n",
    "    normalizer = sigma / sigma_prime\n",
    "    normalized_timeseries = timeseries_arr / normalizer[:, np.newaxis, :]\n",
    "    return normalized_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"02:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=100,\n",
    "#     n_workers_min=20,\n",
    "#     memory=\"16GB\",\n",
    "#     working_directory=\"/home/de64/scratch/de64/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "\n",
    "# small testing deployment\n",
    "\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    n_workers_min=100,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_lineage = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Cell_Cycle/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "# final_output_df_lineage = final_output_df_lineage.loc[:final_output_df_lineage.divisions[4]-1] #getting a small subset\n",
    "final_output_df_lineage = final_output_df_lineage.dropna(\n",
    "    subset=[\"Final timepoints\", \"Division: major_axis_length\"]\n",
    ")\n",
    "final_output_df_lineage = (\n",
    "    final_output_df_lineage.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .repartition(npartitions=100)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "final_output_df_lineage_timepoints = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Observations/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "# final_output_df_lineage_timepoints = final_output_df_lineage_timepoints.loc[:final_output_df_lineage_timepoints.divisions[4]-1] #getting a small subset\n",
    "final_output_df_lineage_timepoints = final_output_df_lineage_timepoints.dropna(\n",
    "    subset=[\"major_axis_length\", \"minor_axis_length\", \"mCherry mean_intensity\"]\n",
    ")\n",
    "final_output_df_lineage_timepoints = (\n",
    "    final_output_df_lineage_timepoints.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .repartition(npartitions=100)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "final_output_df_lineage_growth = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Growth_Observations/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "# final_output_df_lineage_delta_timepoints = final_output_df_lineage_delta_timepoints.loc[:final_output_df_lineage_delta_timepoints.divisions[4]-1] #getting a small subset\n",
    "final_output_df_lineage_growth = final_output_df_lineage_growth.dropna(\n",
    "    subset=[\"Growth Rate: Volume\"]\n",
    ")\n",
    "final_output_df_lineage_growth = (\n",
    "    final_output_df_lineage_growth.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .repartition(npartitions=100)\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_lineage_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Filter for \"Normal\" Sizes at Start\n",
    "\n",
    "1) Fit a gaussian model to each of the specified feature params during the first t timepoints of the experiment (using a subsample for speed) \n",
    "2) Compute a normalized probability trenchwise for these features under the gaussian model, during the first t timepoints of the experiment\n",
    "3) Eliminate trenches that are under some p percentile value of this probability for each feature\n",
    "4) Display histograms for each property as well as the resulting theshold\n",
    "\n",
    "Note that these features should be the only features examined in the resulting analysis. For the notebook, I am looking at:\n",
    "- Birth length (Lb)\n",
    "- Division length (Ld)\n",
    "- Mean Area Increment\n",
    "- Mean Length Increment\n",
    "- Mean Width\n",
    "- Cell cycle duration (Delta t)\n",
    "- Mean mCherry Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_early_outliers(\n",
    "    final_output_df_lineage,\n",
    "    final_output_df_lineage_timepoints,\n",
    "    final_output_df_lineage_growth,\n",
    "    early_time_cutoff=7200,\n",
    "    gaussian_subsample_rates=[0.2, 0.1, 0.1],\n",
    "    percentile_threshold=10,\n",
    "    cell_cycle_params=[\"Division: major_axis_length\"],\n",
    "    timepoint_params=[\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"mCherry mean_intensity\",\n",
    "    ],\n",
    "    growth_params=[\"Growth Rate: Volume\"],\n",
    "    plot_values_names=[\n",
    "        \"Division Length\",\n",
    "        \"Length\",\n",
    "        \"Width\",\n",
    "        \"mCherry Intensity\",\n",
    "        \"Growth Rate\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    final_output_df_trench_groupby = final_output_df_lineage.groupby(\n",
    "        \"phenotype trenchid\", sort=False\n",
    "    )\n",
    "    final_output_df_lineage_timepoints_groupby = (\n",
    "        final_output_df_lineage_timepoints.groupby(\"phenotype trenchid\", sort=False)\n",
    "    )\n",
    "    final_output_df_lineage_growth_groupby = final_output_df_lineage_growth.groupby(\n",
    "        \"phenotype trenchid\", sort=False\n",
    "    )\n",
    "\n",
    "    early_tpt_df = final_output_df_trench_groupby.apply(\n",
    "        lambda x: x[x[\"Final time (s)\"] < early_time_cutoff].reset_index(drop=True)\n",
    "    ).persist()\n",
    "    early_tpt_df_timepoints = final_output_df_lineage_timepoints_groupby.apply(\n",
    "        lambda x: x[x[\"Observation time (s)\"] < early_time_cutoff].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "    ).persist()\n",
    "    early_tpt_df_growth = final_output_df_lineage_growth_groupby.apply(\n",
    "        lambda x: x[x[\"Measurement time (s)\"] < early_time_cutoff].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "    ).persist()\n",
    "\n",
    "    for filter_param in cell_cycle_params:\n",
    "        early_param_series = early_tpt_df[filter_param]\n",
    "        all_param_values = (\n",
    "            early_param_series.sample(frac=gaussian_subsample_rates[0])\n",
    "            .compute()\n",
    "            .tolist()\n",
    "        )\n",
    "        gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "        gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "        early_param_series = dd.from_pandas(\n",
    "            early_param_series.compute().droplevel(1), npartitions=50\n",
    "        )\n",
    "        trench_probability = early_param_series.groupby(\"phenotype trenchid\").apply(\n",
    "            lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "        )\n",
    "\n",
    "        final_output_df_lineage[\n",
    "            filter_param + \": Probability\"\n",
    "        ] = trench_probability.persist()\n",
    "\n",
    "    for filter_param in timepoint_params:\n",
    "        early_param_series = early_tpt_df_timepoints[filter_param]\n",
    "        all_param_values = (\n",
    "            early_param_series.sample(frac=gaussian_subsample_rates[1])\n",
    "            .compute()\n",
    "            .tolist()\n",
    "        )\n",
    "        gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "        gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "        early_param_series = dd.from_pandas(\n",
    "            early_param_series.compute().droplevel(1), npartitions=50\n",
    "        )\n",
    "        trench_probability = early_param_series.groupby(\"phenotype trenchid\").apply(\n",
    "            lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "        )\n",
    "\n",
    "        final_output_df_lineage_timepoints[\n",
    "            filter_param + \": Probability\"\n",
    "        ] = trench_probability.persist()\n",
    "\n",
    "    for filter_param in growth_params:\n",
    "        early_param_series = early_tpt_df_growth[filter_param]\n",
    "        all_param_values = (\n",
    "            early_param_series.sample(frac=gaussian_subsample_rates[2])\n",
    "            .compute()\n",
    "            .tolist()\n",
    "        )\n",
    "        gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "        gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "        early_param_series = dd.from_pandas(\n",
    "            early_param_series.compute().droplevel(1), npartitions=50\n",
    "        )\n",
    "        trench_probability = early_param_series.groupby(\"phenotype trenchid\").apply(\n",
    "            lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "        )\n",
    "\n",
    "        final_output_df_lineage_growth[\n",
    "            filter_param + \": Probability\"\n",
    "        ] = trench_probability.persist()\n",
    "\n",
    "    final_output_df_onetrench = (\n",
    "        final_output_df_lineage.groupby(\"phenotype trenchid\")\n",
    "        .apply(lambda x: x.iloc[0])\n",
    "        .compute()\n",
    "    )\n",
    "    final_output_df_timepoints_onetrench = (\n",
    "        final_output_df_lineage_timepoints.groupby(\"phenotype trenchid\")\n",
    "        .apply(lambda x: x.iloc[0])\n",
    "        .compute()\n",
    "    )\n",
    "    final_output_df_growth_onetrench = (\n",
    "        final_output_df_lineage_growth.groupby(\"phenotype trenchid\")\n",
    "        .apply(lambda x: x.iloc[0])\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(22, 16))\n",
    "\n",
    "    lineage_query_list = []\n",
    "    plot_idx = 0\n",
    "    for i, filter_param in enumerate(cell_cycle_params):\n",
    "        prob_threshold = np.nanpercentile(\n",
    "            final_output_df_onetrench[filter_param + \": Probability\"].tolist(),\n",
    "            percentile_threshold,\n",
    "        )\n",
    "        query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "        lineage_query_list.append(query)\n",
    "\n",
    "        min_v, max_v = (\n",
    "            np.nanpercentile(\n",
    "                final_output_df_onetrench[filter_param + \": Probability\"], 5\n",
    "            ),\n",
    "            np.max(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "        )\n",
    "\n",
    "        plt.subplot(2, 3, plot_idx + 1)\n",
    "        plt.title(plot_values_names[plot_idx], fontsize=22)\n",
    "        plt.xlabel(\"Unnormalized Likelihood\", fontsize=18)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.hist(\n",
    "            final_output_df_onetrench[\n",
    "                final_output_df_onetrench[filter_param + \": Probability\"]\n",
    "                < prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plt.hist(\n",
    "            final_output_df_onetrench[\n",
    "                final_output_df_onetrench[filter_param + \": Probability\"]\n",
    "                >= prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plot_idx += 1\n",
    "\n",
    "    lineage_timepoint_query_list = []\n",
    "    for i, filter_param in enumerate(timepoint_params):\n",
    "        prob_threshold = np.nanpercentile(\n",
    "            final_output_df_timepoints_onetrench[\n",
    "                filter_param + \": Probability\"\n",
    "            ].tolist(),\n",
    "            percentile_threshold,\n",
    "        )\n",
    "        query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "        lineage_timepoint_query_list.append(query)\n",
    "\n",
    "        min_v, max_v = (\n",
    "            np.nanpercentile(\n",
    "                final_output_df_timepoints_onetrench[filter_param + \": Probability\"], 5\n",
    "            ),\n",
    "            np.max(\n",
    "                final_output_df_timepoints_onetrench[filter_param + \": Probability\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        plt.subplot(2, 3, plot_idx + 1)\n",
    "        plt.title(plot_values_names[plot_idx], fontsize=22)\n",
    "        plt.xlabel(\"Unnormalized Likelihood\", fontsize=18)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.hist(\n",
    "            final_output_df_timepoints_onetrench[\n",
    "                final_output_df_timepoints_onetrench[filter_param + \": Probability\"]\n",
    "                < prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plt.hist(\n",
    "            final_output_df_timepoints_onetrench[\n",
    "                final_output_df_timepoints_onetrench[filter_param + \": Probability\"]\n",
    "                >= prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plot_idx += 1\n",
    "\n",
    "    lineage_growth_query_list = []\n",
    "    for i, filter_param in enumerate(growth_params):\n",
    "        prob_threshold = np.nanpercentile(\n",
    "            final_output_df_growth_onetrench[filter_param + \": Probability\"].tolist(),\n",
    "            percentile_threshold,\n",
    "        )\n",
    "        query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "        lineage_growth_query_list.append(query)\n",
    "\n",
    "        min_v, max_v = (\n",
    "            np.nanpercentile(\n",
    "                final_output_df_growth_onetrench[filter_param + \": Probability\"], 5\n",
    "            ),\n",
    "            np.max(final_output_df_growth_onetrench[filter_param + \": Probability\"]),\n",
    "        )\n",
    "\n",
    "        plt.subplot(2, 3, plot_idx + 1)\n",
    "        plt.title(plot_values_names[plot_idx], fontsize=22)\n",
    "        plt.xlabel(\"Unnormalized Likelihood\", fontsize=18)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.hist(\n",
    "            final_output_df_growth_onetrench[\n",
    "                final_output_df_growth_onetrench[filter_param + \": Probability\"]\n",
    "                < prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plt.hist(\n",
    "            final_output_df_growth_onetrench[\n",
    "                final_output_df_growth_onetrench[filter_param + \": Probability\"]\n",
    "                >= prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plot_idx += 1\n",
    "\n",
    "    compiled_lineage_query = \" and \".join(lineage_query_list)\n",
    "    compiled_lineage_timepoint_query = \" and \".join(lineage_timepoint_query_list)\n",
    "    compiled_lineage_growth_query = \" and \".join(lineage_growth_query_list)\n",
    "\n",
    "    final_output_df_onetrench_filtered = final_output_df_onetrench.query(\n",
    "        compiled_lineage_query\n",
    "    )\n",
    "    final_output_df_timepoints_onetrench_filtered = (\n",
    "        final_output_df_timepoints_onetrench.query(compiled_lineage_timepoint_query)\n",
    "    )\n",
    "    final_output_df_growth_onetrench_filtered = final_output_df_growth_onetrench.query(\n",
    "        compiled_lineage_growth_query\n",
    "    )\n",
    "\n",
    "    all_idx_list = sorted(\n",
    "        (\n",
    "            set(final_output_df_onetrench_filtered.index.tolist())\n",
    "            & set(final_output_df_timepoints_onetrench_filtered.index.tolist())\n",
    "        )\n",
    "        & set(final_output_df_growth_onetrench_filtered.index.tolist())\n",
    "    )\n",
    "\n",
    "    final_output_df_filtered = final_output_df_lineage.loc[all_idx_list].persist()\n",
    "    final_output_df_timepoints_filtered = final_output_df_lineage_timepoints.loc[\n",
    "        all_idx_list\n",
    "    ].persist()\n",
    "    final_output_df_growth_filtered = final_output_df_lineage_growth.loc[\n",
    "        all_idx_list\n",
    "    ].persist()\n",
    "\n",
    "    return (\n",
    "        final_output_df_filtered,\n",
    "        final_output_df_timepoints_filtered,\n",
    "        final_output_df_growth_filtered,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    final_output_df_filtered,\n",
    "    final_output_df_timepoints_filtered,\n",
    "    final_output_df_growth_filtered,\n",
    ") = remove_early_outliers(\n",
    "    final_output_df_lineage,\n",
    "    final_output_df_lineage_timepoints,\n",
    "    final_output_df_lineage_growth,\n",
    "    early_time_cutoff=7200,\n",
    "    gaussian_subsample_rates=[0.2, 0.1, 0.1],\n",
    "    percentile_threshold=10,\n",
    ")\n",
    "\n",
    "plt.savefig(\"Prob_threshold_Replicate_3.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_output_df_filtered) / len(final_output_df_lineage)\n",
    "len(final_output_df_timepoints_filtered) / len(final_output_df_lineage_timepoints)\n",
    "len(final_output_df_growth_filtered) / len(final_output_df_lineage_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.cancel(final_output_df_lineage)\n",
    "dask_controller.daskclient.cancel(final_output_df_lineage_timepoints)\n",
    "dask_controller.daskclient.cancel(final_output_df_lineage_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df, label, min_time, max_time, time_label=\"time (s)\", flatten_vals=True\n",
    "):\n",
    "    if flatten_vals:\n",
    "        masked_label_series = df.apply(\n",
    "            lambda x: np.array(x[label])[\n",
    "                (np.array(x[time_label]) >= min_time)\n",
    "                * (np.array(x[time_label]) <= max_time)\n",
    "            ],\n",
    "            axis=1,\n",
    "            meta=\"object\",\n",
    "        )\n",
    "        flattened_vals = np.concatenate(masked_label_series.compute().tolist())\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        masked_label_series = (\n",
    "            df.groupby(\"phenotype trenchid\")\n",
    "            .apply(\n",
    "                lambda x: np.array(x[label])[\n",
    "                    (np.array(x[time_label]) >= min_time)\n",
    "                    * (np.array(x[time_label]) <= max_time)\n",
    "                ],\n",
    "                meta=\"object\",\n",
    "            )\n",
    "            .persist()\n",
    "        )\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "def get_feature_stats(df, feature_label, min_time, max_time, time_label=\"time (s)\"):\n",
    "    feature_vals = get_timepoint_values(\n",
    "        df, feature_label, min_time, max_time, time_label=time_label\n",
    "    )\n",
    "    feature_median = np.median(feature_vals)\n",
    "    feature_iqr = sp.stats.iqr(feature_vals)\n",
    "    return feature_median, feature_iqr\n",
    "\n",
    "\n",
    "def compute_score(df, feature_label, feature_median, feature_iqr):\n",
    "    scores = 1.35 * ((df[feature_label] - feature_median) / feature_iqr)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_feature_scores(\n",
    "    df, feature_label, init_time_range=(0, 7200), time_label=\"time (s)\"\n",
    "):\n",
    "    feature_median, feature_iqr = get_feature_stats(\n",
    "        df, feature_label, init_time_range[0], init_time_range[1], time_label=time_label\n",
    "    )\n",
    "    scores = compute_score(df, feature_label, feature_median, feature_iqr)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_scores(\n",
    "    df, feature_labels, init_time_range=(0, 7200), time_label=\"time (s)\"\n",
    "):\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_scores(\n",
    "            df, feature_label, init_time_range=init_time_range, time_label=time_label\n",
    "        )\n",
    "        df[feature_label + \": z score\"] = feature_scores.persist()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Properties (maybe go back to the per trench normalization?)\n",
    "\n",
    "1) Yeo-Johnson transform the data to get a more normal-like distribution.\n",
    "2) Convert transformed values to time-dependent z-scores using the following formula:\n",
    "\n",
    "$$ z(i,k,t) = 1.35 \\times \\frac{F_{i,k,t} - median_{t\\in \\tau}(F_{i,t})}{iqr_{t\\in \\tau}(F_{i,t})} $$\n",
    "\n",
    "where $F_{i,k,t}$ are the feature values for feature i, trench k at time t. $\\tau$ are the initial pre-induction timepoints. \n",
    "\n",
    "Essentially this is a z-score using the more outlier robust median and interquartile range to define the differences from normal bahavior. The 1.35 factor scales the values such that z-scores represent number of standard deviations from the mean for a normal distribution. Finally the values are normalized by initial behaviors trenchwise by the $median_{t\\in \\tau}(F_{i,k,t})$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_yj_transform(\n",
    "    final_output_df_filtered,\n",
    "    yeo_subsample=0.05,\n",
    "    early_time_cutoff=7200,\n",
    "    params_to_transform=[\"Division: major_axis_length\"],\n",
    "    time_label=\"time (s)\",\n",
    "):\n",
    "\n",
    "    subsample_df = final_output_df_filtered.sample(frac=yeo_subsample).persist()\n",
    "\n",
    "    for i, param in enumerate(params_to_transform):\n",
    "        all_param_values = subsample_df[param].astype(float).compute().tolist()\n",
    "        all_param_values = np.array(all_param_values)\n",
    "        all_param_values = all_param_values[~np.isnan(all_param_values)]\n",
    "        l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "        final_output_df_filtered[param + \": Yeo-Johnson\"] = (\n",
    "            final_output_df_filtered[param]\n",
    "            .apply(\n",
    "                lambda x: sp.stats.yeojohnson(x, lmbda=l_norm),\n",
    "                meta=(param + \": Yeo-Johnson\", float),\n",
    "            )\n",
    "            .persist()\n",
    "        )\n",
    "\n",
    "    final_output_df_filtered = get_all_feature_scores(\n",
    "        final_output_df_filtered,\n",
    "        [param + \": Yeo-Johnson\" for param in params_to_transform],\n",
    "        init_time_range=(0, early_time_cutoff),\n",
    "        time_label=time_label,\n",
    "    )\n",
    "    trenchiddf = final_output_df_filtered.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True\n",
    "    )\n",
    "\n",
    "    return final_output_df_filtered, trenchiddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_filtered, trenchiddf = apply_yj_transform(\n",
    "    final_output_df_filtered,\n",
    "    yeo_subsample=0.05,\n",
    "    early_time_cutoff=7200,\n",
    "    params_to_transform=[\"Division: major_axis_length\"],\n",
    "    time_label=\"time (s)\",\n",
    ")\n",
    "trenchiddf = trenchiddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_timepoints_filtered, trenchiddf_timepoints = apply_yj_transform(\n",
    "    final_output_df_timepoints_filtered,\n",
    "    yeo_subsample=0.01,\n",
    "    early_time_cutoff=7200,\n",
    "    params_to_transform=[\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"mCherry mean_intensity\",\n",
    "    ],\n",
    "    time_label=\"Observation time (s)\",\n",
    ")\n",
    "trenchiddf_timepoints = trenchiddf_timepoints.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_growth_filtered, trenchiddf_growth = apply_yj_transform(\n",
    "    final_output_df_growth_filtered,\n",
    "    yeo_subsample=0.01,\n",
    "    early_time_cutoff=7200,\n",
    "    params_to_transform=[\"Growth Rate: Volume\"],\n",
    "    time_label=\"Measurement time (s)\",\n",
    ")\n",
    "trenchiddf_growth = trenchiddf_growth.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to interpolate trenchwise instead of sgRNAwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric import kernel_regression\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import sklearn as skl\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def timeseries_kernel_reg(\n",
    "    df,\n",
    "    y_label,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    kernel_bins,\n",
    "    kernel_bandwidth,\n",
    "    time_label=\"Cell Cycle\",\n",
    "):\n",
    "    def kernel_reg(\n",
    "        x_arr,\n",
    "        y_arr,\n",
    "        start=min_tpt,\n",
    "        end=max_tpt,\n",
    "        kernel_bins=kernel_bins,\n",
    "        kernel_bandwidth=kernel_bandwidth,\n",
    "    ):\n",
    "        intervals = np.linspace(start, end, num=kernel_bins, dtype=float)\n",
    "        w = kernel_regression.KernelReg(\n",
    "            y_arr,\n",
    "            x_arr,\n",
    "            \"c\",\n",
    "            reg_type=\"lc\",\n",
    "            bw=np.array([kernel_bandwidth]),\n",
    "            ckertype=\"gaussian\",\n",
    "        ).fit(intervals)[0]\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    if time_label == \"Cell Cycle\":\n",
    "        # if this is the cell cycle df, do the following\n",
    "        kernel_result = df.groupby(\"phenotype trenchid\").apply(\n",
    "            lambda x: kernel_reg(\n",
    "                (x[\"Final time (s)\"].values - (x[\"Delta time (s)\"].values / 2)),\n",
    "                x[y_label].values,\n",
    "            )[1],\n",
    "            meta=(y_label, object),\n",
    "        )\n",
    "    else:\n",
    "        kernel_result = df.groupby(\"phenotype trenchid\").apply(\n",
    "            lambda x: kernel_reg(\n",
    "                x[time_label].values,\n",
    "                x[y_label].values,\n",
    "            )[1],\n",
    "            meta=(y_label, object),\n",
    "        )\n",
    "    return kernel_result\n",
    "\n",
    "\n",
    "def get_all_kernel_regs(\n",
    "    df,\n",
    "    y_label_list,\n",
    "    min_tpt=0,\n",
    "    max_tpt=36000,\n",
    "    kernel_bins=20,\n",
    "    kernel_bandwidth=2700,\n",
    "    time_label=\"Cell Cycle\",\n",
    "):\n",
    "    out_df = copy.copy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        kernel_result = timeseries_kernel_reg(\n",
    "            out_df,\n",
    "            y_label,\n",
    "            min_tpt,\n",
    "            max_tpt,\n",
    "            kernel_bins,\n",
    "            kernel_bandwidth,\n",
    "            time_label=time_label,\n",
    "        )\n",
    "        out_df[\"Kernel Trace: \" + y_label] = kernel_result.persist()\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_cycle_params_to_transform = [\"Division: major_axis_length\"]\n",
    "cell_cycle_score_params = [\n",
    "    param + \": Yeo-Johnson: z score\" for param in cell_cycle_params_to_transform\n",
    "]\n",
    "cell_cycle_other_params = [\n",
    "    \"Birth: area\",\n",
    "    \"Division: area\",\n",
    "    \"Delta: area\",\n",
    "    \"Birth: major_axis_length\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Delta: major_axis_length\",\n",
    "    \"Birth: minor_axis_length\",\n",
    "    \"Division: minor_axis_length\",\n",
    "    \"Delta: minor_axis_length\",\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Delta: Volume\",\n",
    "    \"Birth: Surface Area\",\n",
    "    \"Division: Surface Area\",\n",
    "    \"Delta: Surface Area\",\n",
    "    \"Final timepoints\",\n",
    "    \"Delta Timepoints\",\n",
    "    \"Final time (s)\",\n",
    "    \"Delta time (s)\",\n",
    "]\n",
    "\n",
    "cell_cycle_params_to_trace = cell_cycle_score_params + cell_cycle_other_params\n",
    "\n",
    "# making an observation grid to project vals onto\n",
    "\n",
    "trenchiddf = get_all_kernel_regs(\n",
    "    trenchiddf,\n",
    "    cell_cycle_params_to_trace,\n",
    "    min_tpt=0,\n",
    "    max_tpt=36000,\n",
    "    kernel_bins=20,\n",
    "    kernel_bandwidth=2700,\n",
    "    time_label=\"Cell Cycle\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_cycle_timepoints_params_to_transform = [\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    "    \"mCherry mean_intensity\",\n",
    "]\n",
    "cell_cycle_timepoints_score_params = [\n",
    "    param + \": Yeo-Johnson: z score\"\n",
    "    for param in cell_cycle_timepoints_params_to_transform\n",
    "]\n",
    "cell_cycle_timepoints_other_params = [\n",
    "    \"area\",\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "    \"mCherry mean_intensity\",\n",
    "]\n",
    "\n",
    "cell_cycle_timepoints_params_to_trace = (\n",
    "    cell_cycle_timepoints_score_params + cell_cycle_timepoints_other_params\n",
    ")\n",
    "\n",
    "# making an observation grid to project vals onto\n",
    "\n",
    "trenchiddf_timepoints = get_all_kernel_regs(\n",
    "    trenchiddf_timepoints,\n",
    "    cell_cycle_timepoints_params_to_trace,\n",
    "    min_tpt=0,\n",
    "    max_tpt=36000,\n",
    "    kernel_bins=20,\n",
    "    kernel_bandwidth=2700,\n",
    "    time_label=\"Observation time (s)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_params_to_transform = [\"Growth Rate: Volume\"]\n",
    "growth_score_params = [\n",
    "    param + \": Yeo-Johnson: z score\" for param in growth_params_to_transform\n",
    "]\n",
    "growth_other_params = [\n",
    "    \"Growth Rate: Volume\",\n",
    "    \"Growth Rate: major_axis_length\",\n",
    "    \"Growth Rate: Surface Area\",\n",
    "]\n",
    "\n",
    "growth_params_to_trace = growth_score_params + growth_other_params\n",
    "\n",
    "# making an observation grid to project vals onto\n",
    "\n",
    "trenchiddf_growth = get_all_kernel_regs(\n",
    "    trenchiddf_growth,\n",
    "    growth_params_to_trace,\n",
    "    min_tpt=0,\n",
    "    max_tpt=36000,\n",
    "    kernel_bins=20,\n",
    "    kernel_bandwidth=2700,\n",
    "    time_label=\"Measurement time (s)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can move this around to compute earlier, or write to disk\n",
    "all_traced_params = (\n",
    "    cell_cycle_params_to_trace\n",
    "    + cell_cycle_timepoints_params_to_trace\n",
    "    + growth_params_to_trace\n",
    ")\n",
    "all_traced_params = [\"Kernel Trace: \" + item for item in all_traced_params]\n",
    "\n",
    "trenchiddf_out = trenchiddf.groupby(\"phenotype trenchid\").apply(lambda x: x.iloc[0])\n",
    "trenchiddf_timepoints_out = trenchiddf_timepoints.groupby(\"phenotype trenchid\").apply(\n",
    "    lambda x: x.iloc[0]\n",
    ")\n",
    "trenchiddf_growth_out = trenchiddf_growth.groupby(\"phenotype trenchid\").apply(\n",
    "    lambda x: x.iloc[0]\n",
    ")\n",
    "\n",
    "traced_col_names = [\n",
    "    col_name\n",
    "    for col_name in trenchiddf_out.columns.tolist()\n",
    "    if col_name in all_traced_params\n",
    "]\n",
    "trenchiddf_out = trenchiddf_out[\n",
    "    [\n",
    "        \"Global CellID\",\n",
    "        \"File Parquet Index\",\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"initial timepoints\",\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"CellID\",\n",
    "        \"Trench Score\",\n",
    "        \"Mother CellID\",\n",
    "        \"Daughter CellID 1\",\n",
    "        \"Daughter CellID 2\",\n",
    "        \"Sister CellID\",\n",
    "        \"Centroid X\",\n",
    "        \"Centroid Y\",\n",
    "        \"Kymograph File Parquet Index\",\n",
    "        \"Kymograph FOV Parquet Index\",\n",
    "        \"FOV Parquet Index\",\n",
    "    ]\n",
    "    + traced_col_names\n",
    "]\n",
    "\n",
    "traced_col_names = [\n",
    "    col_name\n",
    "    for col_name in trenchiddf_timepoints_out.columns.tolist()\n",
    "    if col_name in all_traced_params\n",
    "]\n",
    "trenchiddf_timepoints_out = trenchiddf_timepoints_out[traced_col_names]\n",
    "\n",
    "traced_col_names = [\n",
    "    col_name\n",
    "    for col_name in trenchiddf_growth_out.columns.tolist()\n",
    "    if col_name in all_traced_params\n",
    "]\n",
    "trenchiddf_growth_out = trenchiddf_growth_out[traced_col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp until split up a bit at earlier stages to save memory\n",
    "trenchiddf_merged_out = dd.concat(\n",
    "    [trenchiddf_out, trenchiddf_timepoints_out, trenchiddf_growth_out], axis=1\n",
    ")\n",
    "trenchiddf_merged_out = trenchiddf_merged_out.compute()\n",
    "\n",
    "dask_controller.daskclient.cancel(final_output_df_filtered)\n",
    "dask_controller.daskclient.cancel(final_output_df_timepoints_filtered)\n",
    "dask_controller.daskclient.cancel(final_output_df_growth_filtered)\n",
    "\n",
    "dask_controller.reset_worker_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchiddf_merged_out_pandas = dd.from_pandas(trenchiddf_merged_out, npartitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchiddf_merged_out_pandas = trenchiddf_merged_out_pandas.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_barcodes = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Final_Barcodes_df/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .groupby(\"phenotype trenchid\", sort=False)\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .persist()\n",
    ")\n",
    "final_output_df_barcodes = final_output_df_barcodes.reset_index().set_index(\n",
    "    \"oDEPool7_id\", drop=True\n",
    ")\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.reset_index().set_index(\"phenotype trenchid\").persist()\n",
    ")\n",
    "\n",
    "trenchiddf_merged_out_pandas = trenchiddf_merged_out_pandas.merge(\n",
    "    final_output_df_barcodes[\n",
    "        [\n",
    "            \"oDEPool7_id\",\n",
    "            \"Barcode\",\n",
    "            \"sgRNA\",\n",
    "            \"Closest Hamming Distance\",\n",
    "            \"EcoWG1_id\",\n",
    "            \"Gene\",\n",
    "            \"N Mismatch\",\n",
    "            \"Category\",\n",
    "            \"TargetID\",\n",
    "            \"barcodeid\",\n",
    "        ]\n",
    "    ],\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").persist()\n",
    "\n",
    "n_obs = (\n",
    "    trenchiddf_merged_out_pandas.groupby(\"oDEPool7_id\", sort=False)\n",
    "    .apply(lambda x: len(x.index.unique()), meta=int)\n",
    "    .compute()\n",
    ")\n",
    "n_obs = pd.DataFrame(n_obs).rename({0: \"N Observations\"}, axis=1).sort_index()\n",
    "trenchiddf_merged_out_pandas = trenchiddf_merged_out_pandas.merge(\n",
    "    n_obs, on=\"oDEPool7_id\", how=\"inner\", right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_transformed_params = (\n",
    "    cell_cycle_params_to_transform\n",
    "    + cell_cycle_timepoints_params_to_transform\n",
    "    + growth_params_to_transform\n",
    ")\n",
    "kernel_score_params = [\n",
    "    \"Kernel Trace: \" + param + \": Yeo-Johnson: z score\"\n",
    "    for param in all_transformed_params\n",
    "]\n",
    "\n",
    "feature_vector_series = trenchiddf_merged_out_pandas.apply(\n",
    "    lambda x: np.array(x[kernel_score_params].tolist()), axis=1\n",
    ")\n",
    "trenchiddf_merged_out_pandas[\"Feature Vector\"] = feature_vector_series\n",
    "trenchiddf_merged_nan_filtered = trenchiddf_merged_out_pandas[\n",
    "    ~trenchiddf_merged_out_pandas[\"Feature Vector\"].apply(\n",
    "        lambda x: np.any(np.isnan(x)), meta=(\"No NaN\", bool)\n",
    "    )\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strong_effect_threshold = 15\n",
    "\n",
    "# zero_vector = np.zeros((1,trenchiddf_nan_filtered[\"Feature Vector\"].iloc[0].shape[0]))\n",
    "# feature_arr = np.array(trenchiddf_nan_filtered[\"Feature Vector\"].tolist())\n",
    "# feature_arr_abs = np.abs(feature_arr)\n",
    "# trenchiddf_nan_filtered[\"Integrated Feature Vector\"] = [item for item in sp.integrate.simpson(feature_arr_abs,axis=2)]\n",
    "# trenchiddf_nan_filtered[\"Integrated Feature Max\"] = trenchiddf_nan_filtered[\"Integrated Feature Vector\"].apply(lambda x: np.max(x))\n",
    "# trenchiddf_nan_filtered[\"Integrated Euclidean Norm\"] = np.linalg.norm(np.array(trenchiddf_nan_filtered[\"Integrated Feature Vector\"].tolist()), axis=1)\n",
    "\n",
    "# sgrnadf_strong_effect = trenchiddf_nan_filtered[trenchiddf_nan_filtered[\"Integrated Feature Max\"]>=strong_effect_threshold]\n",
    "# min_v,max_v = np.min(trenchiddf_nan_filtered[\"Integrated Feature Max\"]),np.percentile(trenchiddf_nan_filtered[\"Integrated Feature Max\"],99)\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.title(\"Integrated Feature Max\")\n",
    "# plt.hist(trenchiddf_nan_filtered[trenchiddf_nan_filtered[\"Integrated Feature Max\"]<strong_effect_threshold][\"Integrated Feature Max\"].tolist(),bins=50,range=(min_v,max_v))\n",
    "# plt.hist(trenchiddf_nan_filtered[trenchiddf_nan_filtered[\"Integrated Feature Max\"]>=strong_effect_threshold][\"Integrated Feature Max\"].tolist(),bins=50,range=(min_v,max_v))\n",
    "# plt.show()\n",
    "\n",
    "# unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"][sgrnadf_strong_effect[\"Gene\"].apply(lambda x: type(x)==str)].tolist(), return_counts=True)\n",
    "# plt.title(\"sgRNAs per Gene\")\n",
    "# plt.xticks(range(0,20,2),labels=range(0,20,2))\n",
    "# plt.hist(gene_counts,bins=np.arange(20)-0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trenchiddf_merged_nan_filtered.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-03-12_gene_cluster_df_no_filter.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
