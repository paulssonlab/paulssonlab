{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finds all PAMs on a strand of a sequence record\n",
    "## strand specifies the strand (1,-1) that the search is being conducted on\n",
    "## startcoord specifies the beginning coordinate of the seqrecord on the underlying reference genome\n",
    "def find_strand_pams(seqrecord, strand, startcoord=0):\n",
    "    if strand == 1:\n",
    "        seq = str(seqrecord.seq)\n",
    "    else:\n",
    "        seq = str(seqrecord.seq.reverse_complement())\n",
    "\n",
    "    pam_reg = re.compile(\"(?=(CC))\")  ## corrected to get overlapping PAMs as well\n",
    "    pam_starts = [item.start(0) for item in re.finditer(pam_reg, str(seq))]\n",
    "\n",
    "    pam_list = []\n",
    "\n",
    "    if strand == 1:\n",
    "        for item in pam_starts:\n",
    "            if len(seq[item + 3 : item + 23]) == 20:\n",
    "                start = startcoord + item + 3\n",
    "                end = startcoord + item + 23\n",
    "                sequence = seq[item + 3 : item + 23]\n",
    "                pam_list.append([start, end, sequence, strand])\n",
    "    else:\n",
    "        for item in pam_starts:\n",
    "            if len(seq[item + 3 : item + 23]) == 20:\n",
    "                start = startcoord + len(seq) - item - 23\n",
    "                end = startcoord + len(seq) - item - 3\n",
    "                sequence = seq[item + 3 : item + 23]\n",
    "                pam_list.append([start, end, sequence, strand])\n",
    "    return pam_list\n",
    "\n",
    "\n",
    "## Finds all PAMs in target sequence record\n",
    "## startcoord specifies the beginning coordinate of the seqrecord on the underlying reference genome\n",
    "## target_strand specifies the strand (1,-1) that the seq record is on\n",
    "def find_pams(seqrecord, startcoord=0, target_strand=1):\n",
    "    fwd_pams = find_strand_pams(seqrecord, 1, startcoord=startcoord)\n",
    "    rev_pams = find_strand_pams(seqrecord, -1, startcoord=startcoord)\n",
    "    pam_df = pd.DataFrame(\n",
    "        fwd_pams + rev_pams, columns=[\"start\", \"end\", \"sequence\", \"ref_strand\"]\n",
    "    )\n",
    "    if target_strand == 1:\n",
    "        pam_df[\"target_strand\"] = 1\n",
    "    else:\n",
    "        pam_df[\"target_strand\"] = -pam_df[\"ref_strand\"]\n",
    "    pam_df = pam_df.reset_index()\n",
    "    pam_df[\"targetid\"] = pam_df[\"index\"]\n",
    "    del pam_df[\"index\"]\n",
    "    return pam_df\n",
    "\n",
    "\n",
    "## Uses a reference csv of bad seeds to eliminate bad seeds from the PAM DataFrame\n",
    "def remove_bad_seeds(\n",
    "    pam_df,\n",
    "    bad_seed_list=[\n",
    "        \"AGGAA\",\n",
    "        \"TGACT\",\n",
    "        \"ACCCA\",\n",
    "        \"AAAGG\",\n",
    "        \"GAGGC\",\n",
    "        \"CGGAA\",\n",
    "        \"ATATG\",\n",
    "        \"AACTA\",\n",
    "        \"TGGAA\",\n",
    "        \"CACTC\",\n",
    "        \"GTATA\",\n",
    "        \"TATAG\",\n",
    "        \"GACTC\",\n",
    "        \"GGGAC\",\n",
    "        \"ATACA\",\n",
    "        \"GAGAA\",\n",
    "        \"GGGAA\",\n",
    "        \"CAATA\",\n",
    "        \"ATTAA\",\n",
    "        \"CACCA\",\n",
    "        \"AGGGG\",\n",
    "        \"AAGGA\",\n",
    "        \"CGACT\",\n",
    "        \"CGGGC\",\n",
    "        \"ATCAG\",\n",
    "        \"ATGAA\",\n",
    "        \"TAGTT\",\n",
    "        \"GGGGT\",\n",
    "        \"TATAA\",\n",
    "        \"CAATC\",\n",
    "    ],\n",
    "):\n",
    "    #     bad_seed_df = pd.read_csv(bad_seed_path)# HERE\n",
    "    #     bad_seed_list = bad_seed_df[\"seeds\"].tolist()\n",
    "    ## reverse complement to match target sequence\n",
    "    pam_df = pam_df[\n",
    "        pam_df[\"sgRNA Sequence\"].apply(lambda x: x[-5:] not in bad_seed_list)\n",
    "    ]\n",
    "    return pam_df\n",
    "\n",
    "\n",
    "### Convert a target sequence to a spacer sequence\n",
    "def target_to_spacer(target_str):\n",
    "    target = Seq(target_str.upper())\n",
    "    spacer = target.reverse_complement()\n",
    "    spacer = str(spacer)\n",
    "    return spacer\n",
    "\n",
    "\n",
    "def check_restriction_site(in_str, rest_str=\"GGTCTC\"):\n",
    "    test_str = Seq(in_str.upper())\n",
    "    rc_test_str = test_str.reverse_complement()\n",
    "    if (rest_str in test_str) or (rest_str in rc_test_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_all_sites(in_str, rest_strs=[\"GAAGAC\", \"CGTCTC\", \"GGTCTC\"]):\n",
    "    out_test = False\n",
    "    for rest_str in rest_strs:\n",
    "        rest_site = check_restriction_site(in_str, rest_str=rest_str)\n",
    "        out_test = out_test or rest_site\n",
    "    return out_test\n",
    "\n",
    "\n",
    "### Generate a mismatch series of the target query with num_mismatch substitutions starting from the PAM distal end\n",
    "def generate_mismatch_series(in_str, mismatch_ns):\n",
    "    flip_dict = {\n",
    "        \"A\": [\"T\", \"C\", \"G\"],\n",
    "        \"T\": [\"A\", \"C\", \"G\"],\n",
    "        \"C\": [\"T\", \"A\", \"G\"],\n",
    "        \"G\": [\"T\", \"C\", \"A\"],\n",
    "    }\n",
    "\n",
    "    out_series = []\n",
    "    in_str_len = len(in_str)\n",
    "    list_str = list(in_str)\n",
    "    new_str = copy.copy(list_str)\n",
    "    for i in range(max(mismatch_ns) + 1):\n",
    "        new_char = np.random.choice(flip_dict[list_str[in_str_len - i - 1]])\n",
    "        new_str[in_str_len - i - 1] = new_char\n",
    "        if i in mismatch_ns:\n",
    "            out_series.append(\"\".join(new_str))\n",
    "    return out_series\n",
    "\n",
    "\n",
    "def generate_mismatch_df(pam_df, mismatch_ns=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "    mismatch_df = []\n",
    "    for i, row in pam_df.iterrows():\n",
    "        targetid = row[\"EcoWG1_id\"]\n",
    "        sgrna_seq = row[\"sgRNA Sequence\"]\n",
    "        gene = row[\"Gene\"]\n",
    "        ref_strand = row[\"Strand\"]\n",
    "        target_seq = row[\"Target Sequence\"]\n",
    "        target_sites = row[\"Target Sites\"]\n",
    "        N_target_sites = row[\"N Target Sites\"]\n",
    "\n",
    "        rest_test = check_all_sites(sgrna_seq, rest_strs=[\"GAAGAC\", \"CGTCTC\", \"GGTCTC\"])\n",
    "        if not rest_test:\n",
    "            rest_test_2 = True\n",
    "            mismatch_df.append(\n",
    "                [\n",
    "                    targetid,\n",
    "                    sgrna_seq,\n",
    "                    gene,\n",
    "                    ref_strand,\n",
    "                    target_seq,\n",
    "                    target_sites,\n",
    "                    N_target_sites,\n",
    "                    0,\n",
    "                    \"Target\",\n",
    "                ]\n",
    "            )\n",
    "            while rest_test_2:\n",
    "                mismatch_list = generate_mismatch_series(target_seq, mismatch_ns)\n",
    "                rest_test_2 = any(\n",
    "                    [\n",
    "                        check_all_sites(\n",
    "                            mismatch_seq, rest_strs=[\"GAAGAC\", \"CGTCTC\", \"GGTCTC\"]\n",
    "                        )\n",
    "                        for mismatch_seq in mismatch_list\n",
    "                    ]\n",
    "                )\n",
    "            mismatch_df += [\n",
    "                [\n",
    "                    targetid,\n",
    "                    target_to_spacer(item),\n",
    "                    gene,\n",
    "                    ref_strand,\n",
    "                    target_seq,\n",
    "                    target_sites,\n",
    "                    N_target_sites,\n",
    "                    mismatch_ns[j] + 1,\n",
    "                    \"Target\",\n",
    "                ]\n",
    "                for j, item in enumerate(mismatch_list)\n",
    "            ]\n",
    "\n",
    "    mismatch_df = pd.DataFrame(\n",
    "        mismatch_df,\n",
    "        columns=[\n",
    "            \"EcoWG1_id\",\n",
    "            \"sgRNA Sequence\",\n",
    "            \"Gene\",\n",
    "            \"Strand\",\n",
    "            \"Target Sequence\",\n",
    "            \"Target Sites\",\n",
    "            \"N Target Sites\",\n",
    "            \"N Mismatch\",\n",
    "            \"Category\",\n",
    "        ],\n",
    "    )\n",
    "    return mismatch_df\n",
    "\n",
    "\n",
    "### Add bsai sites and primer sequences to the side for cloning\n",
    "def add_adaptor_seqs(spacer, target):\n",
    "    site_1 = \"CGTACGACGGAAGACATTAGTCCC\"\n",
    "    site_2 = \"TGTTGGAGACGCGTCTCCTAGT\"\n",
    "    site_3 = \"GTTTTCGTCTTCTTAAGGTGCC\"\n",
    "    output_seq = site_1 + target + site_2 + spacer + site_3\n",
    "    return output_seq\n",
    "\n",
    "\n",
    "## Converts a string to an integer representation\n",
    "def str_to_int(string):\n",
    "    code = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "    conv_str = np.array(list(map(lambda x: code[x], string)))\n",
    "    return conv_str\n",
    "\n",
    "\n",
    "## Determines the maximum matching (minimum edit distance) of each target sequence\n",
    "## to sequences in the reference\n",
    "## subseq_range can be set to subset the sequences to a region of interest (i.e. the PAM adjacent region)\n",
    "## remove_matching_starts can be set to eliminate sequences from consideration when they are at the same location\n",
    "def compare_seqs(\n",
    "    target_df, reference_df, subseq_range=None, remove_matching_starts=True\n",
    "):\n",
    "    target_arr = target_df[\"Target Sequence\"].values\n",
    "    reference_arr = reference_df[\"sequence\"].values\n",
    "    target_int_arr = np.array(list(map(str_to_int, target_arr)), dtype=\"uint8\")\n",
    "    reference_int_arr = np.array(list(map(str_to_int, reference_arr)), dtype=\"uint8\")\n",
    "\n",
    "    if subseq_range != None:\n",
    "        target_int_arr = target_int_arr[:, subseq_range]\n",
    "        reference_int_arr = reference_int_arr[:, subseq_range]\n",
    "\n",
    "    bool_arr = target_int_arr[:, np.newaxis, :] == reference_int_arr[np.newaxis, :, :]\n",
    "    agreement_arr = np.sum(bool_arr, axis=2, dtype=int)\n",
    "\n",
    "    if remove_matching_starts:\n",
    "        matching_starts = np.where(\n",
    "            target_df[\"start\"].values[:, np.newaxis]\n",
    "            == reference_df[\"start\"].values[np.newaxis, :]\n",
    "        )[1]\n",
    "        agreement_arr[:, matching_starts] = 0\n",
    "    most_agreement = np.max(agreement_arr, axis=1)\n",
    "    return most_agreement\n",
    "\n",
    "\n",
    "# Negative Control: sgRNAs that target nothing\n",
    "def generate_random_sequences(num_seqs, str_len=20, match_target=False):\n",
    "    str_arr = np.random.choice([\"A\", \"C\", \"G\", \"T\"], size=(num_seqs, str_len))\n",
    "    str_list = np.apply_along_axis(\"\".join, 1, str_arr).tolist()\n",
    "    if match_target:\n",
    "        df_out = [\n",
    "            [k, None, item, None, None, item, None, None, None, \"NoTarget\"]\n",
    "            for k, item in enumerate(str_list)\n",
    "        ]\n",
    "    else:\n",
    "        df_out = [\n",
    "            [\n",
    "                k,\n",
    "                None,\n",
    "                target_to_spacer(item),\n",
    "                None,\n",
    "                None,\n",
    "                item,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                \"OnlyPlasmid\",\n",
    "            ]\n",
    "            for k, item in enumerate(str_list)\n",
    "        ]\n",
    "\n",
    "    df_out = pd.DataFrame(\n",
    "        df_out,\n",
    "        columns=[\n",
    "            \"TargetID\",\n",
    "            \"EcoWG1_id\",\n",
    "            \"sgRNA Sequence\",\n",
    "            \"Gene\",\n",
    "            \"Strand\",\n",
    "            \"Target Sequence\",\n",
    "            \"Target Sites\",\n",
    "            \"N Target Sites\",\n",
    "            \"N Mismatch\",\n",
    "            \"Category\",\n",
    "        ],\n",
    "    )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EcoWG1 DAta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcoWG1 = pd.read_csv(\"./data_EcoWG1_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcoWG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(EcoWG1[\"T4\"], bins=50, range=(-15, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TraDIS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TraDIS = pd.read_csv(\"./data_Goodall_TraDIS.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TraDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_TraDIS = TraDIS[TraDIS[\"Essential\"]]\n",
    "essential_TraDIS_list = essential_TraDIS[\"Gene\"].to_list()\n",
    "\n",
    "gene_synonyms = pd.read_csv(\"./data_gene_synonyms.csv\", skiprows=1)\n",
    "gene_synonyms.columns = [\"EcoWG1\", \"TraDIS\"]\n",
    "gene_synonyms_Eco_to_Trad = {\n",
    "    key: val\n",
    "    for entry in gene_synonyms.apply(\n",
    "        lambda x: {x[\"EcoWG1\"]: x[\"TraDIS\"]}, axis=1\n",
    "    ).tolist()\n",
    "    for key, val in entry.items()\n",
    "}\n",
    "gene_synonyms_Trad_to_Eco = {val: key for key, val in gene_synonyms_Eco_to_Trad.items()}\n",
    "\n",
    "essential_TraDIS_list_Eco = [\n",
    "    gene_synonyms_Trad_to_Eco[item]\n",
    "    for item in essential_TraDIS_list\n",
    "    if item in gene_synonyms_Trad_to_Eco.keys()\n",
    "]\n",
    "\n",
    "TraDIS_essential_mask = EcoWG1[\"gene\"].isin(essential_TraDIS_list_Eco)\n",
    "EcoWG1[\"TraDIS Essential\"] = TraDIS_essential_mask\n",
    "EcoWG1_TraDIS_essential = EcoWG1[TraDIS_essential_mask]\n",
    "EcoWG1_TraDIS_nonessential = EcoWG1[~TraDIS_essential_mask]\n",
    "\n",
    "EcoWG1_gene_groupby = EcoWG1.groupby(\"gene\").apply(lambda x: np.min(x[\"T4\"]))\n",
    "EcoWG1_TraDIS_essential_gene_groupby = EcoWG1_TraDIS_essential.groupby(\"gene\").apply(\n",
    "    lambda x: np.min(x[\"T4\"])\n",
    ")\n",
    "EcoWG1_TraDIS_nonessential_gene_groupby = (\n",
    "    EcoWG1[~TraDIS_essential_mask].groupby(\"gene\").apply(lambda x: np.min(x[\"T4\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "y_true = (\n",
    "    EcoWG1.groupby(\"gene\")\n",
    "    .apply(lambda x: np.min(x[\"TraDIS Essential\"]))\n",
    "    .astype(int)\n",
    "    .values\n",
    ")\n",
    "y_scores = -EcoWG1.groupby(\"gene\").apply(lambda x: np.min(x[\"T4\"])).values\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "plt.hist(\n",
    "    EcoWG1_TraDIS_nonessential_gene_groupby,\n",
    "    bins=50,\n",
    "    range=(-10, 5),\n",
    "    density=True,\n",
    "    label=\"Non-Essential\",\n",
    ")\n",
    "plt.hist(\n",
    "    EcoWG1_TraDIS_essential_gene_groupby,\n",
    "    bins=50,\n",
    "    range=(-10, 5),\n",
    "    density=True,\n",
    "    label=\"Essential\",\n",
    ")\n",
    "plt.xlabel(\"Log2 Fold Change\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./2021-04-28_lab_meeting_fig_1.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.savefig(\"./2021-04-28_lab_meeting_fig_2.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = np.min(-thresholds[recall[1:] > 0.8])\n",
    "\n",
    "EcoWG1_essential_list = EcoWG1_gene_groupby[EcoWG1_gene_groupby < thr].index.to_list()\n",
    "EcoWG1_essential_mask = EcoWG1[\"gene\"].isin(\n",
    "    EcoWG1_essential_list + essential_TraDIS_list_Eco\n",
    ")\n",
    "EcoWG1_essential = EcoWG1[EcoWG1_essential_mask]\n",
    "EcoWG1_nonessential = EcoWG1[~EcoWG1_essential_mask]\n",
    "\n",
    "EcoWG1_essential = EcoWG1_essential.reset_index(drop=False)\n",
    "EcoWG1_essential = EcoWG1_essential[[\"index\", \"Unnamed: 0\", \"gene\", \"ori\"]]\n",
    "EcoWG1_essential.columns = [\"EcoWG1_id\", \"sgRNA Sequence\", \"Gene\", \"Strand\"]\n",
    "EcoWG1_essential[\"Target Sequence\"] = [\n",
    "    str(Seq(item).reverse_complement()) for item in EcoWG1_essential[\"sgRNA Sequence\"]\n",
    "]\n",
    "\n",
    "n_genes = len(EcoWG1_essential[\"Gene\"].unique())\n",
    "print(\"Guides per gene: \" + str(len(EcoWG1_essential) / n_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = SeqIO.read(\"./U00096_3.gb\", \"gb\")\n",
    "\n",
    "\n",
    "def find_loc(search_str, strand, reference=str(genome.seq)):\n",
    "    ref_len = len(reference)\n",
    "    comp_search_str = str(Seq(search_str).reverse_complement())\n",
    "    search = re.finditer(search_str, reference)\n",
    "    comp_search = re.finditer(comp_search_str, reference)\n",
    "    start_end_list = []\n",
    "    for item in search:\n",
    "        start_end_list.append((item.start(), item.end(), \"+\"))\n",
    "    for item in comp_search:\n",
    "        start_end_list.append((ref_len - item.end(), ref_len - item.start(), \"-\"))\n",
    "    return start_end_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcoWG1_essential[\"Target Sites\"] = EcoWG1_essential.apply(\n",
    "    lambda x: find_loc(x[\"Target Sequence\"], x[\"Strand\"]), axis=1\n",
    ")\n",
    "\n",
    "EcoWG1_essential[\"N Target Sites\"] = EcoWG1_essential[\"Target Sites\"].apply(\n",
    "    lambda x: len(x)\n",
    ")\n",
    "EcoWG1_essential = remove_bad_seeds(EcoWG1_essential)  # some leftover bad seeds?\n",
    "EcoWG1_essential = generate_mismatch_df(EcoWG1_essential)\n",
    "\n",
    "EcoWG1_ids = EcoWG1_essential[\"EcoWG1_id\"].unique().tolist()\n",
    "EcoWG1_ids_to_targetid = dict(zip(EcoWG1_ids, range(len(EcoWG1_ids))))\n",
    "EcoWG1_essential[\"TargetID\"] = EcoWG1_essential[\"EcoWG1_id\"].apply(\n",
    "    lambda x: EcoWG1_ids_to_targetid[x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.7243150684931505 * 584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_targetid = max(EcoWG1_essential[\"TargetID\"]) + 1\n",
    "init_index = max(EcoWG1_essential.index) + 1\n",
    "n_controls = 20\n",
    "seed_sequence_size = 9\n",
    "\n",
    "genome = SeqIO.read(\"./U00096_3.gb\", \"gb\")\n",
    "genome_pam_df = find_pams(genome)\n",
    "\n",
    "negative_control_1 = generate_random_sequences(200, match_target=False)\n",
    "negative_control_1 = remove_bad_seeds(negative_control_1)\n",
    "no_targets = (\n",
    "    compare_seqs(\n",
    "        negative_control_1,\n",
    "        genome_pam_df,\n",
    "        subseq_range=range(0, seed_sequence_size),\n",
    "        remove_matching_starts=False,\n",
    "    )\n",
    "    < seed_sequence_size\n",
    ")\n",
    "negative_control_1 = negative_control_1[no_targets]\n",
    "rest_sites = negative_control_1.apply(\n",
    "    lambda x: check_all_sites(x[\"sgRNA Sequence\"]), axis=1\n",
    ")\n",
    "negative_control_1 = negative_control_1[~rest_sites]\n",
    "negative_control_1 = negative_control_1.sample(n_controls)\n",
    "\n",
    "negative_control_2 = generate_random_sequences(200, match_target=True)\n",
    "negative_control_2 = remove_bad_seeds(negative_control_2)\n",
    "no_targets = (\n",
    "    compare_seqs(\n",
    "        negative_control_2,\n",
    "        genome_pam_df,\n",
    "        subseq_range=range(0, seed_sequence_size),\n",
    "        remove_matching_starts=False,\n",
    "    )\n",
    "    < seed_sequence_size\n",
    ")\n",
    "negative_control_2 = negative_control_2[no_targets]\n",
    "rest_sites = negative_control_2.apply(\n",
    "    lambda x: check_all_sites(x[\"sgRNA Sequence\"]), axis=1\n",
    ")\n",
    "negative_control_2 = negative_control_2[~rest_sites]\n",
    "negative_control_2 = negative_control_2.sample(n_controls)\n",
    "\n",
    "essential_gene_id_list = [7645, 9611]\n",
    "negative_control_3 = EcoWG1_essential[\n",
    "    EcoWG1_essential[\"EcoWG1_id\"].isin(essential_gene_id_list)\n",
    "]\n",
    "negative_control_3[\"Target Sequence\"] = negative_control_3[\"sgRNA Sequence\"]\n",
    "negative_control_3[\"Category\"] = \"OnlyChr\"\n",
    "del negative_control_3[\"TargetID\"]\n",
    "\n",
    "non_essential_genes = [\"lacZ\", \"malT\", \"galM\", \"mak\"]\n",
    "negative_control_4 = EcoWG1[EcoWG1[\"gene\"].isin(non_essential_genes)].reset_index(\n",
    "    drop=False\n",
    ")\n",
    "negative_control_4 = negative_control_4.drop(\n",
    "    [\"T1\", \"T2\", \"T3\", \"T4\", \"TraDIS Essential\"], axis=1\n",
    ")\n",
    "negative_control_4.columns = [\"EcoWG1_id\", \"sgRNA Sequence\", \"Gene\", \"Strand\"]\n",
    "negative_control_4[\"N Mismatch\"] = None\n",
    "\n",
    "negative_control_5 = copy.deepcopy(negative_control_4)\n",
    "\n",
    "negative_control_4[\"Target Sequence\"] = negative_control_4[\"sgRNA Sequence\"].apply(\n",
    "    target_to_spacer\n",
    ")\n",
    "negative_control_4[\"Target Sites\"] = negative_control_4.apply(\n",
    "    lambda x: find_loc(x[\"Target Sequence\"], x[\"Strand\"]), axis=1\n",
    ")\n",
    "negative_control_4[\"N Target Sites\"] = negative_control_4[\"Target Sites\"].apply(\n",
    "    lambda x: len(x)\n",
    ")\n",
    "negative_control_4[\"Category\"] = \"NonEssential\"\n",
    "\n",
    "negative_control_5[\"Target Sequence\"] = negative_control_5[\"sgRNA Sequence\"]\n",
    "negative_control_5[\"Target Sites\"] = negative_control_5.apply(\n",
    "    lambda x: find_loc(target_to_spacer(x[\"Target Sequence\"]), x[\"Strand\"]), axis=1\n",
    ")\n",
    "negative_control_5[\"N Target Sites\"] = negative_control_5[\"Target Sites\"].apply(\n",
    "    lambda x: len(x)\n",
    ")\n",
    "negative_control_5[\"Category\"] = \"NonEssentialOnlyChr\"\n",
    "\n",
    "negative_controls = pd.concat(\n",
    "    [\n",
    "        negative_control_1,\n",
    "        negative_control_2,\n",
    "        negative_control_3,\n",
    "        negative_control_4,\n",
    "        negative_control_5,\n",
    "    ]\n",
    ")\n",
    "negative_controls[\"TargetID\"] = np.array(range(len(negative_controls))) + init_targetid\n",
    "negative_controls = negative_controls.reset_index(drop=True)\n",
    "negative_controls.index = negative_controls.index + init_index\n",
    "\n",
    "output_df = pd.concat([EcoWG1_essential, negative_controls], axis=0)\n",
    "\n",
    "output_df[\"Sequence To Order\"] = output_df.apply(\n",
    "    lambda x: add_adaptor_seqs(x[\"sgRNA Sequence\"], x[\"Target Sequence\"]), axis=1\n",
    ")\n",
    "output_df[\"bbsi site\"] = output_df[\"Sequence To Order\"].apply(\n",
    "    lambda x: check_restriction_site(x[10:97], rest_str=\"GAAGAC\")\n",
    ")\n",
    "output_df[\"bsmbi site\"] = output_df[\"Sequence To Order\"].apply(\n",
    "    lambda x: check_restriction_site(x[:54], rest_str=\"CGTCTC\")\n",
    "    or check_restriction_site(x[56:], rest_str=\"CGTCTC\")\n",
    ")\n",
    "output_df[\"bsai site\"] = output_df[\"Sequence To Order\"].apply(\n",
    "    check_restriction_site, rest_str=\"GGTCTC\"\n",
    ")\n",
    "no_internal_sites = (\n",
    "    ~output_df[\"bbsi site\"] & ~output_df[\"bsai site\"] & ~output_df[\"bsmbi site\"]\n",
    ")\n",
    "included_df = output_df[no_internal_sites]\n",
    "removed_df = output_df[~no_internal_sites]\n",
    "included_df = included_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df[\"sgRNA Sequence\"][:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent Recovered: \" + str(np.sum(no_internal_sites) / len(no_internal_sites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tomorrow, clean up code, spot check all categories and orientations, and send\n",
    "### Make sure to get funding ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df.to_csv(\"./2021-04-20_Essential_KO_Lib_df.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction (8/26/2021)\n",
    "\n",
    "In the original design, the coordinates were misassigned. Here I open and repair the file, to preserve the randomly selected sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = SeqIO.read(\"./U00096_3.gb\", \"gb\")\n",
    "reference = str(genome.seq)\n",
    "ref_len = len(reference)\n",
    "old_df = pd.read_csv(\n",
    "    \"./2021-04-20_Essential_KO_Lib_df_original.tsv\", sep=\"\\t\", usecols=range(1, 15)\n",
    ")\n",
    "old_df.insert(0, \"oDEPool7_id\", old_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df[\"Target Sites\"][~old_df[\"Target Sites\"].isna()] = old_df[\"Target Sites\"][\n",
    "    ~old_df[\"Target Sites\"].isna()\n",
    "].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df[\"Target Sites\"][~old_df[\"Target Sites\"].isna()] = old_df[\"Target Sites\"][\n",
    "    ~old_df[\"Target Sites\"].isna()\n",
    "].apply(\n",
    "    lambda x: [\n",
    "        (target_site[0] + 1, target_site[1], target_site[2])\n",
    "        if target_site[2] == \"+\"\n",
    "        else (ref_len - target_site[1] + 1, ref_len - target_site[0], target_site[2])\n",
    "        for target_site in x\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.to_csv(\"./2021-04-20_Essential_KO_Lib_df_coords_corrected.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controls\n",
    "\n",
    "- Negative Control: sgRNAs that target nothing\n",
    "- Negative Control: sgRNAs that only target essential genes\n",
    "- Negative Control: sgRNAs that only target plasmid FP\n",
    "- Negative Control: sgRNAs that target genes with no effect on growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_targetid = 100\n",
    "n_controls = 20\n",
    "seed_sequence_size = 9\n",
    "\n",
    "genome = SeqIO.read(\"./U00096_3.gb\", \"gb\")\n",
    "genome_pam_df = find_pams(genome)\n",
    "\n",
    "negative_control_1 = generate_random_sequences(100, match_target=False)\n",
    "negative_control_1 = remove_bad_seeds(negative_control_1)\n",
    "no_targets = (\n",
    "    compare_seqs(\n",
    "        negative_control_1,\n",
    "        genome_pam_df,\n",
    "        subseq_range=range(0, seed_sequence_size),\n",
    "        remove_matching_starts=False,\n",
    "    )\n",
    "    < seed_sequence_size\n",
    ")\n",
    "negative_control_1 = negative_control_1[no_targets]\n",
    "negative_control_1 = negative_control_1.sample(n_controls)\n",
    "\n",
    "negative_control_2 = generate_random_sequences(100, match_target=True)\n",
    "negative_control_2 = remove_bad_seeds(negative_control_2)\n",
    "no_targets = (\n",
    "    compare_seqs(\n",
    "        negative_control_2,\n",
    "        genome_pam_df,\n",
    "        subseq_range=range(0, seed_sequence_size),\n",
    "        remove_matching_starts=False,\n",
    "    )\n",
    "    < seed_sequence_size\n",
    ")\n",
    "negative_control_2 = negative_control_2[no_targets]\n",
    "negative_control_2 = negative_control_2.sample(n_controls)\n",
    "\n",
    "negative_controls = pd.concat([negative_control_1, negative_control_2])\n",
    "negative_controls[\"TargetID\"] = np.array(range(len(negative_controls))) + init_targetid\n",
    "negative_controls = negative_controls.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_gene_id_list = [7645, 9611]\n",
    "negative_control_3 = output_df[output_df[\"EcoWG1_id\"].isin(essential_gene_id_list)]\n",
    "negative_control_3[\"Target Sequence\"] = negative_control_3[\"sgRNA Sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_control_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcoWG1 = pd.read_csv(\"./data_EcoWG1_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_essential_genes = [\"lacZ\", \"malT\", \"galM\", \"mak\"]\n",
    "negative_control_4 = EcoWG1[EcoWG1[\"gene\"].isin(non_essential_genes)].reset_index(\n",
    "    drop=False\n",
    ")\n",
    "negative_control_4 = negative_control_4.drop([\"T1\", \"T2\", \"T3\", \"T4\"], axis=1)\n",
    "negative_control_4.columns = [\"EcoWG1_id\", \"sgRNA Sequence\", \"Gene\", \"Strand\"]\n",
    "negative_control_4[\"Target Sequence\"] = negative_control_4[\"sgRNA Sequence\"].apply(\n",
    "    target_to_spacer\n",
    ")\n",
    "negative_control_4[\"Target Sites\"] = negative_control_4.apply(\n",
    "    lambda x: find_loc(x[\"Target Sequence\"], x[\"Strand\"]), axis=1\n",
    ")\n",
    "negative_control_4[\"N Target Sites\"] = negative_control_4[\"Target Sites\"].apply(\n",
    "    lambda x: len(x)\n",
    ")\n",
    "negative_control_4[\"N Mismatch\"] = None\n",
    "negative_control_4[\"Category\"] = \"NonEssential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_control_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"./2021-04-20_Essential_KO_Lib_df.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = pd.read_csv(\"./2021-04-20_Essential_KO_Lib_df_old.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seqs = list(\n",
    "    set(new_df[\"Sequence To Order\"].tolist())\n",
    "    - set(old_df[\"Sequence To Order\"].tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_barcodes = 150000\n",
    "MM_throughput = 700000\n",
    "N_guides = 30349\n",
    "p_choose = 1 / N_guides\n",
    "\n",
    "barcode_to_guide = {\n",
    "    i: item for i, item in enumerate(np.random.choice(range(N_guides), size=N_barcodes))\n",
    "}\n",
    "occur_dist = np.random.binomial(N_barcodes, p=p_choose, size=(N_barcodes))\n",
    "guide_df = pd.DataFrame(\n",
    "    [(barcode_to_guide[i], item) for i, item in enumerate(occur_dist)]\n",
    ")\n",
    "zero_df = pd.DataFrame.from_dict(\n",
    "    dict(zip(list(range(N_guides)), [0 for i in range(N_guides)])), orient=\"index\"\n",
    ").reset_index(drop=False)\n",
    "zero_df.columns = [0, 1]\n",
    "guide_df = pd.concat([guide_df, zero_df])\n",
    "guide_dist = guide_df.groupby(0).sum().values.T[0]\n",
    "\n",
    "freq_dist = guide_dist / np.sum(guide_dist)\n",
    "mother_machine_dist = np.random.choice(\n",
    "    range(len(freq_dist)), p=freq_dist, size=(MM_throughput)\n",
    ")\n",
    "\n",
    "unique, count = np.unique(mother_machine_dist, return_counts=True)\n",
    "guide_to_countMM = dict(zip(unique, count))\n",
    "guide_to_countMM = {\n",
    "    i: guide_to_countMM[i] if i in guide_to_countMM.keys() else 0\n",
    "    for i in range(N_guides)\n",
    "}\n",
    "count = np.array([item for item in guide_to_countMM.values()])\n",
    "ttl_count = np.sum(count)\n",
    "count_freq = count / ttl_count\n",
    "\n",
    "dropped = list(set(range(1, len(freq_dist) + 1)) - set(unique))\n",
    "\n",
    "plt.hist(guide_dist, density=True, bins=50, range=(0, 100))\n",
    "plt.show()\n",
    "print(\"Prob Droupout = \" + str((np.sum(guide_dist == 0)) / len(guide_dist)))\n",
    "print(\"Prob Well Rep = \" + str((np.sum(guide_dist > 1)) / len(guide_dist)))\n",
    "\n",
    "plt.hist(count, density=True, bins=31, range=(0, 30))\n",
    "plt.xlabel(\"N Observations\")\n",
    "plt.ylabel(\"% of Library\")\n",
    "plt.savefig(\"./2021-04-28_lab_meeting_fig_3.png\", dpi=300)\n",
    "plt.show()\n",
    "print(\"Prob Droupout = \" + str((np.sum(count == 0)) / len(count)))\n",
    "print(\"Prob Well Rep = \" + str((np.sum(count > 5)) / len(count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Simulation (from old sims)\n",
    "\n",
    "did in other notebook; seems I need about 10 trenches per sgRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Time\n",
    "\n",
    "Add mutations to library to make KO series\n",
    "\n",
    "Note the rules in the paper to copy:\n",
    "\n",
    "\"We further established guide RNA design rules to select the best possible guides for targeting a gene of interest. Our rules attempt to select guides whose predicted activity falls in the top quartile while avoiding off-targets and toxic seed sequences. We specifically avoid off-targets with 11 nucleotides of perfect identity or more between the seed sequence and the non-template strand of a gene, as well as off-targets with nine nucleotides of perfect identity or more to promoter regions in either orientation.\"\n",
    "\n",
    "- No off targets with 11 nt of identity or more between the seed sequence and the non-template strand of a CDS\n",
    "- No off targets with 9 nt of identity or more between the seed sequence and promoter regions in either orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finds all PAMs on a strand of a sequence record\n",
    "## strand specifies the strand (1,-1) that the search is being conducted on\n",
    "## startcoord specifies the beginning coordinate of the seqrecord on the underlying reference genome\n",
    "def find_strand_pams(seqrecord, strand, startcoord=0):\n",
    "    if strand == 1:\n",
    "        seq = str(seqrecord.seq)\n",
    "    else:\n",
    "        seq = str(seqrecord.seq.reverse_complement())\n",
    "\n",
    "    pam_reg = re.compile(\"(?=(CC))\")\n",
    "    pam_starts = [item.start(0) for item in re.finditer(pam_reg, str(seq))]\n",
    "\n",
    "    pam_list = []\n",
    "\n",
    "    if strand == 1:\n",
    "        for item in pam_starts:\n",
    "            if len(seq[item + 3 : item + 23]) == 20:\n",
    "                start = startcoord + item + 3\n",
    "                end = startcoord + item + 23\n",
    "                sequence = seq[item + 3 : item + 23]\n",
    "                pam_list.append([start, end, sequence, strand])\n",
    "    else:\n",
    "        for item in pam_starts:\n",
    "            if len(seq[item + 3 : item + 23]) == 20:\n",
    "                start = startcoord + len(seq) - item - 23\n",
    "                end = startcoord + len(seq) - item - 3\n",
    "                sequence = seq[item + 3 : item + 23]\n",
    "                pam_list.append([start, end, sequence, strand])\n",
    "    return pam_list\n",
    "\n",
    "\n",
    "## Finds all PAMs in target sequence record\n",
    "## startcoord specifies the beginning coordinate of the seqrecord on the underlying reference genome\n",
    "## target_strand specifies the strand (1,-1) that the seq record is on\n",
    "def find_pams(seqrecord, startcoord=0, target_strand=1):\n",
    "    fwd_pams = find_strand_pams(seqrecord, 1, startcoord=startcoord)\n",
    "    rev_pams = find_strand_pams(seqrecord, -1, startcoord=startcoord)\n",
    "    pam_df = pd.DataFrame(\n",
    "        fwd_pams + rev_pams, columns=[\"start\", \"end\", \"sequence\", \"ref_strand\"]\n",
    "    )\n",
    "    if target_strand == 1:\n",
    "        pam_df[\"target_strand\"] = 1\n",
    "    else:\n",
    "        pam_df[\"target_strand\"] = -pam_df[\"ref_strand\"]\n",
    "    pam_df = pam_df.reset_index()\n",
    "    pam_df[\"targetid\"] = pam_df[\"index\"]\n",
    "    del pam_df[\"index\"]\n",
    "    return pam_df\n",
    "\n",
    "\n",
    "## Uses a reference csv of bad seeds to eliminate bad seeds from the PAM DataFrame\n",
    "def remove_bad_seeds(\n",
    "    pam_df,\n",
    "    bad_seed_list=[\n",
    "        \"AGGAA\",\n",
    "        \"TGACT\",\n",
    "        \"ACCCA\",\n",
    "        \"AAAGG\",\n",
    "        \"GAGGC\",\n",
    "        \"CGGAA\",\n",
    "        \"ATATG\",\n",
    "        \"AACTA\",\n",
    "        \"TGGAA\",\n",
    "        \"CACTC\",\n",
    "        \"GTATA\",\n",
    "        \"TATAG\",\n",
    "        \"GACTC\",\n",
    "        \"GGGAC\",\n",
    "        \"ATACA\",\n",
    "        \"GAGAA\",\n",
    "        \"GGGAA\",\n",
    "        \"CAATA\",\n",
    "        \"ATTAA\",\n",
    "        \"CACCA\",\n",
    "        \"AGGGG\",\n",
    "        \"AAGGA\",\n",
    "        \"CGACT\",\n",
    "        \"CGGGC\",\n",
    "        \"ATCAG\",\n",
    "        \"ATGAA\",\n",
    "        \"TAGTT\",\n",
    "        \"GGGGT\",\n",
    "        \"TATAA\",\n",
    "        \"CAATC\",\n",
    "    ],\n",
    "):\n",
    "    #     bad_seed_df = pd.read_csv(bad_seed_path)# HERE\n",
    "    #     bad_seed_list = bad_seed_df[\"seeds\"].tolist()\n",
    "    ## reverse complement to match target sequence\n",
    "    bad_seed_list = [\n",
    "        str(Seq(item.upper()).reverse_complement()) for item in bad_seed_list\n",
    "    ]\n",
    "    pam_df = pam_df[pam_df[\"sequence\"].apply(lambda x: x[:5] not in bad_seed_list)]\n",
    "    return pam_df\n",
    "\n",
    "\n",
    "## Converts a string to an integer representation\n",
    "def str_to_int(string):\n",
    "    code = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "    conv_str = np.array(list(map(lambda x: code[x], string)))\n",
    "    return conv_str\n",
    "\n",
    "\n",
    "## Determines the maximum matching (minimum edit distance) of each target sequence\n",
    "## to sequences in the reference\n",
    "## subseq_range can be set to subset the sequences to a region of interest (i.e. the PAM adjacent region)\n",
    "## remove_matching_starts can be set to eliminate sequences from consideration when they are at the same location\n",
    "def compare_seqs(\n",
    "    target_df, reference_df, subseq_range=None, remove_matching_starts=True\n",
    "):\n",
    "    target_arr = target_df[\"sequence\"].values\n",
    "    reference_arr = reference_df[\"sequence\"].values\n",
    "    target_int_arr = np.array(list(map(str_to_int, target_arr)), dtype=\"uint8\")\n",
    "    reference_int_arr = np.array(list(map(str_to_int, reference_arr)), dtype=\"uint8\")\n",
    "\n",
    "    if subseq_range != None:\n",
    "        target_int_arr = target_int_arr[:, subseq_range]\n",
    "        reference_int_arr = reference_int_arr[:, subseq_range]\n",
    "\n",
    "    bool_arr = target_int_arr[:, np.newaxis, :] == reference_int_arr[np.newaxis, :, :]\n",
    "    agreement_arr = np.sum(bool_arr, axis=2, dtype=int)\n",
    "\n",
    "    if remove_matching_starts:\n",
    "        matching_starts = np.where(\n",
    "            target_df[\"start\"].values[:, np.newaxis]\n",
    "            == reference_df[\"start\"].values[np.newaxis, :]\n",
    "        )[1]\n",
    "        agreement_arr[:, matching_starts] = 0\n",
    "    most_agreement = np.max(agreement_arr, axis=1)\n",
    "    return most_agreement\n",
    "\n",
    "\n",
    "### Exhaustively generate mismatched versions of the query with num_mismatch substitutions starting from the PAM distal end\n",
    "def generate_all_mismatchs(in_str, num_mismatch):\n",
    "    flip_dict = {\n",
    "        \"A\": [\"T\", \"C\", \"G\"],\n",
    "        \"T\": [\"A\", \"C\", \"G\"],\n",
    "        \"C\": [\"T\", \"A\", \"G\"],\n",
    "        \"G\": [\"T\", \"C\", \"A\"],\n",
    "    }\n",
    "    in_str_len = len(in_str)\n",
    "    prod = list(\n",
    "        itertools.product(\n",
    "            *[flip_dict[in_str[in_str_len - i - 1]] for i in range(num_mismatch)][::-1]\n",
    "        )\n",
    "    )\n",
    "    new_strs = [in_str[: in_str_len - num_mismatch] + \"\".join(item) for item in prod]\n",
    "    return new_strs\n",
    "\n",
    "\n",
    "### Randomly (with replacement) generate mismatched versions of the query with num_mismatch substitutions\n",
    "def generate_mismatch(in_str, num_mismatch):\n",
    "    flip_dict = {\n",
    "        \"A\": [\"T\", \"C\", \"G\"],\n",
    "        \"T\": [\"A\", \"C\", \"G\"],\n",
    "        \"C\": [\"T\", \"A\", \"G\"],\n",
    "        \"G\": [\"T\", \"C\", \"A\"],\n",
    "    }\n",
    "    in_str_len = len(in_str)\n",
    "    list_str = list(in_str)\n",
    "    new_str = copy.copy(list_str)\n",
    "    for i in range(num_mismatch):\n",
    "        new_char = np.random.choice(flip_dict[list_str[in_str_len - i - 1]])\n",
    "        new_str[in_str_len - i - 1] = new_char\n",
    "    new_str = \"\".join(new_str)\n",
    "    return new_str\n",
    "\n",
    "\n",
    "### Generate a mismatch DataFrame with PAMs containing k[i] mismatches, taking n_samples for each k[i]\n",
    "### If k[i]<max_exhaustive, then this is done exhaustively\n",
    "def generate_mismatch_df(pam_df, ks=[1, 2, 4, 8, 10], n_samples=50, max_exhaustive=5):\n",
    "    mismatch_df = []\n",
    "    for i, row in pam_df.iterrows():\n",
    "        seq = row[\"sequence\"]\n",
    "        start = row[\"start\"]\n",
    "        end = row[\"end\"]\n",
    "        ref_strand = row[\"ref_strand\"]\n",
    "        target_strand = row[\"target_strand\"]\n",
    "        target_id = row[\"targetid\"]\n",
    "        mismatch_df.append(\n",
    "            [target_id, start, end, seq, ref_strand, target_strand, 0, \"Target\"]\n",
    "        )\n",
    "        for k in ks:\n",
    "            if k <= max_exhaustive:\n",
    "                mismatch_list = generate_all_mismatchs(seq, k)\n",
    "                random.shuffle(mismatch_list)\n",
    "                mismatch_list = mismatch_list[:n_samples]\n",
    "            else:\n",
    "                mismatch_list = list(\n",
    "                    set([generate_mismatch(seq, k) for i in range(n_samples)])\n",
    "                )\n",
    "            mismatch_df += [\n",
    "                [target_id, start, end, item, ref_strand, target_strand, k, \"Target\"]\n",
    "                for item in mismatch_list\n",
    "            ]\n",
    "\n",
    "    mismatch_df = pd.DataFrame(\n",
    "        mismatch_df,\n",
    "        columns=[\n",
    "            \"targetid\",\n",
    "            \"start\",\n",
    "            \"end\",\n",
    "            \"sequence\",\n",
    "            \"ref_strand\",\n",
    "            \"target_strand\",\n",
    "            \"num_mismatch\",\n",
    "            \"category\",\n",
    "        ],\n",
    "    )\n",
    "    return mismatch_df\n",
    "\n",
    "\n",
    "### Convert a target sequence to a spacer sequence\n",
    "def target_to_spacer(target_str):\n",
    "    target = Seq(target_str.upper())\n",
    "    spacer = target.reverse_complement()\n",
    "    spacer = str(spacer)\n",
    "    return spacer\n",
    "\n",
    "\n",
    "### Add bsai sites and primer sequences to the side for cloning\n",
    "def add_bsaI_sites(spacer):  ##make more general later\n",
    "    site_1 = \"AGGCACTTGCTCGTACGACGGAAGACATTAGT\"\n",
    "    site_2 = \"GTTTTCGTCTTCTTAAGGTGCCGGGCCCACAT\"\n",
    "    output_seq = site_1 + spacer + site_2\n",
    "    return output_seq\n",
    "\n",
    "\n",
    "### Convert a target sequence to a spacer with bsai sites and basic PCR primers\n",
    "def target_to_padded_spacer(target_str):\n",
    "    spacer = target_to_spacer(target_str)\n",
    "    padded_spacer = add_bsaI_sites(spacer)\n",
    "    return padded_spacer\n",
    "\n",
    "\n",
    "def check_restriction_site(in_str, rest_str=\"GGTCTC\"):\n",
    "    test_str = Seq(in_str.upper())\n",
    "    rc_test_str = test_str.reverse_complement()\n",
    "    if (rest_str in test_str) or (rest_str in rc_test_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_output_seqs(sgRNA_df, remove_guides=False):\n",
    "    output_df = copy.deepcopy(sgRNA_df)\n",
    "    output_df[\"sequence_to_order\"] = output_df[\"sequence\"].apply(\n",
    "        target_to_padded_spacer\n",
    "    )\n",
    "    output_df[\"bbsi site\"] = output_df[\"sequence_to_order\"].apply(\n",
    "        lambda x: check_restriction_site(x[28:56], rest_str=\"GAAGAC\")\n",
    "    )\n",
    "    output_df[\"bsai site\"] = output_df[\"sequence_to_order\"].apply(\n",
    "        check_restriction_site, rest_str=\"GGTCTC\"\n",
    "    )\n",
    "    if remove_guides:\n",
    "        no_internal_sites = ~output_df[\"bbsi site\"] & ~output_df[\"bsai site\"]\n",
    "        output_df = output_df[no_internal_sites]\n",
    "        print(\n",
    "            \"Percent Recovered: \"\n",
    "            + str(np.sum(no_internal_sites) / len(no_internal_sites))\n",
    "        )\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def generate_random_sequences(num_seqs, str_len=20, init_targetid=0):\n",
    "    str_arr = np.random.choice([\"A\", \"C\", \"G\", \"T\"], size=(num_seqs, str_len))\n",
    "    str_list = np.apply_along_axis(\"\".join, 1, str_arr).tolist()\n",
    "    df_out = [\n",
    "        [init_targetid + k, -1, -1, item, 1, 1, 0, \"Dummy\"]\n",
    "        for k, item in enumerate(str_list)\n",
    "    ]\n",
    "\n",
    "    df_out = pd.DataFrame(\n",
    "        df_out,\n",
    "        columns=[\n",
    "            \"targetid\",\n",
    "            \"start\",\n",
    "            \"end\",\n",
    "            \"sequence\",\n",
    "            \"ref_strand\",\n",
    "            \"target_strand\",\n",
    "            \"num_mismatch\",\n",
    "            \"category\",\n",
    "        ],\n",
    "    )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = SeqIO.read(\"./DE161_reference.gb\", \"gb\")\n",
    "\n",
    "ref_start = 807758\n",
    "ref_end = 808585\n",
    "target = genome[ref_start:ref_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pam_df = find_pams(target, startcoord=ref_start, target_strand=-1)\n",
    "genome_pam_df = find_pams(genome)\n",
    "\n",
    "target_pam_df = remove_bad_seeds(target_pam_df, \"./bad_seed_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pam_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sequence_size = 10\n",
    "most_agreement = compare_seqs(\n",
    "    target_pam_df, genome_pam_df, range(0, seed_sequence_size)\n",
    ")\n",
    "past_threshold = most_agreement < seed_sequence_size\n",
    "target_pam_df_nooff = target_pam_df[past_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent Past Threshold: \" + str(np.sum(past_threshold) / len(past_threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df = generate_mismatch_df(\n",
    "    target_pam_df_nooff, ks=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], n_samples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get CDS site used previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_df = mismatch_df[mismatch_df[\"target_strand\"] == 1].reset_index(drop=True)\n",
    "site_df = guides_df[guides_df[\"start\"] == 808416].reset_index(drop=True)\n",
    "site_subsample = site_df.groupby(\"num_mismatch\").sample(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_subsample = get_output_seqs(site_subsample, remove_guides=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_subsample[\"sequence_to_order\"].apply(lambda x: x[32:52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_subsample.to_csv(\"test_guide_reorder.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add random sequences to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_df = pd.concat(\n",
    "    [\n",
    "        mismatch_df,\n",
    "        generate_random_sequences(50, init_targetid=max(mismatch_df[\"targetid\"])),\n",
    "    ]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pool for library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_dummy_seqs = 20\n",
    "ttl_replicates = 3\n",
    "\n",
    "output_df = get_output_seqs(library_df)\n",
    "bad_targetids = (\n",
    "    output_df[output_df[\"bbsi site\"] & (output_df[\"num_mismatch\"] == 0)][\n",
    "        \"targetid\"\n",
    "    ].tolist()\n",
    "    + output_df[output_df[\"bsai site\"] & (output_df[\"num_mismatch\"] == 0)][\n",
    "        \"targetid\"\n",
    "    ].tolist()\n",
    ")\n",
    "bad_targetids = sorted(bad_targetids)\n",
    "nobadtarget_df = output_df[~output_df[\"targetid\"].isin(bad_targetids)]\n",
    "norestsite_df = nobadtarget_df[\n",
    "    ~nobadtarget_df[\"bbsi site\"] & ~nobadtarget_df[\"bsai site\"]\n",
    "].reset_index(drop=True)\n",
    "output_mismatch_df = (\n",
    "    norestsite_df[norestsite_df[\"num_mismatch\"] != 0]\n",
    "    .groupby([\"targetid\", \"num_mismatch\"])\n",
    "    .apply(lambda x: x[:ttl_replicates])\n",
    ")\n",
    "output_mismatch_df = output_mismatch_df.reset_index(level=2, drop=True)\n",
    "\n",
    "output_match_df = norestsite_df[norestsite_df[\"num_mismatch\"] == 0]\n",
    "final_library_df = pd.concat([output_match_df, output_mismatch_df])\n",
    "final_library_df = (\n",
    "    final_library_df.set_index([\"targetid\", \"num_mismatch\"]).sort_index().reset_index()\n",
    ")\n",
    "final_dummy_targetids = final_library_df[final_library_df[\"category\"] == \"Dummy\"][\n",
    "    \"targetid\"\n",
    "].tolist()[:ttl_dummy_seqs]\n",
    "final_library_df = final_library_df[\n",
    "    (final_library_df[\"category\"] == \"Target\")\n",
    "    | (final_library_df[\"targetid\"].isin(final_dummy_targetids))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_library_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_library_df.to_csv(\"2020-11-01_mVenus_library_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_series = final_library_df.groupby([\"targetid\", \"num_mismatch\"]).size()\n",
    "size_df = pd.DataFrame(size_seriescolumns=[\"size\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(size_df[size_df[\"num_mismatch\"] == 1][\"size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
