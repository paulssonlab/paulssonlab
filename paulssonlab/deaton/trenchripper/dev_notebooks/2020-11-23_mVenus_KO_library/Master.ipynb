{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrenchRipper Master Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook contains the entire `TrenchRipper` pipline, divided into simple steps. This pipline is ideal for Mother <br>Machine image data where cells possess fluorescent segmentation markers. Segmentation on phase or brightfield data <br>is being developed, but is still an experimental feature.\n",
    "\n",
    "The steps in this pipeline are as follows:\n",
    "1. Extracting your Mother Machine data (.nd2) into hdf5 format\n",
    "2. Identifying and cropping individual trenches into kymographs\n",
    "3. Segmenting cells with a fluorescent marker\n",
    "4. Determining lineages and object properties\n",
    "\n",
    "In each step, the user will dynamically specify parameters using a series of interactive diagnostics on their dataset. <br>Following this, a parameter file will be written to disk and then used to deploy a parallel computation on the <br>dataset, either locally or on a SLURM cluster.\n",
    "\n",
    "\n",
    "This is intended as an end-to-end solution to analyzing Mother Machine data. As such, **it is not trivial to plug data <br>directly into intermediate steps**, as it will lack the correct formatting and associated metadata. A notable <br>exception to this is using another program to segment data. The library references binary segmentation masks using <br>only metadata derived from their associated kymographs. As such, it is possible to generate segmentations on these <br>kymographs elsewhere and place them into the segmentation data path to have `TrenchRipper` act on those <br>segmentations instead. More on this in the segmentation section..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Run this section to import all relavent packages and libraries used in this notebook. You must run this everytime you open a new python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedir = \"/n/files/SysBio/PAULSSON LAB/Personal Folders/Daniel/Nanopore_Data/2020-11-23_lDE13/no_sample/20201124_0006_MN35044_FAO84917_7fb03513\"\n",
    "targetdir = \"/home/de64/scratch/de64/2020-11-23_lDE13_sequencing\"\n",
    "tr.trcluster.transferjob(sourcedir, targetdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mVenus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/mVenus_headpath\"\n",
    "# hdf5inputpath = \"/home/de64/scratch/de64/2020-11-07_lDE11/run\"\n",
    "nd2file = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/induction_real.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_extractor = marlin_extractor(hdf5inputpath, headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.ndextract.hdf5_fov_extractor(\n",
    "    nd2file,\n",
    "    headpath,\n",
    "    tpts_per_file=50,\n",
    "    ignore_fovmetadata=False,\n",
    "    nd2reader_override={\"z_levels\": [], \"z_coordinates\": []},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_extractor = tr.ndextract.tiff_extractor(\n",
    "#     tiffpath,\n",
    "#     headpath,\n",
    "#     [\"Phase\",\"YFP\"],tpts_per_file=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ '/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002',\n",
    " '/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/ezrdm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/mbm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/Sb7_L35',\n",
    " '/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9',\n",
    " '/n/scratch3/users/d/de64/Vibrio_2_1_TOP',\n",
    " '/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms',\n",
    " '/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X',\n",
    " '/n/scratch3/users/d/de64/Main_Experiment',\n",
    " '/n/scratch3/users/d/de64/bde17_gotime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=200, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "At this point you may want to use your output. The output of this step is a set of `.hdf5` files stored in <br>`headpath/kymograph`. The image data stored in these files takes the form of `(K,T,Y,X)` arrays <br>where K is the trench index, T is time, and Y,X are the crop dimensions.\n",
    "\n",
    "These arrays are accessible using keys of the form `\"[Trench Row Number]/[Image Channel]\"`. <br>For example, looking up phase channel data of trenches in the topmost row of an image will require <br>the key `\"0/Phase\"` The metadata associated with these files is a large pandas dataframe relating <br>crops to original FOVs, accessible using the \"kymograph\" key on `headpath/metadata.hdf5`\n",
    "\n",
    "To assist in accessing this file, you may use the `trenchripper.pandas_hdf5_handler` object to <br>interface with this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "from distributed.client import futures_of\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy.signal\n",
    "import skimage as sk\n",
    "from time import sleep\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_measurements(\n",
    "    kymographpath, channels, file_idx, output_name, img_fn, *args, **kwargs\n",
    "):\n",
    "    df = dd.read_parquet(kymographpath + \"/metadata\")\n",
    "\n",
    "    working_dfs = []\n",
    "\n",
    "    proc_file_path = kymographpath + \"/kymograph_\" + str(file_idx) + \".hdf5\"\n",
    "    with h5py.File(proc_file_path, \"r\") as infile:\n",
    "        working_filedf = df[df[\"File Index\"] == file_idx].compute()\n",
    "        trench_idx_list = working_filedf[\"File Trench Index\"].unique().tolist()\n",
    "        for trench_idx in trench_idx_list:\n",
    "            trench_df = working_filedf[\n",
    "                working_filedf[\"File Trench Index\"] == trench_idx\n",
    "            ]\n",
    "\n",
    "            for channel in channels:\n",
    "                kymo_arr = infile[channel][trench_idx]\n",
    "                fn_out = [\n",
    "                    img_fn(kymo_arr[i], *args, **kwargs)\n",
    "                    for i in range(kymo_arr.shape[0])\n",
    "                ]\n",
    "                trench_df[channel + \" \" + output_name] = fn_out\n",
    "            working_dfs.append(trench_df)\n",
    "\n",
    "    out_df = pd.concat(working_dfs)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_all_image_measurements(\n",
    "    headpath, output_path, channels, output_name, img_fn, *args, **kwargs\n",
    "):\n",
    "    kymographpath = headpath + \"/kymograph\"\n",
    "    df = dd.read_parquet(kymographpath + \"/metadata\")\n",
    "\n",
    "    file_list = df[\"File Index\"].unique().compute().tolist()\n",
    "\n",
    "    delayed_list = []\n",
    "\n",
    "    for file_idx in file_list:\n",
    "        df_delayed = delayed(get_image_measurements)(\n",
    "            kymographpath, channels, file_idx, output_name, img_fn, *args, **kwargs\n",
    "        )\n",
    "        delayed_list.append(df_delayed.persist())\n",
    "\n",
    "    ## filtering out non-failed dataframes ##\n",
    "    all_delayed_futures = []\n",
    "    for item in delayed_list:\n",
    "        all_delayed_futures += futures_of(item)\n",
    "    while any(future.status == \"pending\" for future in all_delayed_futures):\n",
    "        sleep(0.1)\n",
    "\n",
    "    good_delayed = []\n",
    "    for item in delayed_list:\n",
    "        if all([future.status == \"finished\" for future in futures_of(item)]):\n",
    "            good_delayed.append(item)\n",
    "\n",
    "    ## compiling output dataframe ##\n",
    "    df_out = dd.from_delayed(good_delayed).persist()\n",
    "    df_out[\"FOV Parquet Index\"] = df_out.index\n",
    "    df_out = df_out.set_index(\"FOV Parquet Index\", drop=True, sorted=False)\n",
    "    df_out = df_out.repartition(partition_size=\"25MB\").persist()\n",
    "\n",
    "    dd.to_parquet(\n",
    "        df_out,\n",
    "        output_path,\n",
    "        engine=\"fastparquet\",\n",
    "        compression=\"gzip\",\n",
    "        write_metadata_file=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/mVenus_headpath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymograph_metadata = dd.read_parquet(headpath + \"/kymograph/metadata\")\n",
    "\n",
    "# kymograph_metadata = pd.read_parquet(headpath + \"/kymograph/metadata\")\n",
    "# kymograph_metadata = kymograph_metadata[kymograph_metadata[\"fov\"]>125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_image_measurements(\n",
    "    headpath,\n",
    "    headpath + \"/percentiles\",\n",
    "    [\"mCherry\", \"YFP\"],\n",
    "    \"90th Percentile\",\n",
    "    np.percentile,\n",
    "    90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymograph_metadata = pd.read_parquet(headpath + \"/percentiles\")\n",
    "kymograph_metadata = kymograph_metadata.set_index([\"trenchid\", \"timepoints\"], drop=True)\n",
    "kymograph_metadata = kymograph_metadata.loc[(slice(None), range(10, 20)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchid_groupby = kymograph_metadata.groupby([\"trenchid\"])\n",
    "yfp_ratio_mean = trenchid_groupby.apply(\n",
    "    lambda x: np.mean(x[\"YFP 90th Percentile\"] / x[\"mCherry 90th Percentile\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yfp_ratio_mean, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluorescence Segmentation\n",
    "\n",
    "Now that you have copped your data into kymographs, we will now perform segmentation/cell detection <br>\n",
    "on your kymographs. Currently, this pipeline only supports segmentation of fluorescence images; however, <br>\n",
    "segmentation of transmitted light imaging techniques is in development.\n",
    "\n",
    "The output of this step will be a set of `segmentation_[File #].hdf5` files stored in `headpath/fluorsegmentation`.<br>\n",
    "The image data stored in these files takes the exact same form as the kymograph data, `(K,T,Y,X)` arrays <br>\n",
    "where K is the trench index, T is time, and Y,X are the crop dimensions. These arrays are accessible using <br>\n",
    "keys of the form `\"[Trench Row Number]\"`.\n",
    "\n",
    "Since no metadata is generated by this step, it is possible to use another segmentation algorithm on the kymograph <br>\n",
    "data. The output of segmentation must be split into `segmentation_[File #].hdf5` files, where `[File #]` agrees with the<br>\n",
    "corresponding `kymograph_[File #].hdf5` file. Additionally, the `(K,T,Y,X)` arrays must be of the same shape as the <br>\n",
    "kymograph arrays and accessible at the corresponding `\"[Trench Row Number]\"` key. These files must be placed into <br>\n",
    "their own folder at `headpath/foldername`. This folder may then be used in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the interactive segmentation class\n",
    "\n",
    "As a first step, initialize the `tr.fluo_segmentation_interactive` class that will be handling all steps of generating a segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation = tr.fluo_segmentation_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose channel to segment on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.choose_seg_channel_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**fov_idx (int)** :\n",
    "\n",
    "**n_trenches (int)** :\n",
    "\n",
    "**t_range (tuple)** :\n",
    "\n",
    "**t_subsample_step (int)** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.import_array_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_processed_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Cell Mask Envelope\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**cell_mask_method (str)** : Thresholding method, can be a local or global Otsu threshold.\n",
    "\n",
    "**cell_otsu_scaling (float)** : Scaling factor applied to determined threshold.\n",
    "\n",
    "**local_otsu_r (int)** : Radius of thresholding kernel used in the local otsu thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_cell_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_eig_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_dist_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_marker_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"1GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = tr.segment.fluo_segmentation_cluster(headpath, paramfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment.dask_segment(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Properties (No Lineage)\n",
    "\n",
    "Note this does not require a dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = tr.analysis.regionprops_extractor(\n",
    "    headpath, \"fluorsegmentation\", intensity_channel_list=[\"mCherry\", \"YFP\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.export_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "from distributed.client import futures_of\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import scipy.signal\n",
    "import skimage as sk\n",
    "from time import sleep\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(headpath + \"/metadata.hdf5\")\n",
    "meta_handle = meta_handle.read_df(\"global\", read_metadata=True).metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_props = pd.read_pickle(headpath + \"/analysis.pkl\").loc[\n",
    "    (slice(None), slice(None), list(range(10, 20)), slice(None))\n",
    "]\n",
    "region_props = region_props.reset_index()\n",
    "region_props = region_props.set_index(\n",
    "    [\"trenchid\", \"timepoints\", \"Intensity Channel\", \"Objectid\"], drop=True\n",
    ")\n",
    "region_props = region_props.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_props[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mchy_df = region_props.loc[(slice(None), slice(None), [\"mCherry\"]), :].droplevel(\n",
    "    \"Intensity Channel\"\n",
    ")\n",
    "yfp_df = region_props.loc[(slice(None), slice(None), [\"YFP\"]), :].droplevel(\n",
    "    \"Intensity Channel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df = yfp_df[\"mean_intensity\"] / mchy_df[\"mean_intensity\"]\n",
    "\n",
    "scaled_yfp = (yfp_df[\"mean_intensity\"] - yfp_df[\"mean_intensity\"].min()) / (\n",
    "    yfp_df[\"mean_intensity\"].max() - yfp_df[\"mean_intensity\"].min()\n",
    ")\n",
    "scaled_mchy = (mchy_df[\"mean_intensity\"] - mchy_df[\"mean_intensity\"].min()) / (\n",
    "    mchy_df[\"mean_intensity\"].max() - mchy_df[\"mean_intensity\"].min()\n",
    ")\n",
    "scaled_ratio_df = scaled_yfp / scaled_mchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = ratio_df.groupby(\"trenchid\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mchy_df[\"mean_intensity\"], bins=200, range=(0, 3000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yfp_df[\"mean_intensity\"], bins=200, range=(0, 15000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ratio_df, bins=200, range=(0, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scaled_ratio_df, bins=200, range=(0, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_df, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# hdf5inputpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/run\"\n",
    "\n",
    "# metadata_files = []\n",
    "# for root, _, files in os.walk(hdf5inputpath):\n",
    "#             metadata_files.extend(\n",
    "#                 [\n",
    "#                     os.path.join(root, f)\n",
    "#                     for f in files\n",
    "#                     if \"metadata\" in os.path.splitext(f)[0]\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "# for i in range(1,9):\n",
    "#     indf = pd.read_hdf(metadata_files[0],key='data/' + str(i))\n",
    "#     indf.to_hdf(metadata_files[0][:-5] + \"_t=\" + str(i) + \".hdf5\", \"data\" ,\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import h5py_cache\n",
    "import tifffile\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as ipyw\n",
    "\n",
    "from nd2reader import ND2Reader\n",
    "from tifffile import imsave, imread\n",
    "from paulssonlab.deaton.trenchripper.trenchripper.utils import (\n",
    "    pandas_hdf5_handler,\n",
    "    writedir,\n",
    ")\n",
    "from parse import compile\n",
    "\n",
    "\n",
    "class marlin_extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf5inputpath,\n",
    "        headpath,\n",
    "        tpts_per_file=100,\n",
    "        parsestr=\"fov={fov:d}_config={channel}_t={timepoints:d}.hdf5\",\n",
    "        metaparsestr=\"metadata_t={timepoint:d}.hdf5\",\n",
    "        zero_base_keys=[\"timepoints\"],\n",
    "    ):  # note this chunk size has a large role in downstream steps...make sure is less than 1 MB\n",
    "        \"\"\"Utility to import hdf5 format files from MARLIN Runs.\n",
    "\n",
    "        Attributes:\n",
    "            headpath (str): base directory for data analysis\n",
    "            tiffpath (str): directory where tiff files are located\n",
    "            metapath (str): metadata path\n",
    "            hdf5path (str): where to store hdf5 data\n",
    "            tpts_per_file (int): number of timepoints to put in each hdf5 file\n",
    "            parsestr (str): format of filenames from which to extract metadata (using parse library)\n",
    "        \"\"\"\n",
    "        self.hdf5inputpath = hdf5inputpath\n",
    "        self.headpath = headpath\n",
    "        self.metapath = self.headpath + \"/metadata.hdf5\"\n",
    "        self.hdf5path = self.headpath + \"/hdf5\"\n",
    "        self.tpts_per_file = tpts_per_file\n",
    "        self.parsestr = parsestr\n",
    "        self.metaparsestr = metaparsestr\n",
    "        self.zero_base_keys = zero_base_keys\n",
    "\n",
    "        self.organism = \"\"\n",
    "        self.microscope = \"\"\n",
    "        self.notes = \"\"\n",
    "\n",
    "    def get_metadata(\n",
    "        self,\n",
    "        hdf5inputpath,\n",
    "        parsestr=\"fov={fov:d}_config={channel}_t={timepoints:d}.hdf5\",\n",
    "        metaparsestr=\"metadata_t={timepoint:d}.hdf5\",\n",
    "        zero_base_keys=[\"timepoints\"],\n",
    "    ):\n",
    "        parser = compile(parsestr)\n",
    "        parse_keys = [\n",
    "            item.split(\"}\")[0].split(\":\")[0] for item in parsestr.split(\"{\")[1:]\n",
    "        ] + [\"image_paths\"]\n",
    "\n",
    "        exp_metadata = {}\n",
    "        fov_metadata = {key: [] for key in parse_keys}\n",
    "\n",
    "        hdf5_files = []\n",
    "        metadata_files = []\n",
    "        for root, _, files in os.walk(hdf5inputpath):\n",
    "            hdf5_files.extend(\n",
    "                [\n",
    "                    os.path.join(root, f)\n",
    "                    for f in files\n",
    "                    if \"config\" in os.path.splitext(f)[0]\n",
    "                ]\n",
    "            )\n",
    "            metadata_files.extend(\n",
    "                [\n",
    "                    os.path.join(root, f)\n",
    "                    for f in files\n",
    "                    if \"metadata\" in os.path.splitext(f)[0]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        with h5py.File(hdf5_files[0], \"r\") as infile:\n",
    "            hdf5_shape = infile[\"data\"].shape\n",
    "        exp_metadata[\"height\"] = hdf5_shape[0]\n",
    "        exp_metadata[\"width\"] = hdf5_shape[1]\n",
    "        #     exp_metadata['pixel_microns'] = tags['65326']\n",
    "\n",
    "        for f in hdf5_files:\n",
    "            match = parser.search(f)\n",
    "            # ignore any files that don't match the regex\n",
    "            if match is not None:\n",
    "                # Add to dictionary\n",
    "                fov_frame_dict = match.named\n",
    "                for key, value in fov_frame_dict.items():\n",
    "                    fov_metadata[key].append(value)\n",
    "                fov_metadata[\"image_paths\"].append(f)\n",
    "\n",
    "        for zero_base_key in zero_base_keys:\n",
    "            if 0 not in fov_metadata[zero_base_key]:\n",
    "                fov_metadata[zero_base_key] = [\n",
    "                    item - 1 for item in fov_metadata[zero_base_key]\n",
    "                ]\n",
    "\n",
    "        channels = list(set(fov_metadata[\"channel\"]))\n",
    "        exp_metadata[\"channels\"] = channels\n",
    "        exp_metadata[\"num_fovs\"] = len(set(fov_metadata[\"fov\"]))\n",
    "        exp_metadata[\"frames\"] = list(set(fov_metadata[\"timepoints\"]))\n",
    "        exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "        exp_metadata[\"pixel_microns\"] = 0.16136255757596  ##hack assuming ti5 40x\n",
    "        fov_metadata = pd.DataFrame(fov_metadata)\n",
    "        fov_metadata = fov_metadata.set_index([\"fov\", \"timepoints\"]).sort_index()\n",
    "\n",
    "        output_fov_metadata = []\n",
    "        step = len(channels)\n",
    "        for i in range(0, len(fov_metadata), step):\n",
    "            rows = fov_metadata[i : i + step]\n",
    "            channel_path_entry = dict(zip(rows[\"channel\"], rows[\"image_paths\"]))\n",
    "            fov_entry = rows.index.get_level_values(\"fov\").unique()[0]\n",
    "            timepoint_entry = rows.index.get_level_values(\"timepoints\").unique()[0]\n",
    "            fov_metadata_entry = {\n",
    "                \"fov\": fov_entry,\n",
    "                \"timepoints\": timepoint_entry,\n",
    "                \"channel_paths\": channel_path_entry,\n",
    "            }\n",
    "            output_fov_metadata.append(fov_metadata_entry)\n",
    "        fov_metadata = pd.DataFrame(output_fov_metadata).set_index(\n",
    "            [\"fov\", \"timepoints\"]\n",
    "        )\n",
    "\n",
    "        metaparser = compile(metaparsestr)\n",
    "        meta_df_out = []\n",
    "        for metadata_file in metadata_files:\n",
    "            match = metaparser.search(metadata_file)\n",
    "            if match is not None:\n",
    "                timepoint = match.named[\"timepoint\"]\n",
    "                meta_df = pd.read_hdf(metadata_file)\n",
    "                meta_df[\"timepoints\"] = timepoint\n",
    "                meta_df_out.append(meta_df)\n",
    "        meta_df_out = pd.concat(meta_df_out)\n",
    "        if 0 not in meta_df_out[\"timepoints\"].unique().tolist():\n",
    "            meta_df_out[\"timepoints\"] = meta_df_out[\"timepoints\"] - 1\n",
    "        meta_df_out = meta_df_out.groupby([\"fov\", \"timepoints\"], as_index=False)\n",
    "        meta_df_out = meta_df_out.apply(lambda x: x[0:1])\n",
    "        meta_df_out = meta_df_out.set_index([\"fov\", \"timepoints\"], drop=True)\n",
    "        fov_metadata = fov_metadata.join(meta_df_out)\n",
    "\n",
    "        return exp_metadata, fov_metadata\n",
    "\n",
    "    def assignidx(self, fov_metadata):\n",
    "        numfovs = len(fov_metadata.index.get_level_values(\"fov\").unique().tolist())\n",
    "        timepoints_per_fov = len(\n",
    "            fov_metadata.index.get_level_values(\"timepoints\").unique().tolist()\n",
    "        )\n",
    "\n",
    "        files_per_fov = (timepoints_per_fov // self.tpts_per_file) + 1\n",
    "        remainder = timepoints_per_fov % self.tpts_per_file\n",
    "        ttlfiles = numfovs * files_per_fov\n",
    "        fov_file_idx = np.repeat(list(range(files_per_fov)), self.tpts_per_file)[\n",
    "            : -(self.tpts_per_file - remainder)\n",
    "        ]\n",
    "        file_idx = np.concatenate(\n",
    "            [fov_file_idx + (fov_idx * files_per_fov) for fov_idx in range(numfovs)]\n",
    "        )\n",
    "        fov_img_idx = np.repeat(\n",
    "            np.array(list(range(self.tpts_per_file)))[np.newaxis, :],\n",
    "            files_per_fov,\n",
    "            axis=0,\n",
    "        )\n",
    "        fov_img_idx = fov_img_idx.flatten()[: -(self.tpts_per_file - remainder)]\n",
    "        img_idx = np.concatenate([fov_img_idx for fov_idx in range(numfovs)])\n",
    "\n",
    "        fov_idx = np.repeat(list(range(numfovs)), timepoints_per_fov)\n",
    "        timepoint_idx = np.repeat(\n",
    "            np.array(list(range(timepoints_per_fov)))[np.newaxis, :], numfovs, axis=0\n",
    "        ).flatten()\n",
    "\n",
    "        outdf = copy.deepcopy(fov_metadata)\n",
    "        outdf[\"File Index\"] = file_idx\n",
    "        outdf[\"Image Index\"] = img_idx\n",
    "        return outdf\n",
    "\n",
    "    def writemetadata(self, t_range=None, fov_list=None):\n",
    "\n",
    "        exp_metadata, fov_metadata = self.get_metadata(\n",
    "            self.hdf5inputpath,\n",
    "            parsestr=self.parsestr,\n",
    "            zero_base_keys=self.zero_base_keys,\n",
    "        )\n",
    "\n",
    "        if t_range is not None:\n",
    "            exp_metadata[\"frames\"] = exp_metadata[\"frames\"][t_range[0] : t_range[1] + 1]\n",
    "            exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "            fov_metadata = fov_metadata.loc[\n",
    "                pd.IndexSlice[:, slice(t_range[0], t_range[1])], :\n",
    "            ]  # 4 -> 70\n",
    "\n",
    "        if fov_list is not None:\n",
    "            fov_metadata = fov_metadata.loc[list(fov_list)]\n",
    "            exp_metadata[\"fields_of_view\"] = list(fov_list)\n",
    "\n",
    "        self.chunk_shape = (1, exp_metadata[\"height\"], exp_metadata[\"width\"])\n",
    "        chunk_bytes = 2 * np.multiply.accumulate(np.array(self.chunk_shape))[-1]\n",
    "        self.chunk_cache_mem_size = 2 * chunk_bytes\n",
    "        exp_metadata[\"chunk_shape\"], exp_metadata[\"chunk_cache_mem_size\"] = (\n",
    "            self.chunk_shape,\n",
    "            self.chunk_cache_mem_size,\n",
    "        )\n",
    "        exp_metadata[\"Organism\"], exp_metadata[\"Microscope\"], exp_metadata[\"Notes\"] = (\n",
    "            self.organism,\n",
    "            self.microscope,\n",
    "            self.notes,\n",
    "        )\n",
    "        self.meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "\n",
    "        assignment_metadata = self.assignidx(fov_metadata)\n",
    "        assignment_metadata.astype({\"File Index\": int, \"Image Index\": int})\n",
    "\n",
    "        self.meta_handle.write_df(\"global\", assignment_metadata, metadata=exp_metadata)\n",
    "\n",
    "    def read_metadata(self):\n",
    "        writedir(self.hdf5path, overwrite=True)\n",
    "        self.writemetadata()\n",
    "        metadf = self.meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        self.metadata = metadf.metadata\n",
    "        metadf = metadf.reset_index(inplace=False)\n",
    "        metadf = metadf.set_index(\n",
    "            [\"File Index\", \"Image Index\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        self.metadf = metadf.sort_index()\n",
    "\n",
    "    def set_params(self, fov_list, t_range, organism, microscope, notes):\n",
    "        self.fov_list = fov_list\n",
    "        self.t_range = t_range\n",
    "        self.organism = organism\n",
    "        self.microscope = microscope\n",
    "        self.notes = notes\n",
    "\n",
    "    def inter_set_params(self):\n",
    "        self.read_metadata()\n",
    "        t0, tf = (self.metadata[\"frames\"][0], self.metadata[\"frames\"][-1])\n",
    "        available_fov_list = self.metadf[\"fov\"].unique().tolist()\n",
    "        selection = ipyw.interactive(\n",
    "            self.set_params,\n",
    "            {\"manual\": True},\n",
    "            fov_list=ipyw.SelectMultiple(options=available_fov_list),\n",
    "            t_range=ipyw.IntRangeSlider(\n",
    "                value=[t0, tf],\n",
    "                min=t0,\n",
    "                max=tf,\n",
    "                step=1,\n",
    "                description=\"Time Range:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "            organism=ipyw.Textarea(\n",
    "                value=\"\",\n",
    "                placeholder=\"Organism imaged in this experiment.\",\n",
    "                description=\"Organism:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "            microscope=ipyw.Textarea(\n",
    "                value=\"\",\n",
    "                placeholder=\"Microscope used in this experiment.\",\n",
    "                description=\"Microscope:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "            notes=ipyw.Textarea(\n",
    "                value=\"\",\n",
    "                placeholder=\"General experiment notes.\",\n",
    "                description=\"Notes:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "        )\n",
    "        display(selection)\n",
    "\n",
    "    def extract(self, dask_controller, retries=1):\n",
    "        dask_controller.futures = {}\n",
    "\n",
    "        self.writemetadata(t_range=self.t_range, fov_list=self.fov_list)\n",
    "        metadf = self.meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        self.metadata = metadf.metadata\n",
    "        metadf = metadf.reset_index(inplace=False)\n",
    "        metadf = metadf.set_index(\n",
    "            [\"File Index\", \"Image Index\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        self.metadf = metadf.sort_index()\n",
    "\n",
    "        def writehdf5(fovnum, num_entries, timepoint_list, file_idx):\n",
    "            y_dim = self.metadata[\"height\"]\n",
    "            x_dim = self.metadata[\"width\"]\n",
    "            filedf = self.metadf.loc[file_idx].reset_index(inplace=False)\n",
    "            filedf = filedf.set_index(\n",
    "                [\"timepoints\"], drop=True, append=False, inplace=False\n",
    "            )\n",
    "            filedf = filedf.sort_index()\n",
    "\n",
    "            with h5py_cache.File(\n",
    "                self.hdf5path + \"/hdf5_\" + str(file_idx) + \".hdf5\",\n",
    "                \"w\",\n",
    "                chunk_cache_mem_size=self.chunk_cache_mem_size,\n",
    "            ) as h5pyfile:\n",
    "                for i, channel in enumerate(self.metadata[\"channels\"]):\n",
    "                    hdf5_dataset = h5pyfile.create_dataset(\n",
    "                        str(channel),\n",
    "                        (num_entries, y_dim, x_dim),\n",
    "                        chunks=self.chunk_shape,\n",
    "                        dtype=\"uint16\",\n",
    "                    )\n",
    "                    for j in range(len(timepoint_list)):\n",
    "                        frame = timepoint_list[j]\n",
    "                        entry = filedf.loc[frame][\"channel_paths\"]\n",
    "                        file_path = entry[channel]\n",
    "                        with h5py_cache.File(file_path, \"r\") as infile:\n",
    "                            img = infile[\"data\"][:]\n",
    "                        hdf5_dataset[j, :, :] = img\n",
    "            return \"Done.\"\n",
    "\n",
    "        file_list = self.metadf.index.get_level_values(\"File Index\").unique().values\n",
    "        num_jobs = len(file_list)\n",
    "        random_priorities = np.random.uniform(size=(num_jobs,))\n",
    "\n",
    "        for k, file_idx in enumerate(file_list):\n",
    "            priority = random_priorities[k]\n",
    "            filedf = self.metadf.loc[file_idx]\n",
    "\n",
    "            fovnum = filedf[0:1][\"fov\"].values[0]\n",
    "            num_entries = len(filedf.index.get_level_values(\"Image Index\").values)\n",
    "            timepoint_list = filedf[\"timepoints\"].tolist()\n",
    "\n",
    "            future = dask_controller.daskclient.submit(\n",
    "                writehdf5,\n",
    "                fovnum,\n",
    "                num_entries,\n",
    "                timepoint_list,\n",
    "                file_idx,\n",
    "                retries=retries,\n",
    "                priority=priority,\n",
    "            )\n",
    "            dask_controller.futures[\"extract file: \" + str(file_idx)] = future\n",
    "\n",
    "        extracted_futures = [\n",
    "            dask_controller.futures[\"extract file: \" + str(file_idx)]\n",
    "            for file_idx in file_list\n",
    "        ]\n",
    "        pause_for_extract = dask_controller.daskclient.gather(\n",
    "            extracted_futures, errors=\"skip\"\n",
    "        )\n",
    "\n",
    "        futures_name_list = [\"extract file: \" + str(file_idx) for file_idx in file_list]\n",
    "        failed_files = [\n",
    "            futures_name_list[k]\n",
    "            for k, item in enumerate(extracted_futures)\n",
    "            if item.status != \"finished\"\n",
    "        ]\n",
    "        failed_file_idx = [int(item.split(\":\")[1]) for item in failed_files]\n",
    "        outdf = self.meta_handle.read_df(\"global\", read_metadata=False)\n",
    "\n",
    "        tempmeta = outdf.reset_index(inplace=False)\n",
    "        tempmeta = tempmeta.set_index(\n",
    "            [\"File Index\", \"Image Index\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        failed_fovs = tempmeta.loc[failed_file_idx][\"fov\"].unique().tolist()\n",
    "\n",
    "        outdf = outdf.drop(failed_fovs)\n",
    "\n",
    "        if self.t_range != None:\n",
    "            outdf = outdf.reset_index(inplace=False)\n",
    "            outdf[\"timepoints\"] = outdf[\"timepoints\"] - self.t_range[0]\n",
    "            outdf = outdf.set_index(\n",
    "                [\"fov\", \"timepoints\"], drop=True, append=False, inplace=False\n",
    "            )\n",
    "\n",
    "        self.meta_handle.write_df(\"global\", outdf, metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/FISH_headpath\"\n",
    "hdf5inputpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = marlin_extractor(hdf5inputpath, headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=200, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "At this point you may want to use your output. The output of this step is a set of `.hdf5` files stored in <br>`headpath/kymograph`. The image data stored in these files takes the form of `(K,T,Y,X)` arrays <br>where K is the trench index, T is time, and Y,X are the crop dimensions.\n",
    "\n",
    "These arrays are accessible using keys of the form `\"[Trench Row Number]/[Image Channel]\"`. <br>For example, looking up phase channel data of trenches in the topmost row of an image will require <br>the key `\"0/Phase\"` The metadata associated with these files is a large pandas dataframe relating <br>crops to original FOVs, accessible using the \"kymograph\" key on `headpath/metadata.hdf5`\n",
    "\n",
    "To assist in accessing this file, you may use the `trenchripper.pandas_hdf5_handler` object to <br>interface with this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "from distributed.client import futures_of\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy.signal\n",
    "import skimage as sk\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_measurements(\n",
    "    kymographpath, channels, file_idx, output_name, img_fn, *args, **kwargs\n",
    "):\n",
    "    df = dd.read_parquet(kymographpath + \"/metadata\")\n",
    "\n",
    "    working_dfs = []\n",
    "\n",
    "    proc_file_path = kymographpath + \"/kymograph_\" + str(file_idx) + \".hdf5\"\n",
    "    with h5py.File(proc_file_path, \"r\") as infile:\n",
    "        working_filedf = df[df[\"File Index\"] == file_idx].compute()\n",
    "        trench_idx_list = working_filedf[\"File Trench Index\"].unique().tolist()\n",
    "        for trench_idx in trench_idx_list:\n",
    "            trench_df = working_filedf[\n",
    "                working_filedf[\"File Trench Index\"] == trench_idx\n",
    "            ]\n",
    "\n",
    "            for channel in channels:\n",
    "                kymo_arr = infile[channel][trench_idx]\n",
    "                fn_out = [\n",
    "                    img_fn(kymo_arr[i], *args, **kwargs)\n",
    "                    for i in range(kymo_arr.shape[0])\n",
    "                ]\n",
    "                trench_df[channel + \" \" + output_name] = fn_out\n",
    "            working_dfs.append(trench_df)\n",
    "\n",
    "    out_df = pd.concat(working_dfs)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_all_image_measurements(\n",
    "    headpath, output_path, channels, output_name, img_fn, *args, **kwargs\n",
    "):\n",
    "    kymographpath = headpath + \"/kymograph\"\n",
    "    df = dd.read_parquet(kymographpath + \"/metadata\")\n",
    "\n",
    "    file_list = df[\"File Index\"].unique().compute().tolist()\n",
    "\n",
    "    delayed_list = []\n",
    "\n",
    "    for file_idx in file_list:\n",
    "        df_delayed = delayed(get_image_measurements)(\n",
    "            kymographpath, channels, file_idx, output_name, img_fn, *args, **kwargs\n",
    "        )\n",
    "        delayed_list.append(df_delayed.persist())\n",
    "\n",
    "    ## filtering out non-failed dataframes ##\n",
    "    all_delayed_futures = []\n",
    "    for item in delayed_list:\n",
    "        all_delayed_futures += futures_of(item)\n",
    "    while any(future.status == \"pending\" for future in all_delayed_futures):\n",
    "        sleep(0.1)\n",
    "\n",
    "    good_delayed = []\n",
    "    for item in delayed_list:\n",
    "        if all([future.status == \"finished\" for future in futures_of(item)]):\n",
    "            good_delayed.append(item)\n",
    "\n",
    "    ## compiling output dataframe ##\n",
    "    df_out = dd.from_delayed(good_delayed).persist()\n",
    "    df_out[\"FOV Parquet Index\"] = df_out.index\n",
    "    df_out = df_out.set_index(\"FOV Parquet Index\", drop=True, sorted=False)\n",
    "    df_out = df_out.repartition(partition_size=\"25MB\").persist()\n",
    "\n",
    "    dd.to_parquet(\n",
    "        df_out,\n",
    "        output_path,\n",
    "        engine=\"fastparquet\",\n",
    "        compression=\"gzip\",\n",
    "        write_metadata_file=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/FISH_headpath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymograph_metadata = dd.read_parquet(headpath + \"/kymograph/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_image_measurements(\n",
    "    headpath,\n",
    "    headpath + \"/percentiles\",\n",
    "    [\"RFP\", \"Cy5\", \"Cy7\"],\n",
    "    \"98th Percentile\",\n",
    "    np.percentile,\n",
    "    98,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage as sk\n",
    "import scipy.signal\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import sklearn.mixture\n",
    "\n",
    "\n",
    "def get_background_dist(values):\n",
    "    hist_range = (0, np.percentile(values, 60))\n",
    "    freq, val = np.histogram(values, bins=100, range=hist_range)\n",
    "    mu_n = val[np.argmax(freq)]\n",
    "    lower_tail = values[values < mu_n]\n",
    "    std_n = sp.stats.halfnorm.fit(-lower_tail)[1]\n",
    "    return mu_n, std_n\n",
    "\n",
    "\n",
    "def get_background_dist_peak(values):\n",
    "    hist_range = (0, np.percentile(values, 90))\n",
    "    hist_count, hist_vals = np.histogram(values, bins=100, range=hist_range)\n",
    "    peaks = sp.signal.find_peaks(hist_count, distance=20)[0]\n",
    "    mu_n = hist_vals[peaks[0]]\n",
    "    lower_tail = values[values < mu_n]\n",
    "    std_n = sp.stats.halfnorm.fit(-lower_tail)[1]\n",
    "    return mu_n, std_n\n",
    "\n",
    "\n",
    "def get_background_thr(values, background_fn, background_scaling=5.0):\n",
    "    mu_n, std_n = background_fn(values)\n",
    "    back_thr = mu_n + background_scaling * std_n\n",
    "    return back_thr\n",
    "\n",
    "\n",
    "def get_signal_sum(df):\n",
    "    trench_group = df.groupby([\"trenchid\"])\n",
    "    barcodes = trench_group.apply(\n",
    "        lambda x: np.array(\n",
    "            x[\"RFP 98th Percentile\"].tolist()\n",
    "            + x[\"Cy5 98th Percentile\"].tolist()\n",
    "            + x[\"Cy7 98th Percentile\"].tolist()\n",
    "        ).astype(float)\n",
    "    )\n",
    "\n",
    "    short = barcodes.apply(lambda x: len(x) != 24)\n",
    "    for idx in np.where(short)[0]:\n",
    "        barcodes[idx] = np.array([0.0 for i in range(24)])\n",
    "\n",
    "    barcodes_arr = np.array(barcodes.to_list())\n",
    "    barcodes_arr_no_short = np.array(barcodes[~short].to_list())\n",
    "\n",
    "    signal_sum = np.sum(barcodes_arr, axis=1)\n",
    "    signal_sum_no_short = np.sum(barcodes_arr_no_short, axis=1)\n",
    "\n",
    "    signal_filter_thr = get_background_thr(\n",
    "        signal_sum_no_short, get_background_dist_peak\n",
    "    )\n",
    "    print(\"Signal Threshold: \" + str(signal_filter_thr))\n",
    "    high_signal_mask = signal_sum > signal_filter_thr\n",
    "    high_signal_barcodes = barcodes[high_signal_mask]\n",
    "\n",
    "    trenchid_list = high_signal_barcodes.index.get_level_values(\n",
    "        \"trenchid\"\n",
    "    ).values.tolist()\n",
    "    high_signal_metadata = df.loc[trenchid_list]\n",
    "\n",
    "    return signal_sum, high_signal_barcodes\n",
    "\n",
    "\n",
    "def get_gmm_params(values):\n",
    "    gmm = skl.mixture.GaussianMixture(n_components=2, n_init=10)\n",
    "    gmm.fit(values.reshape(-1, 1))\n",
    "    #     probs = gmm.predict_proba(values.reshape(-1,1))\n",
    "    return gmm.means_, (gmm.covariances_) ** (1 / 2)\n",
    "\n",
    "\n",
    "def get_gmm_hard_assign(values):\n",
    "    gmm = skl.mixture.GaussianMixture(n_components=2, n_init=10)\n",
    "    gmm.fit(values.reshape(-1, 1))\n",
    "    lower_mean_idx = np.argmin(gmm.means_)\n",
    "    assign = gmm.predict(values.reshape(-1, 1))\n",
    "    if lower_mean_idx == 1:\n",
    "        assign = (-assign) + 1\n",
    "    return assign\n",
    "\n",
    "\n",
    "def get_gmm_probs(values):\n",
    "    gmm = skl.mixture.GaussianMixture(n_components=2, n_init=10)\n",
    "    gmm.fit(values.reshape(-1, 1))\n",
    "    probs = gmm.predict_proba(values.reshape(-1, 1))\n",
    "    return probs\n",
    "\n",
    "\n",
    "def str_to_bool(string):\n",
    "    code = {\"1\": True, \"0\": False}\n",
    "    conv_str = np.array(list(map(lambda x: code[x], string)))\n",
    "    return conv_str\n",
    "\n",
    "\n",
    "def bool_to_str(integer):\n",
    "    rev_code = {True: \"1\", False: \"0\"}\n",
    "    conv_int = \"\".join(list(map(lambda x: rev_code[x], integer)))\n",
    "    return conv_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymograph_metadata = pd.read_parquet(headpath + \"/percentiles\")\n",
    "kymograph_metadata = kymograph_metadata.set_index([\"trenchid\", \"timepoints\"], drop=True)\n",
    "signal_sum, high_signal_barcodes_series = get_signal_sum(kymograph_metadata)\n",
    "high_signal_barcodes = np.array([item for item in high_signal_barcodes_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_signal_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(signal_sum, bins=100, range=(0, 200000))\n",
    "plt.title(\"Sum of all barcode signal\")\n",
    "plt.xlabel(\"Summed Intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_list = []\n",
    "for i in range(24):\n",
    "    #     if i == 1:\n",
    "    #         assign = high_signal_barcodes[:, i]>1100\n",
    "    #     else:\n",
    "    assign = get_gmm_hard_assign(high_signal_barcodes[:, i])\n",
    "    assign_list.append(assign)\n",
    "assign_arr = np.array(assign_list, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(24, 8))\n",
    "colors = [\"salmon\", \"violet\", \"grey\"]\n",
    "\n",
    "for i in range(high_signal_barcodes.shape[1]):\n",
    "    row_idx = i // 8\n",
    "    column_idx = i % 8\n",
    "    color = colors[row_idx]\n",
    "    max_val = np.percentile(high_signal_barcodes[:, i].flatten(), 99)\n",
    "\n",
    "    bins = np.linspace(0, max_val, num=50)\n",
    "    on_arr = high_signal_barcodes[:, i][assign_arr[i]]\n",
    "    off_arr = high_signal_barcodes[:, i][~assign_arr[i]]\n",
    "\n",
    "    on_frq, on_edges = np.histogram(on_arr, bins)\n",
    "    off_frq, off_edges = np.histogram(off_arr, bins)\n",
    "    ttl_frq = np.sum(on_frq) + np.sum(off_frq)\n",
    "    on_frq, off_frq = (on_frq / ttl_frq, off_frq / ttl_frq)\n",
    "\n",
    "    axes[row_idx, column_idx].bar(\n",
    "        off_edges[:-1], off_frq, width=np.diff(off_edges), align=\"edge\", color=\"black\"\n",
    "    )\n",
    "    axes[row_idx, column_idx].bar(\n",
    "        on_edges[:-1], on_frq, width=np.diff(on_edges), align=\"edge\", color=color\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "fig.text(0.5, -0.04, \"Trench Intensity\", ha=\"center\", size=26)\n",
    "fig.text(-0.01, 0.5, \"# Trenches\", va=\"center\", rotation=\"vertical\", size=26)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./2020-10-10_lDE11_figure_1.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barcode_to_FISH(barcodestr):\n",
    "    cycleorder = [\n",
    "        0,\n",
    "        1,\n",
    "        6,\n",
    "        7,\n",
    "        12,\n",
    "        13,\n",
    "        18,\n",
    "        19,\n",
    "        2,\n",
    "        3,\n",
    "        8,\n",
    "        9,\n",
    "        14,\n",
    "        15,\n",
    "        20,\n",
    "        21,\n",
    "        4,\n",
    "        5,\n",
    "        10,\n",
    "        11,\n",
    "        16,\n",
    "        17,\n",
    "        22,\n",
    "        23,\n",
    "    ]\n",
    "\n",
    "    barcode = [bool(int(item)) for item in list(barcodestr)]\n",
    "    FISH_barcode = np.array([barcode[i] for i in cycleorder])\n",
    "    FISH_barcode = \"\".join(FISH_barcode.astype(int).astype(str))\n",
    "\n",
    "    return FISH_barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lDE13_nanopore.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lDE13_nanopore = pd.read_csv(\"./lDE13_final_df.tsv\", delimiter=\"\\t\", index_col=0)\n",
    "lDE13_lookup = {}\n",
    "for _, row in lDE13_nanopore.iterrows():\n",
    "    lDE13_lookup[barcode_to_FISH(row[\"24bit_barcode\"])] = row\n",
    "lDE13_lookup_df = pd.DataFrame(lDE13_lookup).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_strs = np.apply_along_axis(\n",
    "    lambda x: \"\".join(x.astype(int).astype(str)), 0, assign_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_df = pd.DataFrame(high_signal_barcodes_series, columns=[\"Barcode Signal\"])\n",
    "barcode_df[\"Barcode\"] = assign_strs\n",
    "barcode_df = barcode_df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lDE13_lookup_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = []\n",
    "for _, row in barcode_df.iterrows():\n",
    "    try:\n",
    "        lDE_row = lDE13_lookup_df.loc[row[\"Barcode\"]]\n",
    "        entry = pd.concat([row, lDE_row])\n",
    "        mergeddf.append(entry)\n",
    "    except:\n",
    "        pass\n",
    "mergeddf = pd.DataFrame(mergeddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(barcode_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mergeddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called = len(mergeddf) / len(barcode_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf.to_csv(\"./trench_sgrna_map.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = pd.read_csv(\"./trench_sgrna_map.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/2020-11-23_mVenus_KO_library/mVenus_headpath\"\n",
    "\n",
    "region_props = pd.read_pickle(headpath + \"/analysis.pkl\").loc[\n",
    "    (slice(None), slice(None), list(range(2, 5)), slice(None))\n",
    "]\n",
    "region_props = region_props.reset_index()\n",
    "region_props = region_props.set_index(\n",
    "    [\"trenchid\", \"timepoints\", \"Intensity Channel\", \"Objectid\"], drop=True\n",
    ")\n",
    "region_props = region_props.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_first_tpt = kymograph_metadata.loc[(slice(None), 0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trenchid_map(kymodf1, kymodf2):\n",
    "    trenchid_map = {}\n",
    "    for fov in kymo_first_tpt[\"fov\"].unique().tolist():\n",
    "        df1_chunk = kymodf1[kymodf1[\"fov\"] == fov]\n",
    "        df2_chunk = kymodf2[kymodf2[\"fov\"] == fov]\n",
    "\n",
    "        df1_xy = df1_chunk[[\"y (local)\", \"x (local)\"]].values\n",
    "        df2_xy = df2_chunk[[\"y (local)\", \"x (local)\"]].values\n",
    "\n",
    "        ymat = np.subtract.outer(df1_xy[:, 0], df2_xy[:, 0])\n",
    "        xmat = np.subtract.outer(df1_xy[:, 1], df2_xy[:, 1])\n",
    "        distmat = (ymat**2 + xmat**2) ** (1 / 2)\n",
    "        mapping = np.argmin(distmat, axis=1)\n",
    "\n",
    "        df1_trenchids = df1_chunk.index.get_level_values(\"trenchid\").tolist()\n",
    "        df2_trenchids = df2_chunk.index.get_level_values(\"trenchid\").tolist()\n",
    "\n",
    "        trenchid_map.update(\n",
    "            {\n",
    "                trenchid: df2_trenchids[mapping[i]]\n",
    "                for i, trenchid in enumerate(df1_trenchids)\n",
    "            }\n",
    "        )\n",
    "    return trenchid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchid_map = get_trenchid_map(kymo_first_tpt, region_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mchy_df[\"mean_intensity\"], range=(0, 3000), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mchy_df = region_props.loc[(slice(None), slice(None), [\"mCherry\"]), :].droplevel(\n",
    "    \"Intensity Channel\"\n",
    ")\n",
    "yfp_df = region_props.loc[(slice(None), slice(None), [\"YFP\"]), :].droplevel(\n",
    "    \"Intensity Channel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "mean_mchy = mchy_df.groupby(\"trenchid\").mean()\n",
    "mean_yfp = yfp_df.groupby(\"trenchid\").mean()\n",
    "\n",
    "output_df = copy.deepcopy(mergeddf)\n",
    "output_df[\"mean_mchy\"] = mergeddf.apply(\n",
    "    lambda x: mean_mchy.loc[trenchid_map[x[\"trenchid\"]]][\"mean_intensity\"], axis=1\n",
    ")\n",
    "output_df[\"mean_yfp\"] = mergeddf.apply(\n",
    "    lambda x: mean_yfp.loc[trenchid_map[x[\"trenchid\"]]][\"mean_intensity\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"./final_output_df.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "output_df = pd.read_csv(\"./final_output_df.tsv\", delimiter=\"\\t\")\n",
    "lDE13_nanopore = pd.read_csv(\"./lDE13_final_df.tsv\", delimiter=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_df[\"barcodeid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_df[\"sgrnaid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetid = 41\n",
    "targetid = 53\n",
    "example_target = output_df[output_df[\"targetid\"] == targetid]\n",
    "nanopore_ex = (\n",
    "    lDE13_nanopore[lDE13_nanopore[\"targetid\"] == targetid]\n",
    "    .groupby(\"sgrnaid\")\n",
    "    .apply(lambda x: x[0:1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_target[example_target[\"num_mismatch\"] == 0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "g = sns.swarmplot(\n",
    "    x=20 - example_target[\"num_mismatch\"],\n",
    "    y=example_target[\"mean_yfp\"],\n",
    "    palette=\"cividis_r\",\n",
    "    size=6,\n",
    ")\n",
    "plt.xlabel(\"Number Matched\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylim(0, 14000)\n",
    "plt.ylabel(\"Mean mVenus Intensity\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(\"2020-12-05_DAC_fig_5A.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.swarmplot(\n",
    "    x=example_target[\"sgrnaid\"].max() - example_target[\"sgrnaid\"],\n",
    "    y=example_target[\"mean_yfp\"],\n",
    "    palette=\"cividis_r\",\n",
    ")\n",
    "plt.xlabel(\"sgRNAid\", fontsize=30)\n",
    "# plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Mean YFP Intensity\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "# plt.ylim(0,1)\n",
    "for i in range(3, 27, 6):\n",
    "    g.axvspan(i - 0.5, i + 2.5, color=\"C0\", alpha=0.1, lw=0)\n",
    "g.axvspan(27 - 0.5, 27 + 1.5, color=\"C0\", alpha=0.1, lw=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.swarmplot(\n",
    "    x=20 - example_target[\"num_mismatch\"],\n",
    "    y=example_target[\"mean_yfp\"],\n",
    "    palette=\"cividis_r\",\n",
    ")\n",
    "plt.xlabel(\"Number Matched\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Mean YFP Intensity\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "# plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mchy_bkd = 150.0\n",
    "max_percentile = np.percentile(ratio_df, 99)\n",
    "\n",
    "ratio_df = (yfp_df[\"mean_intensity\"] - yfp_df[\"mean_intensity\"].min()) / (\n",
    "    mchy_df[\"mean_intensity\"] - mchy_bkd\n",
    ")\n",
    "ratio_df = ratio_df.groupby(\"trenchid\").median()\n",
    "\n",
    "# low_val = np.percentile(ratio_df,1.)\n",
    "# high_val = np.percentile(ratio_df,95.)\n",
    "# ratio_df = (ratio_df-low_val)/(high_val-low_val)\n",
    "# ratio_df = pd.DataFrame(ratio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ratio_df, bins=100, range=(0.0, 8.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ratio_df[\"mean_intensity\"], bins=100, range=(0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "output_df = copy.deepcopy(mergeddf)\n",
    "output_df[\"mean_intensity\"] = mergeddf.apply(\n",
    "    lambda x: ratio_df.loc[trenchid_map[x[\"trenchid\"]]][\"mean_intensity\"].values, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"./final_output_df.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_csv(\"./final_output_df.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "sgrnaids, ncount = np.unique(output_df[\"sgrnaid\"], return_counts=True)\n",
    "well_sampled_sgrnaids = sgrnaids[ncount > 10]\n",
    "\n",
    "output_df = output_df[output_df[\"sgrnaid\"].isin(well_sampled_sgrnaids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnaid_groupby = output_df.groupby(\"sgrnaid\")\n",
    "yfp = sgrnaid_groupby.apply(lambda x: (x[\"mean_yfp\"] / x[\"mean_mchy\"]))\n",
    "mean_yfp = yfp.groupby(\"sgrnaid\").apply(lambda x: np.mean(x))\n",
    "mean_yfp_argsort = np.argsort(mean_yfp).values[::-1].tolist()\n",
    "mean_yfp_argsort = mean_yfp.index.values[mean_yfp_argsort]\n",
    "yfp_sorted = yfp.loc[mean_yfp_argsort, :].to_frame()\n",
    "yfp_sorted.columns = [\"YFP Ratio\"]\n",
    "yfp_sorted = yfp_sorted.droplevel(1)\n",
    "yfp_sorted = yfp_sorted.reset_index()\n",
    "sgrna_to_index = dict(\n",
    "    zip(yfp_sorted[\"sgrnaid\"].unique(), range(len(yfp_sorted[\"sgrnaid\"].unique())))\n",
    ")\n",
    "yfp_sorted[\"Order\"] = yfp_sorted[\"sgrnaid\"].apply(lambda x: sgrna_to_index[x])\n",
    "\n",
    "yfp_sorted = yfp_sorted[yfp_sorted[\"Order\"] > 20]\n",
    "\n",
    "min_yfp = np.min(yfp_sorted.groupby(\"Order\")[\"YFP Ratio\"].mean())\n",
    "max_yfp = np.max(yfp_sorted.groupby(\"Order\")[\"YFP Ratio\"].mean())\n",
    "yfp_sorted[\"Relative YFP Ratio\"] = (yfp_sorted[\"YFP Ratio\"] - min_yfp) / (\n",
    "    max_yfp - min_yfp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "g = sns.lineplot(\n",
    "    data=yfp_sorted, x=\"Order\", y=\"Relative YFP Ratio\", n_boot=100, linewidth=5.0\n",
    ")\n",
    "# g.set_yscale(\"log\")\n",
    "# g.set_yticks([0.01,0.1,1.])\n",
    "plt.xlabel(\"Rank\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Mean Normalized Intensity\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2021-03-31_Qbio_fig_1.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_target = output_df[output_df[\"targetid\"] == 53]\n",
    "# example_target = output_df[output_df[\"targetid\"]==54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_target.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[\"num_mismatch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "for _, row in example_target.iterrows():\n",
    "    for item in row[\"mean_intensity\"]:\n",
    "        output_data.append([row[\"num_mismatch\"], item])\n",
    "output_data = np.array(output_data)[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data[::100, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mismatch in range(0, 11, 3):\n",
    "    mismatch_mask = output_data[:, 0] == mismatch\n",
    "    masked_output = output_data[mismatch_mask]\n",
    "    g = sns.distplot(\n",
    "        masked_output[:, 1], kde=True, norm_hist=False, hist=False, bins=100\n",
    "    )\n",
    "#     g.set_xscale(\"log\")\n",
    "#     g.set_xticks([-2,-1,0.])\n",
    "# plt.xlabel(\"Number Mismatch\",fontsize=30)\n",
    "# plt.xticks(fontsize=30)\n",
    "# plt.xlim(-1.8,0.5)\n",
    "# plt.ylabel(\"Mean Insensity Ratio\",fontsize=30)\n",
    "# plt.yticks(fontsize=30)\n",
    "# plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "# g = sns.swarmplot(x=20-example_target[\"num_mismatch\"], y=example_target[\"mean_intensity\"], palette=\"cividis\");\n",
    "# g.set_yscale(\"log\")\n",
    "# g.set_yticks([0.01,0.1,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(example_target[\"num_mismatch\"],example_target[\"mean_intensity\"])\n",
    "sns.swarmplot(\n",
    "    x=\"num_mismatch\", y=\"mean_intensity\", data=example_target, palette=\"cividis\"\n",
    ")\n",
    "plt.xlabel(\"Number Mismatch\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Mean Insensity Ratio\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.swarmplot(\n",
    "    x=20 - example_target[\"num_mismatch\"],\n",
    "    y=example_target[\"mean_intensity\"],\n",
    "    palette=\"cividis\",\n",
    ")\n",
    "g.set_yscale(\"log\")\n",
    "g.set_yticks([0.01, 0.1, 1.0])\n",
    "plt.xlabel(\"Number Mismatch\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Mean Insensity Ratio\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.groupby(\"targetid\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_list = [992, 1001, 1014, 1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sgrna in sgrna_list:\n",
    "    sns.distplot(\n",
    "        example_target[example_target[\"sgrnaid\"] == sgrna][\"mean_intensity\"],\n",
    "        norm_hist=True,\n",
    "        hist=False,\n",
    "    )\n",
    "plt.xlabel(\"Number Mismatch\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Mean Insensity Ratio\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "# plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_df = example_target.groupby(\"sgrnaid\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_list = [992, 994, 1001, 1014, 1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_target.groupby(\"sgrnaid\").apply(lambda x: np.min(x[\"num_mismatch\"]))[\n",
    "    nunique_df[\"trenchid\"] > 10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_df[\"trenchid\"] > 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(example_target[\"sgrnaid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_cds_mask = (\n",
    "    (output_df[\"target_strand\"] == 1)\n",
    "    & (output_df[\"category\"] == \"Target\")\n",
    "    & (output_df[\"end\"] <= 808475)\n",
    "    & (output_df[\"end\"] >= 807758)\n",
    ")\n",
    "\n",
    "promoter_mask = (output_df[\"category\"] == \"Target\") & (output_df[\"end\"] > 808475)\n",
    "\n",
    "cds_antisense_mask = (\n",
    "    (output_df[\"target_strand\"] == -1)\n",
    "    & (output_df[\"category\"] == \"Target\")\n",
    "    & (output_df[\"end\"] <= 808475)\n",
    "    & (output_df[\"end\"] >= 807758)\n",
    ")\n",
    "\n",
    "dummy_mask = output_df[\"category\"] == \"Dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_cds_df = output_df[on_cds_mask]\n",
    "promoter_df = output_df[promoter_mask]\n",
    "antisense_df = output_df[cds_antisense_mask]\n",
    "dummy_df = output_df[dummy_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(on_cds_df[\"targetid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for targetid in on_cds_df[\"targetid\"].unique():\n",
    "    subdf = on_cds_df[on_cds_df[\"targetid\"] == targetid]\n",
    "    sns.lineplot(subdf[\"num_mismatch\"], subdf[\"mean_yfp\"], ci=None, color=\"C0\")\n",
    "for targetid in promoter_df[\"targetid\"].unique():\n",
    "    subdf = promoter_df[promoter_df[\"targetid\"] == targetid]\n",
    "    sns.lineplot(subdf[\"num_mismatch\"], subdf[\"mean_yfp\"], ci=None, color=\"C1\")\n",
    "for targetid in antisense_df[\"targetid\"].unique():\n",
    "    subdf = antisense_df[antisense_df[\"targetid\"] == targetid]\n",
    "    sns.lineplot(subdf[\"num_mismatch\"], subdf[\"mean_yfp\"], ci=None, color=\"grey\")\n",
    "# plt.ylim(0.,1.)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "g = sns.swarmplot(\n",
    "    x=20 - example_target[\"num_mismatch\"],\n",
    "    y=example_target[\"mean_yfp\"],\n",
    "    palette=\"cividis_r\",\n",
    "    size=6,\n",
    ")\n",
    "plt.xlabel(\"Number Matched\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylim(0, 14000)\n",
    "plt.ylabel(\"Mean YFP Intensity\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(\"2020-12-05_DAC_fig_5A.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 - subdf[\"num_mismatch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "for targetid in on_cds_df[\"targetid\"].unique():\n",
    "    subdf = on_cds_df[on_cds_df[\"targetid\"] == targetid]\n",
    "    #     plt.plot(20-subdf[\"num_mismatch\"],subdf[\"mean_yfp\"],color=\"C0\")\n",
    "    sns.lineplot(20 - subdf[\"num_mismatch\"], subdf[\"mean_yfp\"], ci=None, color=\"C0\")\n",
    "plt.xlabel(\"Number Matched\", fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylim(0, 14000)\n",
    "plt.ylabel(\"Mean mVenus Intensity\", fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(\"2020-12-05_DAC_fig_5B.png\", dpi=150)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(on_cds_df[\"mean_intensity\"], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dummy_df[\"mean_intensity\"], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
