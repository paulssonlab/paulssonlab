import pandas as pd
from functools import lru_cache
import shlex
from paulssonlab.shenker.sequencing.common import (
    get_workdir,
    REMOTE_HOST,
    REMOTE_BASE_DIR,
    get_registry,
    get_registry_seq,
)

REMOTE_DATA_DIR = f"{REMOTE_BASE_DIR}/!!Jacob Quinn Shenker/200806/200806_arkin_bistable_sequencing"

@lru_cache
def get_samples():
    samples = pd.read_csv("data/samples.tsv", sep="\t", names=["sample", "references"], index_col=False)
    samples = {row.sample.replace(".fastq", ""): row.references.split(",") for row in samples.itertuples()}
    return samples

workdir: get_workdir(make_links=True)

localrules: all, download_samples, download_reads, have_samples, get_references, any2fasta, merge_fasta, finish

rule all:
    input:
        "output/.done"

rule download_samples:
    output:
        "data/samples.tsv",
    shell:
        f"scp \"{REMOTE_HOST}:{shlex.quote(REMOTE_DATA_DIR)}/samples.tsv\" {{output:q}}"

rule download_reads:
    output:
        "data/{sample}.fastq",
    shell:
        f"scp \"{REMOTE_HOST}:{shlex.quote(REMOTE_DATA_DIR)}/{{wildcards.sample:q}}.fastq\" {{output:q}}"

checkpoint have_samples:
    input:
        "data/samples.tsv"
    output:
        touch("output/.have_samples")

rule get_references:
    output:
        "references/{reference}.gb"
    run:
        import time
        for retry in range(5):
            try:
                get_registry_seq(get_registry(), wildcards.reference, output[0])
            except:
                time.sleep(0.3)
            else:
                break

rule any2fasta:
    input:
        "references/{reference}.gb"
    output:
        temp("references/{reference}.fasta")
    conda:
        "envs/any2fasta.yml"
    shell:
        "any2fasta -q {input:q} | seqkit replace -p '(.*)' -r '{wildcards.reference}' > {output:q}"

def wait_merge_fasta(wildcards):
    checkpoints.have_samples.get(**wildcards)
    samples = get_samples()
    files = expand("references/{reference}.fasta", reference=samples[wildcards.sample])
    return files

rule merge_fasta:
    input:
        wait_merge_fasta
    output:
        "output/{sample}/reference.fasta"
    wildcard_constraints:
        input=".+"
    shell:
        "cat {input:q} > {output:q}"

rule bowtie2_build:
    input:
        reference="output/{sample}/reference.fasta"
    output:
        multiext(
            "output/{sample}/index",
            ".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2",
        )
    log:
        "logs/bowtie2_build/{sample}.log"
    conda:
        "envs/mapping.yml"
    params:
        indexbase=lambda wildcards, output: output[0].replace(".1.bt2", ""),
        extra=""
    group: "mapping"
    threads: 8
    shell:
        "bowtie2-build --threads {threads} {params.extra} {input.reference:q} {params.indexbase:q} &> {log:q}"

rule bowtie2_interleaved:
    input:
        reads="data/{sample}.fastq",
        index=multiext(
            "output/{sample}/index",
            ".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2",
        )
    output:
        # TODO: marking this temp causes this rule to be re-run every time
        # temp("output/{sample}/alignments.bam")
        "output/{sample}/alignments.bam"
    log:
        "logs/bowtie2/{sample}.log"
    conda:
        "envs/mapping.yml"
    params:
        indexbase=lambda wildcards, input: input.index[0].replace(".1.bt2", ""),
        extra="--end-to-end"
    group: "mapping"
    threads: 8
    shell:
        "(bowtie2 --threads {threads} {params.extra} -x {params.indexbase:q} --interleaved {input.reads:q} "
        "| samtools view -Sbh -o {output:q} -) 2> {log:q}"

rule samtools_sort:
    input:
        "output/{sample}/alignments.bam"
    output:
        "output/{sample}/alignments.sorted.bam"
    conda:
        "envs/mapping.yml"
    group: "mapping"
    shell:
        "samtools sort -O bam {input:q} -o {output:q}"

rule samtools_index:
    input:
        "output/{sample}/alignments.sorted.bam"
    output:
        "output/{sample}/alignments.sorted.bam.bai"
    conda:
        "envs/mapping.yml"
    group: "mapping"
    shell:
        "samtools index {input:q} {output:q}"

rule call_variants:
    input:
        alignments="output/{sample}/alignments.sorted.bam",
        reference="output/{sample}/reference.fasta"
    output:
        "output/{sample}/calls.bcf"
    log:
        "logs/call_variants/{sample}.log"
    conda:
        "envs/mapping.yml"
    params:
        mpileup_args="--max-depth 2000 --max-idepth 2000",
        call_args="--ploidy 1"
    group: "consensus"
    shell:
        "(bcftools mpileup -Ou {params.mpileup_args} -f {input.reference:q} {input.alignments:q}"
        " | bcftools call -mv -Ob {params.call_args} -o {output:q}) 2> {log:q}"

rule filter_variants:
    input:
        "output/{sample}/calls.bcf"
    output:
        "output/{sample}/calls.filtered.bcf"
    log:
        "logs/filter_variants/{sample}.log"
    conda:
        "envs/mapping.yml"
    params:
        args="-i'%QUAL>20'"
    group: "consensus"
    shell:
        "bcftools filter {params.args} -Ob {input:q} -o {output:q} 2> {log:q}"

rule index_variants:
    input:
        "output/{sample}/calls.filtered.bcf"
    output:
        "output/{sample}/calls.filtered.bcf.csi"
    conda:
        "envs/mapping.yml"
    group: "consensus"
    shell:
        "bcftools index {input:q}"

rule get_consensus:
    input:
        calls="output/{sample}/calls.filtered.bcf",
        index="output/{sample}/calls.filtered.bcf.csi",
        reference="output/{sample}/reference.fasta"
    output:
        "output/{sample}/consensus.fasta"
    log:
        "logs/get_consensus/{sample}.log"
    conda:
        "envs/mapping.yml"
    group: "consensus"
    shell:
        "cat {input.reference:q} | bcftools consensus {input.calls:q} -o {output:q} 2> {log:q}"

rule extract_consensus:
    input:
        "output/{sample}/consensus.fasta"
    output:
        directory("output/{sample}/consensus.fasta.split")
    log:
        "logs/extract_consensus/{sample}.log"
    conda:
        "envs/mapping.yml"
    params:
        outdir=lambda wildcards, input: f"{input}.split"
    group: "consensus"
    shell:
        "(cat {input:q} | seqkit replace -p '^(\\S+).*' -r '{wildcards.sample}_$1'"
        " | seqkit split - --by-id -f -O {params.outdir:q}) 2> {log:q}"

def wait_finish(wildcards):
    checkpoints.have_samples.get(**wildcards)
    samples = get_samples()
    return expand("output/{sample}/consensus.fasta.split", sample=samples.keys())

rule finish:
    input:
        wait_finish
    output:
        touch("output/.done")
