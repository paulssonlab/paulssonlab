{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains the entire `TrenchRipper` pipline, divided into simple steps. This pipline is ideal for Mother <br>Machine image data where cells possess fluorescent segmentation markers. Segmentation on phase or brightfield data <br>is being developed, but is still an experimental feature.\n",
    "\n",
    "The steps in this pipeline are as follows:\n",
    "1. Extracting your Mother Machine data (.nd2) into hdf5 format\n",
    "2. Identifying and cropping individual trenches into kymographs\n",
    "3. Segmenting cells with a fluorescent marker\n",
    "4. Determining lineages and object properties\n",
    "\n",
    "In each step, the user will dynamically specify parameters using a series of interactive diagnostics on their dataset. <br>Following this, a parameter file will be written to disk and then used to deploy a parallel computation on the <br>dataset, either locally or on a SLURM cluster.\n",
    "\n",
    "\n",
    "This is intended as an end-to-end solution to analyzing Mother Machine data. As such, **it is not trivial to plug data <br>directly into intermediate steps**, as it will lack the correct formatting and associated metadata. A notable <br>exception to this is using another program to segment data. The library references binary segmentation masks using <br>only metadata derived from their associated kymographs. As such, it is possible to generate segmentations on these <br>kymographs elsewhere and place them into the segmentation data path to have `TrenchRipper` act on those <br>segmentations instead. More on this in the segmentation section..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Run this section to import all relavent packages and libraries used in this notebook. You must run this everytime you open a new python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Growth/Division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/\"\n",
    "# hdf5inputpath = \"/home/de64/scratch/de64/sync_folder/2021-01-28_lDE14/run/\"\n",
    "nd2file = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Experiment001.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_extractor = tr.marlin_extractor(hdf5inputpath, headpath, metaparsestr='metadata_{timepoint:d}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.ndextract.hdf5_fov_extractor(\n",
    "    nd2file,\n",
    "    headpath,\n",
    "    tpts_per_file=50,\n",
    "    ignore_fovmetadata=False,\n",
    "    nd2reader_override={\"z_levels\": [], \"z_coordinates\": []},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_extractor = tr.ndextract.tiff_extractor(\n",
    "#     tiffpath,\n",
    "#     headpath,\n",
    "#     [\"Phase\",\"YFP\"],tpts_per_file=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/metadata.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle.read_df(\"global\", read_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "[ '/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002',\n",
    " '/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/ezrdm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/mbm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/Sb7_L35',\n",
    " '/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9',\n",
    " '/n/scratch3/users/d/de64/Vibrio_2_1_TOP',\n",
    " '/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms',\n",
    " '/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X',\n",
    " '/n/scratch3/users/d/de64/Main_Experiment',\n",
    " '/n/scratch3/users/d/de64/bde17_gotime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath, persist_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y Percentile 95\n",
    "Y Foreground Percentile 65\n",
    "Y Smoothing Kernel 29\n",
    "Y Percentile Threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_consensus_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimum Trench Length 50\n",
    "Midpoint Distance Tolerance 50\n",
    "Y Padding 20\n",
    "Trench Length 120\n",
    "Orientation Detection Method 0\n",
    "Expected Number of Rows (Manual Orientation Detection) 8\n",
    "Alternate Orientation True\n",
    "Alternate Orientation Over Rows? False\n",
    "Use Median Drift? False\n",
    "Consensus Orientations [1, 0, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X Percentile 90\n",
    "X Background Kernel 51\n",
    "X Smoothing Kernel 5\n",
    "Otsu Threshold Scaling 0.25\n",
    "Minimum X Threshold 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=50, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)  ##HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Fluorescence Segmentation\n",
    "\n",
    "Now that you have copped your data into kymographs, we will now perform segmentation/cell detection <br>\n",
    "on your kymographs. Currently, this pipeline only supports segmentation of fluorescence images; however, <br>\n",
    "segmentation of transmitted light imaging techniques is in development.\n",
    "\n",
    "The output of this step will be a set of `segmentation_[File #].hdf5` files stored in `headpath/fluorsegmentation`.<br>\n",
    "The image data stored in these files takes the exact same form as the kymograph data, `(K,T,Y,X)` arrays <br>\n",
    "where K is the trench index, T is time, and Y,X are the crop dimensions. These arrays are accessible using <br>\n",
    "keys of the form `\"[Trench Row Number]\"`.\n",
    "\n",
    "Since no metadata is generated by this step, it is possible to use another segmentation algorithm on the kymograph <br>\n",
    "data. The output of segmentation must be split into `segmentation_[File #].hdf5` files, where `[File #]` agrees with the<br>\n",
    "corresponding `kymograph_[File #].hdf5` file. Additionally, the `(K,T,Y,X)` arrays must be of the same shape as the <br>\n",
    "kymograph arrays and accessible at the corresponding `\"[Trench Row Number]\"` key. These files must be placed into <br>\n",
    "their own folder at `headpath/foldername`. This folder may then be used in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "##### Initialize the interactive segmentation class\n",
    "\n",
    "As a first step, initialize the `tr.fluo_segmentation_interactive` class that will be handling all steps of generating a segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation = tr.fluo_segmentation_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "##### Choose channel to segment on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.choose_seg_channel_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Import data\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**fov_idx (int)** :\n",
    "\n",
    "**n_trenches (int)** :\n",
    "\n",
    "**t_range (tuple)** :\n",
    "\n",
    "**t_subsample_step (int)** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.import_array_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_processed_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Determine Cell Mask Envelope\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**cell_mask_method (str)** : Thresholding method, can be a local or global Otsu threshold.\n",
    "\n",
    "**cell_otsu_scaling (float)** : Scaling factor applied to determined threshold.\n",
    "\n",
    "**local_otsu_r (int)** : Radius of thresholding kernel used in the local otsu thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_cell_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_eig_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_dist_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_marker_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Generate Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    n_workers=400,\n",
    "    memory=\"3GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segment = tr.segment.fluo_segmentation_cluster(headpath, paramfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segment.dask_segment(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Stop Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## Region Properties (No Lineage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE\n",
    "analyzer = tr.analysis.regionprops_extractor(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    intensity_channel_list=[\"mCherry\"],\n",
    "    include_background=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.export_all_data(n_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "## Lineage Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = tr.tracking.scorefn(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    u_size=0.22,\n",
    "    sig_size=0.08,\n",
    "    u_pos=0.21,\n",
    "    sig_pos=0.1,\n",
    "    w_merge=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function.interactive_scorefn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver = tr.tracking.tracking_solver(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    ScoreFn=score_function,\n",
    "    edge_limit=2,\n",
    ")\n",
    "data, orientation = score_function.output.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver.interactive_tracking(data, orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tracking_Solver.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "### Generate Lineage Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=200,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver = tr.tracking.tracking_solver(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    paramfile=True,\n",
    "    volume_estimation=True,\n",
    "    props_list=[\"area\", \"major_axis_length\", \"minor_axis_length\"],\n",
    "    props_to_unpack={},\n",
    "    pixel_scaling_factors={\"area\": 2, \"major_axis_length\": 1, \"minor_axis_length\": 1},\n",
    "    intensity_props_list=[\"mean_intensity\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "file_idx = 0\n",
    "\n",
    "kymo_meta = dd.read_parquet(headpath + \"/kymograph/metadata\")\n",
    "kymo_meta = kymo_meta.set_index(\"File Parquet Index\", sorted=True)\n",
    "n_partitions, divisions = (kymo_meta.npartitions, kymo_meta.divisions)\n",
    "# writedir(kymo_meta.lineagepath,overwrite=False)\n",
    "\n",
    "kymo_df = dd.read_parquet(headpath + \"/kymograph/metadata\")\n",
    "kymo_df = kymo_df.reset_index(drop=False).set_index(\n",
    "    \"File Parquet Index\", sorted=True, npartitions=n_partitions, divisions=divisions\n",
    ")\n",
    "\n",
    "start_idx = int(str(file_idx) + \"00000000\")\n",
    "end_idx = int(str(file_idx) + \"99999999\")\n",
    "\n",
    "kymo_df = kymo_df.loc[start_idx:end_idx].compute(scheduler=\"threads\")\n",
    "trench_idx_list = kymo_df[\"File Trench Index\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_out = Tracking_Solver.lineage_trace(kymo_df, file_idx, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_idx = 0\n",
    "\n",
    "kymo_df = dd.read_parquet(Tracking_Solver.headpath + \"/kymograph/metadata\")\n",
    "kymo_df = kymo_df.reset_index(drop=False).set_index(\n",
    "    \"File Parquet Index\", sorted=True, npartitions=n_partitions, divisions=divisions\n",
    ")\n",
    "\n",
    "start_idx = int(str(file_idx) + \"00000000\")\n",
    "end_idx = int(str(file_idx) + \"99999999\")\n",
    "\n",
    "kymo_df = kymo_df.loc[start_idx:end_idx].compute(scheduler=\"threads\")\n",
    "trench_idx_list = kymo_df[\"File Trench Index\"].unique().tolist()\n",
    "\n",
    "mergeddf = []\n",
    "for file_trench_idx in trench_idx_list:\n",
    "    try:\n",
    "        df_out = Tracking_Solver.lineage_trace(kymo_df, file_idx, file_trench_idx)\n",
    "        mergeddf.append(df_out)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if file_trench_idx > 3:\n",
    "        break\n",
    "mergeddf = pd.concat(mergeddf).reset_index()\n",
    "parq_file_idx = mergeddf.apply(\n",
    "    lambda x: int(\n",
    "        f'{int(x[\"File Index\"]):08}{int(x[\"File Trench Index\"]):04}{int(x[\"timepoints\"]):04}'\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "parq_file_idx.index = mergeddf.index\n",
    "mergeddf[\"File Parquet Index\"] = parq_file_idx\n",
    "del mergeddf[\"File Index\"]\n",
    "del mergeddf[\"File Trench Index\"]\n",
    "del mergeddf[\"timepoints\"]\n",
    "del mergeddf[\"index\"]\n",
    "\n",
    "# kymo_df = kymo_df.join(mergeddf.set_index(\"File Parquet Index\")).dropna()\n",
    "\n",
    "# kymo_df[\"Mother CellID\"] = kymo_df[\"Mother CellID\"].astype(int)\n",
    "# kymo_df[\"Daughter CellID 1\"] = kymo_df[\"Daughter CellID 1\"].astype(int)\n",
    "# kymo_df[\"Daughter CellID 2\"] = kymo_df[\"Daughter CellID 2\"].astype(int)\n",
    "# kymo_df[\"Sister CellID\"] = kymo_df[\"Sister CellID\"].astype(int)\n",
    "# kymo_df[\"CellID\"] = kymo_df[\"CellID\"].astype(int)\n",
    "# kymo_df[\"Global CellID\"] = kymo_df[\"Global CellID\"].astype(int)\n",
    "\n",
    "# #remove old indices\n",
    "# del kymo_df[\"FOV Parquet Index\"]\n",
    "\n",
    "# parq_file_idx = kymo_df.apply(lambda x: int(f'{int(x[\"File Index\"]):08}{int(x[\"File Trench Index\"]):04}{int(x[\"timepoints\"]):04}{int(x[\"CellID\"]):04}'), axis=1)\n",
    "# parq_fov_idx = kymo_df.apply(lambda x: int(f'{int(x[\"fov\"]):04}{int(x[\"row\"]):04}{int(x[\"trench\"]):04}{int(x[\"timepoints\"]):04}{int(x[\"CellID\"]):04}'), axis=1)\n",
    "\n",
    "# kymo_df[\"File Parquet Index\"] = parq_file_idx\n",
    "# kymo_df[\"FOV Parquet Index\"] = parq_fov_idx\n",
    "\n",
    "# kymo_df = kymo_df.set_index(\"File Parquet Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mergeddf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = Tracking_Solver.lineage_trace_file(0, n_partitions, divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = test_df[test_df[\"Global CellID\"].isin([30005, 30006, 30007])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = df_out.loc[100].loc[0].loc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(\n",
    "    ((init[\"Surface Area\"] / init[\"mCherry mean_intensity\"]) * 1600).sample(50), bins=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tracking_Solver.lineage_trace_file(100,n_partitions,divisions)\n",
    "# df_out = self.lineage_trace(kymo_df,file_idx,file_trench_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver.compute_all_lineages(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2: Barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/\"\n",
    ")\n",
    "hdf5inputpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/run/\"\n",
    ")\n",
    "# nd2file = \"/home/de64/scratch/de64/sync_folder/2021-05-27_lDE18_20x_run_1/run_100ms_mchry100_yfp50.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.marlin_extractor(\n",
    "    hdf5inputpath,\n",
    "    headpath,\n",
    "    pixel_microns=0.2125,\n",
    "    metaparsestr=\"metadata_{timepoint:d}.hdf5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "[ '/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002',\n",
    " '/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/ezrdm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/mbm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/Sb7_L35',\n",
    " '/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9',\n",
    " '/n/scratch3/users/d/de64/Vibrio_2_1_TOP',\n",
    " '/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms',\n",
    " '/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X',\n",
    " '/n/scratch3/users/d/de64/Main_Experiment',\n",
    " '/n/scratch3/users/d/de64/bde17_gotime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath, persist_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on w
hich you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_consensus_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimum Trench Length 80\n",
    "Midpoint Distance Tolerance 50\n",
    "Y Padding 20\n",
    "Trench Length 120\n",
    "Orientation Detection Method 1\n",
    "Expected Number of Rows (Manual Orientation Detection) 10\n",
    "Alternate Orientation True\n",
    "Alternate Orientation Over Rows? False\n",
    "Use Median Drift? False\n",
    "Consensus Orientations [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "X Percentile 98\n",
    "X Background Kernel 61\n",
    "X Smoothing Kernel 1\n",
    "Otsu Threshold Scaling 0.75\n",
    "Minimum X Threshold 3000\n",
    "Trench Width 10\n",
    "Trench Presence Threshold 1.0\n",
    "All Channels ['BF', 'Cy5', 'Cy7', 'RFP']\n",
    "Filter Channel None\n",
    "Invert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=200, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "## FISH Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {},
   "source": [
    "#### Get Barcode Signal (Percentile Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tr.get_all_image_measurements(\n",
    "    dask_controller,\n",
    "    headpath,\n",
    "    headpath + \"/percentiles\",\n",
    "    [\"RFP\", \"Cy5\", \"Cy7\"],\n",
    "    \"98th Percentile\",\n",
    "    np.percentile,\n",
    "    98,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "#### Determine Barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test = tr.fish_analysis(\n",
    "    headpath,\n",
    "    \"./lDE20_final_df.tsv\",\n",
    "    \"./lDE20_final_df.json\",\n",
    "    hamming_thr=1,\n",
    "    channel_names=[\"RFP 98th Percentile\", \"Cy5 98th Percentile\", \"Cy7 98th Percentile\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.plot_signal_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.get_bit_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.bit_threshold_list = [\n",
    "    1100,\n",
    "    800,\n",
    "    1400,\n",
    "    1200,\n",
    "    1000,\n",
    "    1200,\n",
    "    1200,\n",
    "    1000,\n",
    "    1000,\n",
    "    1400,\n",
    "    7000,\n",
    "    6000,\n",
    "    12000,\n",
    "    4000,\n",
    "    6000,\n",
    "    2200,\n",
    "    4500,\n",
    "    5000,\n",
    "    4000,\n",
    "    5000,\n",
    "    500,\n",
    "    1000,\n",
    "    600,\n",
    "    600,\n",
    "    700,\n",
    "    600,\n",
    "    700,\n",
    "    700,\n",
    "    900,\n",
    "    1000,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.plot_bit_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.output_barcode_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 3: Combined Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"02:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=100,\n",
    "#     memory=\"8GB\",\n",
    "#     working_directory=headpath + \"/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/metadata.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1.0 - percent_called_w_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215",
   "metadata": {},
   "source": [
    "#### Import Regionprops Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/analysis\"\n",
    ")\n",
    "n_partitions, divisions = analysis_df.npartitions, analysis_df.divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pixel_microns = 0.2125\n",
    "\n",
    "trench_timepoint_df = (\n",
    "    analysis_df.reset_index(drop=False)\n",
    "    .set_index(\"Trenchid Timepoint Index\", sorted=True)\n",
    "    .persist()\n",
    ")\n",
    "trench_timepoint_groupby = trench_timepoint_df.groupby(\"Trenchid Timepoint Index\")\n",
    "mchy_intensity_bkd = trench_timepoint_groupby.apply(\n",
    "    lambda x: x.iloc[0][\"mean_intensity\"], meta=float\n",
    ")\n",
    "trench_timepoint_df[\"background_intensity\"] = mchy_intensity_bkd\n",
    "output_df = trench_timepoint_df.reset_index(drop=False).set_index(\n",
    "    \"File Parquet Index\", sorted=True, npartitions=n_partitions, divisions=divisions\n",
    ")\n",
    "del trench_timepoint_df\n",
    "output_df[\"mean_intensity_wo_bkd\"] = (\n",
    "    output_df[\"mean_intensity\"] - output_df[\"background_intensity\"]\n",
    ")\n",
    "output_df = output_df[output_df[\"Objectid\"] != 0]\n",
    "output_df = output_df.reset_index(drop=False).set_index(\n",
    "    \"Trenchid Timepoint Index\", sorted=True\n",
    ")\n",
    "\n",
    "### Scale by pixel microns\n",
    "output_df[\"area\"] = output_df[\"area\"] * (pixel_microns**2)\n",
    "output_df[\"centroid_y\"] = output_df[\"centroid_y\"] * (pixel_microns)\n",
    "output_df[\"centroid_x\"] = output_df[\"centroid_x\"] * (pixel_microns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trench_timepoint_groupby = output_df.groupby(\"Trenchid Timepoint Index\")\n",
    "# size_series = trreset_indexch_timepoint_groupby.apply(lambda x:x[\"area\"].tolist(), meta=list).to_frame()\n",
    "# size_series = output_df[[\"trenchid\"]].join(size_series)\n",
    "# size_series = size_series.rename(columns = {0:\"Size List\"})\n",
    "# size_series = size_series.groupby(\"trenchid\").apply(lambda x:x[\"Size List\"].tolist(), meta=list).to_frame().repartition(partition_size='25MB')\n",
    "\n",
    "# intensity_series = trench_timepoint_groupby.apply(lambda x:x[\"mean_intensity\"].tolist(), meta=list).to_frame()\n",
    "# intensity_series = output_df[[\"trenchid\"]].join(intensity_series)\n",
    "# intensity_series = intensity_series.rename(columns = {0:\"mCherry Intensity List\"})\n",
    "# intensity_series = intensity_series.groupby(\"trenchid\").apply(lambda x:x[\"mCherry Intensity List\"].tolist(), meta=list).to_frame().repartition(partition_size='25MB')\n",
    "\n",
    "# scalar_dict = {\"Size List\":size_series,\"mCherry Intensity List\":intensity_series}\n",
    "\n",
    "# del output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df_trenchid_idx = output_df.reset_index(drop=False).set_index(\n",
    "    \"trenchid\", sorted=True\n",
    ")\n",
    "\n",
    "init_df_idx = output_df_trenchid_idx.index.unique().compute().to_list()\n",
    "valid_barcode_df = barcode_df[\n",
    "    barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "].compute()\n",
    "barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "    lambda x: trenchid_map[x]\n",
    ")\n",
    "valid_init_df_indices = barcode_df_mapped_trenchids.isin(init_df_idx)\n",
    "barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "called_df = called_df.set_index(\"phenotype trenchid\")\n",
    "final_output_df = output_df_trenchid_idx.loc[called_df.index.compute().tolist()].join(\n",
    "    called_df\n",
    ")\n",
    "final_output_df[\"phenotype trenchid\"] = final_output_df.index\n",
    "final_output_df = final_output_df.reset_index(drop=True).set_index(\n",
    "    \"File Parquet Index\", sorted=True\n",
    ")\n",
    "\n",
    "del final_output_df[\"Barcode Signal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_parquet(\n",
    "    final_output_df,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Final_Analysis\",\n",
    "    engine=\"fastparquet\",\n",
    "    compression=\"gzip\",\n",
    "    write_metadata_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225",
   "metadata": {},
   "source": [
    "#### Analysis Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_og = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Final_Analysis\"\n",
    ")\n",
    "final_output_df = final_output_df_og[\n",
    "    [\"sgRNA\", \"timepoints\", \"area\", \"mean_intensity_wo_bkd\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_og[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oscillaty_boi = final_output_df_og[\n",
    "    final_output_df_og[\"sgRNA\"] == \"CACTTTGAGTACGATGGTGT\"\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(oscillaty_boi.groupby(\"timepoints\")[\"area\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(oscillaty_boi.groupby(\"timepoints\")[\"mean_intensity\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "mean_series_area = (\n",
    "    final_output_df.groupby([\"sgRNA\", \"timepoints\"])[\"area\"].mean().persist()\n",
    ")\n",
    "mean_series_intensity = (\n",
    "    final_output_df.groupby([\"sgRNA\", \"timepoints\"])[\"mean_intensity_wo_bkd\"]\n",
    "    .mean()\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "time_vectors_area = (\n",
    "    mean_series_area.compute().groupby(\"sgRNA\").apply(lambda x: np.array(x))\n",
    ")\n",
    "valid_time_vectors_area = time_vectors_area[time_vectors_area.apply(len) == 143]\n",
    "\n",
    "time_vectors_intensity = (\n",
    "    mean_series_intensity.compute().groupby(\"sgRNA\").apply(lambda x: np.array(x))\n",
    ")\n",
    "valid_time_vectors_intensity = time_vectors_intensity[\n",
    "    time_vectors_intensity.apply(len) == 143\n",
    "]\n",
    "\n",
    "time_vectors_area_X = np.array(valid_time_vectors_area.tolist())\n",
    "time_vectors_area_X_filtered = sp.signal.medfilt(\n",
    "    time_vectors_area_X, kernel_size=(1, 11)\n",
    ")\n",
    "\n",
    "time_vectors_intensity_X = np.array(valid_time_vectors_intensity.tolist())\n",
    "time_vectors_intensity_X_filtered = sp.signal.medfilt(\n",
    "    time_vectors_intensity_X, kernel_size=(1, 11)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_area = skl.cluster.KMeans(n_clusters=4, random_state=0)\n",
    "kmeans_area.fit(time_vectors_area_X_filtered[:, :-10])\n",
    "\n",
    "kmeans_intensity = skl.cluster.KMeans(n_clusters=4, random_state=0)\n",
    "kmeans_intensity.fit(time_vectors_intensity_X_filtered[:, :-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_center_area = kmeans_area.transform(time_vectors_area_X_filtered[:, :-10])\n",
    "dist_to_center_area = np.min(dist_to_center_area, axis=1)\n",
    "predicted_area_clust = kmeans_area.predict(time_vectors_area_X_filtered[:, :-10])\n",
    "\n",
    "dist_to_center_intensity = kmeans_intensity.transform(\n",
    "    time_vectors_intensity_X_filtered[:, :-10]\n",
    ")\n",
    "predicted_intensity_clust = kmeans_intensity.predict(\n",
    "    time_vectors_intensity_X_filtered[:, :-10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_area, counts_area = np.unique(predicted_area_clust, return_counts=True)\n",
    "unique_intensity, counts_intensity = np.unique(\n",
    "    predicted_intensity_clust, return_counts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 16))\n",
    "\n",
    "ax[0].plot(\n",
    "    kmeans_area.cluster_centers_.T[:, 0],\n",
    "    label=\"Cluster 0\",\n",
    "    linewidth=2,\n",
    "    color=\"#56B4E9\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    kmeans_area.cluster_centers_.T[:, 1],\n",
    "    label=\"Cluster 1\",\n",
    "    linewidth=2,\n",
    "    color=\"#E69F00\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    kmeans_area.cluster_centers_.T[:, 2],\n",
    "    label=\"Cluster 2\",\n",
    "    linewidth=2,\n",
    "    color=\"#009E73\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    kmeans_area.cluster_centers_.T[:, 3],\n",
    "    label=\"Cluster 3\",\n",
    "    linewidth=2,\n",
    "    color=\"#CC79A7\",\n",
    ")\n",
    "ax[0].set_xlabel(\"Time ($\\Delta$ t = 4 mins)\", fontsize=20)\n",
    "ax[0].tick_params(labelsize=20)\n",
    "ax[0].set_ylabel(\"Average Cell Size ($\\mu^2$)\", fontsize=20)\n",
    "ax[0].legend(fontsize=20)\n",
    "\n",
    "ax[1].bar(\n",
    "    [\"0\", \"1\", \"2\", \"3\"],\n",
    "    counts_area,\n",
    "    bottom=[0, 1, 2, 3],\n",
    "    linewidth=1,\n",
    "    color=[\"#56B4E9\", \"#E69F00\", \"#009E73\", \"#CC79A7\"],\n",
    ")\n",
    "ax[1].set_xlabel(\"Cluster #\", fontsize=20)\n",
    "ax[1].tick_params(labelsize=20)\n",
    "ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "# plt.savefig(\"2021-07-13_cluster_fig.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 16))\n",
    "\n",
    "ax[0].plot(\n",
    "    kmeans_intensity.cluster_centers_.T[:, 0],\n",
    "    label=\"Cluster 0\",\n",
    "    linewidth=2,\n",
    "    color=\"#56B4E9\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    kmeans_intensity.cluster_centers_.T[:, 1],\n",
    "    label=\"Cluster 1\",\n",
    "    linewidth=2,\n",
    "    color=\"#E69F00\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    kmeans_intensity.cluster_centers_.T[:, 2],\n",
    "    label=\"Cluster 2\",\n",
    "    linewidth=2,\n",
    "    color=\"#009E73\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    kmeans_intensity.cluster_centers_.T[:, 3],\n",
    "    label=\"Cluster 3\",\n",
    "    linewidth=2,\n",
    "    color=\"#CC79A7\",\n",
    ")\n",
    "ax[0].set_xlabel(\"Time ($\\Delta$ t = 4 mins)\", fontsize=20)\n",
    "ax[0].tick_params(labelsize=20)\n",
    "ax[0].set_ylabel(\"Average Intensity (AU)\", fontsize=20)\n",
    "ax[0].legend(fontsize=20)\n",
    "\n",
    "ax[1].bar(\n",
    "    [\"0\", \"1\", \"2\", \"3\"],\n",
    "    counts_intensity,\n",
    "    bottom=[0, 1, 2, 3],\n",
    "    linewidth=1,\n",
    "    color=[\"#56B4E9\", \"#E69F00\", \"#009E73\", \"#CC79A7\"],\n",
    ")\n",
    "ax[1].set_xlabel(\"Cluster #\", fontsize=20)\n",
    "ax[1].tick_params(labelsize=20)\n",
    "ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "# plt.savefig(\"2021-07-13_cluster_fig.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_mask_area = predicted_area_clust == 3\n",
    "hits_df = valid_time_vectors_area[filter_mask_area].to_frame()\n",
    "hits_df[\"Distance to Centroid\"] = dist_to_center_area[filter_mask_area]\n",
    "hits_df[\"Distance to Stable Intensity Centroid\"] = dist_to_center_intensity[\n",
    "    filter_mask_area, 0\n",
    "]\n",
    "hits_df = hits_df.rename(columns={\"area\": \"avg_area_trace\"})\n",
    "# hits_df = hits_df.sample(n=100)\n",
    "sgRNA_filtered_df = final_output_df_og[\n",
    "    final_output_df_og[\"sgRNA\"].isin(hits_df.index.tolist())\n",
    "]\n",
    "sgRNA_filtered_df_for_hist = sgRNA_filtered_df.groupby(\"phenotype trenchid\").apply(\n",
    "    lambda x: x.iloc[0]\n",
    ")\n",
    "sgRNA_filtered_df_for_hist = sgRNA_filtered_df_for_hist.set_index(\"sgRNA\")\n",
    "sgRNA_filtered_df_for_hist = sgRNA_filtered_df_for_hist.join(hits_df).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgRNA_filtered_df_for_hist.to_pickle(\"./short_cell_cluster_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsZ_df = final_output_df_og[final_output_df_og[\"Gene\"] == \"ftsZ\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsz_targets = ftsZ_df[\"TargetID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsZ_df[ftsZ_df[\"TargetID\"] == ftsz_targets[2]][\"N Mismatch\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df = ftsZ_df[ftsZ_df[\"TargetID\"] == ftsz_targets[2]]\n",
    "target_mean_series = target_df.groupby([\"N Mismatch\", \"timepoints\"])[\"area\"].mean()\n",
    "target_time_vectors = target_mean_series.groupby(\"N Mismatch\").apply(\n",
    "    lambda x: np.array(x)\n",
    ")\n",
    "target_time_vectors_X = np.array(target_time_vectors.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(target_time_vectors_X[0].T, label=\"N Mismatch = 8\")\n",
    "plt.plot(target_time_vectors_X[1].T, label=\"N Mismatch = 9\")\n",
    "plt.plot(target_time_vectors_X[2].T, label=\"N Mismatch = 10\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsz_mean_series = ftsZ_df.groupby([\"sgRNA\", \"timepoints\"])[\"area\"].mean()\n",
    "ftsz_time_vectors = ftsz_mean_series.groupby(\"sgRNA\").apply(lambda x: np.array(x))\n",
    "ftsz_time_vectors_X = np.array(ftsz_time_vectors.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ftsz_time_vectors_X.T)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = ftsZ_df[ftsZ_df[\"TargetID\"] == ftsz_targets[3]]\n",
    "target_mean_series = target_df.groupby([\"N Mismatch\", \"timepoints\"])[\"area\"].mean()\n",
    "target_time_vectors = target_mean_series.groupby(\"N Mismatch\").apply(\n",
    "    lambda x: np.array(x)\n",
    ")\n",
    "target_time_vectors_X = np.array(target_time_vectors.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(target_time_vectors_X.T)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_series = final_output_df.groupby([\"sgRNA\", \"timepoints\"])[\"area\"].mean().persist()\n",
    "\n",
    "time_vectors = mean_series.compute().groupby(\"sgRNA\").apply(lambda x: np.array(x))\n",
    "valid_time_vectors = time_vectors[time_vectors.apply(len) == 143]\n",
    "\n",
    "time_vectors_X = np.array(valid_time_vectors.tolist())\n",
    "time_vectors_X_filtered = sp.signal.medfilt(time_vectors_X, kernel_size=(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clust = kmeans.predict(time_vectors_X_filtered[:, :-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_center[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.predicted_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256",
   "metadata": {},
   "source": [
    "#### Remeber kmeans++ good for oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = skl.mixture.BayesianGaussianMixture(n_components=5)\n",
    "gmm.fit(time_vectors_X_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gmm.means_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vectors = mean_series.groupby(\"sgRNA\").apply(lambda x: np.array(x))\n",
    "time_vectors_X = np.array(time_vectors.tolist())\n",
    "centers, indices = skl.cluster.kmeans_plusplus(\n",
    "    time_vectors_X, n_clusters=3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centers, indices = skl.cluster.kmeans_plusplus(\n",
    "    time_vectors_X, n_clusters=3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(centers.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr.unlinked_kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270",
   "metadata": {},
   "source": [
    "### Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2021-05-27_lDE18_20x_run_1/Barcodes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    n_workers=6,\n",
    "    memory=\"64GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgRNA_filtered_df_for_hist[\"Distance to Centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_level = 0.7\n",
    "min_intensity = 500\n",
    "n_trench_min = 3\n",
    "filtered_df = barcode_only_df[\n",
    "    (barcode_only_df[\"mVenus Mean Intensity: Median\"] > min_intensity)\n",
    "    & (\n",
    "        (\n",
    "            barcode_only_df[\"mVenus Fano Factor: SEM\"]\n",
    "            / barcode_only_df[\"mVenus Fano Factor: Mean\"]\n",
    "        )\n",
    "        < filter_level\n",
    "    )\n",
    "    & (\n",
    "        (\n",
    "            barcode_only_df[\"mVenus Mean Intensity: SEM\"]\n",
    "            / barcode_only_df[\"mVenus Mean Intensity: Mean\"]\n",
    "        )\n",
    "        < filter_level\n",
    "    )\n",
    "    & (barcode_only_df[\"N Trenches\"] >= n_trench_min)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-05-27_lDE18_20x_run_1/mVenus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgRNA_filtered_df_for_hist.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = sgRNA_filtered_df_for_hist[\n",
    "    [\n",
    "        \"EcoWG1_id\",\n",
    "        \"Gene\",\n",
    "        \"Target Sites\",\n",
    "        \"N Target Sites\",\n",
    "        \"N Mismatch\",\n",
    "        \"Category\",\n",
    "        \"TargetID\",\n",
    "        \"phenotype trenchid\",\n",
    "        \"avg_area_trace\",\n",
    "        \"Distance to Centroid\",\n",
    "    ]\n",
    "]\n",
    "hist, trenchid_table, edges, select_histcolumn, select_trenchid = tr.linked_histogram(\n",
    "    working_df,\n",
    "    \"Distance to Centroid\",\n",
    "    trenchids_as_list=False,\n",
    "    maxperc=99,\n",
    "    trenchid_column=\"phenotype trenchid\",\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trenchid_table.opts(width=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_kymograph_display = tr.linked_kymograph_for_hist(\n",
    "    kymo_xarr,\n",
    "    working_df,\n",
    "    \"Distance to Centroid\",\n",
    "    edges,\n",
    "    select_histcolumn,\n",
    "    select_trenchid,\n",
    "    trenchid_column=\"phenotype trenchid\",\n",
    "    y_scale=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_kymograph_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    scatter,\n",
    "    trenchid_table,\n",
    "    unpack_trenchid_table,\n",
    "    select_scatter,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_scatter(\n",
    "    filtered_df,\n",
    "    \"mVenus Mean Intensity: Median\",\n",
    "    \"mVenus Fano Factor: Median\",\n",
    "    trenchids_as_list=True,\n",
    "    maxperc=99,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    height=600,\n",
    "    logx=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trenchid_table + unpack_trenchid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter_kymograph_display = tr.linked_kymograph_for_scatter(\n",
    "    kymo_xarr,\n",
    "    filtered_df,\n",
    "    \"mVenus Mean Intensity: Median\",\n",
    "    \"mVenus Fano Factor: Median\",\n",
    "    select_scatter,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter_kymograph_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "fano_outlier_percentile = 99\n",
    "\n",
    "filtered_df[\"Log mVenus Mean Intensity\"] = np.log10(\n",
    "    filtered_df[\"mVenus Mean Intensity: Median\"]\n",
    ").values\n",
    "filtered_df[\"Log mVenus Fano Factor\"] = np.log10(\n",
    "    filtered_df[\"mVenus Fano Factor: Median\"]\n",
    ").values\n",
    "is_nan_mask = np.logical_or(\n",
    "    np.isnan(filtered_df[\"Log mVenus Mean Intensity\"]),\n",
    "    np.isnan(filtered_df[\"Log mVenus Fano Factor\"]),\n",
    ")\n",
    "\n",
    "log_intensity = filtered_df[\"Log mVenus Mean Intensity\"][~is_nan_mask].values\n",
    "log_fano = filtered_df[\"Log mVenus Fano Factor\"][~is_nan_mask].values\n",
    "\n",
    "reg = LinearRegression().fit(log_intensity.reshape(-1, 1), log_fano)\n",
    "r = reg.score(log_intensity.reshape(-1, 1), log_fano)\n",
    "print(\"R squared: \" + str(r))\n",
    "\n",
    "corrected_fano = filtered_df[\"Log mVenus Fano Factor\"].values - (\n",
    "    reg.coef_[0] * filtered_df[\"Log mVenus Mean Intensity\"].values + reg.intercept_\n",
    ")\n",
    "perc = np.nanpercentile(corrected_fano, fano_outlier_percentile)\n",
    "high_fano_df = filtered_df[corrected_fano > perc]\n",
    "high_fano_complement_df = filtered_df[corrected_fano <= perc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divergence analysis\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ref_seq = \"TTGACGGCTAGCTCAGTCCTAGGTACAGTGCTAGC\"\n",
    "ref_seq = np.array([char for char in ref_seq])\n",
    "\n",
    "filtered_df[\"Promoter Divergence\"] = 35 - filtered_df[\"Promoter\"].apply(\n",
    "    lambda x: np.sum(np.array([char for char in x]) == ref_seq)\n",
    ")\n",
    "filtered_df[\"-10 Divergence\"] = 12 - filtered_df[\"Promoter\"].apply(\n",
    "    lambda x: np.sum(np.array([char for char in x])[-12:] == ref_seq[-12:])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin_df = filtered_df[filtered_df[\"Promoter Divergence\"] < 5]\n",
    "sns.violinplot(\n",
    "    data=violin_df,\n",
    "    x=\"Promoter Divergence\",\n",
    "    y=\"mVenus Mean Intensity: Median\",\n",
    ")\n",
    "plt.ylim(0, 15000)\n",
    "plt.show()\n",
    "\n",
    "violin_df = filtered_df[filtered_df[\"-10 Divergence\"] < 5]\n",
    "sns.violinplot(\n",
    "    data=violin_df,\n",
    "    x=\"-10 Divergence\",\n",
    "    y=\"mVenus Mean Intensity: Median\",\n",
    ")\n",
    "plt.ylim(0, 15000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293",
   "metadata": {},
   "source": [
    "## Step 3b: Combined Analysis with Lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/metadata.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1.0 - percent_called_w_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305",
   "metadata": {},
   "source": [
    "#### Import Lineage Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, query_list, client=False, repartition=False):\n",
    "    # filter_list must be in df.query format (see pandas docs)\n",
    "\n",
    "    # returns persisted dataframe either in cluster or local\n",
    "\n",
    "    compiled_query = \" and \".join(query_list)\n",
    "    out_df = df.query(compiled_query)\n",
    "    if client:\n",
    "        out_df = client.daskclient.persist(out_df)\n",
    "    else:\n",
    "        out_df = out_df.persist()\n",
    "\n",
    "    if repartition:\n",
    "        init_size = len(df)\n",
    "        final_size = len(out_df)\n",
    "        ratio = init_size // final_size\n",
    "        out_df = out_df.repartition(npartitions=(df.npartitions // ratio) + 1)\n",
    "\n",
    "        if client:\n",
    "            out_df = client.daskclient.persist(out_df)\n",
    "        else:\n",
    "            out_df = out_df.persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_first_cell_timepoint(df):\n",
    "    min_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmin().tolist()\n",
    "    init_cells =
 df.loc[min_tpts]\n",
    "    return init_cells\n",
    "\n",
    "\n",
    "def get_last_cell_timepoint(df):\n",
    "    max_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmax().tolist()\n",
    "    fin_cells = df.loc[max_tpts]\n",
    "    return fin_cells\n",
    "\n",
    "\n",
    "def get_first_last_cell_dfs(df, persist=False):\n",
    "    ### NOTE: this functions requires that the input df has partitions aligned with trenchids, so\n",
    "    ### that mother and siblings are in the same partition.\n",
    "\n",
    "    init_cells = df.map_partitions(get_first_cell_timepoint)\n",
    "    fin_cells = df.map_partitions(get_last_cell_timepoint)\n",
    "    if persist:\n",
    "        init_cells = init_cells.persist()\n",
    "        fin_cells = fin_cells.persist()\n",
    "    return init_cells, fin_cells\n",
    "\n",
    "\n",
    "def get_df_from_series_index(df, delayed_series, partition_info=None):\n",
    "    # Hack to avoid automatic partition alignment in map_partitions\n",
    "    # Allows for mismatched index lookup\n",
    "\n",
    "    n = partition_info[\"number\"]\n",
    "    list_of_indices = delayed_series[n].tolist()\n",
    "    df_out = df.loc[list_of_indices]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_relative_dfs(query_df, reference_df, persist_relatives=False):\n",
    "    init_cells, fin_cells = get_first_last_cell_dfs(query_df, persist=False)\n",
    "    cell_min_tpt_df, cell_max_tpt_df = get_first_last_cell_dfs(\n",
    "        reference_df, persist=False\n",
    "    )\n",
    "\n",
    "    init_cells = (\n",
    "        init_cells.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    fin_cells = (\n",
    "        fin_cells.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    cell_min_tpt_df = (\n",
    "        cell_min_tpt_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    cell_max_tpt_df = (\n",
    "        cell_max_tpt_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "    #     min_tpts = query_df.groupby(['Global CellID'])['timepoints'].idxmin().to_frame().set_index(\"timepoints\",sorted=True,drop=False).repartition(divisions=query_df.divisions,force=True)\n",
    "    #     max_tpts = query_df.groupby(['Global CellID'])['timepoints'].idxmax().to_frame().set_index(\"timepoints\",sorted=True,drop=False).repartition(divisions=query_df.divisions,force=True)\n",
    "    #     init_cells = query_df.loc[min_tpts['timepoints']].reset_index(drop=False).set_index('Global CellID',sorted=True).persist()\n",
    "    #     fin_cells = query_df.loc[max_tpts['timepoints']].reset_index(drop=False).set_index('Global CellID',sorted=True).persist()\n",
    "\n",
    "    #     cells_max_tpt = reference_df.groupby(['Global CellID'])['timepoints'].idxmax().to_frame().set_index(\"timepoints\",sorted=True,drop=False).repartition(divisions=reference_df.divisions,force=True)\n",
    "    #     cells_min_tpt = reference_df.groupby(['Global CellID'])['timepoints'].idxmin().to_frame().set_index(\"timepoints\",sorted=True,drop=False).repartition(divisions=reference_df.divisions,force=True)\n",
    "    #     cell_max_tpt_df = reference_df.loc[cells_max_tpt['timepoints']].reset_index(drop=False).set_index('Global CellID',sorted=True).persist()\n",
    "    #     cell_min_tpt_df = reference_df.loc[cells_min_tpt['timepoints']].reset_index(drop=False).set_index('Global CellID',sorted=True).persist()\n",
    "\n",
    "    mother_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_max_tpt_df,\n",
    "        init_cells[\"Mother CellID\"].to_delayed(),\n",
    "        meta=cell_max_tpt_df.head()[:0],\n",
    "    )\n",
    "    sister_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_min_tpt_df,\n",
    "        init_cells[\"Sister CellID\"].to_delayed(),\n",
    "        meta=cell_min_tpt_df.head()[:0],\n",
    "    )\n",
    "    daughter_1_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_min_tpt_df,\n",
    "        fin_cells[\"Daughter CellID 1\"].to_delayed(),\n",
    "        meta=cell_min_tpt_df.head()[:0],\n",
    "    )\n",
    "    daughter_2_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_min_tpt_df,\n",
    "        fin_cells[\"Daughter CellID 2\"].to_delayed(),\n",
    "        meta=cell_min_tpt_df.head()[:0],\n",
    "    )\n",
    "\n",
    "    #     mother_df = cell_max_tpt_df.loc[init_cells[\"Mother CellID\"].compute().tolist()]\n",
    "    #     sister_df = cell_min_tpt_df.loc[init_cells[\"Sister CellID\"].compute().tolist()]\n",
    "    #     daughter_1_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 1\"].compute().tolist()]\n",
    "    #     daughter_2_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 2\"].compute().tolist()]\n",
    "\n",
    "    #     mother_df[\"init_cells Index\"] = init_cells.index.values\n",
    "    #     sister_df[\"init_cells Index\"] = init_cells.index.values\n",
    "    #     daughter_1_df[\"init_cells Index\"] = init_cells.index.values\n",
    "    #     daughter_2_df[\"init_cells Index\"] = init_cells.index.values\n",
    "\n",
    "    mother_df = mother_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "    sister_df = sister_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "    daughter_1_df = daughter_1_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "    daughter_2_df = daughter_2_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "\n",
    "    if persist_relatives:\n",
    "        mother_df = mother_df.persist()\n",
    "        sister_df = sister_df.persist()\n",
    "        daughter_1_df = daughter_1_df.persist()\n",
    "        daughter_2_df = daughter_2_df.persist()\n",
    "\n",
    "    return init_cells, fin_cells, mother_df, sister_df, daughter_1_df, daughter_2_df\n",
    "\n",
    "\n",
    "def get_init_and_final_size(\n",
    "    query_df, reference_df, size_metrics=[\"area\", \"major_axis_length\"]\n",
    "):\n",
    "    ##query contains cells of interest\n",
    "    ##reference contains all cells that may be retrieved (mothers,sisters,daughters)\n",
    "\n",
    "    (\n",
    "        init_cells,\n",
    "        fin_cells,\n",
    "        mother_df,\n",
    "        sister_df,\n",
    "        daughter_1_df,\n",
    "        daughter_2_df,\n",
    "    ) = get_relative_dfs(query_df, reference_df)\n",
    "\n",
    "    init_cells_noidx, fin_cells_noidx = (\n",
    "        init_cells.reset_index(drop=False).persist(),\n",
    "        fin_cells.reset_index(drop=False).persist(),\n",
    "    )\n",
    "\n",
    "    adjusted_init_size = {}\n",
    "    adjusted_final_size = {}\n",
    "    adjusted_del_size = {}\n",
    "\n",
    "    ### Ineffecient, but not sure how to avoid\n",
    "    for metric in size_metrics:\n",
    "        #         init_cells_noidx = init_cells[metric].values.compute()\n",
    "        #         fin_cells_arr = fin_cells[metric].values.compute()\n",
    "        #         mother_df_arr = mother_df[metric].values.compute()\n",
    "        #         sister_df_arr = sister_df[metric].values.compute()\n",
    "        #         daughter_1_df_arr = daughter_1_df[metric].values.compute()\n",
    "        #         daughter_2_df_arr = daughter_2_df[metric].values.compute()\n",
    "\n",
    "        interp_mother_final_size = (\n",
    "            (init_cells_noidx[metric] + sister_df[metric]) * mother_df[metric]\n",
    "        ) ** (1 / 2)\n",
    "        sister_frac = init_cells_noidx[metric] / (\n",
    "            sister_df[metric] + init_cells_noidx[metric]\n",
    "        )\n",
    "        adjusted_init_size[metric] = sister_frac * interp_mother_final_size\n",
    "\n",
    "        adjusted_final_size[metric] = (\n",
    "            (daughter_1_df[metric] + daughter_2_df[metric]) * fin_cells_noidx[metric]\n",
    "        ) ** (1 / 2)\n",
    "\n",
    "        adjusted_del_size[metric] = (\n",
    "            adjusted_final_size[metric] - adjusted_init_size[metric]\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        init_cells_noidx,\n",
    "        fin_cells_noidx,\n",
    "        adjusted_init_size,\n",
    "        adjusted_final_size,\n",
    "        adjusted_del_size,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_promoter_synthesis_rate(cellid_groupby, intensity_label, size_metric_label):\n",
    "    del_intensity_series = (\n",
    "        cellid_groupby[intensity_label]\n",
    "        .apply(lambda x: x.values[1:] - x.values[:-1])\n",
    "        .to_frame(name=\"del intensity\")\n",
    "    )\n",
    "    mean_intensity_series = (\n",
    "        cellid_groupby[intensity_label]\n",
    "        .apply(lambda x: (x.values[1:] + x.values[:-1]) / 2)\n",
    "        .to_frame(name=\"mean intensity\")\n",
    "    )\n",
    "    del_size_series = (\n",
    "        cellid_groupby[size_metric_label]\n",
    "        .apply(lambda x: x.values[1:] - x.values[:-1])\n",
    "        .to_frame(name=\"del size\")\n",
    "    )\n",
    "    mean_size_series = (\n",
    "        cellid_groupby[size_metric_label]\n",
    "        .apply(lambda x: (x.values[1:] + x.values[:-1]) / 2)\n",
    "        .to_frame(name=\"mean size\")\n",
    "    )\n",
    "    pro_syn_df = dd.concat(\n",
    "        [\n",
    "            del_intensity_series,\n",
    "            mean_intensity_series,\n",
    "            del_size_series,\n",
    "            mean_size_series,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    promoter_activity_series = pro_syn_df.apply(\n",
    "        lambda x: np.nanmean(\n",
    "            x[\"del intensity\"]\n",
    "            + (x[\"mean intensity\"] * (x[\"del size\"] / x[\"mean size\"]))\n",
    "        ),\n",
    "        axis=1,\n",
    "        meta=float,\n",
    "    )\n",
    "    return promoter_activity_series\n",
    "\n",
    "\n",
    "def get_growth_and_division_stats(query_df, reference_df, delta_t_min=4):\n",
    "    (\n",
    "        init_cells_noidx,\n",
    "        fin_cells_noidx,\n",
    "        adjusted_init_size,\n",
    "        adjusted_final_size,\n",
    "        adjusted_del_size,\n",
    "    ) = get_init_and_final_size(\n",
    "        query_df, reference_df, size_metrics=[\"area\", \"major_axis_length\"]\n",
    "    )\n",
    "\n",
    "    init_cells_noidx[\"delS\"] = adjusted_del_size[\"area\"]\n",
    "    init_cells_noidx[\"Sb\"] = adjusted_init_size[\"area\"]\n",
    "    init_cells_noidx[\"Sd\"] = adjusted_final_size[\"area\"]\n",
    "\n",
    "    init_cells_noidx[\"delL\"] = adjusted_del_size[\"major_axis_length\"]\n",
    "    init_cells_noidx[\"Lb\"] = adjusted_init_size[\"major_axis_length\"]\n",
    "    init_cells_noidx[\"Ld\"] = adjusted_final_size[\"major_axis_length\"]\n",
    "\n",
    "    init_cells_noidx[\"final timepoints\"] = fin_cells_noidx[\"timepoints\"]\n",
    "    del_t = init_cells_noidx[\"final timepoints\"] - init_cells_noidx[\"timepoints\"]\n",
    "    init_cells_noidx[\"Delta t\"] = del_t\n",
    "\n",
    "    init_cells = init_cells_noidx.set_index(\"Global CellID\", sorted=True)\n",
    "\n",
    "    query_df_cellid_sorted = (\n",
    "        query_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    query_df[\"Global CellID-timepoints Index\"] = query_df.apply(\n",
    "        lambda x: int(f'{int(x[\"Global CellID\"]):04}{int(x[\"timepoints\"]):04}'), axis=1\n",
    "    )\n",
    "    query_df_cellid_sorted = (\n",
    "        query_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID-timepoints Index\", sorted=False)\n",
    "        .set_index(\"Global CellID\", sorted=True)\n",
    "        .persist()\n",
    "    )\n",
    "    del query_df\n",
    "\n",
    "    mean_cell_area_incr = query_df_cellid_sorted.groupby(\"Global CellID\")[\"area\"].apply(\n",
    "        lambda x: np.mean(x[1:].values - x[:-1].values)\n",
    "    )\n",
    "    mean_cell_length_incr = query_df_cellid_sorted.groupby(\"Global CellID\")[\n",
    "        \"major_axis_length\"\n",
    "    ].apply(lambda x: np.mean(x[1:].values - x[:-1].values))\n",
    "\n",
    "    mean_cell_width = query_df_cellid_sorted.groupby(\"Global CellID\")[\n",
    "        \"minor_axis_length\"\n",
    "    ].apply(lambda x: np.mean(x.values))\n",
    "    mean_mchy_intensity = query_df_cellid_sorted.groupby(\"Global CellID\")[\n",
    "        \"mCherry mean_intensity\"\n",
    "    ].apply(lambda x: np.mean(x.values))\n",
    "\n",
    "    area_normed_mchy_intensity = get_promoter_synthesis_rate(\n",
    "        query_df_cellid_sorted.groupby(\"Global CellID\"),\n",
    "        \"mCherry mean_intensity\",\n",
    "        \"area\",\n",
    "    )\n",
    "    length_normed_mchy_intensity = get_promoter_synthesis_rate(\n",
    "        query_df_cellid_sorted.groupby(\"Global CellID\"),\n",
    "        \"mCherry mean_intensity\",\n",
    "        \"major_axis_length\",\n",
    "    )\n",
    "\n",
    "    init_cells[\"Mean Area Increment\"] = mean_cell_area_incr\n",
    "    init_cells[\"Mean Length Increment\"] = mean_cell_length_incr\n",
    "    init_cells[\"Mean Width\"] = mean_cell_width\n",
    "    init_cells[\"Mean mCherry Intensity\"] = mean_mchy_intensity\n",
    "\n",
    "    init_cells[\n",
    "        \"Mean mCherry Promoter Activity (area normed)\"\n",
    "    ] = area_normed_mchy_intensity\n",
    "    init_cells[\n",
    "        \"Mean mCherry Promoter Activity (length normed)\"\n",
    "    ] = length_normed_mchy_intensity\n",
    "\n",
    "    #     init_cells_trenchid_idx = init_cells.set_index(\"trenchid\",sorted=False).persist()\n",
    "    #     init_cells_trenchid_groupby = init_cells_trenchid_idx.groupby(\"trenchid\")\n",
    "\n",
    "    ## Filtering by cell cycle length to eliminate artifact\n",
    "    init_cells = init_cells[init_cells[\"Delta t\"] >= delta_t_min]\n",
    "\n",
    "    trenchid_df = (\n",
    "        query_df_cellid_sorted.reset_index(drop=False)\n",
    "        .set_index(\"trenchid\", sorted=True)\n",
    "        .groupby(\"trenchid\")\n",
    "        .apply(lambda x: x.iloc[0])\n",
    "        .persist()\n",
    "    )\n",
    "    init_cells_trenchid_idx = (\n",
    "        init_cells.reset_index(drop=False).set_index(\"trenchid\", sorted=True).persist()\n",
    "    )\n",
    "\n",
    "    init_cells_trenchid_groupby = init_cells_trenchid_idx.groupby(\n",
    "        \"trenchid\", sort=False\n",
    "    )\n",
    "\n",
    "    trenchid_df[\"Mean delS\"] = init_cells_trenchid_groupby[\"delS\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean Sb\"] = init_cells_trenchid_groupby[\"Sb\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean Sd\"] = init_cells_trenchid_groupby[\"Sd\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean delL\"] = init_cells_trenchid_groupby[\"delL\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean Lb\"] = init_cells_trenchid_groupby[\"Lb\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean Ld\"] = init_cells_trenchid_groupby[\"Ld\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean Area Increment\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Area Increment\"\n",
    "    ].apply(lambda x: np.nanmean(x.values), meta=float)\n",
    "    trenchid_df[\"Mean Length Increment\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Length Increment\"\n",
    "    ].apply(lambda x: np.nanmean(x.values), meta=float)\n",
    "    trenchid_df[\"Mean Width\"] = init_cells_trenchid_groupby[\"Mean Width\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Mean mCherry Intensity\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Intensity\"\n",
    "    ].apply(lambda x: np.nanmean(x.values), meta=float)\n",
    "    trenchid_df[\"Mean Delta t\"] = init_cells_trenchid_groupby[\"Delta t\"].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\n",
    "        \"Mean mCherry Promoter Activity (area normed)\"\n",
    "    ] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Promoter Activity (area normed)\"\n",
    "    ].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\n",
    "        \"Mean mCherry Promoter Activity (length normed)\"\n",
    "    ] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Promoter Activity (length normed)\"\n",
    "    ].apply(\n",
    "        lambda x: np.nanmean(x.values), meta=float\n",
    "    )\n",
    "\n",
    "    trenchid_df[\"Standard Deviation delS\"] = init_cells_trenchid_groupby[\"delS\"].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Standard Deviation Sb\"] = init_cells_trenchid_groupby[\"Sb\"].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Standard Deviation Sd\"] = init_cells_trenchid_groupby[\"Sd\"].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Standard Deviation delL\"] = init_cells_trenchid_groupby[\"delL\"].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Standard Deviation Lb\"] = init_cells_trenchid_groupby[\"Lb\"].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Standard Deviation Ld\"] = init_cells_trenchid_groupby[\"Ld\"].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\"Standard Deviation Area Increment\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Area Increment\"\n",
    "    ].apply(lambda x: np.nanstd(x.values), meta=float)\n",
    "    trenchid_df[\"Standard Deviation Length Increment\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Length Increment\"\n",
    "    ].apply(lambda x: np.nanstd(x.values), meta=float)\n",
    "    trenchid_df[\"Standard Deviation Width\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Width\"\n",
    "    ].apply(lambda x: np.nanstd(x.values), meta=float)\n",
    "    trenchid_df[\"Standard Deviation mCherry Intensity\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Intensity\"\n",
    "    ].apply(lambda x: np.nanstd(x.values), meta=float)\n",
    "    trenchid_df[\"Standard Deviation Delta t\"] = init_cells_trenchid_groupby[\n",
    "        \"Delta t\"\n",
    "    ].apply(lambda x: np.nanstd(x.values), meta=float)\n",
    "    trenchid_df[\n",
    "        \"Standard Deviation mCherry Promoter Activity (area normed)\"\n",
    "    ] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Promoter Activity (area normed)\"\n",
    "    ].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "    trenchid_df[\n",
    "        \"Standard Deviation mCherry Promoter Activity (length normed)\"\n",
    "    ] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Promoter Activity (length normed)\"\n",
    "    ].apply(\n",
    "        lambda x: np.nanstd(x.values), meta=float\n",
    "    )\n",
    "\n",
    "    trenchid_df[\"delS list\"] = init_cells_trenchid_groupby[\"delS\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"Sb list\"] = init_cells_trenchid_groupby[\"Sb\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"Sd list\"] = init_cells_trenchid_groupby[\"Sd\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"delL list\"] = init_cells_trenchid_groupby[\"delL\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"Lb list\"] = init_cells_trenchid_groupby[\"Lb\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"Ld list\"] = init_cells_trenchid_groupby[\"Ld\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"Mean Area Increment list\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Area Increment\"\n",
    "    ].apply(lambda x: x.tolist(), meta=list)\n",
    "    trenchid_df[\"Mean Length Increment list\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean Length Increment\"\n",
    "    ].apply(lambda x: x.tolist(), meta=list)\n",
    "    trenchid_df[\"Mean Width list\"] = init_cells_trenchid_groupby[\"Mean Width\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"Mean mCherry Intensity list\"] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Intensity\"\n",
    "    ].apply(lambda x: x.tolist(), meta=list)\n",
    "    trenchid_df[\"Delta t list\"] = init_cells_trenchid_groupby[\"Delta t\"].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\n",
    "        \"Mean mCherry Promoter Activity list (area normed)\"\n",
    "    ] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Promoter Activity (area normed)\"\n",
    "    ].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\n",
    "        \"Mean mCherry Promoter Activity list (length normed)\"\n",
    "    ] = init_cells_trenchid_groupby[\n",
    "        \"Mean mCherry Promoter Activity (length normed)\"\n",
    "    ].apply(\n",
    "        lambda x: x.tolist(), meta=list\n",
    "    )\n",
    "    trenchid_df[\"cell timepoints list\"] = init_cells_trenchid_groupby[\n",
    "        \"timepoints\"\n",
    "    ].apply(lambda x: x.tolist(), meta=list)\n",
    "    trenchid_df[\"final cell timepoints list\"] = init_cells_trenchid_groupby[\n",
    "        \"final timepoints\"\n",
    "    ].apply(lambda x: x.tolist(), meta=list)\n",
    "\n",
    "    return trenchid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307",
   "metadata": {},
   "outputs": [],
   "source": [
    "list([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308",
   "metadata": {},
   "source": [
    "### Import Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/lineage\"\n",
    ")\n",
    "\n",
    "##temp fix\n",
    "lineage_df[\"CellID\"] = lineage_df[\"CellID\"].astype(int)\n",
    "lineage_df[\"Global CellID\"] = lineage_df[\"Global CellID\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310",
   "metadata": {},
   "source": [
    "### Growth/Div Function (time independent) \n",
    "\n",
    "Only useful in a pseudo steady state regime where stable behaviors are present; since all measurements are averaged over all cells in time. Use to look into time windows for each trench.\n",
    "\n",
    "**Write birth size convergence later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_df = filter_df(\n",
    "    lineage_df, [\"`Trench Score` < -75\"], client=dask_controller, repartition=False\n",
    ").persist()\n",
    "query_df = filter_df(\n",
    "    lineage_df,\n",
    "    [\n",
    "        \"`Mother CellID` != -1\",\n",
    "        \"`Daughter CellID 1` != -1\",\n",
    "        \"`Daughter CellID 2` != -1\",\n",
    "        \"`Sister CellID` != -1\",\n",
    "        \"`Trench Score` < -75\",\n",
    "        \"`timepoints` > 70\",\n",
    "    ],\n",
    "    client=dask_controller,\n",
    "    repartition=False,\n",
    ").persist()\n",
    "trenchid_df = get_growth_and_division_stats(query_df, reference_df).persist()\n",
    "\n",
    "del reference_df\n",
    "del query_df\n",
    "del lineage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trenchid_df = get_growth_and_division_stats(query_df, reference_df).persist()\n",
    "\n",
    "del reference_df\n",
    "del query_df\n",
    "del lineage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313",
   "metadata": {},
   "source": [
    "#### Issue with linear regression for adder?\n",
    "\n",
    "Producing a mean of 0.5 in the library...doesn't make sense\n",
    "\n",
    "This ended up being remedied (mostly) by setting windows for the regime in which the slope is measured and rescaling to the mean added size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_df_trenchid_idx = trenchid_df.reset_index(drop=False).set_index(\"trenchid\",sorted=True)\n",
    "\n",
    "init_df_idx = trenchid_df.index.unique().compute().to_list()\n",
    "valid_barcode_df = barcode_df[\n",
    "    barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "].compute()\n",
    "barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "    lambda x: trenchid_map[x]\n",
    ")\n",
    "valid_init_df_indices = barcode_df_mapped_trenchids.isin(init_df_idx)\n",
    "barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "called_df = called_df.set_index(\"phenotype trenchid\")\n",
    "final_output_df = trenchid_df.loc[called_df.index.compute().tolist()].join(called_df)\n",
    "final_output_df[\"phenotype trenchid\"] = final_output_df.index\n",
    "final_output_df = final_output_df.reset_index(drop=True).set_index(\n",
    "    \"File Parquet Index\", sorted=True\n",
    ")\n",
    "\n",
    "del final_output_df[\"Barcode Signal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317",
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_output_df[\"dir0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del final_output_df[\"dir0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Restore parquet use if reworking labeling scheme to label raw lineage df\n",
    "\n",
    "# dd.to_parquet(\n",
    "#     final_output_df,\n",
    "#     \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis\",\n",
    "#     engine=\"fastparquet\",\n",
    "#     compression=\"gzip\",\n",
    "#     write_metadata_file=True,\n",
    "#     overwrite=True,\n",
    "# )\n",
    "\n",
    "final_output_df = final_output_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321",
   "metadata": {},
   "source": [
    "#### Adder Analysis (seems dubious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"02:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=100,\n",
    "#     memory=\"8GB\",\n",
    "#     working_directory=headpath + \"/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_df_og = dd.read_parquet(\"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_linreg(df, x_label, y_label):\n",
    "    X = df[x_label].reshape(-1, 1)\n",
    "    y = df[y_label]\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    score = reg.score(X, y)\n",
    "    slope, inter = reg.coef_[0], reg.intercept_\n",
    "    return score, slope, inter\n",
    "\n",
    "\n",
    "merged_sb_series = (\n",
    "    final_output_df_pd.groupby(\"sgRNA\")\n",
    "    .apply(lambda x: np.array([val for item in x[\"Sb list\"].tolist() for val in item]))\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Sb list\"})\n",
    ")\n",
    "merged_dels_series = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: np.array([val for item in x[\"delS list\"].tolist() for val in item])\n",
    ")\n",
    "merged_sb_series[\"delS list\"] = merged_dels_series\n",
    "merged_sb_series[\"N obs\"] = merged_sb_series[\"Sb list\"].apply(lambda x: len(x))\n",
    "merged_sb_series[\"Mean delS\"] = merged_sb_series.apply(\n",
    "    lambda x: np.mean(x[\"delS list\"]), axis=1\n",
    ")\n",
    "merged_sb_series[\"Mean Sb\"] = merged_sb_series.apply(\n",
    "    lambda x: np.mean(x[\"Sb list\"]), axis=1\n",
    ")\n",
    "\n",
    "merged_sb_series[\"Sb/udelS list\"] = (\n",
    "    merged_sb_series[\"Sb list\"] / merged_sb_series[\"Mean delS\"]\n",
    ")\n",
    "merged_sb_series[\"delS/udelS list\"] = (\n",
    "    merged_sb_series[\"delS list\"] / merged_sb_series[\"Mean delS\"]\n",
    ")\n",
    "# merged_sb_series = merged_sb_series[merged_sb_series[\"N obs\"]>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sb_series[\"Mean Growth Increment\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: np.mean(x[\"Mean Growth Increment\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sb_dask_df = dd.from_pandas(merged_sb_series, chunksize=100).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "min_birth_size, max_birth_size = 0.8, 1.4\n",
    "bins = 10\n",
    "\n",
    "del_birth_size = max_birth_size - min_birth_size\n",
    "intervals = np.linspace(min_birth_size, max_birth_size, num=bins, dtype=float)\n",
    "\n",
    "\n",
    "def lowess_reg(\n",
    "    x_arr, y_arr, start=min_birth_size, end=max_birth_size, bins=bins, frac=0.66\n",
    "):\n",
    "    intervals = np.linspace(start, end, num=bins, dtype=float)\n",
    "    w = lowess(y_arr, x_arr, frac=frac, xvals=intervals, it=1)\n",
    "    reg_x, reg_y = (intervals, w)\n",
    "    return reg_x, reg_y\n",
    "\n",
    "\n",
    "lowess_result = merged_sb_dask_df.apply(\n",
    "    lambda x: lowess_reg(x[\"Sb/udelS list\"], x[\"delS/udelS list\"])[1],\n",
    "    axis=1,\n",
    "    meta=float,\n",
    ").compute()\n",
    "merged_sb_series[\"LOWESS Trace\"] = lowess_result\n",
    "merged_sb_series[\"LOWESS Slope\"] = merged_sb_series[\"LOWESS Trace\"].apply(\n",
    "    lambda x: (x[-1] - x[0]) / del_birth_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_mask = merged_sb_series[\"LOWESS Trace\"].apply(lambda x: ~np.any(np.isnan(x)))\n",
    "no_neg_mask = merged_sb_series[\"LOWESS Trace\"].apply(lambda x: ~np.any(x < 0.0))\n",
    "no_filament_mask = merged_sb_series[\"LOWESS Trace\"].apply(lambda x: ~np.any(x > 4.0))\n",
    "filter_mask = no_nan_mask * no_neg_mask * no_filament_mask\n",
    "merged_sb_series_nonan = merged_sb_series[filter_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lowess_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "\n",
    "lowess_arr = np.array(merged_sb_series_nonan[\"LOWESS Trace\"].tolist())\n",
    "kmeans_lowess = skl.cluster.KMeans(n_clusters=3, random_state=0)\n",
    "kmeans_lowess.fit(lowess_arr)\n",
    "dist_to_center_lowess = kmeans_lowess.transform(lowess_arr)\n",
    "dist_to_center_lowess = np.min(dist_to_center_lowess, axis=1)\n",
    "predicted_lowess_clust = kmeans_lowess.predict(lowess_arr)\n",
    "unique_lowess, counts_lowess = np.unique(predicted_lowess_clust, return_counts=True)\n",
    "\n",
    "merged_sb_series_nonan[\"Cluster Assignment\"] = predicted_lowess_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    merged_sb_series_nonan[\"LOWESS Slope\"],\n",
    "    merged_sb_series_nonan[\"Mean Growth Increment\"],\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    merged_sb_series_nonan[\"LOWESS Slope\"],\n",
    "    merged_sb_series_nonan[\"Mean Growth Increment\"],\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    "    c=merged_sb_series_nonan[\"Cluster Assignment\"],\n",
    ")\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    merged_sb_series_nonan[\"LOWESS Slope\"],\n",
    "    merged_sb_series_nonan[\"Mean Sb\"],\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    "    c=merged_sb_series_nonan[\"Cluster Assignment\"],\n",
    ")\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(1.0, 5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_df = final_output_df_pd.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "sgrna_df[\"N Observations\"] = final_output_df_pd.groupby(\"sgRNA\").size()\n",
    "sgrna_df[\"phenotype trenchids\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = merged_sb_series_nonan.join(\n",
    "    sgrna_df[\n",
    "        [\n",
    "            \"Gene\",\n",
    "            \"N Observations\",\n",
    "            \"phenotype trenchids\",\n",
    "            \"Category\",\n",
    "            \"N Target Sites\",\n",
    "            \"N Mismatch\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "working_df = working_df[working_df[\"N Observations\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_df = merged_sb_series_nonan[['EcoWG1_id', 'Gene','Target Sites','N Target Sites',\\\n",
    "#                          'N Mismatch', 'Category', 'TargetID', 'phenotype trenchids',\\\n",
    "#                          \"Mean Mean Sb\",\"Mean Mean Growth Increment\"]].compute()\n",
    "\n",
    "(\n",
    "    hist,\n",
    "    trenchid_table,\n",
    "    unpack_trenchid_table,\n",
    "    edges,\n",
    "    select_histcolumn,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_histogram(\n",
    "    working_df,\n",
    "    \"LOWESS Slope\",\n",
    "    trenchids_as_list=True,\n",
    "    minperc=0,\n",
    "    maxperc=100,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trenchid_table + unpack_trenchid_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_kymograph_display = tr.linked_kymograph_for_hist(\n",
    "    kymo_xarr,\n",
    "    working_df,\n",
    "    \"LOWESS Slope\",\n",
    "    edges,\n",
    "    select_histcolumn,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_kymograph_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 32))\n",
    "\n",
    "for i in range(kmeans_lowess.n_clusters):\n",
    "    ax[0].plot(\n",
    "        intervals,\n",
    "        kmeans_lowess.cluster_centers_.T[:, i],\n",
    "        label=\"Cluster \" + str(i),\n",
    "        linewidth=2,\n",
    "    )  # ,color=\"#56B4E9\")\n",
    "# ax[0].plot(intervals,kmeans_lowess.cluster_centers_.T[:,1],label=\"Cluster 1\",linewidth=2,color=\"#E69F00\")\n",
    "# ax[0].plot(intervals,kmeans_lowess.cluster_centers_.T[:,2],label=\"Cluster 2\",linewidth=2,color=\"#009E73\")\n",
    "# ax[0].plot(kmeans_area.cluster_centers_.T[:,3],label=\"Cluster 3\",linewidth=2,color=\"#CC79A7\")\n",
    "ax[0].set_xlabel(\"Time ($\\Delta$ t = 4 mins)\", fontsize=20)\n",
    "ax[0].tick_params(labelsize=20)\n",
    "ax[0].set_ylabel(\"Average Cell Size ($\\mu^2$)\", fontsize=20)\n",
    "ax[0].legend(fontsize=20)\n",
    "# ax[0].set_ylim(0.7,1.5)\n",
    "# ax[0].set_xlim(0.7,1.5)\n",
    "\n",
    "ax[1].bar(\n",
    "    [str(i) for i in range(0, kmeans_lowess.n_clusters)],\n",
    "    counts_lowess,\n",
    "    bottom=range(0, kmeans_lowess.n_clusters),\n",
    "    linewidth=1,\n",
    ")  # color=[\"#56B4E9\",\"#E69F00\",\"#009E73\"])\n",
    "ax[1].set_xlabel(\"Cluster #\", fontsize=20)\n",
    "ax[1].tick_params(labelsize=20)\n",
    "ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "# plt.savefig(\"2021-07-13_cluster_fig.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346",
   "metadata": {},
   "source": [
    "#### Pseudo Steady State states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd[\"CV Sb\"] = (\n",
    "    final_output_df_pd[\"Standard Deviation Sb\"] / final_output_df_pd[\"Mean Sb\"]\n",
    ")\n",
    "final_output_df_pd[\"CV delS\"] = (\n",
    "    final_output_df_pd[\"Standard Deviation delS\"] / final_output_df_pd[\"Mean delS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_df = final_output_df_pd.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "sgrna_df[\"N Observations\"] = final_output_df_pd.groupby(\"sgRNA\").size()\n",
    "sgrna_df[\"phenotype trenchids\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "sgrna_df[\"Median: CV Sb\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: np.median(x[\"CV Sb\"])\n",
    ")\n",
    "sgrna_df[\"Median: CV delS\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: np.median(x[\"CV delS\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = sgrna_df[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"N Observations\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"Category\",\n",
    "        \"N Target Sites\",\n",
    "        \"N Mismatch\",\n",
    "        \"Median: CV Sb\",\n",
    "        \"Median: CV delS\",\n",
    "    ]\n",
    "]\n",
    "working_df = working_df[working_df[\"N Observations\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plt.hist(final_output_df_pd['CV Sb'],bins=50,range=(0,2),alpha=0.7)\n",
    "# plt.hist(final_output_df_pd['CV delS'],bins=50,range=(0,2), alpha=0.7)\n",
    "# plt.show()\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plt.scatter(final_output_df_pd['CV Sb'],final_output_df_pd['CV delS'], s=1, alpha=0.7)\n",
    "# plt.xlim(0,2)\n",
    "# plt.ylim(0,2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_df = merged_sb_series_nonan[['EcoWG1_id', 'Gene','Target Sites','N Target Sites',\\\n",
    "#                          'N Mismatch', 'Category', 'TargetID', 'phenotype trenchids',\\\n",
    "#                          \"Mean Mean Sb\",\"Mean Mean Growth Increment\"]].compute()\n",
    "\n",
    "(\n",
    "    hist,\n",
    "    trenchid_table,\n",
    "    unpack_trenchid_table,\n",
    "    edges,\n",
    "    select_histcolumn,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_histogram(\n",
    "    working_df,\n",
    "    \"Median: CV delS\",\n",
    "    trenchids_as_list=True,\n",
    "    minperc=1,\n",
    "    maxperc=100,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trenchid_table + unpack_trenchid_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_kymograph_display = tr.linked_kymograph_for_hist(\n",
    "    kymo_xarr,\n",
    "    working_df,\n",
    "    \"Median: CV delS\",\n",
    "    edges,\n",
    "    select_histcolumn,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_kymograph_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357",
   "metadata": {},
   "source": [
    "### Growth/Div Function (time dependent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"02:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=100,\n",
    "#     memory=\"8GB\",\n",
    "#     working_directory=headpath + \"/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/lineage\"\n",
    ")\n",
    "\n",
    "##temp fix\n",
    "lineage_df[\"CellID\"] = lineage_df[\"CellID\"].astype(int)\n",
    "lineage_df[\"Global CellID\"] = lineage_df[\"Global CellID\"].astype(int)\n",
    "\n",
    "reference_df = filter_df(\n",
    "    lineage_df, [\"`Trench Score` < -75\"], client=dask_controller, repartition=False\n",
    ").persist()\n",
    "query_df = filter_df(\n",
    "    lineage_df,\n",
    "    [\n",
    "        \"`Mother CellID` != -1\",\n",
    "        \"`Daughter CellID 1` != -1\",\n",
    "        \"`Daughter CellID 2` != -1\",\n",
    "        \"`Sister CellID` != -1\",\n",
    "        \"`Trench Score` < -75\",\n",
    "    ],\n",
    "    client=dask_controller,\n",
    "    repartition=False,\n",
    ").persist()\n",
    "trenchid_df = get_growth_and_division_stats(query_df, reference_df).persist()\n",
    "\n",
    "del reference_df\n",
    "del query_df\n",
    "del lineage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/metadata.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1.0 - percent_called_w_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_df_trenchid_idx = trenchid_df.reset_index(drop=False).set_index(\"trenchid\",sorted=True)\n",
    "\n",
    "init_df_idx = trenchid_df.index.unique().compute().to_list()\n",
    "valid_barcode_df = barcode_df[\n",
    "    barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "].compute()\n",
    "barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "    lambda x: trenchid_map[x]\n",
    ")\n",
    "valid_init_df_indices = barcode_df_mapped_trenchids.isin(init_df_idx)\n",
    "barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "called_df = called_df.set_index(\"phenotype trenchid\")\n",
    "final_output_df = trenchid_df.loc[called_df.index.compute().tolist()].join(called_df)\n",
    "final_output_df[\"phenotype trenchid\"] = final_output_df.index\n",
    "final_output_df = final_output_df.reset_index(drop=True).set_index(\n",
    "    \"File Parquet Index\", sorted=True\n",
    ")\n",
    "\n",
    "del final_output_df[\"Barcode Signal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371",
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_output_df[\"dir0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df = final_output_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis_with_time_delt_filt.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374",
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375",
   "metadata": {},
   "source": [
    "#### Euclidean Clustering: Growth/Division Rate over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=25,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis_with_time.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_pd_groupby = final_output_df_pd.groupby(\"sgRNA\")\n",
    "\n",
    "merged_timeseries_df = (\n",
    "    final_output_df_pd_groupby.apply(\n",
    "        lambda x: np.array(\n",
    "            [val for item in x[\"final cell timepoints list\"].tolist() for val in item]\n",
    "        )\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"final cell timepoints\"})\n",
    ")\n",
    "merged_timeseries_df[\"init cell timepoints\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"cell timepoints list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Delta t list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Delta t list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Mean mCherry Intensity list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"Mean mCherry Intensity list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Mean Width list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Mean Width list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Mean Area Increment list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"Mean Area Increment list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Mean Length Increment list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"Mean Length Increment list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "\n",
    "merged_timeseries_df[\"Sb list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Sb list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Sd list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Sd list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"delS list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"delS list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Lb list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Lb list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Ld list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Ld list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"delL list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"delL list\"].tolist() for val in item])\n",
    ")\n",
    "\n",
    "merged_timeseries_df[\"phenotype trenchids\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "merged_timeseries_df[\"Gene\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"Gene\"].iloc[0]\n",
    ")\n",
    "merged_timeseries_df[\"N Mismatch\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"N Mismatch\"].iloc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382",
   "metadata": {},
   "source": [
    "##### Lowess regression of Growth/Division timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def timeseries_lowess_reg(df, t_label, y_label, min_tpt, max_tpt, bins, frac=0.33):\n",
    "    del_tpt = max_tpt - min_tpt\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins, dtype=float)\n",
    "\n",
    "    def lowess_reg(x_arr, y_arr, start=min_tpt, end=max_tpt, bins=bins, frac=frac):\n",
    "        intervals = np.linspace(start, end, num=bins, dtype=float)\n",
    "        w = lowess(y_arr, x_arr, frac=frac, xvals=intervals, it=1)\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    lowess_result = df.apply(\n",
    "        lambda x: lowess_reg(x[t_label], x[y_label])[1], axis=1, meta=float\n",
    "    )\n",
    "\n",
    "    df[\"LOWESS Trace: \" + y_label] = lowess_result\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_timeseries_df_dask = dd.from_pandas(merged_timeseries_df, chunksize=50).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tpt, max_tpt = 0, 143\n",
    "bins = 5\n",
    "frac = 0.4\n",
    "\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Mean Area Increment list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Mean Length Increment list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Mean mCherry Intensity list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Mean Width list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Delta t list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Sb list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"delS list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"Lb list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")\n",
    "merged_timeseries_df_dask = timeseries_lowess_reg(\n",
    "    merged_timeseries_df_dask,\n",
    "    \"final cell timepoints\",\n",
    "    \"delL list\",\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_timeseries_df_final = merged_timeseries_df_dask.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 147\n",
    "plt.plot(merged_timeseries_df_final[\"LOWESS Trace: Delta t list\"][idx])\n",
    "plt.scatter(\n",
    "    (\n",
    "        (merged_timeseries_df_final[\"final cell timepoints\"][idx] / (max_tpt - min_tpt))\n",
    "        * (bins - 1)\n",
    "    ),\n",
    "    merged_timeseries_df_final[\"Delta t list\"][idx],\n",
    ")\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388",
   "metadata": {},
   "source": [
    "#### Filtering out \"Normal\" Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perc_exclusion = 60\n",
    "\n",
    "# std_dev = merged_timeseries_df_final[\"LOWESS Trace: Mean Growth Increment list\"].apply(lambda x: np.nanstd(x))\n",
    "# std_dev_high_perc = np.nanpercentile(std_dev,perc_exclusion)\n",
    "# std_dev_mask = std_dev>std_dev_high_perc\n",
    "\n",
    "# perc_recovery = np.sum(std_dev_mask)/len(std_dev_mask)\n",
    "# print(perc_recovery)\n",
    "\n",
    "# merged_timeseries_df_final_masked = merged_timeseries_df_final[std_dev_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391",
   "metadata": {},
   "outputs": [],
   "source": [
    "d
ef euclidean_timeseries_clustering(df, timseries_label, n_clusters=3):\n",
    "    timeseries_arr = np.array(df[timseries_label].tolist())\n",
    "    km = TimeSeriesKMeans(\n",
    "        n_clusters=n_clusters, metric=\"euclidean\", max_iter=5, random_state=0\n",
    "    ).fit(timeseries_arr)\n",
    "    dist_to_center = km.transform(timeseries_arr)\n",
    "    dist_to_center = np.min(dist_to_center, axis=1)\n",
    "    predicted_clust = km.predict(timeseries_arr)\n",
    "    unique, counts = np.unique(predicted_clust, return_counts=True)\n",
    "\n",
    "    return km, predicted_clust, dist_to_center, unique, counts\n",
    "\n",
    "\n",
    "def plot_clustering(\n",
    "    df,\n",
    "    km,\n",
    "    counts,\n",
    "    PCA_label,\n",
    "    t_label,\n",
    "    y_label,\n",
    "    min_tpt=min_tpt,\n",
    "    max_tpt=max_tpt,\n",
    "    bins=bins,\n",
    "    principle_pairs=[[0, 1]],\n",
    "    scatter_s=3,\n",
    "    scatter_alpha=0.7,\n",
    "    cmap=\"Set1\",\n",
    "    indices_of_interest=[],\n",
    "):\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins, dtype=float)\n",
    "    n_plots = 2 + len(principle_pairs)\n",
    "    fig, ax = plt.subplots(nrows=n_plots, ncols=1, figsize=(16, 16 * n_plots))\n",
    "    cmap_colors = plt.get_cmap(cmap).colors\n",
    "\n",
    "    for i in range(km.n_clusters):\n",
    "        ax[0].plot(\n",
    "            intervals,\n",
    "            km.cluster_centers_[i, :, 0],\n",
    "            label=\"Cluster \" + str(i),\n",
    "            linewidth=2,\n",
    "            color=cmap_colors[i % len(cmap_colors)],\n",
    "        )  # ,color=\"#56B4E9\")\n",
    "    # ax[0].plot(intervals,kmeans_lowess.cluster_centers_.T[:,1],label=\"Cluster 1\",linewidth=2,color=\"#E69F00\")\n",
    "    # ax[0].plot(intervals,kmeans_lowess.cluster_centers_.T[:,2],label=\"Cluster 2\",linewidth=2,color=\"#009E73\")\n",
    "    # ax[0].plot(kmeans_area.cluster_centers_.T[:,3],label=\"Cluster 3\",linewidth=2,color=\"#CC79A7\")\n",
    "    ax[0].set_xlabel(t_label, fontsize=20)\n",
    "    ax[0].tick_params(labelsize=20)\n",
    "    ax[0].set_ylabel(y_label, fontsize=20)\n",
    "    ax[0].legend(fontsize=20)\n",
    "    # ax[0].set_ylim(0.7,1.5)\n",
    "    # ax[0].set_xlim(0.7,1.5)\n",
    "\n",
    "    ax[1].bar(\n",
    "        [str(i) for i in range(0, km.n_clusters)],\n",
    "        counts,\n",
    "        bottom=range(0, km.n_clusters),\n",
    "        linewidth=1,\n",
    "        color=[cmap_colors[i % len(cmap_colors)] for i in range(km.n_clusters)],\n",
    "    )  # color=[\"#56B4E9\",\"#E69F00\",\"#009E73\"])\n",
    "    ax[1].set_xlabel(\"Cluster #\", fontsize=20)\n",
    "    ax[1].tick_params(labelsize=20)\n",
    "    ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "    ax[1].set_ylabel(\"N sgRNAs\", fontsize=20)\n",
    "    # plt.savefig(\"2021-07-13_cluster_fig.png\",dpi=300)\n",
    "\n",
    "    category_colors = np.array(\n",
    "        [cmap_colors[i % len(cmap_colors)] for i in range(km.n_clusters)]\n",
    "    )[df[\"Predicted Cluster\"].tolist()]\n",
    "\n",
    "    for i, principle_pair in enumerate(principle_pairs):\n",
    "        x_principle = np.array(\n",
    "            df[PCA_label].apply(lambda x: x[principle_pair[0]]).tolist()\n",
    "        )\n",
    "        y_principle = np.array(\n",
    "            df[PCA_label].apply(lambda x: x[principle_pair[1]]).tolist()\n",
    "        )\n",
    "        ax[i + 2].scatter(\n",
    "            x_principle,\n",
    "            y_principle,\n",
    "            s=scatter_s,\n",
    "            alpha=scatter_alpha,\n",
    "            c=category_colors,\n",
    "        )\n",
    "        if len(indices_of_interest) != 0:\n",
    "            ax[i + 2].scatter(\n",
    "                x_principle[indices_of_interest],\n",
    "                y_principle[indices_of_interest],\n",
    "                s=20,\n",
    "                alpha=1,\n",
    "                c=\"none\",\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "\n",
    "        ax[i + 2].tick_params(labelsize=20)\n",
    "        ax[i + 2].set_xlabel(\"PC \" + str(principle_pair[0] + 1), fontsize=20)\n",
    "        ax[i + 2].set_ylabel(\"PC \" + str(principle_pair[1] + 1), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_nan_mask = merged_timeseries_df_final[\n",
    "    \"LOWESS Trace: Mean Length Increment list\"\n",
    "].apply(lambda x: ~np.any(np.isnan(x)))\n",
    "no_neg_mask = merged_timeseries_df_final[\n",
    "    \"LOWESS Trace: Mean Length Increment list\"\n",
    "].apply(lambda x: ~np.any(x < 0.0))\n",
    "no_high_mask = merged_timeseries_df_final[\n",
    "    \"LOWESS Trace: Mean Length Increment list\"\n",
    "].apply(lambda x: ~np.any(x > 1.5))\n",
    "filter_mask = no_nan_mask * no_neg_mask * no_high_mask\n",
    "merged_timeseries_df_mean_length = merged_timeseries_df_final[filter_mask].reset_index(\n",
    "    drop=False\n",
    ")\n",
    "\n",
    "idx_of_interest = merged_timeseries_df_mean_length[\n",
    "    merged_timeseries_df_mean_length.apply(lambda x: \"rpo\" in str(x[\"Gene\"]), axis=1)\n",
    "].index.values\n",
    "\n",
    "km, predicted_clust, dist_to_center, unique, counts = euclidean_timeseries_clustering(\n",
    "    merged_timeseries_df_mean_length,\n",
    "    \"LOWESS Trace: Mean Length Increment list\",\n",
    "    n_clusters=6,\n",
    ")\n",
    "\n",
    "merged_timeseries_df_mean_length[\"Predicted Cluster\"] = predicted_clust\n",
    "merged_timeseries_df_mean_length[\"Distance to Centroid\"] = dist_to_center\n",
    "\n",
    "PCA_output = skl.decomposition.PCA().fit_transform(\n",
    "    np.array(\n",
    "        merged_timeseries_df_mean_length[\n",
    "            \"LOWESS Trace: Mean Length Increment list\"\n",
    "        ].tolist()\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df_mean_length[\"PCA of LOWESS Trace: Mean Length Increment list\"] = [\n",
    "    PCA_output[i] for i in range(PCA_output.shape[0])\n",
    "]\n",
    "\n",
    "plot_clustering(\n",
    "    merged_timeseries_df_mean_length,\n",
    "    km,\n",
    "    counts,\n",
    "    \"PCA of LOWESS Trace: Mean Length Increment list\",\n",
    "    \"Time ($\\Delta$ t = 4 mins)\",\n",
    "    \"Average Growth Rate $\\dfrac{\\mu}{4 mins}$\",\n",
    "    principle_pairs=[[0, 1], [0, 2], [1, 2]],\n",
    "    indices_of_interest=idx_of_interest,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean Width list\"].apply(\n",
    "    lambda x: ~np.any(np.isnan(x))\n",
    ")\n",
    "# no_neg_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean Area Increment list\"].apply(lambda x: ~np.any(x<0.))\n",
    "# no_high_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean Area Increment list\"].apply(lambda x: ~np.any(x>1.5))\n",
    "filter_mask = no_nan_mask  # *no_neg_mask*no_high_mask\n",
    "merged_timeseries_df_mean_width = merged_timeseries_df_final[filter_mask].reset_index(\n",
    "    drop=False\n",
    ")\n",
    "\n",
    "idx_of_interest = merged_timeseries_df_mean_width[\n",
    "    merged_timeseries_df_mean_width.apply(lambda x: \"rpo\" in str(x[\"Gene\"]), axis=1)\n",
    "].index.values\n",
    "\n",
    "km, predicted_clust, dist_to_center, unique, counts = euclidean_timeseries_clustering(\n",
    "    merged_timeseries_df_mean_width, \"LOWESS Trace: Mean Width list\", n_clusters=8\n",
    ")\n",
    "\n",
    "merged_timeseries_df_mean_width[\"Predicted Cluster\"] = predicted_clust\n",
    "merged_timeseries_df_mean_width[\"Distance to Centroid\"] = dist_to_center\n",
    "\n",
    "PCA_output = skl.decomposition.PCA().fit_transform(\n",
    "    np.array(merged_timeseries_df_mean_width[\"LOWESS Trace: Mean Width list\"].tolist())\n",
    ")\n",
    "merged_timeseries_df_mean_width[\"PCA of LOWESS Trace: Mean Width list\"] = [\n",
    "    PCA_output[i] for i in range(PCA_output.shape[0])\n",
    "]\n",
    "\n",
    "plot_clustering(\n",
    "    merged_timeseries_df_mean_width,\n",
    "    km,\n",
    "    counts,\n",
    "    \"PCA of LOWESS Trace: Mean Width list\",\n",
    "    \"Time ($\\Delta$ t = 4 mins)\",\n",
    "    \"Average Width ($\\mu$)\",\n",
    "    principle_pairs=[[0, 1], [0, 2], [1, 2]],\n",
    "    indices_of_interest=idx_of_interest,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perc_exclusion = 30\n",
    "\n",
    "# std_dev = merged_timeseries_df_final[\"LOWESS Trace: Delta t list\"].apply(lambda x: np.nanstd(x))\n",
    "# std_dev_high_perc = np.nanpercentile(std_dev,perc_exclusion)\n",
    "# std_dev_mask = std_dev>std_dev_high_perc\n",
    "\n",
    "# perc_recovery = np.sum(std_dev_mask)/len(std_dev_mask)\n",
    "# print(perc_recovery)\n",
    "\n",
    "# merged_timeseries_df_final_masked = merged_timeseries_df_final[std_dev_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_nan_mask = merged_timeseries_df_final[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: ~np.any(np.isnan(x))\n",
    ")\n",
    "no_neg_mask = merged_timeseries_df_final[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: ~np.any(x < 0.0)\n",
    ")\n",
    "no_high_mask = merged_timeseries_df_final[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: ~np.any(x > 30)\n",
    ")\n",
    "filter_mask = no_nan_mask * no_neg_mask * no_high_mask\n",
    "merged_timeseries_df_mean_delt = merged_timeseries_df_final[filter_mask].reset_index(\n",
    "    drop=False\n",
    ")\n",
    "\n",
    "idx_of_interest = merged_timeseries_df_mean_delt[\n",
    "    merged_timeseries_df_mean_delt.apply(lambda x: \"rps\" in str(x[\"Gene\"]), axis=1)\n",
    "].index.values\n",
    "\n",
    "km, predicted_clust, dist_to_center, unique, counts = euclidean_timeseries_clustering(\n",
    "    merged_timeseries_df_mean_delt, \"LOWESS Trace: Delta t list\", n_clusters=8\n",
    ")\n",
    "\n",
    "merged_timeseries_df_mean_delt[\"Predicted Cluster\"] = predicted_clust\n",
    "merged_timeseries_df_mean_delt[\"Distance to Centroid\"] = dist_to_center\n",
    "\n",
    "PCA_output = skl.decomposition.PCA().fit_transform(\n",
    "    np.array(merged_timeseries_df_mean_delt[\"LOWESS Trace: Delta t list\"].tolist())\n",
    ")\n",
    "merged_timeseries_df_mean_delt[\"PCA of LOWESS Trace: Delta t list\"] = [\n",
    "    PCA_output[i] for i in range(PCA_output.shape[0])\n",
    "]\n",
    "\n",
    "plot_clustering(\n",
    "    merged_timeseries_df_mean_delt,\n",
    "    km,\n",
    "    counts,\n",
    "    \"PCA of LOWESS Trace: Delta t list\",\n",
    "    \"Time ($\\Delta$ t = 4 mins)\",\n",
    "    \"Average Delta t of division\",\n",
    "    principle_pairs=[[0, 1], [0, 2], [1, 2]],\n",
    "    indices_of_interest=idx_of_interest,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_mask = merged_timeseries_df_final[\n",
    "    \"LOWESS Trace: Mean mCherry Intensity list\"\n",
    "].apply(lambda x: ~np.any(np.isnan(x)))\n",
    "# no_neg_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean mCherry Intensity list\"].apply(lambda x: ~np.any(x<0.))\n",
    "# no_high_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean mCherry Intensity list\"].apply(lambda x: ~np.any(x>30))\n",
    "filter_mask = no_nan_mask  # *no_neg_mask*no_high_mask\n",
    "merged_timeseries_df_mean_mchy = merged_timeseries_df_final[filter_mask].reset_index(\n",
    "    drop=False\n",
    ")\n",
    "\n",
    "idx_of_interest = merged_timeseries_df_mean_mchy[\n",
    "    merged_timeseries_df_mean_mchy.apply(lambda x: \"rps\" in str(x[\"Gene\"]), axis=1)\n",
    "].index.values\n",
    "\n",
    "km, predicted_clust, dist_to_center, unique, counts = euclidean_timeseries_clustering(\n",
    "    merged_timeseries_df_mean_mchy,\n",
    "    \"LOWESS Trace: Mean mCherry Intensity list\",\n",
    "    n_clusters=3,\n",
    ")\n",
    "\n",
    "merged_timeseries_df_mean_mchy[\"Predicted Cluster\"] = predicted_clust\n",
    "merged_timeseries_df_mean_mchy[\"Distance to Centroid\"] = dist_to_center\n",
    "\n",
    "PCA_output = skl.decomposition.PCA().fit_transform(\n",
    "    np.array(\n",
    "        merged_timeseries_df_mean_mchy[\n",
    "            \"LOWESS Trace: Mean mCherry Intensity list\"\n",
    "        ].tolist()\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df_mean_mchy[\"PCA of LOWESS Trace: Mean mCherry Intensity list\"] = [\n",
    "    PCA_output[i] for i in range(PCA_output.shape[0])\n",
    "]\n",
    "\n",
    "plot_clustering(\n",
    "    merged_timeseries_df_mean_mchy,\n",
    "    km,\n",
    "    counts,\n",
    "    \"PCA of LOWESS Trace: Mean mCherry Intensity list\",\n",
    "    \"Time ($\\Delta$ t = 4 mins)\",\n",
    "    \"Average mCherry Intensity\",\n",
    "    principle_pairs=[[0, 1], [0, 2], [1, 2]],\n",
    "    indices_of_interest=idx_of_interest,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "normalizer = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0)\n",
    "\n",
    "spliced_trace = merged_timeseries_df_final.apply(\n",
    "    lambda x: np.stack(\n",
    "        [\n",
    "            x[\"LOWESS Trace: Mean mCherry Intensity list\"],\n",
    "            x[\"LOWESS Trace: Mean Width list\"],\n",
    "            x[\"LOWESS Trace: Delta t list\"],\n",
    "            x[\"LOWESS Trace: Mean Length Increment list\"],\n",
    "        ]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "spliced_trace = np.array(spliced_trace.tolist()).swapaxes(1, 2)\n",
    "spliced_trace = normalizer.fit_transform(spliced_trace).reshape(\n",
    "    spliced_trace.shape[0], -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliced_trace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = merged_timeseries_df_spliced_trace[\n",
    "    merged_timeseries_df_spliced_trace[\"Predicted Cluster\"] == 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(test[\"Gene\"].tolist(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique[counts > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_timeseries_df_spliced_trace = merged_timeseries_df_final\n",
    "merged_timeseries_df_spliced_trace[\"Spliced Trace\"] = [\n",
    "    spliced_trace[i] for i in range(spliced_trace.shape[0])\n",
    "]\n",
    "no_nan_mask = merged_timeseries_df_spliced_trace[\"Spliced Trace\"].apply(\n",
    "    lambda x: ~np.any(np.isnan(x))\n",
    ")\n",
    "# no_neg_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean mCherry Intensity list\"].apply(lambda x: ~np.any(x<0.))\n",
    "# no_high_mask = merged_timeseries_df_final[\"LOWESS Trace: Mean mCherry Intensity list\"].apply(lambda x: ~np.any(x>30))\n",
    "filter_mask = no_nan_mask  # *no_neg_mask*no_high_mask\n",
    "merged_timeseries_df_spliced_trace = merged_timeseries_df_spliced_trace[\n",
    "    filter_mask\n",
    "].reset_index(drop=False)\n",
    "\n",
    "idx_of_interest = merged_timeseries_df_spliced_trace[\n",
    "    merged_timeseries_df_spliced_trace.apply(lambda x: \"fts\" in str(x[\"Gene\"]), axis=1)\n",
    "].index.values\n",
    "\n",
    "km, predicted_clust, dist_to_center, unique, counts = euclidean_timeseries_clustering(\n",
    "    merged_timeseries_df_spliced_trace, \"Spliced Trace\", n_clusters=10\n",
    ")\n",
    "\n",
    "merged_timeseries_df_spliced_trace[\"Predicted Cluster\"] = predicted_clust\n",
    "merged_timeseries_df_spliced_trace[\"Distance to Centroid\"] = dist_to_center\n",
    "\n",
    "PCA_output = skl.decomposition.PCA().fit_transform(\n",
    "    np.array(merged_timeseries_df_spliced_trace[\"Spliced Trace\"].tolist())\n",
    ")\n",
    "merged_timeseries_df_spliced_trace[\"PCA of Spliced Trace\"] = [\n",
    "    PCA_output[i] for i in range(PCA_output.shape[0])\n",
    "]\n",
    "\n",
    "plot_clustering(\n",
    "    merged_timeseries_df_spliced_trace,\n",
    "    km,\n",
    "    counts,\n",
    "    \"PCA of Spliced Trace\",\n",
    "    \"Time ($\\Delta$ t = 4 mins)\",\n",
    "    \"Average mCherry Intensity\",\n",
    "    principle_pairs=[[0, 1], [0, 2], [0, 3], [0, 4]],\n",
    "    indices_of_interest=idx_of_interest,\n",
    "    min_tpt=0,\n",
    "    max_tpt=20,\n",
    "    bins=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403",
   "metadata": {},
   "source": [
    "#### Non-euclidian clustering to discover transient perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tslearn.shapelets import LearningShapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_exclusion = 60\n",
    "\n",
    "std_dev = merged_timeseries_df_final[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: np.nanstd(x)\n",
    ")\n",
    "std_dev_high_perc = np.nanpercentile(std_dev, perc_exclusion)\n",
    "std_dev_mask = std_dev > std_dev_high_perc\n",
    "\n",
    "perc_recovery = np.sum(std_dev_mask) / len(std_dev_mask)\n",
    "print(perc_recovery)\n",
    "\n",
    "merged_timeseries_df_final_masked = merged_timeseries_df_final[std_dev_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_nan_mask = merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].apply(lambda x: ~np.any(np.isnan(x)))\n",
    "no_neg_mask = merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].apply(lambda x: ~np.any(x<0.))\n",
    "no_high_mask = merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].apply(lambda x: ~np.any(x>30)\n",
    "filter_mask=no_nan_mask*no_neg_mask*no_high_mask\n",
    "merged_timeseries_df_knn = merged_timeseries_df_final_masked[filter_mask].reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_arr = np.array(\n",
    "    merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowess_arr = TimeSeriesScalerMeanVariance().fit_transform(lowess_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = TimeSeriesKMeans(\n",
    "    n_clusters=3, metric=\"dtw\", max_iter=3, max_iter_barycenter=5, random_state=0\n",
    ").fit(lowess_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411",
   "metadata": {},
   "source": [
    "#### K Nearent Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.neighbors import KNeighborsTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_mask = merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: ~np.any(np.isnan(x))\n",
    ")\n",
    "no_neg_mask = merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: ~np.any(x < 0.0)\n",
    ")\n",
    "no_high_mask = merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].apply(\n",
    "    lambda x: ~np.any(x > 30)\n",
    ")\n",
    "filter_mask = no_nan_mask * no_neg_mask * no_high_mask\n",
    "merged_timeseries_df_knn = merged_timeseries_df_final_masked[filter_mask].reset_index(\n",
    "    drop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_arr = np.array(merged_timeseries_df_knn[\"LOWESS Trace: Delta t list\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_interest = merged_timeseries_df_knn[\n",
    "    merged_timeseries_df_knn.apply(lambda x: \"fts\" in str(x[\"Gene\"]), axis=1)\n",
    "].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_queries = 2\n",
    "n_neighbors = 4\n",
    "\n",
    "knn = KNeighborsTimeSeries(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "knn.fit(lowess_arr)\n",
    "ind = knn.kneighbors(lowess_arr[idx_of_interest], return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_timeseries_df_knn.loc[ind[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_arr = np.array(\n",
    "    merged_timeseries_df_final_masked[\"LOWESS Trace: Delta t list\"].tolist()\n",
    ")\n",
    "lowess_arr = TimeSeriesScalerMeanVariance().fit_transform(lowess_arr)\n",
    "km = TimeSeriesKMeans(\n",
    "    n_clusters=3, metric=\"dtw\", max_iter=5, max_iter_barycenter=5, random_state=0\n",
    ").fit(lowess_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km.cluster_centers_[:, :, 0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422",
   "metadata": {},
   "source": [
    "### Growth/Div (time dependent with bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=25,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427",
   "metadata": {},
   "source": [
    "#### Binned Plots for Exemplary Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/lDE20_Lineage_Analysis_with_time.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd_groupby = final_output_df_pd.groupby(\"sgRNA\")\n",
    "\n",
    "merged_timeseries_df = (\n",
    "    final_output_df_pd_groupby.apply(\n",
    "        lambda x: np.array(\n",
    "            [val for item in x[\"final cell timepoints list\"].tolist() for val in item]\n",
    "        )\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"final cell timepoints\"})\n",
    ")\n",
    "merged_timeseries_df[\"init cell timepoints\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"cell timepoints list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Delta t list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Delta t list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Mean mCherry Intensity list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"Mean mCherry Intensity list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Mean Width list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Mean Width list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\n",
    "    \"Mean mCherry Promoter Activity list (area normed)\"\n",
    "] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [\n",
    "            val\n",
    "            for item in x[\"Mean mCherry Promoter Activity list (area normed)\"].tolist()\n",
    "            for val in item\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\n",
    "    \"Mean mCherry Promoter Activity list (length normed)\"\n",
    "] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [\n",
    "            val\n",
    "            for item in x[\n",
    "                \"Mean mCherry Promoter Activity list (length normed)\"\n",
    "            ].tolist()\n",
    "            for val in item\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Mean Area Increment list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"Mean Area Increment list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "merged_timeseries_df[\"Mean Length Increment list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array(\n",
    "        [val for item in x[\"Mean Length Increment list\"].tolist() for val in item]\n",
    "    )\n",
    ")\n",
    "\n",
    "merged_timeseries_df[\"Sb list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Sb list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Sd list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Sd list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"delS list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"delS list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Lb list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Lb list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"Ld list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"Ld list\"].tolist() for val in item])\n",
    ")\n",
    "merged_timeseries_df[\"delL list\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: np.array([val for item in x[\"delL list\"].tolist() for val in item])\n",
    ")\n",
    "\n",
    "merged_timeseries_df[\"phenotype trenchids\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "merged_timeseries_df[\"Gene\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"Gene\"].iloc[0]\n",
    ")\n",
    "merged_timeseries_df[\"TargetID\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"TargetID\"].iloc[0]\n",
    ")\n",
    "merged_timeseries_df[\"N Mismatch\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"N Mismatch\"].iloc[0]\n",
    ")\n",
    "merged_timeseries_df[\"Category\"] = final_output_df_pd_groupby.apply(\n",
    "    lambda x: x[\"Category\"].iloc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_timeseries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_bins(df, t_label, y_labels, min_tpt, max_tpt, bins):\n",
    "    del_tpt = max_tpt - min_tpt\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins, dtype=float)\n",
    "\n",
    "    timeseries_bin_dfs = []\n",
    "\n",
    "    for i in range(len(intervals) - 1):\n",
    "        bin_df = {}\n",
    "        bin_df[\"Bin Mean Timepoint\"] = np.mean(intervals[i : i + 2])\n",
    "        bin_df[\"Timepoint Bin\"] = i\n",
    "        for y_label in y_labels:\n",
    "            bin_df[y_label + \": Bin Values\"] = df.apply(\n",
    "                lambda x: x[y_label][\n",
    "                    (x[t_label] >= intervals[i]) & (x[t_label] < intervals[i + 1])\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            bin_df[y_label + \": Mean\"] = bin_df[y_label + \": Bin Values\"].apply(\n",
    "                lambda x: np.mean(x)\n",
    "            )\n",
    "            bin_df[y_label + \": Standard Deviation\"] = bin_df[\n",
    "                y_label + \": Bin Values\"\n",
    "            ].apply(lambda x: np.std(x))\n",
    "            bin_df[y_label + \": CV\"] = (\n",
    "                bin_df[y_label + \": Standard Deviation\"] / bin_df[y_label + \": Mean\"]\n",
    "            )\n",
    "\n",
    "        timeseries_bin_df = pd.DataFrame(bin_df)\n",
    "        timeseries_bin_df.index = df.index\n",
    "        timeseries_bin_dfs.append(timeseries_bin_df)\n",
    "    timeseries_bin_df_output = (\n",
    "        pd.concat(timeseries_bin_dfs).join(df).set_index(\"sgRNA\").sort_index()\n",
    "    )\n",
    "\n",
    "    return timeseries_bin_df_output\n",
    "\n",
    "\n",
    "def filter_strong_KOs(df, sampling_thr=3, n_strongest=2):\n",
    "    for i in range(sampling_thr, 0, -1):\n",
    "        sampling_mask = df[\"N Mismatch\"] >= sampling_thr\n",
    "        mismatch_series = df[sampling_mask][\"N Mismatch\"]\n",
    "\n",
    "        for n in range(n_strongest + 1, 0, -1):\n",
    "            if len(mismatch_series) >= n_strongest:\n",
    "                keep_indices = np.argsort(mismatch_series)[:n_strongest]\n",
    "                out_df = df.iloc[keep_indices]\n",
    "\n",
    "                return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_timeseries_index_df = merged_timeseries_df.reset_index(drop=False)\n",
    "merged_timeseries_index_df_best_KO = (\n",
    "    merged_timeseries_index_df.groupby([\"TargetID\"])\n",
    "    .apply(lambda x: filter_strong_KOs(x))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "no_nan_mask = merged_timeseries_index_df_best_KO[\"sgRNA\"].apply(\n",
    "    lambda x: type(x) == str\n",
    ")\n",
    "merged_timeseries_index_df_best_KO = merged_timeseries_index_df_best_KO[no_nan_mask]\n",
    "merged_timeseries_index_df_best_KO = pd.concat(\n",
    "    [\n",
    "        merged_timeseries_index_df_best_KO,\n",
    "        merged_timeseries_df[merged_timeseries_df[\"Category\"] != \"Target\"].reset_index(\n",
    "            drop=False\n",
    "        ),\n",
    "    ]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tpt, max_tpt, bins = 0, 143, 10\n",
    "\n",
    "timeseries_bin_df_output = timeseries_bins(\n",
    "    merged_timeseries_index_df_best_KO,\n",
    "    \"final cell timepoints\",\n",
    "    [\n",
    "        \"Lb list\",\n",
    "        \"Delta t list\",\n",
    "        \"Mean mCherry Intensity list\",\n",
    "        \"Mean Length Increment list\",\n",
    "        \"Mean Area Increment list\",\n",
    "        \"Mean Width list\",\n",
    "        \"Mean mCherry Promoter Activity list (area normed)\",\n",
    "        \"Mean mCherry Promoter Activity list (length normed)\",\n",
    "    ],\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timeseries_bin_df_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_timeseries_index_df_best_KO_sgRNA_sorted = merged_timeseries_index_df_best_KO.set_index(\"sgRNA\")\n",
    "# # indices = merged_timeseries_index_df_best_KO_sgRNA_sorted[merged_timeseries_index_df_best_KO_sgRNA_sorted.apply(lambda x: \"ftsL\" in str(x[\"Gene\"]), axis=1)].index.tolist()\n",
    "# indices = merged_timeseries_index_df_best_KO_sgRNA_sorted.index.tolist()\n",
    "\n",
    "# violin_bins = []\n",
    "# for i in range(bins-1):\n",
    "#     timeseries_bin = timeseries_bin_dfs[i].loc[indices][[\"Lb list: Mean\",'Delta t list: Mean',\"Gene\"]]\n",
    "# #     timeseries_bin = [val for item in timeseries_bin_dfs[i].loc[indices][\"Bin Values\"] for val in item]\n",
    "#     timeseries_bin[\"Timepoint Bin\"] = i\n",
    "\n",
    "#     violin_bins.append(timeseries_bin)\n",
    "# violin_bins = pd.concat(violin_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.violinplot(\n",
    "    data=violin_bins,\n",
    "    x=\"Timepoint Bin\",\n",
    "    y=\"Lb list: Mean\",\n",
    ")\n",
    "# plt.ylim(0,15)\n",
    "# sns.scatterplot(data=violin_bins,x=\"Timepoint Bin\",y=\"Mean\",)\n",
    "# sns.lineplot(data=violin_bins,x=\"Timepoint Bin\",y=\"Mean\",hue=\"Gene\",)\n",
    "# sns.lineplot(data=violin_bins,x=\"Timepoint Bin\",y=\"Mean\",hue=\"sgRNA\")\n",
    "# plt.ylim(0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "        \"Lb list: Mean\"\n",
    "    ],\n",
    "    bins=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_timepoint = 4\n",
    "max_timepoint = 8\n",
    "value_min = 4\n",
    "value_max = 12\n",
    "n_timepoints_min = 2\n",
    "\n",
    "\n",
    "def filter_binned_timeseries_df(\n",
    "    df, label, min_timepoint, max_timepoint, value_min, value_max, n_timepoints_min\n",
    "):\n",
    "    timepoint_mask = (df[\"Timepoint Bin\"] >= min_timepoint) & (\n",
    "        df[\"Timepoint Bin\"] <= max_timepoint\n",
    "    )\n",
    "    masked_df = df[timepoint_mask]\n",
    "    value_mask = masked_df.groupby(\"sgRNA\").apply(\n",
    "        lambda x: np.sum((x[label] >= value_min) & (x[label] <= value_max))\n",
    "        >= n_timepoints_min\n",
    "    )\n",
    "    sg_RNA_hits = masked_df[value_mask].index.tolist()\n",
    "    output_df = df.loc[sg_RNA_hits]\n",
    "    return output_df\n",
    "\n",
    "\n",
    "# hits = violin_bins.groupby(\"sgRNA\").apply(lambda x: (np.sum(x[x[\"Timepoint Bin\"]<7][\"Mean\"]>4)>1)&(np.sum(x[x[\"Timepoint Bin\"]>=7][\"Mean\"]<3)>1))\n",
    "hits = filter_binned_timeseries_df(\n",
    "    timeseries_bin_df_output,\n",
    "    \"Lb list: Mean\",\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    value_min,\n",
    "    value_max,\n",
    "    n_timepoints_min,\n",
    ")\n",
    "\n",
    "# hits = time_window[(time_window[\"Mean\"]>4)&(violin_bins[\"Timepoint Bin\"]<7)]\n",
    "gene_list = hits[\"Gene\"].tolist()\n",
    "unique, counts = np.unique(gene_list, return_counts=True)\n",
    "filamenting_genes = unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440",
   "metadata": {},
   "outputs": [],
   "source": [
    "filamenting_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "        \"Delta t list: Mean\"\n",
    "    ],\n",
    "    range=(0, 15),\n",
    "    bins=50,\n",
    ")\n",
    "plt.xlim(0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timepoint = 4\n",
    "max_timepoint = 8\n",
    "value_min = 14\n",
    "value_max = 30\n",
    "n_timepoints_min = 2\n",
    "\n",
    "# hits = violin_bins.groupby(\"sgRNA\").apply(lambda x: (np.sum(x[x[\"Timepoint Bin\"]<7][\"Mean\"]>4)>1)&(np.sum(x[x[\"Timepoint Bin\"]>=7][\"Mean\"]<3)>1))\n",
    "hits = filter_binned_timeseries_df(\n",
    "    timeseries_bin_df_output,\n",
    "    \"Delta t list: Mean\",\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    value_min,\n",
    "    value_max,\n",
    "    n_timepoints_min,\n",
    ")\n",
    "\n",
    "# hits = time_window[(time_window[\"Mean\"]>4)&(violin_bins[\"Timepoint Bin\"]<7)]\n",
    "gene_list = hits[\"Gene\"].tolist()\n",
    "unique, counts = np.unique(gene_list, return_counts=True)\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "        \"Mean mCherry Intensity list: Mean\"\n",
    "    ],\n",
    "    bins=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timepoint = 4\n",
    "max_timepoint = 8\n",
    "value_min = 2500\n",
    "value_max = 100000\n",
    "n_timepoints_min = 3\n",
    "\n",
    "# hits = violin_bins.groupby(\"sgRNA\").apply(lambda x: (np.sum(x[x[\"Timepoint Bin\"]<7][\"Mean\"]>4)>1)&(np.sum(x[x[\"Timepoint Bin\"]>=7][\"Mean\"]<3)>1))\n",
    "hits = filter_binned_timeseries_df(\n",
    "    timeseries_bin_df_output,\n",
    "    \"Mean mCherry Intensity list: Mean\",\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    value_min,\n",
    "    value_max,\n",
    "    n_timepoints_min,\n",
    ")\n",
    "\n",
    "# hits = time_window[(time_window[\"Mean\"]>4)&(violin_bins[\"Timepoint Bin\"]<7)]\n",
    "gene_list = hits[\"Gene\"].tolist()\n",
    "unique, counts = np.unique(gene_list, return_counts=True)\n",
    "bright_genes = unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "    \"Mean Width list\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "        \"Mean Width list\"\n",
    "    ],\n",
    "    bins=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timepoint = 4\n",
    "max_timepoint = 8\n",
    "value_min = 2500\n",
    "value_max = 100000\n",
    "n_timepoints_min = 3\n",
    "\n",
    "# hits = violin_bins.groupby(\"sgRNA\").apply(lambda x: (np.sum(x[x[\"Timepoint Bin\"]<7][\"Mean\"]>4)>1)&(np.sum(x[x[\"Timepoint Bin\"]>=7][\"Mean\"]<3)>1))\n",
    "hits = filter_binned_timeseries_df(\n",
    "    timeseries_bin_df_output,\n",
    "    \"Mean mCherry Intensity list: Mean\",\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    value_min,\n",
    "    value_max,\n",
    "    n_timepoints_min,\n",
    ")\n",
    "\n",
    "# hits = time_window[(time_window[\"Mean\"]>4)&(violin_bins[\"Timepoint Bin\"]<7)]\n",
    "gene_list = hits[\"Gene\"].tolist()\n",
    "unique, counts = np.unique(gene_list, return_counts=True)\n",
    "bright_genes = unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sorted(list(set(filamenting_genes) - set(bright_genes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449",
   "metadata": {},
   "source": [
    "#### Growth vs Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "        \"Lb list: Mean\"\n",
    "    ],\n",
    "    bins=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    timeseries_bin_df_output[timeseries_bin_df_output[\"Timepoint Bin\"] == 8][\n",
    "        \"Mean Length Increment list: Mean\"\n",
    "    ],\n",
    "    bins=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output[\"Category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "control_df = timeseries_bin_df_output[timeseries_bin_df_output[\"Category\"] != \"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "control_df[\"Mean mCherry Intensity list: Mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_mask = ~(\n",
    "    np.isnan(control_df[\"Mean mCherry Intensity list: Mean\"])\n",
    "    | np.isnan(control_df[\"Lb list: Mean\"])\n",
    ")\n",
    "zero_mask = (\n",
    "    (control_df[\"Mean mCherry Intensity list: Mean\"] > 0)\n",
    "    & (control_df[\"Lb list: Mean\"] > 0)\n",
    "    & (control_df[\"Mean Length Increment list: Mean\"] > 0)\n",
    ")\n",
    "no_nan_control = control_df[(nan_mask * zero_mask)]\n",
    "fractional_increment = (\n",
    "    no_nan_control[\"Mean Length Increment list: Mean\"] / no_nan_control[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "\n",
    "r, p = sp.stats.pearsonr(\n",
    "    no_nan_control[\"Mean mCherry Intensity list: Mean\"], fractional_increment\n",
    ")\n",
    "print(r)\n",
    "r, p = sp.stats.pearsonr(no_nan_control[\"Lb list: Mean\"], fractional_increment)\n",
    "print(r)\n",
    "r, p = sp.stats.pearsonr(\n",
    "    no_nan_control[\"Mean mCherry Intensity list: Mean\"], no_nan_control[\"Lb list: Mean\"]\n",
    ")\n",
    "print(r)\n",
    "### Strong negative correlations between mchy intensity, birth size and growth rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_mask = ~(\n",
    "    np.isnan(timeseries_bin_df_output[\"Mean mCherry Intensity list: Mean\"])\n",
    "    | np.isnan(timeseries_bin_df_output[\"Lb list: Mean\"])\n",
    ")\n",
    "zero_mask = (\n",
    "    (timeseries_bin_df_output[\"Mean mCherry Intensity list: Mean\"] > 0)\n",
    "    & (timeseries_bin_df_output[\"Lb list: Mean\"] > 0)\n",
    "    & (timeseries_bin_df_output[\"Mean Length Increment list: Mean\"] > 0)\n",
    ")\n",
    "no_nan_timeseries_bin_df_output = timeseries_bin_df_output[(nan_mask * zero_mask)]\n",
    "no_nan_timeseries_bin_df_output[\"Fractional Increment Scaled\"] = (\n",
    "    no_nan_timeseries_bin_df_output[\"Mean Length Increment list: Mean\"]\n",
    "    / no_nan_timeseries_bin_df_output[\"Lb list: Mean\"]\n",
    ") * (60 / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson_correlation = no_nan_timeseries_bin_df_output.groupby(\"sgRNA\").apply(lambda x: sp.stats.pearsonr(x['Mean mCherry Intensity list: Mean'],x[\"Lb list: Mean\"])[0] if len(x)>5 else None)\n",
    "pearson_correlation = no_nan_timeseries_bin_df_output.groupby(\"sgRNA\").apply(\n",
    "    lambda x: sp.stats.pearsonr(x[\"Lb list: Mean\"], x[\"Fractional Increment Scaled\"])[0]\n",
    "    if len(x) > 5\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(\n",
    "    no_nan_timeseries_bin_df_output.loc[\n",
    "        (pearson_correlation[pearson_correlation > 0.85]).index.tolist()\n",
    "    ][\"Gene\"].tolist(),\n",
    "    return_counts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique[counts > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pearson_correlation, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "fractional_increment = (\n",
    "    control_df[\"Mean Length Increment list: Mean\"] / control_df[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(control_df[\"Lb list: Mean\"], fractional_increment, alpha=0.6, s=4)\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    control_df[\"Mean mCherry Intensity list: Mean\"],\n",
    "    fractional_increment,\n",
    "    alpha=0.6,\n",
    "    s=6,\n",
    ")\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    control_df[\"Mean mCherry Intensity list: Mean\"],\n",
    "    control_df[\"Lb list: Mean\"],\n",
    "    alpha=0.6,\n",
    "    s=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mchy_promoter_activity(df):\n",
    "#     mchy_series = df['Mean mCherry Intensity list: Bin Values']\n",
    "#     length_incr_series = df['Mean Length Increment list: Bin Values']\n",
    "#     birth_len_series = df['Lb list: Bin Values']\n",
    "#     fractional_increment = (length_incr_series/birth_len_series)\n",
    "#     delta_t_series = df['Delta t list: Bin Values']\n",
    "#     del_mchy_i = mchy_series[1:-1]-mchy_series[:-2]\n",
    "#     del_mchy_f = mchy_series[2:]-mchy_series[1:-1]\n",
    "#     del_mchy = (del_mchy_i+del_mchy_f)/2\n",
    "#     promoter_activity = del_mchy + (fractional_increment*delta_t_series*mchy_series)[1:-1]\n",
    "#     return promoter_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464",
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = range(0, 3)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "timeseries_bin_df_output_sub = timeseries_bin_df_output[\n",
    "    timeseries_bin_df_output[\"Timepoint Bin\"].isin(timepoints)\n",
    "]\n",
    "fractional_increment = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"], fractional_increment, alpha=0.5, s=4\n",
    ")\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\n",
    "        \"Mean mCherry Promoter Activity list (area normed): Mean\"\n",
    "    ],\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(-1, 3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\n",
    "        \"Mean mCherry Promoter Activity list (area normed): Mean\"\n",
    "    ],\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465",
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = range(7, 10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "timeseries_bin_df_output_sub = timeseries_bin_df_output[\n",
    "    timeseries_bin_df_output[\"Timepoint Bin\"].isin(timepoints)\n",
    "]\n",
    "fractional_increment = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"], fractional_increment, alpha=0.5, s=4\n",
    ")\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\n",
    "        \"Mean mCherry Promoter Activity list (area normed): Mean\"\n",
    "    ],\n",
    "    fractional_increment,\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\n",
    "        \"Mean mCherry Promoter Activity list (area normed): Mean\"\n",
    "    ],\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output_sub[\"fractional_increment\"] = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / (\n",
    "        (\n",
    "            timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    "            + timeseries_bin_df_output_sub[\"Ld list: Mean\"]\n",
    "        )\n",
    "        / 2\n",
    "    )\n",
    ") * (60 / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\n",
    "        \"Mean mCherry Promoter Activity list (length normed): Mean\"\n",
    "    ],\n",
    "    timeseries_bin_df_output_sub[\"fractional_increment\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(-1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xlim(0,1000)\n",
    "# plt.ylim(-1,8)\n",
    "(\n",
    "    scatter,\n",
    "    trenchid_table,\n",
    "    unpack_trenchid_table,\n",
    "    select_scatter,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_scatter(\n",
    "    timeseries_bin_df_output_sub,\n",
    "    \"Mean mCherry Promoter Activity list (length normed): Mean\",\n",
    "    \"fractional_increment\",\n",
    "    trenchids_as_list=True,\n",
    "    maxperc=90,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    height=600,\n",
    "    logx=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trenchid_table + unpack_trenchid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter_kymograph_display = tr.linked_kymograph_for_scatter(\n",
    "    kymo_xarr,\n",
    "    timeseries_bin_df_output_sub,\n",
    "    \"Mean mCherry Promoter Activity list (length normed): Mean\",\n",
    "    \"fractional_increment\",\n",
    "    select_scatter,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter_kymograph_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474",
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = range(0, 3)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "timeseries_bin_df_output_sub = timeseries_bin_df_output[\n",
    "    timeseries_bin_df_output[\"Timepoint Bin\"].isin(timepoints)\n",
    "]\n",
    "fractional_increment = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"], fractional_increment, alpha=0.5, s=4\n",
    ")\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Intensity list: Mean\"],\n",
    "    fractional_increment,\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Promoter Activity\"],\n",
    "    fractional_increment,\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Intensity list: Mean\"],\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Promoter Activity\"],\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(0, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475",
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = range(6, 9)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "timeseries_bin_df_output_sub = timeseries_bin_df_output[\n",
    "    timeseries_bin_df_output[\"Timepoint Bin\"].isin(timepoints)\n",
    "]\n",
    "timeseries_bin_df_output_sub[\n",
    "    \"Mean mCherry Promoter Activity\"\n",
    "] = timeseries_bin_df_output_sub.apply(\n",
    "    lambda x: np.mean(mchy_promoter_activity(x)), axis=1\n",
    ")\n",
    "fractional_increment = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"], fractional_increment, alpha=0.5, s=4\n",
    ")\n",
    "plt.show()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Intensity list: Mean\"],\n",
    "    fractional_increment,\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(-1000, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Promoter Activity\"],\n",
    "    fractional_increment,\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(-1000, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Intensity list: Mean\"],\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(-1000, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Mean mCherry Promoter Activity\"],\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"],\n",
    "    alpha=0.5,\n",
    "    s=6,\n",
    ")\n",
    "plt.xlim(-1000, 6000)\n",
    "plt.ylim(-1, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mchy_promoter_activity(df):\n",
    "    mchy_series = df[\"Mean mCherry Intensity list: Bin Values\"]\n",
    "    length_incr_series = df[\"Mean Length Increment list: Bin Values\"][1:]\n",
    "    birth_len_series = df[\"Lb list: Bin Values\"][1:]\n",
    "    fractional_increment = length_incr_series / birth_len_series\n",
    "    delta_t_series = df[\"Delta t list: Bin Values\"][1:]\n",
    "    del_mchy = mchy_series[1:] - mchy_series[:-1]\n",
    "    avg_mchy = (mchy_series[1:] + mchy_series[:-1]) / 2\n",
    "\n",
    "    promoter_activity = del_mchy + fractional_increment * delta_t_series * avg_mchy\n",
    "    return promoter_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"Mean Length Increment list: Mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timeseries_bin_df_output_sub[] = timeseries_bin_df_output_sub.apply(lambda x: mchy_promoter_activity(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "fractional_increment = (\n",
    "    control_df[\"Mean Length Increment list: Mean\"] / control_df[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(control_df[\"Lb list: Mean\"], fractional_increment, alpha=0.6, s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = \"folA\"\n",
    "\n",
    "timepoints = range(0, 5)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "timeseries_bin_df_output_sub = timeseries_bin_df_output[\n",
    "    timeseries_bin_df_output[\"Timepoint Bin\"].isin(timepoints)\n",
    "]\n",
    "query = timeseries_bin_df_output_sub[timeseries_bin_df_output_sub[\"Gene\"] == gene]\n",
    "fractional_increment = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "fractional_increment_query = (\n",
    "    query[\"Mean Length Increment list: Mean\"] / query[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"], fractional_increment, alpha=0.6, s=1\n",
    ")\n",
    "plt.scatter(query[\"Lb list: Mean\"], fractional_increment_query, alpha=0.6, s=4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "timepoints = range(5, 9)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "timeseries_bin_df_output_sub = timeseries_bin_df_output[\n",
    "    timeseries_bin_df_output[\"Timepoint Bin\"].isin(timepoints)\n",
    "]\n",
    "query = timeseries_bin_df_output_sub[timeseries_bin_df_output_sub[\"Gene\"] == gene]\n",
    "fractional_increment = (\n",
    "    timeseries_bin_df_output_sub[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output_sub[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "fractional_increment_query = (\n",
    "    query[\"Mean Length Increment list: Mean\"] / query[\"Lb list: Mean\"]\n",
    ") * (60 / 4)\n",
    "plt.scatter(\n",
    "    timeseries_bin_df_output_sub[\"Lb list: Mean\"], fractional_increment, alpha=0.6, s=1\n",
    ")\n",
    "plt.scatter(query[\"Lb list: Mean\"], fractional_increment_query, alpha=0.6, s=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_bin_df_output[\"Mean Length Increment list: Mean\"] / timeseries_bin_df_output[\n",
    "    \"Lb list: Mean\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_increment = (\n",
    "    timeseries_bin_df_output[\"Mean Length Increment list: Mean\"]\n",
    "    / timeseries_bin_df_output[\"Lb list: Mean\"]\n",
    ") * (4 / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484
",
   "metadata": {},
   "source": [
    "### Gene Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_output_df_pd.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "df[\"phenotype trenchids\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    df, trenchids_as_list=True, trenchid_column=\"phenotype trenchids\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    df,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
