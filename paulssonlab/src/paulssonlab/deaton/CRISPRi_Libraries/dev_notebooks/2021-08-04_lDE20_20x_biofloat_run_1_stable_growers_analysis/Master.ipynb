{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "popt## Steady-state Analysis of lDE20 (with lineage Dataframe ready)\n",
    "\n",
    "- Note that there are fluctuations in the illumination intensity which may be resulting in pathological behavior from the reporter\n",
    "\n",
    "- Consider either normalizing this out or fixing the underlying problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask\n",
    "import warnings\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import (\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    "    cosine_distances,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "\n",
    "\n",
    "import pylab\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    masked_label_series = df.apply(\n",
    "        lambda x: np.array(x[label])[\n",
    "            (np.array(x[time_label]) >= min_timepoint)\n",
    "            * (np.array(x[time_label]) <= max_timepoint)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    if flatten_vals:\n",
    "        flattened_vals = [val for item in masked_label_series.tolist() for val in item]\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "def get_feature_stats(df, feature_label, min_timepoint, max_timepoint):\n",
    "    feature_vals = get_timepoint_values(df, feature_label, min_timepoint, max_timepoint)\n",
    "    feature_median = np.median(feature_vals)\n",
    "    feature_iqr = sp.stats.iqr(feature_vals)\n",
    "    return feature_median, feature_iqr\n",
    "\n",
    "\n",
    "def get_feature_median_bytrench(df, feature_label, min_timepoint, max_timepoint):\n",
    "    masked_label_series = get_timepoint_values(\n",
    "        final_output_df_pd_filtered,\n",
    "        feature_label,\n",
    "        min_timepoint,\n",
    "        max_timepoint,\n",
    "        flatten_vals=False,\n",
    "    )\n",
    "    trench_median_series = masked_label_series.apply(lambda x: np.nanmedian(x))\n",
    "    return trench_median_series\n",
    "\n",
    "\n",
    "def compute_score(\n",
    "    df,\n",
    "    feature_label,\n",
    "    trench_median_series,\n",
    "    feature_median,\n",
    "    feature_iqr,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    scaling_factor = 1.35 * (feature_median / feature_iqr)\n",
    "\n",
    "    if timepoint_range == None:\n",
    "        scores = (\n",
    "            (df[feature_label].apply(lambda x: np.array(x))) / trench_median_series\n",
    "        ) - 1.0\n",
    "    else:\n",
    "        scores = (\n",
    "            (\n",
    "                df[feature_label].apply(\n",
    "                    lambda x: np.array(x)[\n",
    "                        (np.array(x[time_label]) >= timepoint_range[0])\n",
    "                        * (np.array(x[time_label]) <= timepoint_range[1])\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            / trench_median_series\n",
    "        ) - 1.0\n",
    "    scores = scaling_factor * scores\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_feature_scores(\n",
    "    df,\n",
    "    feature_label,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    feature_median, feature_iqr = get_feature_stats(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    trench_median_series = get_feature_median_bytrench(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    scores = compute_score(\n",
    "        df,\n",
    "        feature_label,\n",
    "        trench_median_series,\n",
    "        feature_median,\n",
    "        feature_iqr,\n",
    "        time_label=time_label,\n",
    "        timepoint_range=timepoint_range,\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_scores(\n",
    "    df,\n",
    "    feature_labels,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_scores(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "            timepoint_range=timepoint_range,\n",
    "        )\n",
    "        df[feature_label + \": score\"] = feature_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sgrnadf_from_scoredf(\n",
    "    scoredf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    scoredf_groupby = scoredf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        scoredf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": score\"] = scoredf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label + \": score\"].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = scoredf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = scoredf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = scoredf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = scoredf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = scoredf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = scoredf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "# No longer using this\n",
    "# def filter_strong_KOs(df,sampling_thr = 4, n_strongest=2):\n",
    "\n",
    "#     for i in range(sampling_thr,0,-1):\n",
    "#         sampling_mask = df[\"N Observations\"]>=sampling_thr\n",
    "#         mismatch_series = df[sampling_mask][\"N Mismatch\"]\n",
    "\n",
    "#         for n in range(n_strongest,0,-1):\n",
    "#             if len(mismatch_series)>=n:\n",
    "#                 keep_indices = np.argsort(mismatch_series)[:n]\n",
    "#                 out_df = df[sampling_mask].iloc[keep_indices]\n",
    "\n",
    "#                 return out_df\n",
    "\n",
    "\n",
    "def normalize_timeseries(feature_vector_series, lmbda=0.5):\n",
    "    timeseries_arr = np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    sigma = np.std(timeseries_arr, axis=1)\n",
    "    if lmbda > 0.0:\n",
    "        sigma_prime = ((sigma + 1) ** lmbda - 1) / lmbda  ##yeo-johnson\n",
    "    elif lmbda == 0.0:\n",
    "        sigma_prime = np.log(sigma + 1)\n",
    "    else:\n",
    "        raise ValueError(\"lmbda cannot be negative\")\n",
    "    normalizer = sigma / sigma_prime\n",
    "    normalized_timeseries = timeseries_arr / normalizer[:, np.newaxis, :]\n",
    "    return normalized_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_pd = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-07-26_lDE20_Lineage_Analysis.pkl\"\n",
    ")\n",
    "final_output_df_pd = final_output_df_pd[\n",
    "    ~final_output_df_pd[\"final cell timepoints list\"].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for \"Normal\" Sizes at Start\n",
    "\n",
    "1) Fit a gaussian model to each of the specified feature params during the first t timepoints of the experiment (using a subsample for speed) \n",
    "2) Compute a normalized probability trenchwise for these features under the gaussian model, during the first t timepoints of the experiment\n",
    "3) Eliminate trenches that are under some p percentile value of this probability for each feature\n",
    "4) Display histograms for each property as well as the resulting theshold\n",
    "\n",
    "Note that these features should be the only features examined in the resulting analysis. For the notebook, I am looking at:\n",
    "- Birth length (Lb)\n",
    "- Division length (Ld)\n",
    "- Mean Area Increment\n",
    "- Mean Length Increment\n",
    "- Mean Width\n",
    "- Cell cycle duration (Delta t)\n",
    "- Mean mCherry Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_timepoint_cutoff = 30\n",
    "gaussian_subsample = 0.2\n",
    "percentile_threshold = 10\n",
    "\n",
    "filter_params = [\n",
    "    \"Lb list\",\n",
    "    \"Ld list\",\n",
    "    \"Mean Area Increment list\",\n",
    "    \"Mean Length Increment list\",\n",
    "    \"Mean Width list\",\n",
    "    \"Mean mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]\n",
    "\n",
    "final_output_df_pd_dask = dd.from_pandas(final_output_df_pd, npartitions=200).persist()\n",
    "dask.distributed.wait(final_output_df_pd_dask)\n",
    "final_output_df_pd_dask[\"Early Timepoint Mask\"] = final_output_df_pd_dask[\n",
    "    \"cell timepoints list\"\n",
    "].apply(\n",
    "    lambda x: np.array([item if (type(item) is int) else 10000000 for item in x])\n",
    "    < early_timepoint_cutoff,\n",
    "    meta=(None, \"object\"),\n",
    ")\n",
    "\n",
    "for filter_param in filter_params:\n",
    "    early_param_series = final_output_df_pd_dask.apply(\n",
    "        lambda x: np.array(x[filter_param])[x[\"Early Timepoint Mask\"]]\n",
    "        if type(x[filter_param]) is list\n",
    "        else np.array([]),\n",
    "        axis=1,\n",
    "        meta=(None, \"object\"),\n",
    "    )\n",
    "    all_param_values = [\n",
    "        val\n",
    "        for item in early_param_series.sample(frac=gaussian_subsample)\n",
    "        .compute()\n",
    "        .tolist()\n",
    "        for val in item\n",
    "    ]\n",
    "    gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    final_output_df_pd[filter_param + \": Probability\"] = early_param_series.apply(\n",
    "        lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "    ).persist()\n",
    "\n",
    "plt.figure(figsize=(22, 16))\n",
    "query_list = []\n",
    "for i, filter_param in enumerate(filter_params):\n",
    "    prob_threshold = np.nanpercentile(\n",
    "        final_output_df_pd[filter_param + \": Probability\"].tolist(),\n",
    "        percentile_threshold,\n",
    "    )\n",
    "    query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "    query_list.append(query)\n",
    "\n",
    "    min_v, max_v = np.min(final_output_df_pd[filter_param + \": Probability\"]), np.max(\n",
    "        final_output_df_pd[filter_param + \": Probability\"]\n",
    "    )\n",
    "\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.title(filter_param)\n",
    "    plt.hist(\n",
    "        final_output_df_pd[\n",
    "            final_output_df_pd[filter_param + \": Probability\"] < prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "    plt.hist(\n",
    "        final_output_df_pd[\n",
    "            final_output_df_pd[filter_param + \": Probability\"] >= prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "plt.show()\n",
    "\n",
    "compiled_query = \" and \".join(query_list)\n",
    "final_output_df_pd_filtered = final_output_df_pd.query(compiled_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_output_df_pd_filtered) / len(final_output_df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Properties\n",
    "\n",
    "1) Yeo-Johnson transform the data as before (this time I am omitting the label for simplicity).\n",
    "2) Convert transformed values to time-dependent s-scores using the following formula:\n",
    "\n",
    "$$ z(i,k,t) = 1.35 \\times \\frac{median_{t\\in \\tau}(F_{i,t})}{iqr_{t\\in \\tau}(F_{i,t})}\\Bigg(\\frac{F_{i,k,t}}{median_{t\\in \\tau}(F_{i,k,t})} - 1\\Bigg) $$\n",
    "\n",
    "where $F_{i,k,t}$ are the feature values for feature i, trench k at time t. $\\tau$ are the initial pre-induction timepoints. \n",
    "\n",
    "Essentially this is a z-score using the more outlier robust median and interquartile range to define the differences from normal bahavior. The 1.35 factor scales the values such that z-scores represent number of standard deviations from the mean for a normal distribution. Finally the values are normalized by initial behaviors trenchwise by the $median_{t\\in \\tau}(F_{i,k,t})$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Lb list\",\n",
    "    \"Ld list\",\n",
    "    \"delL list\",\n",
    "    \"Mean Area Increment list\",\n",
    "    \"Mean Length Increment list\",\n",
    "    \"Mean Width list\",\n",
    "    \"Mean mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]\n",
    "yeo_subsample = 0.1\n",
    "\n",
    "final_output_df_pd_filtered_dask = dd.from_pandas(\n",
    "    final_output_df_pd_filtered, npartitions=100\n",
    ").persist()\n",
    "dask.distributed.wait(final_output_df_pd_filtered_dask)\n",
    "\n",
    "for i, param in enumerate(params_to_transform):\n",
    "    all_param_values = [\n",
    "        float(val)\n",
    "        for item in final_output_df_pd_filtered_dask[param]\n",
    "        .sample(frac=yeo_subsample)\n",
    "        .compute()\n",
    "        .tolist()\n",
    "        for val in item\n",
    "    ]\n",
    "    l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "    final_output_df_pd_filtered_dask[param] = (\n",
    "        final_output_df_pd_filtered_dask[param]\n",
    "        .apply(\n",
    "            lambda x: sp.stats.yeojohnson(np.array(x).astype(float), lmbda=l_norm),\n",
    "            meta=\"object\",\n",
    "        )\n",
    "        .persist()\n",
    "    )\n",
    "final_output_df_pd_filtered = final_output_df_pd_filtered_dask.compute()\n",
    "\n",
    "scoredf = get_all_feature_scores(final_output_df_pd_filtered, params_to_transform)\n",
    "sgrnadf = get_sgrnadf_from_scoredf(scoredf, params_to_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgRNA Effect Size Filtering (within Gene groups)\n",
    "\n",
    "1) Threshold sgRNAs to include by number of observations\n",
    "2) Use LOWESS to smooth out score timeseries into 20 point timeseries\n",
    "    - changing everything after this\n",
    "3) For each timepoint, measure the euclidean norm of the feature vector and take the maximum over all time as a measure of effect size\n",
    "4) Thrshold sgRNAs for strong effects by applying a threshold to the euclidean norm that will be displayed with histogram\n",
    "5) Display a histogram for the sgRNA number per gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import sklearn as skl\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def timeseries_lowess_reg(df, t_label, y_label, min_tpt, max_tpt, bins, frac=0.33):\n",
    "    del_tpt = max_tpt - min_tpt\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins, dtype=float)\n",
    "\n",
    "    def lowess_reg(x_arr, y_arr, start=min_tpt, end=max_tpt, bins=bins, frac=frac):\n",
    "        intervals = np.linspace(start, end, num=bins, dtype=float)\n",
    "        w = lowess(y_arr, x_arr, frac=frac, xvals=intervals, it=1)\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    lowess_result = df.apply(\n",
    "        lambda x: lowess_reg(x[t_label], x[y_label])[1], axis=1, meta=float\n",
    "    )\n",
    "\n",
    "    return lowess_result\n",
    "\n",
    "\n",
    "def get_all_lowess_regs(\n",
    "    df,\n",
    "    y_label_list,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=0.33,\n",
    "    t_label=\"final cell timepoints list\",\n",
    "    iqr_bins=8,\n",
    "    iqr_window=3,\n",
    "):\n",
    "    out_df = copy.deepcopy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        lowess_result = timeseries_lowess_reg(\n",
    "            df, t_label, y_label, min_tpt, max_tpt, bins, frac=frac\n",
    "        )\n",
    "        out_df[\"LOWESS Trace: \" + y_label] = lowess_result.persist()\n",
    "    #         out_df[\"Binned IQR: \" + y_label] = out_df.apply(lambda x: compute_binned_iqr(x[t_label], x[y_label],min_tpt,max_tpt,iqr_bins,iqr_window), axis=1, meta=\"object\").persist()\n",
    "    #         interp_series = out_df[\"Binned IQR: \" + y_label].apply(lambda x: interp1d_with_nan(np.linspace(min_tpt,max_tpt,num=iqr_bins),x), meta=\"object\").persist()\n",
    "    #         out_df[\"Binned IQR Interpolation: \" + y_label] = interp_series.apply(lambda x: x(np.linspace(min_tpt,max_tpt,num=bins)), meta=\"object\").persist()\n",
    "    #         out_df.apply(lambda x: x[\"Binned IQR Interpolation: \" + y_label](np.linspace(min_tpt,max_tpt,num=bins)), axis=1, meta=\"object\").persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# def compute_binned_std(time_arr, val_arr, min_tpt, max_tpt, bins):\n",
    "#     intervals = np.linspace(min_tpt,max_tpt,num=bins,dtype=float)\n",
    "#     lower_bounds = intervals[:-1]\n",
    "#     upper_bounds = intervals[1:]\n",
    "#     interval_assign = np.where(np.logical_and(np.greater.outer(time_arr,lower_bounds),np.less_equal.outer(time_arr,upper_bounds)))[1]\n",
    "#     std_devs = [np.nanstd(val_arr[interval_assign==i]) for i in range(len(intervals)-1)]\n",
    "#     return std_devs\n",
    "\n",
    "\n",
    "def cumulative_std(val_arr):\n",
    "    intervals = np.array(range(len(val_arr)), dtype=int)\n",
    "    interval_assign = np.where(np.greater.outer(intervals[:-1], intervals[1:]))[1]\n",
    "    iqrs = np.array(\n",
    "        [\n",
    "            iqr(\n",
    "                val_arr[\n",
    "                    (interval_assign <= i + iqr_window_radius)\n",
    "                    * (interval_assign >= (i - iqr_window_radius))\n",
    "                ],\n",
    "                nan_policy=\"omit\",\n",
    "            )\n",
    "            for i in range(len(intervals) - 1)\n",
    "        ]\n",
    "    )\n",
    "    return iqrs\n",
    "\n",
    "\n",
    "# def interp1d_with_nan(timepoint_arr,timeseries_arr):\n",
    "#     '''\n",
    "#     interpolate to fill nan values #https://newbedev.com/interpolate-nan-values-in-a-numpy-array\n",
    "#     '''\n",
    "#     good_vals = np.where(np.isfinite(timeseries_arr))[0]\n",
    "#     filtered_timepoint_arr,filtered_timeseries_arr = timepoint_arr[good_vals],timeseries_arr[good_vals]\n",
    "#     interp = sp.interpolate.interp1d(filtered_timepoint_arr, filtered_timeseries_arr, kind='cubic')\n",
    "#     return interp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to find Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_Observations_thr = 10\n",
    "\n",
    "sgrnadf_wellsampled = sgrnadf[sgrnadf[\"N Observations\"] >= N_Observations_thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_tpt = 0\n",
    "max_tpt = 143\n",
    "bins = 60\n",
    "frac = (1 / bins) * 4\n",
    "iqr_bins = 8  # optimized on minC\n",
    "iqr_window = 3\n",
    "\n",
    "sgrnadf_wellsampled_dask = dd.from_pandas(\n",
    "    sgrnadf_wellsampled, npartitions=100\n",
    ").persist()\n",
    "dask.distributed.wait(sgrnadf_wellsampled_dask)\n",
    "\n",
    "lowess_trace_df = get_all_lowess_regs(\n",
    "    sgrnadf_wellsampled_dask,\n",
    "    sgrnadf_wellsampled.columns[1:8],\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    "    iqr_bins=iqr_bins,\n",
    "    iqr_window=iqr_window,\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_trace_df_nan_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowess_params = ['LOWESS Trace: ' + param + ': score' for param in params_to_transform]\n",
    "# feature_vector_series = lowess_trace_df.apply(lambda x: np.array(x[lowess_params].tolist()), axis=1)\n",
    "# lowess_trace_df[\"Feature Vector\"] = feature_vector_series\n",
    "# lowess_trace_df_nan_filtered = lowess_trace_df[~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))]\n",
    "\n",
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "params = [param + \": score\" for param in params_to_transform]\n",
    "feature_vector_sampled_series = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Sampled Feature Vector\"] = feature_vector_sampled_series\n",
    "feature_vector_series = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Feature Vector\"] = feature_vector_series\n",
    "# lowess_trace_df_nan_filtered = lowess_trace_df[~lowess_trace_df[\"Sampled Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))]\n",
    "lowess_trace_df_nan_filtered = lowess_trace_df[\n",
    "    ~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 984\n",
    "timeseries_vector = lowess_trace_df_nan_filtered[\"Feature Vector\"][idx]\n",
    "times = lowess_trace_df_nan_filtered[\"final cell timepoints list\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lowess_trace_df_nan_filtered[\"N Observations\"][idx])\n",
    "plt.scatter(times, lowess_trace_df_nan_filtered[\"Lb list: score\"][idx], s=2)\n",
    "plt.plot(\n",
    "    np.linspace(min_tpt, max_tpt, bins),\n",
    "    timeseries_vector[0],\n",
    "    c=\"tab:orange\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.ylim(-3.0, 2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import ruptures as rpt\n",
    "\n",
    "# # creation of data\n",
    "# n = 500  # number of samples\n",
    "# n_bkps, sigma = 3, 5  # number of change points, noise standard deviation\n",
    "# signal, bkps = rpt.pw_constant(n, 1, n_bkps, noise_std=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import ruptures as rpt\n",
    "\n",
    "# # creation of data\n",
    "# n = 500  # number of samples\n",
    "# n_bkps, sigma = 3, 5  # number of change points, noise standard deviation\n",
    "# signal, bkps = rpt.pw_constant(n, 1, n_bkps, noise_std=sigma)\n",
    "penalty = 2\n",
    "# change point detection\n",
    "model = \"rbf\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",...\n",
    "# algo = rpt.Binseg(model=model,min_size=5,jump=1).fit(timeseries_vector[:,5:-5].T)\n",
    "# my_bkps = algo.predict(epsilon=3 * timeseries_vector[:,5:-5].shape[1] * (sigma ** 2))\n",
    "# my_bkps = algo.predict(pen=np.log(timeseries_vector[:,5:-5].shape[1]) * timeseries_vector[:,5:-5].shape[0] * sigma ** 2)\n",
    "algo = rpt.Binseg(model=model, min_size=5, jump=1).fit(timeseries_vector[:, 5:-5].T)\n",
    "my_bkps = algo.predict(pen=penalty)\n",
    "\n",
    "# show results\n",
    "rpt.show.display(timeseries_vector[0, 5:-5], my_bkps, my_bkps, figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"rbf\"\n",
    "\n",
    "my_bkps_list = []\n",
    "for idx in range(1000):\n",
    "    timeseries_vector = lowess_trace_df_nan_filtered[\"Feature Vector\"][idx]\n",
    "    times = lowess_trace_df_nan_filtered[\"final cell timepoints list\"][idx]\n",
    "    algo = rpt.Binseg(model=model, min_size=5, jump=1).fit(timeseries_vector[:, 5:-5].T)\n",
    "    my_bkps = algo.predict(pen=penalty)\n",
    "    my_bkps = my_bkps[:-1]\n",
    "    my_bkps_list.append(my_bkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_bkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([val for item in my_bkps_list for val in item], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A POSSIBLY LOWESS-free approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_timeseries_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 546\n",
    "timeseries_vector = lowess_trace_df_nan_filtered[\"Sampled Feature Vector\"][idx]\n",
    "lowess_timeseries_vector = lowess_trace_df_nan_filtered[\"Feature Vector\"][idx]\n",
    "times = lowess_trace_df_nan_filtered[\"final cell timepoints list\"][idx]\n",
    "sorted_timeseries_vector = timeseries_vector[:, np.argsort(times)]\n",
    "sorted_times = times[np.argsort(times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lowess_trace_df_nan_filtered[\"N Observations\"][idx])\n",
    "plt.scatter(times, timeseries_vector[0], s=2)\n",
    "plt.plot(\n",
    "    np.linspace(min_tpt, max_tpt, bins),\n",
    "    lowess_timeseries_vector[0],\n",
    "    c=\"tab:orange\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.ylim(-4.0, 6.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scaled_breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penalty = 200\n",
    "# change point detection\n",
    "# model = \"linear\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",...\n",
    "# algo = rpt.Binseg(model=model,min_size=5,jump=1).fit(timeseries_vector[:,5:-5].T)\n",
    "# my_bkps = algo.predict(epsilon=3 * timeseries_vector[:,5:-5].shape[1] * (sigma ** 2))\n",
    "# my_bkps = algo.predict(pen=np.log(timeseries_vector[:,5:-5].shape[1]) * timeseries_vector[:,5:-5].shape[0] * sigma ** 2)\n",
    "algo = rpt.KernelCPD(kernel=\"linear\", min_size=50).fit(sorted_timeseries_vector.T)\n",
    "my_bkps = algo.predict(pen=penalty)\n",
    "scaled_breakpoints = sorted_times[np.array(my_bkps) - 1]\n",
    "lowess_len = len(lowess_timeseries_vector[0])\n",
    "scaled_breakpoints = scaled_breakpoints * (lowess_len / max_tpt)\n",
    "\n",
    "# show results\n",
    "rpt.show.display(sorted_timeseries_vector[0], my_bkps, my_bkps, figsize=(10, 6))\n",
    "plt.show()\n",
    "\n",
    "rpt.show.display(\n",
    "    (lowess_timeseries_vector[0]),\n",
    "    list(scaled_breakpoints),\n",
    "    list(scaled_breakpoints),\n",
    "    figsize=(10, 6),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bkps_list = []\n",
    "max_bkps_list = []\n",
    "for idx in range(300):\n",
    "    timeseries_vector = lowess_trace_df_nan_filtered[\"Sampled Feature Vector\"][idx]\n",
    "    lowess_timeseries_vector = lowess_trace_df_nan_filtered[\"Feature Vector\"][idx]\n",
    "    times = lowess_trace_df_nan_filtered[\"final cell timepoints list\"][idx]\n",
    "    sorted_timeseries_vector = timeseries_vector[:, np.argsort(times)]\n",
    "    sorted_times = times[np.argsort(times)]\n",
    "\n",
    "    algo = rpt.KernelCPD(kernel=\"linear\", min_size=10).fit(sorted_timeseries_vector.T)\n",
    "    my_bkps = algo.predict(pen=penalty)\n",
    "    #     my_bkps = my_bkps[:-1]\n",
    "    scaled_breakpoints = sorted_times[np.array(my_bkps) - 1]\n",
    "    scaled_breakpoints = scaled_breakpoints[:-1]\n",
    "    if len(scaled_breakpoints) > 0:\n",
    "        max_bkps_list.append(np.max(scaled_breakpoints))\n",
    "    else:\n",
    "        max_bkps_list.append(0)\n",
    "    my_bkps_list.append(scaled_breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([val for item in my_bkps_list for val in item], bins=10)\n",
    "plt.xlim(0, 143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(max_bkps_list, bins=15)\n",
    "plt.xlim(0, 143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOW NEED A PRINCIPLED EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "feature_vector_series = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Feature Vector\"] = feature_vector_series\n",
    "lowess_trace_df_nan_filtered = lowess_trace_df[\n",
    "    ~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vect = np.array(feature_vector_series.tolist())\n",
    "reversed_feature_vect = feature_vect[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_5_timepoints = reversed_feature_vect[:, :, :5]\n",
    "mean_last_5_timepoints = np.mean(last_5_timepoints, axis=2)\n",
    "centered_last_5_timepoints = (\n",
    "    last_5_timepoints - mean_last_5_timepoints[:, :, np.newaxis]\n",
    ")\n",
    "max_deviation = np.max(centered_last_5_timepoints, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(max_deviation[:, 0].flatten(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_feature_vect = feature_vect[::-1]\n",
    "derivative_vect = reversed_feature_vect[:, :, 1:] - reversed_feature_vect[:, :, :-1]\n",
    "smoothed_derivative_vect = sp.signal.medfilt(derivative_vect, kernel_size=(1, 1, 5))\n",
    "abs_smoothed_derivative_vect = np.abs(smoothed_derivative_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(abs_smoothed_derivative_vect[0:2000, 0].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs_smoothed_derivative_vect[:, 0].flatten(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "mean_vector_df = lowess_trace_df[lowess_params]\n",
    "mean_vector_df = pd.concat(\n",
    "    [mean_vector_df[lowess_param].explode() for lowess_param in lowess_params], axis=1\n",
    ")\n",
    "mean_vector_df[\"LOWESS Timepoint\"] = mean_vector_df.groupby(\"sgRNA\").cumcount()\n",
    "mean_vector_df[\"Inverted LOWESS Timepoint\"] = (bins - 1) - mean_vector_df.groupby(\n",
    "    \"sgRNA\"\n",
    ").cumcount()\n",
    "mean_vector_df = (\n",
    "    mean_vector_df.reset_index(drop=False)\n",
    "    .set_index([\"sgRNA\", \"Inverted LOWESS Timepoint\"], drop=True)\n",
    "    .sort_index()\n",
    ")\n",
    "cumulative_var_df = mean_vector_df.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x.expanding(1).var()\n",
    ")\n",
    "var_from_end_df = pd.concat(\n",
    "    [\n",
    "        cumulative_var_df[lowess_param].groupby(\"sgRNA\").apply(lambda x: x.tolist()[1:])\n",
    "        for lowess_param in lowess_params\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "mean_vector_df = lowess_trace_df[lowess_params]\n",
    "mean_vector_df = pd.concat(\n",
    "    [mean_vector_df[lowess_param].explode() for lowess_param in lowess_params], axis=1\n",
    ")\n",
    "mean_vector_df[\"LOWESS Timepoint\"] = mean_vector_df.groupby(\"sgRNA\").cumcount()\n",
    "mean_vector_df[\"Inverted LOWESS Timepoint\"] = (bins - 1) - mean_vector_df.groupby(\n",
    "    \"sgRNA\"\n",
    ").cumcount()\n",
    "mean_vector_df = (\n",
    "    mean_vector_df.reset_index(drop=False)\n",
    "    .set_index([\"sgRNA\", \"Inverted LOWESS Timepoint\"], drop=True)\n",
    "    .sort_index()\n",
    ")\n",
    "cumulative_var_df = mean_vector_df.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x.expanding(1).var()\n",
    ")\n",
    "var_from_end_df = pd.concat(\n",
    "    [\n",
    "        cumulative_var_df[lowess_param].groupby(\"sgRNA\").apply(lambda x: x.tolist()[1:])\n",
    "        for lowess_param in lowess_params\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_over_time_arr = np.array(var_from_end_df[\"LOWESS Trace: Lb list: score\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    var_over_time_arr[:, 2],\n",
    "    bins=50,\n",
    "    histtype=\"step\",\n",
    "    linewidth=3,\n",
    "    density=False,\n",
    "    range=(0.05, 0.5),\n",
    ")  ## Last timepoint\n",
    "plt.hist(\n",
    "    var_over_time_arr[:, 18],\n",
    "    bins=50,\n",
    "    histtype=\"step\",\n",
    "    linewidth=3,\n",
    "    density=False,\n",
    "    range=(0.05, 0.5),\n",
    ")  ## First timepoint\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_pred = -np.concatenate([var_over_time_arr[:, 4], var_over_time_arr[:, 18]])\n",
    "y_true = np.array(\n",
    "    [1 for i in range(len(var_over_time_arr[:, 4]))]\n",
    "    + [0 for i in range(len(var_over_time_arr[:, 18]))]\n",
    ")\n",
    "\n",
    "precision, recall, thresholds = skl.metrics.precision_recall_curve(y_true, probas_pred)\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "max_F1_idx = np.argmax(F1)\n",
    "max_F1_prec, max_F1_recall, max_F1_thr = (\n",
    "    precision[max_F1_idx],\n",
    "    recall[max_F1_idx],\n",
    "    -thresholds[max_F1_idx],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_F1_prec, max_F1_recall, max_F1_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_F1_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(del_var, bins=50, histtype=\"step\", linewidth=3, range=(0.1, 0.5))\n",
    "plt.plot()\n",
    "plt.hist(del_var_init, bins=50, histtype=\"step\", linewidth=3, range=(0.1, 0.5))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    var_over_time_arr_control[:, 0],\n",
    "    bins=50,\n",
    "    histtype=\"step\",\n",
    "    linewidth=3,\n",
    "    density=False,\n",
    "    range=(0.1, 0.3),\n",
    ")  ## Last timepoint\n",
    "plt.hist(\n",
    "    var_over_time_arr_control[:, 4],\n",
    "    bins=50,\n",
    "    histtype=\"step\",\n",
    "    linewidth=3,\n",
    "    density=False,\n",
    "    range=(0.1, 0.3),\n",
    ")  ## 5th to last timepoint\n",
    "plt.hist(\n",
    "    var_over_time_arr_control[:, 18],\n",
    "    bins=50,\n",
    "    histtype=\"step\",\n",
    "    linewidth=3,\n",
    "    density=False,\n",
    "    range=(0.1, 0.3),\n",
    ")  ## First timepoint\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.15 seems like a reasonable threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_trace_df[\"Mean Vector\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_arr = np.array([0, 1, 2])\n",
    "intervals = np.array(range(len(val_arr)), dtype=int)\n",
    "interval_assign = np.where(np.greater.outer(intervals[:-1], intervals[1:]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.greater.outer(intervals[:-1], intervals[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise Linear Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    masked_label_series = df.apply(\n",
    "        lambda x: np.array(x[label])[\n",
    "            (np.array(x[time_label]) >= min_timepoint)\n",
    "            * (np.array(x[time_label]) <= max_timepoint)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    if flatten_vals:\n",
    "        flattened_vals = [val for item in masked_label_series.tolist() for val in item]\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "# def get_feature_stats(df, feature_label, min_timepoint, max_timepoint):\n",
    "#     feature_vals = get_timepoint_values(df, feature_label, min_timepoint, max_timepoint)\n",
    "#     feature_median = np.median(feature_vals)\n",
    "#     feature_iqr = sp.stats.iqr(feature_vals)\n",
    "#     return feature_median,feature_iqr\n",
    "\n",
    "\n",
    "def get_feature_median_bytrench(df, feature_label, min_timepoint, max_timepoint):\n",
    "    masked_label_series = get_timepoint_values(\n",
    "        final_output_df_pd_filtered,\n",
    "        feature_label,\n",
    "        min_timepoint,\n",
    "        max_timepoint,\n",
    "        flatten_vals=False,\n",
    "    )\n",
    "    trench_median_series = masked_label_series.apply(lambda x: np.nanmedian(x))\n",
    "    return trench_median_series\n",
    "\n",
    "\n",
    "def compute_foldchange(\n",
    "    df,\n",
    "    feature_label,\n",
    "    trench_median_series,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    if timepoint_range == None:\n",
    "        foldchange = (\n",
    "            df[feature_label].apply(lambda x: np.array(x))\n",
    "        ) - trench_median_series\n",
    "    else:\n",
    "        foldchange = (\n",
    "            df[feature_label].apply(\n",
    "                lambda x: np.array(x)[\n",
    "                    (np.array(x[time_label]) >= timepoint_range[0])\n",
    "                    * (np.array(x[time_label]) <= timepoint_range[1])\n",
    "                ]\n",
    "            )\n",
    "        ) - trench_median_series\n",
    "    return foldchange\n",
    "\n",
    "\n",
    "def get_feature_foldchange(\n",
    "    df,\n",
    "    feature_label,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    trench_median_series = get_feature_median_bytrench(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    scores = compute_foldchange(\n",
    "        df,\n",
    "        feature_label,\n",
    "        trench_median_series,\n",
    "        time_label=time_label,\n",
    "        timepoint_range=timepoint_range,\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_foldchange(\n",
    "    df,\n",
    "    feature_labels,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_foldchange(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "            timepoint_range=timepoint_range,\n",
    "        )\n",
    "        df[feature_label + \": foldchange\"] = feature_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sgrnadf_from_folddf(\n",
    "    folddf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    folddf_groupby = folddf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        folddf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": foldchange\"] = folddf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [\n",
    "                    val\n",
    "                    for item in x[feature_label + \": foldchange\"].tolist()\n",
    "                    for val in item\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = folddf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = folddf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = folddf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = folddf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = folddf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = folddf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Lb list\",\n",
    "    \"Ld list\",\n",
    "    \"delL list\",\n",
    "    \"Mean Area Increment list\",\n",
    "    \"Mean Length Increment list\",\n",
    "    \"Mean Width list\",\n",
    "    \"Mean mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]\n",
    "# yeo_subsample = 0.1\n",
    "\n",
    "# final_output_df_pd_filtered_dask = dd.from_pandas(final_output_df_pd_filtered,npartitions=100).persist()\n",
    "# dask.distributed.wait(final_output_df_pd_filtered_dask)\n",
    "\n",
    "# for i,param in enumerate(params_to_transform):\n",
    "#     all_param_values = [float(val) for item in final_output_df_pd_filtered_dask[param].sample(frac=yeo_subsample).compute().tolist() for val in item]\n",
    "#     l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "#     final_output_df_pd_filtered_dask[param] = final_output_df_pd_filtered_dask[param].apply(lambda x: sp.stats.yeojohnson(np.array(x).astype(float),lmbda = l_norm), meta='object').persist()\n",
    "# final_output_df_pd_filtered = final_output_df_pd_filtered_dask.compute()\n",
    "\n",
    "# NOT A FOLD CHANGE RN\n",
    "scoredf = get_all_feature_foldchange(final_output_df_pd_filtered, params_to_transform)\n",
    "sgrnadf = get_sgrnadf_from_folddf(scoredf, params_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_wellsampled = sgrnadf[sgrnadf[\"N Observations\"] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tpt = 0\n",
    "max_tpt = 143\n",
    "bins = 20\n",
    "frac = (1 / bins) * 4\n",
    "iqr_bins = 8  # optimized on minC\n",
    "iqr_window = 3\n",
    "\n",
    "sgrnadf_wellsampled_dask = dd.from_pandas(\n",
    "    sgrnadf_wellsampled, npartitions=100\n",
    ").persist()\n",
    "dask.distributed.wait(sgrnadf_wellsampled_dask)\n",
    "\n",
    "lowess_trace_df = get_all_lowess_regs(\n",
    "    sgrnadf_wellsampled_dask,\n",
    "    sgrnadf_wellsampled.columns[1:8],\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    "    iqr_bins=iqr_bins,\n",
    "    iqr_window=iqr_window,\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lowess_trace_df[\"LOWESS Trace: Lb list: foldchange\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import ruptures as rpt\n",
    "\n",
    "penalty = 0.0\n",
    "model = \"linear\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",...\n",
    "\n",
    "timeseries_vector = lowess_trace_df[\"LOWESS Trace: Lb list: foldchange\"][50].reshape(\n",
    "    -1, 1\n",
    ")\n",
    "# algo = rpt.Binseg(model=model,min_size=5,jump=1).fit(timeseries_vector[:,5:-5].T)\n",
    "# my_bkps = algo.predict(epsilon=3 * timeseries_vector[:,5:-5].shape[1] * (sigma ** 2))\n",
    "# my_bkps = algo.predict(pen=np.log(timeseries_vector[:,5:-5].shape[1]) * timeseries_vector[:,5:-5].shape[0] * sigma ** 2)\n",
    "algo = rpt.Binseg(model=model, min_size=5, jump=1).fit(timeseries_vector)\n",
    "my_bkps = algo.predict(n_bkps=1)\n",
    "\n",
    "# show results\n",
    "rpt.show.display(timeseries_vector, my_bkps, my_bkps, figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbit thr\n",
    "\n",
    "steady_state_thr = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_trace_df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_arr = np.array(\n",
    "    get_timepoint_values(lowess_trace_df[0:1], \"Lb list: foldchange\", 100, 143)\n",
    ")\n",
    "time_arr = np.array(\n",
    "    get_timepoint_values(lowess_trace_df[0:1], \"final cell timepoints list\", 100, 143)\n",
    ").reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(time_arr, val_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(df, label, time_label, min_timepoint, max_timepoint):\n",
    "    val_arr = np.array(get_timepoint_values(df, label, min_timepoint, max_timepoint))\n",
    "    time_arr = np.array(\n",
    "        get_timepoint_values(df, time_label, min_timepoint, max_timepoint)\n",
    "    )\n",
    "    good_vals = (~np.isnan(val_arr)) * (~np.isnan(time_arr))\n",
    "    time_arr = time_arr[good_vals].reshape(-1, 1)\n",
    "    val_arr = val_arr[good_vals]\n",
    "    reg = LinearRegression().fit(time_arr, val_arr)\n",
    "    return reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = (\n",
    "    lowess_trace_df[:3000]\n",
    "    .groupby(\"sgRNA\")\n",
    "    .apply(\n",
    "        lambda x: get_slope(\n",
    "            x, \"Lb list: foldchange\", \"final cell timepoints list\", 100, 143\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equilibrated_subset = lowess_trace_df[:3000][abs(regs) < 0.015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(regs), bins=50, range=(0, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lowess_trace_df[\"LOWESS Trace: Lb list: foldchange\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pearson_r(X):\n",
    "    corr_coeff = sp.stats.pearsonr(X[0], X[1])[0]\n",
    "    return corr_coeff\n",
    "\n",
    "\n",
    "def get_corrcoeff(df, label_x, label_y, min_timepoint, max_timepoint):\n",
    "    label_x_arr = np.array(\n",
    "        get_timepoint_values(df, label_x, min_timepoint, max_timepoint)\n",
    "    )\n",
    "    label_y_arr = np.array(\n",
    "        get_timepoint_values(df, label_y, min_timepoint, max_timepoint)\n",
    "    )\n",
    "    good_vals = (~np.isnan(label_x_arr)) * (~np.isnan(label_y_arr))\n",
    "    label_x_arr = label_x_arr[good_vals]\n",
    "    label_y_arr = label_y_arr[good_vals]\n",
    "    X = np.stack([label_x_arr, label_y_arr])\n",
    "    r = get_pearson_r(X)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = (\n",
    "    equilibrated_subset[:5000]\n",
    "    .groupby(\"sgRNA\")\n",
    "    .apply(\n",
    "        lambda x: get_corrcoeff(\n",
    "            x, \"Lb list: foldchange\", \"delL list: foldchange\", 100, 143\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(corrs, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrs[corrs < -0.4].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(\n",
    "    lowess_trace_df.loc[corrs[corrs < -0.4].index.tolist()][\"Gene\"], return_counts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_interest = lowess_trace_df.loc[corrs[corrs < -0.4].index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = np.array(\n",
    "        get_timepoint_values(df_of_interest[i : i + 1], \"Lb list: foldchange\", 100, 143)\n",
    "    )\n",
    "    y = np.array(\n",
    "        get_timepoint_values(\n",
    "            df_of_interest[i : i + 1], \"delL list: foldchange\", 100, 143\n",
    "        )\n",
    "    )\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    masked_label_series = df.apply(\n",
    "        lambda x: np.array(x[label])[\n",
    "            (np.array(x[time_label]) >= min_timepoint)\n",
    "            * (np.array(x[time_label]) <= max_timepoint)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    if flatten_vals:\n",
    "        flattened_vals = [val for item in masked_label_series.tolist() for val in item]\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "# def get_feature_stats(df, feature_label, min_timepoint, max_timepoint):\n",
    "#     feature_vals = get_timepoint_values(df, feature_label, min_timepoint, max_timepoint)\n",
    "#     feature_median = np.median(feature_vals)\n",
    "#     feature_iqr = sp.stats.iqr(feature_vals)\n",
    "#     return feature_median,feature_iqr\n",
    "\n",
    "\n",
    "def get_feature_median_bytrench(df, feature_label, min_timepoint, max_timepoint):\n",
    "    masked_label_series = get_timepoint_values(\n",
    "        final_output_df_pd_filtered,\n",
    "        feature_label,\n",
    "        min_timepoint,\n",
    "        max_timepoint,\n",
    "        flatten_vals=False,\n",
    "    )\n",
    "    trench_median_series = masked_label_series.apply(lambda x: np.nanmedian(x))\n",
    "    return trench_median_series\n",
    "\n",
    "\n",
    "def compute_foldchange(\n",
    "    df,\n",
    "    feature_label,\n",
    "    trench_median_series,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    if timepoint_range == None:\n",
    "        foldchange = (\n",
    "            df[feature_label].apply(lambda x: np.array(x))\n",
    "        ) - trench_median_series\n",
    "    else:\n",
    "        foldchange = (\n",
    "            df[feature_label].apply(\n",
    "                lambda x: np.array(x)[\n",
    "                    (np.array(x[time_label]) >= timepoint_range[0])\n",
    "                    * (np.array(x[time_label]) <= timepoint_range[1])\n",
    "                ]\n",
    "            )\n",
    "        ) - trench_median_series\n",
    "    return foldchange\n",
    "\n",
    "\n",
    "def get_feature_foldchange(\n",
    "    df,\n",
    "    feature_label,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    trench_median_series = get_feature_median_bytrench(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    scores = compute_foldchange(\n",
    "        df,\n",
    "        feature_label,\n",
    "        trench_median_series,\n",
    "        time_label=time_label,\n",
    "        timepoint_range=timepoint_range,\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_foldchange(\n",
    "    df,\n",
    "    feature_labels,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_foldchange(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "            timepoint_range=timepoint_range,\n",
    "        )\n",
    "        df[feature_label + \": foldchange\"] = feature_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sgrnadf_from_folddf(\n",
    "    folddf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    folddf_groupby = folddf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        folddf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": foldchange\"] = folddf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [\n",
    "                    val\n",
    "                    for item in x[feature_label + \": foldchange\"].tolist()\n",
    "                    for val in item\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = folddf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = folddf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = folddf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = folddf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = folddf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = folddf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Lb list\",\n",
    "    \"Ld list\",\n",
    "    \"delL list\",\n",
    "    \"Mean Area Increment list\",\n",
    "    \"Mean Length Increment list\",\n",
    "    \"Mean Width list\",\n",
    "    \"Mean mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]\n",
    "# yeo_subsample = 0.1\n",
    "\n",
    "# final_output_df_pd_filtered_dask = dd.from_pandas(final_output_df_pd_filtered,npartitions=100).persist()\n",
    "# dask.distributed.wait(final_output_df_pd_filtered_dask)\n",
    "\n",
    "# for i,param in enumerate(params_to_transform):\n",
    "#     all_param_values = [float(val) for item in final_output_df_pd_filtered_dask[param].sample(frac=yeo_subsample).compute().tolist() for val in item]\n",
    "#     l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "#     final_output_df_pd_filtered_dask[param] = final_output_df_pd_filtered_dask[param].apply(lambda x: sp.stats.yeojohnson(np.array(x).astype(float),lmbda = l_norm), meta='object').persist()\n",
    "# final_output_df_pd_filtered = final_output_df_pd_filtered_dask.compute()\n",
    "\n",
    "# NOT A FOLD CHANGE RN\n",
    "scoredf = get_all_feature_foldchange(final_output_df_pd_filtered, params_to_transform)\n",
    "sgrnadf = get_sgrnadf_from_folddf(scoredf, params_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def logifunc(x, A, x0, k, off):\n",
    "    return A / (1 + np.exp(-k * (x - x0))) + off\n",
    "\n",
    "\n",
    "def gompfunc(x, a, b, c, off):\n",
    "    return (a * np.exp(-np.exp(b - (c * x)))) + off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tpt = 0\n",
    "max_tpt = 143\n",
    "bins = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "y = sgrnadf[\"Lb list: foldchange\"][idx]\n",
    "x = sgrnadf[\"final cell timepoints list\"][idx]\n",
    "x_data = np.linspace(min_tpt, max_tpt, num=bins)\n",
    "plt.scatter(x, y, label=\"Logistic function\")\n",
    "\n",
    "popt, pcov = curve_fit(\n",
    "    gompfunc,\n",
    "    x,\n",
    "    y,\n",
    "    p0=[0.0, 1.0, 1.0, 0.0],\n",
    ")\n",
    "midpoint = (popt[1] - np.log(np.log(2))) / popt[2]\n",
    "plt.plot(x_data, gompfunc(x_data, *popt), \"r-\", label=\"Fitted function\")\n",
    "plt.legend()\n",
    "print(midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "y = sgrnadf[\"Lb list: foldchange\"][idx]\n",
    "x = sgrnadf[\"final cell timepoints list\"][idx]\n",
    "x_data = np.linspace(min_tpt, max_tpt, num=bins)\n",
    "\n",
    "popt, pcov = curve_fit(logifunc, x, y, p0=[50, 185, 0.1, -222])\n",
    "midpoint = popt[1]\n",
    "plt.scatter(x, y, label=\"Logistic function\")\n",
    "plt.plot(x_data, logifunc(x_data, *popt), \"r-\", label=\"Fitted function\")\n",
    "plt.legend()\n",
    "print(midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sgrnadf[\"Lb list: foldchange\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sgrnadf[\"Lb list: foldchange\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sgrnadf[\"Lb list: foldchange\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on Mean Behavior Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "feature_vector_series = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Feature Vector\"] = feature_vector_series\n",
    "lowess_trace_df_nan_filtered = lowess_trace_df[\n",
    "    ~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_effect_threshold = 35\n",
    "\n",
    "zero_vector = np.zeros(\n",
    "    (1, lowess_trace_df_nan_filtered[\"Feature Vector\"].iloc[0].shape[0])\n",
    ")\n",
    "feature_arr = np.array(lowess_trace_df_nan_filtered[\"Feature Vector\"].tolist())\n",
    "flattened_feature_arr = np.swapaxes(feature_arr, 1, 2).reshape(-1, feature_arr.shape[1])\n",
    "dist_arr = euclidean_distances(flattened_feature_arr, zero_vector).reshape(\n",
    "    feature_arr.shape[0], feature_arr.shape[2]\n",
    ")\n",
    "lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"] = sp.integrate.simpson(\n",
    "    dist_arr\n",
    ")\n",
    "# lowess_trace_df[\"Max Euclidean Norm\"] = np.max(dist_arr,axis=1)\n",
    "\n",
    "sgrnadf_strong_effect = lowess_tlowess_trace_df_nan_filteredrace_df[\n",
    "    lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = (\n",
    "    np.min(lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"]),\n",
    "    np.percentile(lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"], 99),\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Euclidean Norm\")\n",
    "plt.hist(\n",
    "    lowess_trace_df_nan_filtered[\n",
    "        lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"]\n",
    "        < strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    lowess_trace_df_nan_filtered[\n",
    "        lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"]\n",
    "        >= strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"], return_counts=True)\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_rep_example_series = (\n",
    "    sgrnadf_strong_effect.reset_index(drop=False)\n",
    "    .groupby(\"TargetID\")\n",
    "    .apply(lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])])\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(\"sgRNA\", drop=True)\n",
    ")\n",
    "\n",
    "normalized_timeseries = np.swapaxes(\n",
    "    normalize_timeseries(most_rep_example_series[\"Feature Vector\"], lmbda=0.5), 1, 2\n",
    ")\n",
    "most_rep_example_series[\"Normalized Feature Vector\"] = [\n",
    "    normalized_timeseries[i] for i in range(normalized_timeseries.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Distance Metrics\n",
    "\n",
    "Now, I want to evaluate the performance of different distance metrics on the data wrt seperating it maximally while also preserving similarity within replicates\n",
    "\n",
    "- DTW (can be done with cosine similarity) \n",
    "- cosine similarity (same as pearson for z-scores)\n",
    "- cross correlation\n",
    "\n",
    "Seems like soft-DTW is a pretty good option. Going forward with that for now.\n",
    "\n",
    "<!-- In the end cosine similarity was chosen as it produced superior silhouette scores for sets of targets from genes with different phenotypes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_examples_for_distance_metric = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin([\"ftsN\", \"rplA\", \"mreB\", \"tufB\", \"tff\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import (\n",
    "    dtw,\n",
    "    cdist_dtw,\n",
    "    dtw_path_from_metric,\n",
    "    cdist_soft_dtw,\n",
    "    cdist_soft_dtw_normalized,\n",
    ")\n",
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_arr = np.swapaxes(\n",
    "    np.array(\n",
    "        sgrnadf_examples_for_distance_metric[\"Normalized Feature Vector\"].tolist()\n",
    "    ),\n",
    "    1,\n",
    "    2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "\n",
    "    print(\n",
    "        \"Soft-DTW Gamma=\"\n",
    "        + str(gamma)\n",
    "        + \": \"\n",
    "        + str(\n",
    "            tslearn.clustering.silhouette_score(\n",
    "                timeseries_arr,\n",
    "                sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "                metric=\"softdtw\",\n",
    "                gamma=gamma,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"cosine\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Cosine-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"euclidean\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Euclidean-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_dtw_dist_arr = tslearn.metrics.cdist_soft_dtw(timeseries_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(soft_dtw_dist_arr.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting different effects against single genes\n",
    "\n",
    "1) Plot a histogram of minimum soft-DTW similarity within groups of TargetIDs against the same genes (for genes with more than one targetID)\n",
    "2) Use affinity propagation to select the number of phenotype clusters to use per gene (preference still needs to be dialed in, not sure how to optimize on this)\n",
    "3) Among each cluster, represent the final effect as the strongest effect (integrated euc norm) of the members of the cluster\n",
    "\n",
    "~~3) Among each cluster, represent the final effect as the median of the members of the cluster~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_softdtw(feature_vector_series):\n",
    "    dist_mat = cdist_soft_dtw_normalized(\n",
    "        np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    )\n",
    "    timeseries_len = (\n",
    "        feature_vector_series[0].shape[0] * feature_vector_series[0].shape[1]\n",
    "    )\n",
    "    dist_mat = dist_mat / timeseries_len\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def get_upper_right_vals(a):\n",
    "    upper_tri = np.triu(a, k=1)\n",
    "    upper_tri[upper_tri == 0.0] = np.NaN\n",
    "    return upper_tri\n",
    "\n",
    "\n",
    "def get_sgRNA_clusters(df, preference=0.6):\n",
    "    gene_indexed_df = (\n",
    "        df.reset_index(drop=False)\n",
    "        .set_index(\"Gene\")[[\"sgRNA\", \"Normalized Feature Vector\", \"TargetID\"]]\n",
    "        .sort_index()\n",
    "    )\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = pd.Series(\n",
    "        np.zeros(len(gene_indexed_df), dtype=int), dtype=int\n",
    "    )\n",
    "    gene_df_list = []\n",
    "    for gene in gene_indexed_df.index.tolist():\n",
    "        gene_df = gene_indexed_df.loc[[gene]]\n",
    "        if len(gene_df) > 1:\n",
    "            gene_feature_vector = gene_df[\"Normalized Feature Vector\"]\n",
    "            soft_dtw_dist = get_normed_softdtw(gene_feature_vector)\n",
    "            af_labels = (\n",
    "                AffinityPropagation(\n",
    "                    affinity=\"precomputed\", preference=preference, random_state=42\n",
    "                )\n",
    "                .fit_predict(-soft_dtw_dist)\n",
    "                .astype(int)\n",
    "            )\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = af_labels\n",
    "        else:\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = 0\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = gene_indexed_df[\"sgRNA Cluster\"].astype(int)\n",
    "    return gene_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sgrna_replicate_thr = 2\n",
    "pref_factor = 3.0\n",
    "\n",
    "gene_list, counts_list = np.unique(most_rep_example_series[\"Gene\"], return_counts=True)\n",
    "genes_with_many_replicate_sgRNAs = gene_list[counts_list >= n_sgrna_replicate_thr]\n",
    "sgrnadf_many_copies_per_gene = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin(genes_with_many_replicate_sgRNAs)\n",
    "]\n",
    "\n",
    "max_distance_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: np.nanmax(\n",
    "        get_upper_right_vals(get_normed_softdtw(x[\"Normalized Feature Vector\"]))\n",
    "    )\n",
    ")\n",
    "plt.title(\"Maximum soft-DTW Distance per Gene\")\n",
    "plt.hist(max_distance_within_gene, bins=50)\n",
    "plt.show()\n",
    "\n",
    "dist_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: get_upper_right_vals(\n",
    "        get_normed_softdtw(x[\"Normalized Feature Vector\"])\n",
    "    ).flatten()\n",
    ")\n",
    "dist_within_gene = [val for item in dist_within_gene.tolist() for val in item]\n",
    "median_similarity = -np.nanmedian(dist_within_gene)\n",
    "\n",
    "gene_df = get_sgRNA_clusters(\n",
    "    most_rep_example_series, preference=pref_factor * median_similarity\n",
    ")\n",
    "\n",
    "most_rep_example_series[\"sgRNA Cluster\"] = gene_df.set_index(\"sgRNA\")[\"sgRNA Cluster\"]\n",
    "most_rep_example_series[\"sgRNA Cluster Label\"] = most_rep_example_series.apply(\n",
    "    lambda x: str(x[\"Gene\"]) + \"-\" + str(x[\"sgRNA Cluster\"]), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df = most_rep_example_series[\n",
    "    [\n",
    "        \"sgRNA Cluster Label\",\n",
    "        \"Normalized Feature Vector\",\n",
    "        \"Gene\",\n",
    "        \"Integrated Euclidean Norm\",\n",
    "    ]\n",
    "    + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "].reset_index(drop=True)\n",
    "gene_cluster_groupby = gene_cluster_df.groupby(\"sgRNA Cluster Label\")\n",
    "# median_feature_series = gene_cluster_groupby.apply(lambda x: np.median(np.stack(x[\"Feature Vector\"]).astype(float), axis=0)).to_frame().rename(columns={0:\"Feature Vector\"})\n",
    "feature_series = (\n",
    "    gene_cluster_groupby.apply(\n",
    "        lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])][\n",
    "            \"Normalized Feature Vector\"\n",
    "        ]\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Normalized Feature Vector\"})\n",
    ")\n",
    "\n",
    "gene_cluster_df = gene_cluster_groupby.apply(\n",
    "    lambda x: x.iloc[0][\n",
    "        [\"Gene\"]\n",
    "        + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "    ]\n",
    ")\n",
    "gene_cluster_df = gene_cluster_df.join(feature_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: TSNE and Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = get_normed_softdtw(gene_cluster_df[\"Normalized Feature Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(\n",
    "    n_components=2, perplexity=5.0, early_exaggeration=50.0, metric=\"precomputed\"\n",
    ").fit_transform(X_dist - np.min(X_dist))\n",
    "gene_cluster_df[\"TSNE Coords\"] = [X_embedded[i] for i in range(X_embedded.shape[0])]\n",
    "\n",
    "af_labels = (\n",
    "    AffinityPropagation(affinity=\"precomputed\", preference=-0.5)\n",
    "    .fit_predict(-X_dist)\n",
    "    .astype(int)\n",
    ")\n",
    "gene_cluster_df[\"Affinity Clusts\"] = af_labels\n",
    "\n",
    "plt.scatter(\n",
    "    X_embedded[:, 0],\n",
    "    X_embedded[:, 1],\n",
    "    s=3,\n",
    "    alpha=1,\n",
    "    c=gene_cluster_df[\"Affinity Clusts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_labels = [\n",
    "    \"Birth Length\",\n",
    "    \"Division Length\",\n",
    "    \"Area Growth Rate\",\n",
    "    \"Length Growth Rate\",\n",
    "    \"Average Width\",\n",
    "    \"mCherry Intensity\",\n",
    "    \"Cell Cycle Duration\",\n",
    "]\n",
    "\n",
    "hierarchical_labels = gene_cluster_df.index.tolist()\n",
    "\n",
    "\n",
    "def get_leaf_children(tree, leaf_id):\n",
    "    cluster_node = tree[leaf_id]\n",
    "    leaf_children = cluster_node.pre_order(lambda x: x.id)\n",
    "    return leaf_children\n",
    "\n",
    "\n",
    "def assign_dendro_clusts(df, children_labels):\n",
    "    df_out = copy.deepcopy(df)\n",
    "    df_out[\"Dendrogram Clusters\"] = pd.Series(len(df), dtype=int)\n",
    "    for clust_i, indices in enumerate(children_labels):\n",
    "        df_out[\"Dendrogram Clusters\"].iloc[indices] = clust_i\n",
    "    df_out[\"Dendrogram Clusters\"] = df_out[\"Dendrogram Clusters\"].astype(int)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "suppress_thr = 15\n",
    "min_zscore = -2\n",
    "max_zscore = 2\n",
    "\n",
    "\n",
    "def compute_and_plot_dendrogram(\n",
    "    df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "):\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    hierarchical_labels = df.index.tolist()\n",
    "    X = np.array(df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(2, suppress_thr)\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    #     Y = sch.linkage(X, method='weighted', metric='cosine',optimal_ordering=True)\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        truncate_mode=\"lastp\",\n",
    "        show_contracted=True,\n",
    "        p=suppress_thr,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "    children_labels = [get_leaf_children(cluster_tree, leaf) for leaf in Z[\"leaves\"]]\n",
    "\n",
    "    #     fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\\\n",
    "    #                  ax=dendro_ax, orientation='vertical', label='Z-score',\\\n",
    "    #                 use_gridspec=True, location='left', pad=-0.05,aspect=10)\n",
    "\n",
    "    for i, children in enumerate(children_labels):\n",
    "        children_arr = np.array(\n",
    "            df.iloc[children][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "        mean_vector = np.mean(children_arr, axis=0)  # feature,timepoint\n",
    "        # fig = plt.figure(constrained_layout=True, figsize=(20,10))\n",
    "        # gs = fig.add_gridspec(2, 10)\n",
    "        # for v in range(10):\n",
    "        #     inner_gs = gs[0,v].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0, )\n",
    "        #     inner_grid_sub = inner_gs.subplots()\n",
    "        #     for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "        #         ax.plot(mean_vector[c])\n",
    "        #         ax.set(xticks=[], yticks=[])\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                mean_vector.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 10])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=18,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return children_labels\n",
    "\n",
    "\n",
    "def plot_subset(\n",
    "    df_subset,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    feature_labels=feature_labels,\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "):\n",
    "\n",
    "    df_clusts = (\n",
    "        df_subset.sort_index()\n",
    "        .reset_index(drop=False)\n",
    "        .set_index(\"Dendrogram Clusters\")[\n",
    "            [\"sgRNA Cluster Label\", \"Normalized Feature Vector\"]\n",
    "        ]\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(df_clusts), wspace=wspace)\n",
    "\n",
    "    for i in range(len(df_clusts)):\n",
    "        clust_arr = np.array(\n",
    "            df_clusts[\"Normalized Feature Vector\"].iloc[i].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 10.0])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=18,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "        #             imshow_first_ax = fig.add_subplot(gs[0, i])\n",
    "        #             imshow_first_ax.imshow(df_clusts[\"Feature Vector\"].iloc[i].astype(float).reshape(-1,1),cmap=cmap,norm=norm)\n",
    "\n",
    "        #             imshow_first_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "        #             imshow_first_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "        #             imshow_first_ax.set_xlabel(df_clusts[\"sgRNA Cluster Label\"].iloc[i] + \"\\n Cluster \" + str(df_clusts.index[i]), fontsize=14)\n",
    "\n",
    "        #             imshow_first_ax.set_yticks(range(len(feature_labels)))\n",
    "        #             imshow_first_ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(clust_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def make_subset_dendrogram(\n",
    "    sub_df,\n",
    "    title,\n",
    "    feature_labels=feature_labels,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    figsize=(10, 10),\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "    X_dist = get_normed_softdtw(sub_df[\"Normalized Feature Vector\"])\n",
    "    X = np.array(sub_df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(2, len(sub_df))\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        show_contracted=True,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    fig.suptitle(title, fontsize=fontsize)\n",
    "\n",
    "    for i, leaf in enumerate(Z[\"leaves\"]):\n",
    "        leaf_arr = np.array(\n",
    "            sub_df.iloc[leaf][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                leaf_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 8])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(leaf_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "#         imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "#         imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "\n",
    "#         imshow_ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_and_plot_dendrogram(df,X_dist,feature_labels,suppress_thr,min_zscore,max_zscore,cmap=mpl.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_labels = compute_and_plot_dendrogram(\n",
    "    gene_cluster_df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    ")\n",
    "# plt.savefig(\"./Dendrograms/Global_Dendrogram.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df = assign_dendro_clusts(gene_cluster_df, children_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fts\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fts_subset, figsize=(20, 8))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/fts.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpl_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpl\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpl_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpl.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpm\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpm_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpm.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rps\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rps_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rps.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_subset = gene_cluster_df[gene_cluster_df.apply(lambda x: \"rr\" in x[\"Gene\"], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rr_subset, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"tff\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(tff_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/tff.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpo_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpo\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpo_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpo.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"min\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(min_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/min.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"dna\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(dna_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/dna.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/fol.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muk_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"muk\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(muk_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/muk.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mre\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mre_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/mre.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mur_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mur\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mur_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/mur.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nus_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"nus\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(nus_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/nus.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"sec\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(sec_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/sec.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"bam\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(bam_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/bam.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/hol.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hda_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hda\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hda_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/hda.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodZ_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rodZ\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rodZ_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rodz.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_counts = np.unique(\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"], return_counts=True\n",
    ")\n",
    "singleton_clusters = clusters[cluster_counts < 3]\n",
    "small_clusters = clusters[(cluster_counts <= 40) & (cluster_counts >= 3)]\n",
    "big_clusters = clusters[cluster_counts > 40]\n",
    "print(singleton_clusters)\n",
    "print(small_clusters)\n",
    "print(big_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3to4 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([3, 4])]\n",
    "cluster_9to11 = gene_cluster_df[\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"].isin([9, 10, 11])\n",
    "]\n",
    "cluster_2 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([2])]\n",
    "cluster_12 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([12])]\n",
    "cluster_13 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([13])]\n",
    "cluster_14 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters = list(set(small_clusters) - set([3, 4, 9, 10, 11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in remaining_small_clusters:\n",
    "    cluster_df = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([i])]\n",
    "    make_subset_dendrogram(\n",
    "        cluster_df,\n",
    "        \"Cluster \" + str(i) + \" Dendrogram\",\n",
    "        figsize=(int((len(cluster_df) * 5.0) - 9.0), int((len(cluster_df) * 2.0) + 3)),\n",
    "        fontsize=4 + int(len(cluster_df) * 4.0),\n",
    "        linewidth=1 + int(len(cluster_df) * 1.0),\n",
    "    )\n",
    "    plt.show()\n",
    "#     plt.savefig(\"./Dendrograms/Cluster_\" + str(i) + \".png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_3to4,\n",
    "    \"Cluster 3 to 4 Dendrogram\",\n",
    "    figsize=(int((len(cluster_3to4) * 5.0) - 9.0), int((len(cluster_3to4) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_3to4) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_3to4) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_6to8.png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_9to11,\n",
    "    \"Cluster 9 to 11 Dendrogram\",\n",
    "    figsize=(\n",
    "        int((len(cluster_9to11) * 5.0) - 9.0),\n",
    "        int((len(cluster_9to11) * 2.0) + 3),\n",
    "    ),\n",
    "    fontsize=4 + int(len(cluster_9to11) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_9to11) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_9to10.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_2,\n",
    "    \"Cluster 2 Dendrogram\",\n",
    "    figsize=(int((len(cluster_2) * 5.0) - 9.0), int((len(cluster_2) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_2) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_2) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_12,\n",
    "    \"Cluster 12 Dendrogram\",\n",
    "    figsize=(int((len(cluster_12) * 5.0) - 9.0), int((len(cluster_12) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_12) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_12) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_13,\n",
    "    \"Cluster 13 Dendrogram\",\n",
    "    figsize=(int((len(cluster_13) * 5.0) - 9.0), int((len(cluster_13) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_13) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_13) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_14,\n",
    "    \"Cluster 14 Dendrogram\",\n",
    "    figsize=(int((len(cluster_14) * 5.0) - 9.0), int((len(cluster_14) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_14) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_14) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_cluster_df.to_csv(\"2021-07-31_Steady_State_Analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on Mean and Variance Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "\n",
    "lowess_and_iqr_params = [\n",
    "    \"LOWESS Trace: \" + param + \": score\" for param in params_to_transform\n",
    "] + [\"Binned IQR Interpolation: \" + param + \": score\" for param in params_to_transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_trace_df[\"Feature Vector\"] = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_and_iqr_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Feature Vector OnlyLOWESS\"] = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df_nan_filtered = lowess_trace_df[\n",
    "    ~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_effect_threshold = 35\n",
    "\n",
    "zero_vector = np.zeros(\n",
    "    (1, lowess_trace_df[\"Feature Vector OnlyLOWESS\"].iloc[0].shape[0])\n",
    ")\n",
    "feature_arr = np.array(lowess_trace_df[\"Feature Vector OnlyLOWESS\"].tolist())\n",
    "flattened_feature_arr = np.swapaxes(feature_arr, 1, 2).reshape(-1, feature_arr.shape[1])\n",
    "dist_arr = euclidean_distances(flattened_feature_arr, zero_vector).reshape(\n",
    "    feature_arr.shape[0], feature_arr.shape[2]\n",
    ")\n",
    "lowess_trace_df[\"Integrated Euclidean Norm\"] = sp.integrate.simpson(dist_arr)\n",
    "# lowess_trace_df[\"Max Euclidean Norm\"] = np.max(dist_arr,axis=1)\n",
    "\n",
    "sgrnadf_strong_effect = lowess_trace_df[\n",
    "    lowess_trace_df[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = np.min(lowess_trace_df[\"Integrated Euclidean Norm\"]), np.percentile(\n",
    "    lowess_trace_df[\"Integrated Euclidean Norm\"], 99\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Euclidean Norm\")\n",
    "plt.hist(\n",
    "    lowess_trace_df[\n",
    "        lowess_trace_df[\"Integrated Euclidean Norm\"] < strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    lowess_trace_df[\n",
    "        lowess_trace_df[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"], return_counts=True)\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_rep_example_series = (\n",
    "    sgrnadf_strong_effect.reset_index(drop=False)\n",
    "    .groupby(\"TargetID\")\n",
    "    .apply(lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])])\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(\"sgRNA\", drop=True)\n",
    ")\n",
    "\n",
    "normalized_timeseries = np.swapaxes(\n",
    "    normalize_timeseries(most_rep_example_series[\"Feature Vector\"], lmbda=0.5), 1, 2\n",
    ")\n",
    "most_rep_example_series[\"Normalized Feature Vector\"] = [\n",
    "    normalized_timeseries[i] for i in range(normalized_timeseries.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Distance Metrics\n",
    "\n",
    "Now, I want to evaluate the performance of different distance metrics on the data wrt seperating it maximally while also preserving similarity within replicates\n",
    "\n",
    "- DTW (can be done with cosine similarity) \n",
    "- cosine similarity (same as pearson for z-scores)\n",
    "- cross correlation\n",
    "\n",
    "Seems like soft-DTW is a pretty good option. Going forward with that for now.\n",
    "\n",
    "<!-- In the end cosine similarity was chosen as it produced superior silhouette scores for sets of targets from genes with different phenotypes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_examples_for_distance_metric = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin([\"ftsN\", \"rplA\", \"mreB\", \"tufB\", \"tff\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import (\n",
    "    dtw,\n",
    "    cdist_dtw,\n",
    "    dtw_path_from_metric,\n",
    "    cdist_soft_dtw,\n",
    "    cdist_soft_dtw_normalized,\n",
    ")\n",
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_arr = np.swapaxes(\n",
    "    np.array(\n",
    "        sgrnadf_examples_for_distance_metric[\"Normalized Feature Vector\"].tolist()\n",
    "    ),\n",
    "    1,\n",
    "    2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "\n",
    "    print(\n",
    "        \"Soft-DTW Gamma=\"\n",
    "        + str(gamma)\n",
    "        + \": \"\n",
    "        + str(\n",
    "            tslearn.clustering.silhouette_score(\n",
    "                timeseries_arr,\n",
    "                sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "                metric=\"softdtw\",\n",
    "                gamma=gamma,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"cosine\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Cosine-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"euclidean\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Euclidean-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_dtw_dist_arr = tslearn.metrics.cdist_soft_dtw(timeseries_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(soft_dtw_dist_arr.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting different effects against single genes\n",
    "\n",
    "1) Plot a histogram of minimum soft-DTW similarity within groups of TargetIDs against the same genes (for genes with more than one targetID)\n",
    "2) Use affinity propagation to select the number of phenotype clusters to use per gene (preference still needs to be dialed in, not sure how to optimize on this)\n",
    "3) Among each cluster, represent the final effect as the strongest effect (integrated euc norm) of the members of the cluster\n",
    "\n",
    "~~3) Among each cluster, represent the final effect as the median of the members of the cluster~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_softdtw(feature_vector_series):\n",
    "    dist_mat = cdist_soft_dtw_normalized(\n",
    "        np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    )\n",
    "    timeseries_len = (\n",
    "        feature_vector_series[0].shape[0] * feature_vector_series[0].shape[1]\n",
    "    )\n",
    "    dist_mat = dist_mat / timeseries_len\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def get_upper_right_vals(a):\n",
    "    upper_tri = np.triu(a, k=1)\n",
    "    upper_tri[upper_tri == 0.0] = np.NaN\n",
    "    return upper_tri\n",
    "\n",
    "\n",
    "def get_sgRNA_clusters(df, preference=0.6):\n",
    "    gene_indexed_df = (\n",
    "        df.reset_index(drop=False)\n",
    "        .set_index(\"Gene\")[[\"sgRNA\", \"Normalized Feature Vector\", \"TargetID\"]]\n",
    "        .sort_index()\n",
    "    )\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = pd.Series(\n",
    "        np.zeros(len(gene_indexed_df), dtype=int), dtype=int\n",
    "    )\n",
    "    gene_df_list = []\n",
    "    for gene in gene_indexed_df.index.tolist():\n",
    "        gene_df = gene_indexed_df.loc[[gene]]\n",
    "        if len(gene_df) > 1:\n",
    "            gene_feature_vector = gene_df[\"Normalized Feature Vector\"]\n",
    "            soft_dtw_dist = get_normed_softdtw(gene_feature_vector)\n",
    "            af_labels = (\n",
    "                AffinityPropagation(\n",
    "                    affinity=\"precomputed\", preference=preference, random_state=42\n",
    "                )\n",
    "                .fit_predict(-soft_dtw_dist)\n",
    "                .astype(int)\n",
    "            )\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = af_labels\n",
    "        else:\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = 0\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = gene_indexed_df[\"sgRNA Cluster\"].astype(int)\n",
    "    return gene_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sgrna_replicate_thr = 2\n",
    "pref_factor = 3.0\n",
    "\n",
    "gene_list, counts_list = np.unique(most_rep_example_series[\"Gene\"], return_counts=True)\n",
    "genes_with_many_replicate_sgRNAs = gene_list[counts_list >= n_sgrna_replicate_thr]\n",
    "sgrnadf_many_copies_per_gene = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin(genes_with_many_replicate_sgRNAs)\n",
    "]\n",
    "\n",
    "max_distance_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: np.nanmax(\n",
    "        get_upper_right_vals(get_normed_softdtw(x[\"Normalized Feature Vector\"]))\n",
    "    )\n",
    ")\n",
    "plt.title(\"Maximum soft-DTW Distance per Gene\")\n",
    "plt.hist(max_distance_within_gene, bins=50)\n",
    "plt.show()\n",
    "\n",
    "dist_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: get_upper_right_vals(\n",
    "        get_normed_softdtw(x[\"Normalized Feature Vector\"])\n",
    "    ).flatten()\n",
    ")\n",
    "dist_within_gene = [val for item in dist_within_gene.tolist() for val in item]\n",
    "median_similarity = -np.nanmedian(dist_within_gene)\n",
    "\n",
    "gene_df = get_sgRNA_clusters(\n",
    "    most_rep_example_series, preference=pref_factor * median_similarity\n",
    ")\n",
    "\n",
    "most_rep_example_series[\"sgRNA Cluster\"] = gene_df.set_index(\"sgRNA\")[\"sgRNA Cluster\"]\n",
    "most_rep_example_series[\"sgRNA Cluster Label\"] = most_rep_example_series.apply(\n",
    "    lambda x: str(x[\"Gene\"]) + \"-\" + str(x[\"sgRNA Cluster\"]), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df = most_rep_example_series[\n",
    "    [\n",
    "        \"sgRNA Cluster Label\",\n",
    "        \"Normalized Feature Vector\",\n",
    "        \"Gene\",\n",
    "        \"Integrated Euclidean Norm\",\n",
    "    ]\n",
    "    + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "].reset_index(drop=True)\n",
    "gene_cluster_groupby = gene_cluster_df.groupby(\"sgRNA Cluster Label\")\n",
    "# median_feature_series = gene_cluster_groupby.apply(lambda x: np.median(np.stack(x[\"Feature Vector\"]).astype(float), axis=0)).to_frame().rename(columns={0:\"Feature Vector\"})\n",
    "feature_series = (\n",
    "    gene_cluster_groupby.apply(\n",
    "        lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])][\n",
    "            \"Normalized Feature Vector\"\n",
    "        ]\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Normalized Feature Vector\"})\n",
    ")\n",
    "\n",
    "gene_cluster_df = gene_cluster_groupby.apply(\n",
    "    lambda x: x.iloc[0][\n",
    "        [\"Gene\"]\n",
    "        + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "    ]\n",
    ")\n",
    "gene_cluster_df = gene_cluster_df.join(feature_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: TSNE and Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = get_normed_softdtw(gene_cluster_df[\"Normalized Feature Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(\n",
    "    n_components=2, perplexity=5.0, early_exaggeration=50.0, metric=\"precomputed\"\n",
    ").fit_transform(X_dist - np.min(X_dist))\n",
    "gene_cluster_df[\"TSNE Coords\"] = [X_embedded[i] for i in range(X_embedded.shape[0])]\n",
    "\n",
    "af_labels = (\n",
    "    AffinityPropagation(affinity=\"precomputed\", preference=-0.5)\n",
    "    .fit_predict(-X_dist)\n",
    "    .astype(int)\n",
    ")\n",
    "gene_cluster_df[\"Affinity Clusts\"] = af_labels\n",
    "\n",
    "plt.scatter(\n",
    "    X_embedded[:, 0],\n",
    "    X_embedded[:, 1],\n",
    "    s=3,\n",
    "    alpha=1,\n",
    "    c=gene_cluster_df[\"Affinity Clusts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_labels = [\n",
    "    \"Birth Length\",\n",
    "    \"Division Length\",\n",
    "    \"Area Growth Rate\",\n",
    "    \"Length Growth Rate\",\n",
    "    \"Average Width\",\n",
    "    \"mCherry Intensity\",\n",
    "    \"Cell Cycle Duration\",\n",
    "]\n",
    "\n",
    "feature_labels = feature_labels + [label + \": IQR\" for label in feature_labels]\n",
    "\n",
    "hierarchical_labels = gene_cluster_df.index.tolist()\n",
    "\n",
    "\n",
    "def get_leaf_children(tree, leaf_id):\n",
    "    cluster_node = tree[leaf_id]\n",
    "    leaf_children = cluster_node.pre_order(lambda x: x.id)\n",
    "    return leaf_children\n",
    "\n",
    "\n",
    "def assign_dendro_clusts(df, children_labels):\n",
    "    df_out = copy.deepcopy(df)\n",
    "    df_out[\"Dendrogram Clusters\"] = pd.Series(len(df), dtype=int)\n",
    "    for clust_i, indices in enumerate(children_labels):\n",
    "        df_out[\"Dendrogram Clusters\"].iloc[indices] = clust_i\n",
    "    df_out[\"Dendrogram Clusters\"] = df_out[\"Dendrogram Clusters\"].astype(int)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "suppress_thr = 15\n",
    "min_zscore = -2\n",
    "max_zscore = 2\n",
    "\n",
    "\n",
    "def compute_and_plot_dendrogram(\n",
    "    df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "):\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    hierarchical_labels = df.index.tolist()\n",
    "    X = np.array(df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(2, suppress_thr)\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    #     Y = sch.linkage(X, method='weighted', metric='cosine',optimal_ordering=True)\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        truncate_mode=\"lastp\",\n",
    "        show_contracted=True,\n",
    "        p=suppress_thr,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "    children_labels = [get_leaf_children(cluster_tree, leaf) for leaf in Z[\"leaves\"]]\n",
    "\n",
    "    #     fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\\\n",
    "    #                  ax=dendro_ax, orientation='vertical', label='Z-score',\\\n",
    "    #                 use_gridspec=True, location='left', pad=-0.05,aspect=10)\n",
    "\n",
    "    for i, children in enumerate(children_labels):\n",
    "        children_arr = np.array(\n",
    "            df.iloc[children][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "        mean_vector = np.mean(children_arr, axis=0)  # feature,timepoint\n",
    "        # fig = plt.figure(constrained_layout=True, figsize=(20,10))\n",
    "        # gs = fig.add_gridspec(2, 10)\n",
    "        # for v in range(10):\n",
    "        #     inner_gs = gs[0,v].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0, )\n",
    "        #     inner_grid_sub = inner_gs.subplots()\n",
    "        #     for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "        #         ax.plot(mean_vector[c])\n",
    "        #         ax.set(xticks=[], yticks=[])\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                mean_vector.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 10])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=18,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return children_labels\n",
    "\n",
    "\n",
    "def plot_subset(\n",
    "    df_subset,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    feature_labels=feature_labels,\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "):\n",
    "\n",
    "    df_clusts = (\n",
    "        df_subset.sort_index()\n",
    "        .reset_index(drop=False)\n",
    "        .set_index(\"Dendrogram Clusters\")[\n",
    "            [\"sgRNA Cluster Label\", \"Normalized Feature Vector\"]\n",
    "        ]\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(df_clusts), wspace=wspace)\n",
    "\n",
    "    for i in range(len(df_clusts)):\n",
    "        clust_arr = np.array(\n",
    "            df_clusts[\"Normalized Feature Vector\"].iloc[i].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 10.0])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=18,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "        #             imshow_first_ax = fig.add_subplot(gs[0, i])\n",
    "        #             imshow_first_ax.imshow(df_clusts[\"Feature Vector\"].iloc[i].astype(float).reshape(-1,1),cmap=cmap,norm=norm)\n",
    "\n",
    "        #             imshow_first_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "        #             imshow_first_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "        #             imshow_first_ax.set_xlabel(df_clusts[\"sgRNA Cluster Label\"].iloc[i] + \"\\n Cluster \" + str(df_clusts.index[i]), fontsize=14)\n",
    "\n",
    "        #             imshow_first_ax.set_yticks(range(len(feature_labels)))\n",
    "        #             imshow_first_ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(clust_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def make_subset_dendrogram(\n",
    "    sub_df,\n",
    "    title,\n",
    "    feature_labels=feature_labels,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    figsize=(10, 10),\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "    X_dist = get_normed_softdtw(sub_df[\"Normalized Feature Vector\"])\n",
    "    X = np.array(sub_df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(2, len(sub_df))\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        show_contracted=True,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    fig.suptitle(title, fontsize=fontsize)\n",
    "\n",
    "    for i, leaf in enumerate(Z[\"leaves\"]):\n",
    "        leaf_arr = np.array(\n",
    "            sub_df.iloc[leaf][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                leaf_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 8])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(leaf_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "#         imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "#         imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "\n",
    "#         imshow_ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_and_plot_dendrogram(df,X_dist,feature_labels,suppress_thr,min_zscore,max_zscore,cmap=mpl.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_labels = compute_and_plot_dendrogram(\n",
    "    gene_cluster_df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    ")\n",
    "# plt.savefig(\"./Dendrograms/Global_Dendrogram.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df = assign_dendro_clusts(gene_cluster_df, children_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fts\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fts_subset, figsize=(20, 8))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/fts.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpl_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpl\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpl_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpl.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpm\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpm_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpm.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rps\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rps_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rps.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_subset = gene_cluster_df[gene_cluster_df.apply(lambda x: \"rr\" in x[\"Gene\"], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rr_subset, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"tff\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(tff_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/tff.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpo_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpo\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpo_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpo.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"min\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(min_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/min.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"dna\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(dna_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/dna.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/fol.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muk_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"muk\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(muk_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/muk.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mre\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mre_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/mre.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mur_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mur\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mur_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/mur.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nus_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"nus\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(nus_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/nus.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"sec\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(sec_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/sec.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"bam\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(bam_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/bam.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/hol.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hda_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hda\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hda_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/hda.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodZ_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rodZ\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rodZ_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rodz.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_7 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_7,\n",
    "    \"Cluster 7 Dendrogram\",\n",
    "    figsize=(int((len(cluster_7) * 5.0) - 9.0), int((len(cluster_7) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_7) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_7) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_counts = np.unique(\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"], return_counts=True\n",
    ")\n",
    "singleton_clusters = clusters[cluster_counts < 3]\n",
    "small_clusters = clusters[(cluster_counts <= 40) & (cluster_counts >= 3)]\n",
    "big_clusters = clusters[cluster_counts > 40]\n",
    "print(singleton_clusters)\n",
    "print(small_clusters)\n",
    "print(big_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3to4 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([3, 4])]\n",
    "cluster_9to11 = gene_cluster_df[\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"].isin([9, 10, 11])\n",
    "]\n",
    "cluster_2 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([2])]\n",
    "cluster_12 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([12])]\n",
    "cluster_13 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([13])]\n",
    "cluster_14 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters = list(set(small_clusters) - set([3, 4, 9, 10, 11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in remaining_small_clusters:\n",
    "    cluster_df = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([i])]\n",
    "    make_subset_dendrogram(\n",
    "        cluster_df,\n",
    "        \"Cluster \" + str(i) + \" Dendrogram\",\n",
    "        figsize=(int((len(cluster_df) * 5.0) - 9.0), int((len(cluster_df) * 2.0) + 3)),\n",
    "        fontsize=4 + int(len(cluster_df) * 4.0),\n",
    "        linewidth=1 + int(len(cluster_df) * 1.0),\n",
    "    )\n",
    "    plt.show()\n",
    "#     plt.savefig(\"./Dendrograms/Cluster_\" + str(i) + \".png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_3to4,\n",
    "    \"Cluster 3 to 4 Dendrogram\",\n",
    "    figsize=(int((len(cluster_3to4) * 5.0) - 9.0), int((len(cluster_3to4) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_3to4) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_3to4) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_6to8.png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_9to11,\n",
    "    \"Cluster 9 to 11 Dendrogram\",\n",
    "    figsize=(\n",
    "        int((len(cluster_9to11) * 5.0) - 9.0),\n",
    "        int((len(cluster_9to11) * 2.0) + 3),\n",
    "    ),\n",
    "    fontsize=4 + int(len(cluster_9to11) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_9to11) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_9to10.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_2,\n",
    "    \"Cluster 2 Dendrogram\",\n",
    "    figsize=(int((len(cluster_2) * 5.0) - 9.0), int((len(cluster_2) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_2) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_2) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_12,\n",
    "    \"Cluster 12 Dendrogram\",\n",
    "    figsize=(int((len(cluster_12) * 5.0) - 9.0), int((len(cluster_12) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_12) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_12) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_13,\n",
    "    \"Cluster 13 Dendrogram\",\n",
    "    figsize=(int((len(cluster_13) * 5.0) - 9.0), int((len(cluster_13) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_13) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_13) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_14,\n",
    "    \"Cluster 14 Dendrogram\",\n",
    "    figsize=(int((len(cluster_14) * 5.0) - 9.0), int((len(cluster_14) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_14) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_14) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_cluster_df.to_csv(\"2021-07-31_Steady_State_Analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_output_df_pd.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "df[\"phenotype trenchids\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    df, trenchids_as_list=True, trenchid_column=\"phenotype trenchids\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    df,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button  ## NEED OPTION WHETHER OR NOT TO NORM SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
