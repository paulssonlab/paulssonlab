{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Clustering Analysis of lDE20 (with lineage Dataframe ready)\n",
    "\n",
    "- Note that there are fluctuations in the illumination intensity which may be resulting in pathological behavior from the reporter\n",
    "\n",
    "- This has been normalized out in the upstream processing, but try to fix long term\n",
    "\n",
    "- Also consider a flat field correction for the final experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask\n",
    "import warnings\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import (\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    "    cosine_distances,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "\n",
    "\n",
    "import pylab\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgrnadf_from_scoredf(\n",
    "    scoredf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    scoredf_groupby = scoredf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        scoredf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": score\"] = scoredf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label + \": score\"].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = scoredf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = scoredf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = scoredf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = scoredf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = scoredf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = scoredf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "# No longer using this\n",
    "# def filter_strong_KOs(df,sampling_thr = 4, n_strongest=2):\n",
    "\n",
    "#     for i in range(sampling_thr,0,-1):\n",
    "#         sampling_mask = df[\"N Observations\"]>=sampling_thr\n",
    "#         mismatch_series = df[sampling_mask][\"N Mismatch\"]\n",
    "\n",
    "#         for n in range(n_strongest,0,-1):\n",
    "#             if len(mismatch_series)>=n:\n",
    "#                 keep_indices = np.argsort(mismatch_series)[:n]\n",
    "#                 out_df = df[sampling_mask].iloc[keep_indices]\n",
    "\n",
    "#                 return out_df\n",
    "\n",
    "\n",
    "def normalize_timeseries(feature_vector_series, lmbda=0.5):\n",
    "    timeseries_arr = np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    sigma = np.std(timeseries_arr, axis=1)\n",
    "    if lmbda > 0.0:\n",
    "        sigma_prime = ((sigma + 1) ** lmbda - 1) / lmbda  ##yeo-johnson\n",
    "    elif lmbda == 0.0:\n",
    "        sigma_prime = np.log(sigma + 1)\n",
    "    else:\n",
    "        raise ValueError(\"lmbda cannot be negative\")\n",
    "    normalizer = sigma / sigma_prime\n",
    "    normalized_timeseries = timeseries_arr / normalizer[:, np.newaxis, :]\n",
    "    return normalized_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-10_lDE20_Lineage_Analysis\",\n",
    "    engine=\"fastparquet\",\n",
    ")\n",
    "final_output_df = final_output_df.dropna(\n",
    "    subset=[\n",
    "        \"final timepoints\",\n",
    "        \"Mean Exponential Growth Rate: area\",\n",
    "        \"Birth: minor_axis_length\",\n",
    "        \"Birth: Surface Area\",\n",
    "    ]\n",
    ")\n",
    "final_output_df = (\n",
    "    final_output_df.reset_index().set_index(\"phenotype trenchid\", sorted=True).persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_trench_groupby = final_output_df.groupby(\n",
    "    \"phenotype trenchid\", sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Filter for \"Normal\" Sizes at Start\n",
    "\n",
    "1) Fit a gaussian model to each of the specified feature params during the first t timepoints of the experiment (using a subsample for speed) \n",
    "2) Compute a normalized probability trenchwise for these features under the gaussian model, during the first t timepoints of the experiment\n",
    "3) Eliminate trenches that are under some p percentile value of this probability for each feature\n",
    "4) Display histograms for each property as well as the resulting theshold\n",
    "\n",
    "Note that these features should be the only features examined in the resulting analysis. For the notebook, I am looking at:\n",
    "- Birth length (Lb)\n",
    "- Division length (Ld)\n",
    "- Mean Area Increment\n",
    "- Mean Length Increment\n",
    "- Mean Width\n",
    "- Cell cycle duration (Delta t)\n",
    "- Mean mCherry Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_timepoint_cutoff = 30\n",
    "gaussian_subsample = 0.2\n",
    "percentile_threshold = 10\n",
    "\n",
    "filter_params = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta t\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_tpt_df = final_output_df_trench_groupby.apply(\n",
    "    lambda x: x[x[\"final timepoints\"] < early_timepoint_cutoff].reset_index(drop=True)\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filter_param in filter_params:\n",
    "    early_param_series = early_tpt_df[filter_param]\n",
    "    all_param_values = (\n",
    "        early_param_series.sample(frac=gaussian_subsample).compute().tolist()\n",
    "    )\n",
    "    gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    early_param_series = dd.from_pandas(\n",
    "        early_param_series.compute().droplevel(1), npartitions=50\n",
    "    )\n",
    "    trench_probability = early_param_series.groupby(\"phenotype trenchid\").apply(\n",
    "        lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "    )\n",
    "\n",
    "    final_output_df[filter_param + \": Probability\"] = trench_probability.persist()\n",
    "\n",
    "final_output_df_onetrench = (\n",
    "    final_output_df.groupby(\"phenotype trenchid\").apply(lambda x: x.iloc[0]).compute()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(22, 16))\n",
    "query_list = []\n",
    "for i, filter_param in enumerate(filter_params):\n",
    "    prob_threshold = np.nanpercentile(\n",
    "        final_output_df_onetrench[filter_param + \": Probability\"].tolist(),\n",
    "        percentile_threshold,\n",
    "    )\n",
    "    query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "    query_list.append(query)\n",
    "\n",
    "    min_v, max_v = (\n",
    "        np.min(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "        np.max(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "    )\n",
    "\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.title(filter_param)\n",
    "    plt.hist(\n",
    "        final_output_df_onetrench[\n",
    "            final_output_df_onetrench[filter_param + \": Probability\"] < prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "    plt.hist(\n",
    "        final_output_df_onetrench[\n",
    "            final_output_df_onetrench[filter_param + \": Probability\"] >= prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "plt.show()\n",
    "\n",
    "compiled_query = \" and \".join(query_list)\n",
    "final_output_df_onetrench_filtered = final_output_df_onetrench.query(compiled_query)\n",
    "final_output_df_filtered = final_output_df.loc[\n",
    "    final_output_df_onetrench_filtered.index.tolist()\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_output_df_filtered) / len(final_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"final timepoints\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    if flatten_vals:\n",
    "        masked_label_series = df.apply(\n",
    "            lambda x: np.array(x[label])[\n",
    "                (np.array(x[time_label]) >= min_timepoint)\n",
    "                * (np.array(x[time_label]) <= max_timepoint)\n",
    "            ],\n",
    "            axis=1,\n",
    "            meta=\"object\",\n",
    "        )\n",
    "        flattened_vals = np.concatenate(masked_label_series.compute().tolist())\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        masked_label_series = (\n",
    "            df.groupby(\"phenotype trenchid\")\n",
    "            .apply(\n",
    "                lambda x: np.array(x[label])[\n",
    "                    (np.array(x[time_label]) >= min_timepoint)\n",
    "                    * (np.array(x[time_label]) <= max_timepoint)\n",
    "                ],\n",
    "                meta=\"object\",\n",
    "            )\n",
    "            .persist()\n",
    "        )\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "def get_feature_stats(\n",
    "    df, feature_label, min_timepoint, max_timepoint, time_label=\"final timepoints\"\n",
    "):\n",
    "    feature_vals = get_timepoint_values(\n",
    "        df, feature_label, min_timepoint, max_timepoint, time_label=time_label\n",
    "    )\n",
    "    feature_mean = np.mean(feature_vals)\n",
    "    feature_std = np.std(feature_vals)\n",
    "    return feature_mean, feature_std\n",
    "\n",
    "\n",
    "def get_feature_mean_bytrench(df, feature_label, min_timepoint, max_timepoint):\n",
    "    masked_label_series = get_timepoint_values(\n",
    "        df, feature_label, min_timepoint, max_timepoint, flatten_vals=False\n",
    "    )\n",
    "    trench_mean_series = (\n",
    "        masked_label_series.apply(lambda x: np.nanmean(x), meta=float)\n",
    "        .compute()\n",
    "        .sort_index()\n",
    "    )\n",
    "    return trench_mean_series\n",
    "\n",
    "\n",
    "def compute_score(df, feature_label, trench_mean_series, feature_mean, feature_std):\n",
    "    scaling_factor = feature_mean / feature_std\n",
    "    scores = (df[feature_label] / trench_mean_series) - 1.0\n",
    "    scores = scaling_factor * scores\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_feature_scores(\n",
    "    df, feature_label, init_timepoint_range=(0, 20), time_label=\"final timepoints\"\n",
    "):\n",
    "    feature_mean, feature_std = get_feature_stats(\n",
    "        df,\n",
    "        feature_label,\n",
    "        init_timepoint_range[0],\n",
    "        init_timepoint_range[1],\n",
    "        time_label=time_label,\n",
    "    )\n",
    "    trench_mean_series = get_feature_mean_bytrench(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    scores = compute_score(\n",
    "        df, feature_label, trench_mean_series, feature_mean, feature_std\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_scores(\n",
    "    df, feature_labels, init_timepoint_range=(0, 20), time_label=\"final timepoints\"\n",
    "):\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_scores(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "        )\n",
    "        df[feature_label + \": z score\"] = feature_scores.persist()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_filtered.loc[:5].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Properties\n",
    "\n",
    "1) Yeo-Johnson transform the data as before (omitting, but return later).\n",
    "2) Convert transformed values to time-dependent z-scores using the following formula (trying to use means):\n",
    "\n",
    "$$ z(i,k,t) = \\frac{mean_{t\\in \\tau}(F_{i,t})}{std_{t\\in \\tau}(F_{i,t})}\\Bigg(\\frac{F_{i,k,t}}{mean_{t\\in \\tau}(F_{i,k,t})} - 1\\Bigg) $$\n",
    "\n",
    "where $F_{i,k,t}$ are the feature values for feature i, trench k at time t. $\\tau$ are the initial pre-induction timepoints. \n",
    "\n",
    "Essentially this is a z-score using the more outlier robust median and interquartile range to define the differences from normal bahavior. The 1.35 factor scales the values such that z-scores represent number of standard deviations from the mean for a normal distribution. Finally the values are normalized by initial behaviors trenchwise by the $median_{t\\in \\tau}(F_{i,k,t})$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta t\",\n",
    "]\n",
    "\n",
    "# yeo_subsample = 0.1\n",
    "\n",
    "# final_output_df_pd_filtered_dask = dd.from_pandas(final_output_df_pd_filtered,npartitions=100).persist()\n",
    "# dask.distributed.wait(final_output_df_pd_filtered_dask)\n",
    "\n",
    "# for i,param in enumerate(params_to_transform):\n",
    "#     all_param_values = [float(val) for item in final_output_df_pd_filtered_dask[param].sample(frac=yeo_subsample).compute().tolist() for val in item]\n",
    "#     l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "#     final_output_df_pd_filtered_dask[param] = final_output_df_pd_filtered_dask[param].apply(lambda x: sp.stats.yeojohnson(np.array(x).astype(float),lmbda = l_norm), meta='object').persist()\n",
    "# final_output_df_pd_filtered = final_output_df_pd_filtered_dask.compute()\n",
    "\n",
    "scoredf = get_all_feature_scores(final_output_df_filtered, params_to_transform)\n",
    "scoredf_noidx = scoredf.reset_index().set_index(\"Global CellID\", drop=True)\n",
    "sgRNA_dict = {\n",
    "    sgRNA: i\n",
    "    for i, sgRNA in enumerate(\n",
    "        sorted(scoredf_noidx[\"sgRNA\"].unique().compute().tolist())\n",
    "    )\n",
    "}\n",
    "scoredf_noidx[\"sgRNAid\"] = (\n",
    "    scoredf_noidx[\"sgRNA\"].apply(lambda x: sgRNA_dict[x], meta=int).compute()\n",
    ")\n",
    "sgrnadf = scoredf_noidx.reset_index().set_index(\"sgRNAid\", drop=True)\n",
    "sgrnadf[\"N Observations\"] = sgrnadf.groupby(\"sgRNAid\")[\"trenchid\"].apply(\n",
    "    lambda x: len(x.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgRNA Effect Size Filtering (within Gene groups)\n",
    "\n",
    "1) Threshold sgRNAs to include by number of observations\n",
    "2) Use Kernel smoothing to smooth out score timeseries into 20 point timeseries\n",
    "3) For each timepoint, measure the euclidean norm of the feature vector and take the maximum over all time as a measure of effect size\n",
    "4) Threshold sgRNAs for strong effects by applying a threshold to the euclidean norm that will be displayed with histogram\n",
    "5) Display a histogram for the sgRNA number per gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric import kernel_regression\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import sklearn as skl\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# def get_gridded_obs(df,val_label,grid_step=1):\n",
    "#     init_t,final_t = df[\"initial timepoints\"],df[\"final timepoints\"]\n",
    "#     range_series = df.apply(lambda x: list(range(x[\"initial timepoints\"],x[\"final timepoints\"]+1,grid_step)), axis=1)\n",
    "#     val_series = df.apply(lambda x: [x[val_label] for i in range(x[\"initial timepoints\"],x[\"final timepoints\"]+1,grid_step)], axis=1)\n",
    "\n",
    "#     range_arr = np.array([val for item in range_series for val in item])\n",
    "#     val_arr = np.array([val for item in val_series for val in item])\n",
    "\n",
    "# #     out_arr = np.stack([range_arr,val_arr])\n",
    "\n",
    "#     range_df = pd.DataFrame(data={\"time\":range_arr,\"value\":val_arr})\n",
    "#     med = range_df.groupby(pd.cut(range_df[\"time\"], np.arange(0, 143, 10))).mean()\n",
    "\n",
    "# #     med = range_df[\"value\"].rolling(3).median()\n",
    "\n",
    "#     return med\n",
    "\n",
    "\n",
    "def timeseries_kernel_reg(df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth):\n",
    "    def kernel_reg(\n",
    "        x_arr,\n",
    "        y_arr,\n",
    "        start=min_tpt,\n",
    "        end=max_tpt,\n",
    "        kernel_bins=kernel_bins,\n",
    "        kernel_bandwidth=kernel_bandwidth,\n",
    "    ):\n",
    "        intervals = np.linspace(start, end, num=kernel_bins, dtype=float)\n",
    "        w = kernel_regression.KernelReg(\n",
    "            y_arr,\n",
    "            x_arr,\n",
    "            \"c\",\n",
    "            reg_type=\"lc\",\n",
    "            bw=np.array([kernel_bandwidth]),\n",
    "            ckertype=\"gaussian\",\n",
    "        ).fit(intervals)[0]\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    kernel_result = df.groupby(\"sgRNAid\").apply(\n",
    "        lambda x: kernel_reg(\n",
    "            (x[\"final timepoints\"].values + x[\"initial timepoints\"].values) / 2,\n",
    "            x[y_label].values,\n",
    "        )[1],\n",
    "        meta=float,\n",
    "    )\n",
    "\n",
    "    return kernel_result\n",
    "\n",
    "\n",
    "def get_all_kernel_regs(\n",
    "    df, y_label_list, min_tpt, max_tpt, kernel_bins=20, kernel_bandwidth=10\n",
    "):\n",
    "    out_df = copy.copy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        kernel_result = timeseries_kernel_reg(\n",
    "            out_df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth\n",
    "        )\n",
    "        out_df[\"Kernel Trace: \" + y_label] = kernel_result.persist()\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Observations_thr = 4\n",
    "\n",
    "sgrnadf_wellsampled = sgrnadf[sgrnadf[\"N Observations\"] >= N_Observations_thr].persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "LOWESS extrapolates past the edge in an annoying way since it is fitting the slope. Rolling mean/median worth trying...implement after meeting.\n",
    "\n",
    "Take a look at single examples later if I need to make improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making an observation grid to project vals onto\n",
    "\n",
    "min_tpt = 0\n",
    "max_tpt = 143\n",
    "\n",
    "kernel_bins = 20\n",
    "kernel_bandwidth = 10\n",
    "\n",
    "score_params = [param + \": z score\" for param in params_to_transform]\n",
    "\n",
    "trace_df = get_all_kernel_regs(\n",
    "    sgrnadf_wellsampled,\n",
    "    score_params,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    kernel_bins=kernel_bins,\n",
    "    kernel_bandwidth=kernel_bandwidth,\n",
    ")\n",
    "trace_df = trace_df.groupby(\"sgRNAid\").apply(lambda x: x.iloc[0]).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on Mean Behavior Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_params = [\n",
    "    \"Kernel Trace: \" + param + \": z score\" for param in params_to_transform\n",
    "]\n",
    "feature_vector_series = trace_df.apply(\n",
    "    lambda x: np.array(x[kernel_params].tolist()), axis=1\n",
    ")\n",
    "trace_df[\"Feature Vector\"] = feature_vector_series\n",
    "trace_df_nan_filtered = trace_df[\n",
    "    ~trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# # Maybe move this normalization up before effect size selection\n",
    "# X = np.stack(trace_df_nan_filtered[\"Feature Vector\"].tolist(),axis=0)\n",
    "# X = np.swapaxes(X,1,2)\n",
    "# X_norm = TimeSeriesScalerMeanVariance(mu=0.,std=1.).fit_transform(X)\n",
    "# X_norm = np.swapaxes(X_norm,1,2)\n",
    "# trace_df_nan_filtered[\"Normalized Feature Vector\"] = [X_norm[i] for i in range(X_norm.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strong_effect_threshold = 35\n",
    "\n",
    "zero_vector = np.zeros((1, trace_df_nan_filtered[\"Feature Vector\"].iloc[0].shape[0]))\n",
    "feature_arr = np.array(trace_df_nan_filtered[\"Feature Vector\"].tolist())\n",
    "flattened_feature_arr = np.swapaxes(feature_arr, 1, 2).reshape(-1, feature_arr.shape[1])\n",
    "dist_arr = euclidean_distances(flattened_feature_arr, zero_vector).reshape(\n",
    "    feature_arr.shape[0], feature_arr.shape[2]\n",
    ")\n",
    "trace_df_nan_filtered[\"Integrated Euclidean Norm\"] = sp.integrate.simpson(dist_arr)\n",
    "# lowess_trace_df[\"Max Euclidean Norm\"] = np.max(dist_arr,axis=1)\n",
    "\n",
    "sgrnadf_strong_effect = trace_df_nan_filtered[\n",
    "    trace_df_nan_filtered[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = (\n",
    "    np.min(trace_df_nan_filtered[\"Integrated Euclidean Norm\"]),\n",
    "    np.percentile(trace_df_nan_filtered[\"Integrated Euclidean Norm\"], 99),\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Euclidean Norm\")\n",
    "plt.hist(\n",
    "    trace_df_nan_filtered[\n",
    "        trace_df_nan_filtered[\"Integrated Euclidean Norm\"] < strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    trace_df_nan_filtered[\n",
    "        trace_df_nan_filtered[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"], return_counts=True)\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_strong_effect.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-16_gene_cluster_df_full.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_rep_example_series = (\n",
    "    sgrnadf_strong_effect.reset_index(drop=False)\n",
    "    .groupby(\"TargetID\")\n",
    "    .apply(lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])])\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(\"sgRNA\", drop=True)\n",
    ")\n",
    "\n",
    "## THIS IS FOR A LOG TRANSFORMATION, try to do this earlier when it makes more sense....\n",
    "# normalized_timeseries = np.swapaxes(normalize_timeseries(most_rep_example_series[\"Feature Vector\"], lmbda=0.5),1,2)\n",
    "# most_rep_example_series[\"Normalized Feature Vector\"] = [normalized_timeseries[i] for i in range(normalized_timeseries.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Distance Metrics\n",
    "\n",
    "Now, I want to evaluate the performance of different distance metrics on the data wrt seperating it maximally while also preserving similarity within replicates\n",
    "\n",
    "- DTW (can be done with cosine similarity) \n",
    "- cosine similarity (same as pearson for z-scores)\n",
    "- cross correlation\n",
    "\n",
    "Seems like soft-DTW is a pretty good option. Going forward with that for now.\n",
    "\n",
    "<!-- In the end cosine similarity was chosen as it produced superior silhouette scores for sets of targets from genes with different phenotypes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_examples_for_distance_metric = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin([\"ftsN\", \"rplA\", \"mreB\", \"tufB\", \"tff\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import (\n",
    "    dtw,\n",
    "    cdist_dtw,\n",
    "    dtw_path_from_metric,\n",
    "    cdist_soft_dtw,\n",
    "    cdist_soft_dtw_normalized,\n",
    ")\n",
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_arr = np.swapaxes(\n",
    "    np.array(sgrnadf_examples_for_distance_metric[\"Feature Vector\"].tolist()), 1, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "\n",
    "    print(\n",
    "        \"Soft-DTW Gamma=\"\n",
    "        + str(gamma)\n",
    "        + \": \"\n",
    "        + str(\n",
    "            tslearn.clustering.silhouette_score(\n",
    "                timeseries_arr,\n",
    "                sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "                metric=\"softdtw\",\n",
    "                gamma=gamma,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"cosine\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Cosine-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"euclidean\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Euclidean-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_dtw_dist_arr = tslearn.metrics.cdist_soft_dtw(timeseries_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(soft_dtw_dist_arr.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting different effects against single genes\n",
    "\n",
    "1) Plot a histogram of minimum soft-DTW similarity within groups of TargetIDs against the same genes (for genes with more than one targetID)\n",
    "2) Use affinity propagation to select the number of phenotype clusters to use per gene (preference still needs to be dialed in, not sure how to optimize on this)\n",
    "3) Among each cluster, represent the final effect as the strongest effect (integrated euc norm) of the members of the cluster\n",
    "\n",
    "~~3) Among each cluster, represent the final effect as the median of the members of the cluster~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_softdtw(feature_vector_series):\n",
    "    dist_mat = cdist_soft_dtw_normalized(\n",
    "        np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    )\n",
    "    timeseries_len = (\n",
    "        feature_vector_series[0].shape[0] * feature_vector_series[0].shape[1]\n",
    "    )\n",
    "    dist_mat = dist_mat / timeseries_len\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def get_upper_right_vals(a):\n",
    "    upper_tri = np.triu(a, k=1)\n",
    "    upper_tri[upper_tri == 0.0] = np.NaN\n",
    "    return upper_tri\n",
    "\n",
    "\n",
    "def get_sgRNA_clusters(df, preference=0.6):\n",
    "    gene_indexed_df = (\n",
    "        df.reset_index(drop=False)\n",
    "        .set_index(\"Gene\")[[\"sgRNA\", \"Feature Vector\", \"TargetID\"]]\n",
    "        .sort_index()\n",
    "    )\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = pd.Series(\n",
    "        np.zeros(len(gene_indexed_df), dtype=int), dtype=int\n",
    "    )\n",
    "    gene_df_list = []\n",
    "    for gene in gene_indexed_df.index.tolist():\n",
    "        gene_df = gene_indexed_df.loc[[gene]]\n",
    "        if len(gene_df) > 1:\n",
    "            gene_feature_vector = gene_df[\"Feature Vector\"]\n",
    "            soft_dtw_dist = get_normed_softdtw(gene_feature_vector)\n",
    "            af_labels = (\n",
    "                AffinityPropagation(\n",
    "                    affinity=\"precomputed\", preference=preference, random_state=42\n",
    "                )\n",
    "                .fit_predict(-soft_dtw_dist)\n",
    "                .astype(int)\n",
    "            )\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = af_labels\n",
    "        else:\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = 0\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = gene_indexed_df[\"sgRNA Cluster\"].astype(int)\n",
    "    return gene_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sgrna_replicate_thr = 2\n",
    "pref_factor = 3.0\n",
    "\n",
    "gene_list, counts_list = np.unique(most_rep_example_series[\"Gene\"], return_counts=True)\n",
    "genes_with_many_replicate_sgRNAs = gene_list[counts_list >= n_sgrna_replicate_thr]\n",
    "sgrnadf_many_copies_per_gene = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin(genes_with_many_replicate_sgRNAs)\n",
    "]\n",
    "\n",
    "max_distance_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: np.nanmax(get_upper_right_vals(get_normed_softdtw(x[\"Feature Vector\"])))\n",
    ")\n",
    "plt.title(\"Maximum soft-DTW Distance per Gene\")\n",
    "plt.hist(max_distance_within_gene, bins=50)\n",
    "plt.show()\n",
    "\n",
    "dist_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: get_upper_right_vals(get_normed_softdtw(x[\"Feature Vector\"])).flatten()\n",
    ")\n",
    "dist_within_gene = [val for item in dist_within_gene.tolist() for val in item]\n",
    "median_similarity = -np.nanmedian(dist_within_gene)\n",
    "\n",
    "gene_df = get_sgRNA_clusters(\n",
    "    most_rep_example_series, preference=pref_factor * median_similarity\n",
    ")\n",
    "\n",
    "most_rep_example_series[\"sgRNA Cluster\"] = gene_df.set_index(\"sgRNA\")[\"sgRNA Cluster\"]\n",
    "most_rep_example_series[\"sgRNA Cluster Label\"] = most_rep_example_series.apply(\n",
    "    lambda x: str(x[\"Gene\"]) + \"-\" + str(x[\"sgRNA Cluster\"]), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df = most_rep_example_series[\n",
    "    [\"sgRNA Cluster Label\", \"Feature Vector\", \"Gene\", \"Integrated Euclidean Norm\"]\n",
    "    + kernel_params\n",
    "].reset_index(drop=True)\n",
    "gene_cluster_groupby = gene_cluster_df.groupby(\"sgRNA Cluster Label\")\n",
    "# median_feature_series = gene_cluster_groupby.apply(lambda x: np.median(np.stack(x[\"Feature Vector\"]).astype(float), axis=0)).to_frame().rename(columns={0:\"Feature Vector\"})\n",
    "feature_series = (\n",
    "    gene_cluster_groupby.apply(\n",
    "        lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])][\"Feature Vector\"]\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Feature Vector\"})\n",
    ")\n",
    "\n",
    "gene_cluster_df = gene_cluster_groupby.apply(\n",
    "    lambda x: x.iloc[0][[\"Gene\"] + kernel_params]\n",
    ")\n",
    "gene_cluster_df = gene_cluster_df.join(feature_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-16_gene_cluster_df.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: TSNE and Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = get_normed_softdtw(gene_cluster_df[\"Feature Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_embedded = TSNE(\n",
    "    n_components=2, perplexity=5.0, early_exaggeration=50.0, metric=\"precomputed\"\n",
    ").fit_transform(X_dist - np.min(X_dist))\n",
    "gene_cluster_df[\"TSNE Coords\"] = [X_embedded[i] for i in range(X_embedded.shape[0])]\n",
    "\n",
    "af_labels = (\n",
    "    AffinityPropagation(affinity=\"precomputed\", preference=-0.5)\n",
    "    .fit_predict(-X_dist)\n",
    "    .astype(int)\n",
    ")\n",
    "gene_cluster_df[\"Affinity Clusts\"] = af_labels\n",
    "\n",
    "plt.scatter(\n",
    "    X_embedded[:, 0],\n",
    "    X_embedded[:, 1],\n",
    "    s=3,\n",
    "    alpha=1,\n",
    "    c=gene_cluster_df[\"Affinity Clusts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN and Leiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-16_gene_cluster_df.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from tslearn.metrics import cdist_soft_dtw_normalized, cdist_soft_dtw\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "import umap\n",
    "\n",
    "\n",
    "def knn_leiden_dist(dist_arr, n_neighbors=5, n_iterations=2):\n",
    "\n",
    "    knn = skl.neighbors.NearestNeighbors(\n",
    "        n_neighbors=n_neighbors + 1, metric=\"precomputed\", algorithm=\"brute\"\n",
    "    ).fit(\n",
    "        dist_arr\n",
    "    )  # plus one neighbor to account for self-loops\n",
    "\n",
    "    G = knn.kneighbors_graph(dist_arr, mode=\"distance\")\n",
    "    G[G.nonzero()] = 1.0 / (G[G.nonzero()] + 1.0)  ##Distance normalization\n",
    "    G_arr = G.toarray()\n",
    "\n",
    "    ig_G = ig.Graph.Weighted_Adjacency(G_arr, mode=\"undirected\", loops=False)\n",
    "    ig_G.vs[\"index\"] = [i for i in range(ig_G.vcount())]\n",
    "    ig_G.vs[\"weight\"] = [1.0 for i in range(ig_G.vcount())]\n",
    "\n",
    "    part = leidenalg.find_partition(\n",
    "        ig_G,\n",
    "        leidenalg.ModularityVertexPartition,\n",
    "        n_iterations=n_iterations,\n",
    "        weights=\"weight\",\n",
    "    )\n",
    "\n",
    "    return G, ig_G, part\n",
    "\n",
    "\n",
    "def knn_leiden_softdtw(X, n_neighbors=5, n_iterations=2):\n",
    "\n",
    "    norm_soft_dtw_Y = cdist_soft_dtw_normalized(X)  ##This is a distance\n",
    "\n",
    "    G, ig_G, part = knn_leiden_dist(\n",
    "        norm_soft_dtw_Y, n_neighbors=n_neighbors, n_iterations=n_iterations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(gene_cluster_df[\"Feature Vector\"].tolist())\n",
    "X = np.swapaxes(X, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G, ig_G, part = knn_leiden_softdtw(X, n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df[\"Leiden Clusters\"] = part.membership\n",
    "gene_names = gene_cluster_df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Force-Directed Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weight = 10\n",
    "global_graph = part.graph\n",
    "global_graph_arr = global_graph.get_adjacency_sparse()\n",
    "# global_graph.vs[\"label\"] = [gene_names[idx] for idx in global_graph.vs[\"index\"]]\n",
    "pal = ig.drawing.colors.ClusterColoringPalette(len(part))\n",
    "global_graph.vs[\"color\"] = pal.get_many(part.membership)\n",
    "global_graph.vs[\"label\"] = part.membership\n",
    "global_graph.es[\"width\"] = [item * edge_weight for item in global_graph.es[\"weight\"]]\n",
    "\n",
    "global_graph_umap = umap.UMAP(\n",
    "    n_neighbors=5, n_components=2, metric=\"precomputed\", min_dist=0.5\n",
    ").fit_transform(global_graph_arr)\n",
    "global_graph_umap_layout = [\n",
    "    tuple(global_graph_umap[i]) for i in range(global_graph_umap.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = global_graph.layout(\"fruchterman_reingold\", weights=\"weight\")\n",
    "ig.plot(global_graph, layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP Global Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig.plot(global_graph, layout=global_graph_umap_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weight = 10\n",
    "sub_graph = part.subgraph(3)\n",
    "sub_graph_arr = sub_graph.get_adjacency_sparse()\n",
    "sub_graph.vs[\"label\"] = [gene_names[idx] for idx in sub_graph.vs[\"index\"]]\n",
    "sub_graph.es[\"width\"] = [item * edge_weight for item in sub_graph.es[\"weight\"]]\n",
    "\n",
    "sub_graph_umap = umap.UMAP(\n",
    "    n_neighbors=2, n_components=2, metric=\"precomputed\", min_dist=0.5\n",
    ").fit_transform(sub_graph_arr)\n",
    "sub_graph_umap_layout = [\n",
    "    tuple(sub_graph_umap[i]) for i in range(sub_graph_umap.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Force-directed Community Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = sub_graph.layout(\"fruchterman_reingold\", weights=\"weight\")\n",
    "ig.plot(sub_graph, layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP Community Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layout = sub_graph.layout(\"kamada_kawai\",weights=\"weight\")\n",
    "ig.plot(sub_graph, layout=sub_graph_umap_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP Embedding\n",
    "\n",
    "- might be better here to use a less filtered dataset (elim clusters?) to get a better local density estimate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from igraph.drawing.text import TextDrawer\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from tslearn.metrics import cdist_soft_dtw_normalized, cdist_soft_dtw\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "import umap\n",
    "\n",
    "\n",
    "def knn_leiden_dist(dist_arr, n_neighbors=5, n_iterations=2):\n",
    "\n",
    "    knn = skl.neighbors.NearestNeighbors(\n",
    "        n_neighbors=n_neighbors + 1, metric=\"precomputed\", algorithm=\"brute\"\n",
    "    ).fit(\n",
    "        dist_arr\n",
    "    )  # plus one neighbor to account for self-loops\n",
    "\n",
    "    G = knn.kneighbors_graph(dist_arr, mode=\"distance\")\n",
    "    G[G.nonzero()] = 1.0 / (G[G.nonzero()] + 1.0)  ##Distance normalization\n",
    "    G_arr = G.toarray()\n",
    "\n",
    "    ig_G = ig.Graph.Weighted_Adjacency(G_arr, mode=\"undirected\", loops=False)\n",
    "    ig_G.vs[\"index\"] = [i for i in range(ig_G.vcount())]\n",
    "    ig_G.vs[\"weight\"] = [1.0 for i in range(ig_G.vcount())]\n",
    "\n",
    "    part = leidenalg.find_partition(\n",
    "        ig_G,\n",
    "        leidenalg.ModularityVertexPartition,\n",
    "        n_iterations=n_iterations,\n",
    "        weights=\"weight\",\n",
    "    )\n",
    "\n",
    "    return G, ig_G, part\n",
    "\n",
    "\n",
    "def knn_leiden_softdtw(X, n_neighbors=5, n_iterations=2):\n",
    "\n",
    "    norm_soft_dtw_Y = cdist_soft_dtw_normalized(X)  ##This is a distance\n",
    "\n",
    "    G, ig_G, part = knn_leiden_dist(\n",
    "        norm_soft_dtw_Y, n_neighbors=n_neighbors, n_iterations=n_iterations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-16_gene_cluster_df.pkl\"\n",
    ")\n",
    "gene_cluster_df_full = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-16_gene_cluster_df_full.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df_full.columns[:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_traces = [\n",
    "    \"Kernel Trace: Birth: Volume: z score\",\n",
    "    \"Kernel Trace: Division: Volume: z score\",\n",
    "    \"Kernel Trace: Mean Linear Growth Rate: Volume: z score\",\n",
    "    \"Kernel Trace: Mean Exponential Growth Rate: Volume: z score\",\n",
    "    \"Kernel Trace: Mean: minor_axis_length: z score\",\n",
    "    \"Kernel Trace: Mean: mCherry Intensity: z score\",\n",
    "    \"Kernel Trace: Delta t: z score\",\n",
    "]\n",
    "\n",
    "for zscore_trace in zscore_traces:\n",
    "    avg_zscore = gene_cluster_df_full.apply(lambda x: np.mean(x[zscore_trace]), axis=1)\n",
    "    gene_cluster_df_full[zscore_trace + \": Mean\"] = avg_zscore\n",
    "\n",
    "for zscore_trace in zscore_traces:\n",
    "    max_zscore = gene_cluster_df_full.apply(lambda x: np.max(x[zscore_trace]), axis=1)\n",
    "    gene_cluster_df_full[zscore_trace + \": Max\"] = max_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ungrouped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(gene_cluster_df_full[\"Feature Vector\"].tolist())\n",
    "X = np.swapaxes(X, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_norm_soft_dtw(X, chunk_size=100):\n",
    "    X_dask = da.from_array(X, chunks=(chunk_size, X.shape[1], X.shape[2]))\n",
    "    soft_dtw_arr = da.blockwise(\n",
    "        cdist_soft_dtw, \"ik\", X_dask, \"itd\", X_dask, \"ktd\", concatenate=True\n",
    "    ).compute()\n",
    "    d_ii = np.diag(soft_dtw_arr)\n",
    "    norm_soft_dtw_arr = soft_dtw_arr - (\n",
    "        0.5 * (d_ii.reshape((-1, 1)) + d_ii.reshape((1, -1)))\n",
    "    )\n",
    "    return norm_soft_dtw_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_soft_dtw_arr = parallel_norm_soft_dtw(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_community = 6\n",
    "n_neighbors_umap = 20\n",
    "edge_weight = 10\n",
    "\n",
    "G, ig_G, part = knn_leiden_dist(\n",
    "    norm_soft_dtw_arr, n_neighbors=n_neighbors_community, n_iterations=10\n",
    ")\n",
    "gene_cluster_df_full[\"Leiden Clusters\"] = part.membership\n",
    "global_graph = copy.copy(part.graph)\n",
    "global_graph_arr = global_graph.get_adjacency_sparse()\n",
    "# global_graph.vs[\"label\"] = [gene_names[idx] for idx in global_graph.vs[\"index\"]]\n",
    "pal = ig.drawing.colors.ClusterColoringPalette(len(part))\n",
    "global_graph.vs[\"color\"] = pal.get_many(part.membership)\n",
    "global_graph.vs[\"size\"] = [5 for i in global_graph.vs[\"color\"]]\n",
    "# global_graph.vs[\"label\"] = part.membership\n",
    "global_graph.es[\"width\"] = [item * edge_weight for item in global_graph.es[\"weight\"]]\n",
    "\n",
    "global_graph_umap = umap.UMAP(\n",
    "    n_neighbors=n_neighbors_umap, n_components=2, metric=\"precomputed\"\n",
    ").fit_transform(norm_soft_dtw_arr)\n",
    "global_graph_umap_layout = ig.layout.Layout(\n",
    "    [tuple(global_graph_umap[i]) for i in range(global_graph_umap.shape[0])]\n",
    ")\n",
    "\n",
    "graph_plot = ig.plot(global_graph, layout=global_graph_umap_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscore_colors(zscores):\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    zscore_max = max(abs(np.percentile(zscores, 1)), abs(np.percentile(zscores, 99)))\n",
    "    norm = mpl.colors.Normalize(vmin=-zscore_max, vmax=zscore_max)\n",
    "    zscore_colors = cmap(norm(zscores))\n",
    "    return zscore_colors, cmap, norm\n",
    "\n",
    "\n",
    "def umap_plot_zscore(global_graph, zscores, layout):\n",
    "    color_graph = copy.deepcopy(global_graph)\n",
    "    zscore_colors, cmap, norm = get_zscore_colors(zscores)\n",
    "    color_graph.vs[\"color\"] = [mpl.colors.to_rgb(item) for item in zscore_colors]\n",
    "    return color_graph, layout, cmap, norm\n",
    "\n",
    "\n",
    "def get_colorbar(cmap, norm):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.05, 0.80, 0.9, 0.1])\n",
    "\n",
    "    cb = mpl.colorbar.ColorbarBase(ax, orientation=\"horizontal\", cmap=cmap, norm=norm)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def display_zscore_map(df, graph, graph_layout, zscore_label):\n",
    "    zscore_vals = df[zscore_label].tolist()\n",
    "    color_graph, layout, cmap, norm = umap_plot_zscore(graph, zscore_vals, graph_layout)\n",
    "    graph_plot = ig.plot(color_graph, layout=layout)\n",
    "    fig, ax = get_colorbar(cmap, norm)\n",
    "    return graph_plot, fig, ax\n",
    "\n",
    "\n",
    "def make_zscore_file(df, graph, graph_layout, zscore_label, file_label):\n",
    "    graph_plot, fig, ax = display_zscore_map(df, graph, graph_layout, zscore_label)\n",
    "    graph_plot.save(\"./Plots/\" + file_label + \".svg\")\n",
    "    graph_plot.save(\"./Plots/\" + file_label + \".png\")\n",
    "    plt.savefig(\"./Plots/\" + file_label + \"_Cmap.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Birth: Volume: z score: Mean\"\n",
    "file_label = \"Birth_Volume_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Division: Volume: z score: Mean\"\n",
    "file_label = \"Division_Volume_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Mean Linear Growth Rate: Volume: z score: Mean\"\n",
    "file_label = \"Linear_Growth_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Mean Exponential Growth Rate: Volume: z score: Mean\"\n",
    "file_label = \"Exp_Growth_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Mean: minor_axis_length: z score: Mean\"\n",
    "file_label = \"Width_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Mean: mCherry Intensity: z score: Mean\"\n",
    "file_label = \"mCherry_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Delta t: z score: Mean\"\n",
    "file_label = \"Division_Zscore\"\n",
    "\n",
    "make_zscore_file(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    zscore_label,\n",
    "    file_label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Genes of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gene_cluster_df_full[\"Gene\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_list = [True if \"rps\" in gene else False for gene in gene_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_plot_nodes_of_interest(\n",
    "    global_graph, binary_list, layout, unselected_opacity=0.2\n",
    "):\n",
    "    color_dict = {False: (128, 128, 128, unselected_opacity), True: \"yellow\"}\n",
    "    color_graph = copy.deepcopy(global_graph)\n",
    "    color_graph.vs[\"color\"] = [color_dict[item] for item in binary_list]\n",
    "    outline_hex = mpl.colors.rgb2hex(\n",
    "        (0.0, 0.0, 0.0, unselected_opacity), keep_alpha=True\n",
    "    )\n",
    "    return color_graph, layout, outline_hex\n",
    "\n",
    "\n",
    "# should add gene ontology?\n",
    "def highlight_cluster(\n",
    "    df, graph, graph_layout, cluster_label, cluster_id, bbox=(0, 0, 600, 600)\n",
    "):\n",
    "    binary_list = (df[cluster_label] == cluster_id).tolist()\n",
    "    color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "        graph, binary_list, graph_layout\n",
    "    )\n",
    "    graph_plot = ig.plot(\n",
    "        color_graph, layout=layout, bbox=bbox, vertex_frame_color=outline_hex\n",
    "    )\n",
    "    return graph_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_list = [True if \"fts\" in gene else False for gene in gene_list]\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    global_graph, binary_list, global_graph_umap_layout\n",
    ")\n",
    "ig.plot(color_graph, layout=layout, vertex_frame_color=outline_hex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ribosomal Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosomal_proteins = [\n",
    "    \"prmA\",\n",
    "    \"rimM\",\n",
    "    \"rplA\",\n",
    "    \"rplB\",\n",
    "    \"rplC\",\n",
    "    \"rplD\",\n",
    "    \"rplE\",\n",
    "    \"rplF\",\n",
    "    \"rplI\",\n",
    "    \"rplJ\",\n",
    "    \"rplK\",\n",
    "    \"rplL\",\n",
    "    \"rplM\",\n",
    "    \"rplN\",\n",
    "    \"rplO\",\n",
    "    \"rplP\",\n",
    "    \"rplQ\",\n",
    "    \"rplR\",\n",
    "    \"rplS\",\n",
    "    \"rplT\",\n",
    "    \"rplU\",\n",
    "    \"rplV\",\n",
    "    \"rplW\",\n",
    "    \"rplX\",\n",
    "    \"rplY\",\n",
    "    \"rpmA\",\n",
    "    \"rpmB\",\n",
    "    \"rpmC\",\n",
    "    \"rpmD\",\n",
    "    \"rpmE\",\n",
    "    \"rpmF\",\n",
    "    \"rpmG\",\n",
    "    \"rpmH\",\n",
    "    \"rpmI\",\n",
    "    \"rpmJ\",\n",
    "    \"rpsA\",\n",
    "    \"rpsB\",\n",
    "    \"rpsC\",\n",
    "    \"rpsD\",\n",
    "    \"rpsE\",\n",
    "    \"rpsF\",\n",
    "    \"rpsG\",\n",
    "    \"rpsH\",\n",
    "    \"rpsI\",\n",
    "    \"rpsJ\",\n",
    "    \"rpsK\",\n",
    "    \"rpsL\",\n",
    "    \"rpsM\",\n",
    "    \"rpsN\",\n",
    "    \"rpsO\",\n",
    "    \"rpsP\",\n",
    "    \"rpsQ\",\n",
    "    \"rpsR\",\n",
    "    \"rpsS\",\n",
    "    \"rpsT\",\n",
    "    \"rpsU\",\n",
    "    \"sra\",\n",
    "    \"ykgM\",\n",
    "    \"ykgO\",\n",
    "]\n",
    "\n",
    "binary_list = gene_cluster_df_full[\"Gene\"].isin(ribosomal_proteins).tolist()\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    global_graph, binary_list, global_graph_umap_layout\n",
    ")\n",
    "ig.plot(\n",
    "    color_graph, layout=layout, vertex_frame_color=outline_hex, bbox=(0, 0, 400, 400)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_df = gene_cluster_df_full[\n",
    "    gene_cluster_df_full[\"Gene\"].isin(ribosomal_proteins)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_genes, ribosome_gene_counts = np.unique(\n",
    "    ribosome_df[ribosome_df[\"Leiden Clusters\"] == 0][\"Gene\"].tolist(),\n",
    "    return_counts=True,\n",
    ")\n",
    "ribosome_genes = [\n",
    "    str(ribosome_genes[i]) + \":\" + str(ribosome_gene_counts[i])\n",
    "    for i in range(len(ribosome_genes))\n",
    "]\n",
    "\n",
    "\n",
    "gene_names = \" \".join(sorted(ribosome_genes))\n",
    "print(textwrap.fill(gene_names, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_genes, ribosome_gene_counts = np.unique(\n",
    "    ribosome_df[ribosome_df[\"Leiden Clusters\"] == 6][\"Gene\"].tolist(),\n",
    "    return_counts=True,\n",
    ")\n",
    "ribosome_genes = [\n",
    "    str(ribosome_genes[i]) + \":\" + str(ribosome_gene_counts[i])\n",
    "    for i in range(len(ribosome_genes))\n",
    "]\n",
    "\n",
    "\n",
    "gene_names = \" \".join(sorted(ribosome_genes))\n",
    "print(textwrap.fill(gene_names, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rplKAJLrpoBC Operon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rplKAJLrpoBC_df = gene_cluster_df_full[\n",
    "    gene_cluster_df_full[\"Gene\"].isin([\"rplK\", \"rplA\", \"rplJ\", \"rplL\", \"rpoB\", \"rpoC\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rplKAJLrpoBC_df[rplKAJLrpoBC_df[\"Gene\"] == \"rpoC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_df[ribosome_df[\"Gene\"] == \"rplJ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNA Polymerase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnapol_genes = gene_cluster_df_full[\"Gene\"].isin([\"rpoZ\"]).tolist()\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    global_graph, rnapol_genes, global_graph_umap_layout\n",
    ")\n",
    "ig.plot(\n",
    "    color_graph, layout=layout, vertex_frame_color=outline_hex, bbox=(0, 0, 400, 400)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosomal_rnas = [\"rrf\", \"rrs\", \"rrl\"]\n",
    "\n",
    "binary_list = (\n",
    "    gene_cluster_df_full[\"Gene\"]\n",
    "    .apply(lambda x: np.any([rna in x for rna in ribosomal_rnas]))\n",
    "    .tolist()\n",
    ")\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    global_graph, binary_list, global_graph_umap_layout\n",
    ")\n",
    "ig.plot(\n",
    "    color_graph, layout=layout, vertex_frame_color=outline_hex, bbox=(0, 0, 400, 400)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosomal_rnas = [\"rrl\"]\n",
    "\n",
    "binary_list = (\n",
    "    gene_cluster_df_full[\"Gene\"]\n",
    "    .apply(lambda x: np.any([rna in x for rna in ribosomal_rnas]))\n",
    "    .tolist()\n",
    ")\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    global_graph, binary_list, global_graph_umap_layout\n",
    ")\n",
    "ig.plot(\n",
    "    color_graph, layout=layout, vertex_frame_color=outline_hex, bbox=(0, 0, 400, 400)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_rna_df = gene_cluster_df_full[\n",
    "    gene_cluster_df_full[\"Gene\"].apply(\n",
    "        lambda x: np.any([rna in x for rna in ribosomal_rnas])\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_rna_df[\"Gene\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fts Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnapol_genes = gene_cluster_df_full['Gene'].isin([\"rpoZ\"]).tolist()\n",
    "color_graph,layout,outline_hex = umap_plot_nodes_of_interest(global_graph,rnapol_genes,global_graph_umap_layout)\n",
    "ig.plot(color_graph,layout = layout, vertex_frame_color=outline_hex, bbox=(0,0,400,400))rnapol_genes = gene_cluster_df_full['Gene'].isin([\"rpoZ\"]).tolist()\n",
    "color_graph,layout,outline_hex = umap_plot_nodes_of_interest(global_graph,rnapol_genes,global_graph_umap_layout)\n",
    "ig.plot(color_graph,layout = layout, vertex_frame_color=outline_hex, bbox=(0,0,400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribosome_genes, ribosome_gene_counts = np.unique(\n",
    "    ribosome_rna_df[ribosome_rna_df[\"Leiden Clusters\"] == 0][\"Gene\"].tolist(),\n",
    "    return_counts=True,\n",
    ")\n",
    "ribosome_genes = [\n",
    "    str(ribosome_genes[i]) + \":\" + str(ribosome_gene_counts[i])\n",
    "    for i in range(len(ribosome_genes))\n",
    "]\n",
    "\n",
    "\n",
    "gene_names = \" \".join(sorted(ribosome_genes))\n",
    "print(textwrap.fill(gene_names, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GO Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goatools\n",
    "import goatools.base\n",
    "from goatools.base import download_go_basic_obo\n",
    "\n",
    "from goatools.obo_parser import GODag\n",
    "from goatools.anno.gaf_reader import GafReader\n",
    "from goatools.semantic import semantic_similarity\n",
    "from goatools.semantic import TermCounts, get_info_content\n",
    "\n",
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "from goatools.go_enrichment import GOEnrichmentStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ontologies\n",
    "obo_fname = download_go_basic_obo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ecoli association file (ecocyc)\n",
    "gaf_handle = goatools.base.http_get(\n",
    "    \"http://current.geneontology.org/annotations/ecocyc.gaf.gz\", fout=\"./ecocyc.gaf.gz\"\n",
    ")\n",
    "gaf_fname = goatools.base.gunzip(\"./ecocyc.gaf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting ontologies and other nonesense\n",
    "\n",
    "obodag = GODag(obo_fname)\n",
    "objanno = GafReader(gaf_fname)\n",
    "ns2assoc = objanno.get_ns2assc()\n",
    "\n",
    "gene_to_id = {assoc.DB_Symbol: assoc.DB_ID for assoc in objanno.associations}\n",
    "inv_gene_to_id = {assoc.DB_ID: assoc.DB_Symbol for assoc in objanno.associations}\n",
    "synonym_dict = {\n",
    "    synonym: assoc.DB_ID\n",
    "    for assoc in objanno.associations\n",
    "    for synonym in assoc.DB_Synonym\n",
    "}\n",
    "gene_to_id.update(synonym_dict)\n",
    "\n",
    "# background gene set\n",
    "\n",
    "all_genes = gene_cluster_df_full[\"Gene\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enriched_GO_terms(\n",
    "    background_gene_list, gene_list, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    "):\n",
    "\n",
    "    gene_to_id = {assoc.DB_Symbol: assoc.DB_ID for assoc in objanno.associations}\n",
    "    synonym_dict = {\n",
    "        synonym: assoc.DB_ID\n",
    "        for assoc in objanno.associations\n",
    "        for synonym in assoc.DB_Synonym\n",
    "    }\n",
    "    gene_to_id.update(synonym_dict)\n",
    "\n",
    "    # background gene set\n",
    "\n",
    "    all_genes_uniprot = [\n",
    "        gene_to_id[item] for item in background_gene_list if item in gene_to_id.keys()\n",
    "    ]\n",
    "    selected_genes_uniprot = [\n",
    "        gene_to_id[item] for item in gene_list if item in gene_to_id.keys()\n",
    "    ]\n",
    "\n",
    "    print(len(all_genes_uniprot))\n",
    "    print(len(selected_genes_uniprot))\n",
    "\n",
    "    goeaobj = GOEnrichmentStudy(\n",
    "        all_genes_uniprot,  # List of mouse protein-coding genes\n",
    "        ns2assoc[GO_type],  # geneid/GO associations\n",
    "        obodag,  # Ontologies\n",
    "        propagate_counts=True,\n",
    "        alpha=pval,  # default significance cut-off\n",
    "        methods=[\"fdr_bh\"],\n",
    "    )\n",
    "    # defult multipletest correction method\n",
    "\n",
    "    goea_results_all = goeaobj.run_study(selected_genes_uniprot, prt=None)\n",
    "    goea_quiet_sig = [r for r in goea_results_all if r.p_fdr_bh < pval]\n",
    "    goea_quiet_enriched = [r for r in goea_quiet_sig if r.enrichment == \"e\"]\n",
    "    return goea_quiet_enriched\n",
    "\n",
    "\n",
    "def pick_exemplar(go1, go2, termcounts, obodag, info_thr, pval_factor=2.0):\n",
    "\n",
    "    info_1_low = get_info_content(go1.GO, termcounts) < info_thr\n",
    "    info_2_low = get_info_content(go2.GO, termcounts) < info_thr\n",
    "    if info_1_low and not info_2_low:\n",
    "        return go2\n",
    "    elif info_2_low and not info_1_low:\n",
    "        return go1\n",
    "    elif info_2_low and info_1_low:\n",
    "        return go1\n",
    "\n",
    "    pval_ratio = go1.p_fdr_bh / go2.p_fdr_bh\n",
    "\n",
    "    if pval_ratio > pval_factor:\n",
    "        return go2\n",
    "    elif pval_ratio < (1.0 / pval_factor):\n",
    "        return go1\n",
    "\n",
    "    go1_parents = list(obodag[go1.GO].get_all_parents())\n",
    "    go2_parents = list(obodag[go2.GO].get_all_parents())\n",
    "\n",
    "    if go2.GO in go1_parents:\n",
    "        return go2\n",
    "\n",
    "    elif go1.GO in go2_parents:\n",
    "        return go1\n",
    "\n",
    "    return go1\n",
    "\n",
    "\n",
    "def get_filtered_go_terms(\n",
    "    obodag, objanno, goea_list, sim_thr=0.05, info_thr=1.0, GO_type=\"BP\"\n",
    "):\n",
    "\n",
    "    termcounts = TermCounts(obodag, objanno.get_ns2assc()[GO_type])\n",
    "\n",
    "    go_term_list = [item.GO for item in goea_list]\n",
    "    sim_arr = np.zeros((len(go_term_list), len(go_term_list)))\n",
    "    for i in range(len(go_term_list)):\n",
    "        for j in range(len(go_term_list)):\n",
    "            sim_arr[i, j] = semantic_similarity(\n",
    "                go_term_list[i], go_term_list[j], obodag\n",
    "            )\n",
    "    np.fill_diagonal(sim_arr, 0.0)\n",
    "\n",
    "    working_group_idx = 0\n",
    "    grouped_terms = {}\n",
    "    group_exemplars = {}\n",
    "    go_term_indices = list(range(len(go_term_list)))\n",
    "\n",
    "    while len(go_term_indices) > 0:\n",
    "        i = go_term_indices[0]\n",
    "        most_sim_arg = np.argmax(sim_arr[i])\n",
    "        sim_score = sim_arr[i, most_sim_arg]\n",
    "        if sim_score > sim_thr:\n",
    "            if len(grouped_terms) > 0:\n",
    "                in_other_group_keys = [\n",
    "                    key for key, val in grouped_terms.items() if most_sim_arg in val\n",
    "                ]\n",
    "                if len(in_other_group_keys) == 1:\n",
    "                    other_group_idx = in_other_group_keys[0]\n",
    "                    grouped_terms[other_group_idx] = grouped_terms[other_group_idx] + [\n",
    "                        i\n",
    "                    ]\n",
    "                    group_exemplars[other_group_idx] = pick_exemplar(\n",
    "                        group_exemplars[other_group_idx],\n",
    "                        goea_list[i],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                else:\n",
    "                    grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                    group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                        goea_list[i],\n",
    "                        goea_list[most_sim_arg],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                    working_group_idx += 1\n",
    "                    go_term_indices.remove(most_sim_arg)\n",
    "            else:\n",
    "                grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                    goea_list[i], goea_list[most_sim_arg], termcounts, obodag, info_thr\n",
    "                )\n",
    "                working_group_idx += 1\n",
    "                go_term_indices.remove(most_sim_arg)\n",
    "        go_term_indices.remove(i)\n",
    "\n",
    "    group_exemplars = list(group_exemplars.values())\n",
    "\n",
    "    return group_exemplars\n",
    "\n",
    "\n",
    "def get_GO_assign_dict(selected_goea, cluster_genes_uniprot):\n",
    "    all_study_items = copy.copy(cluster_genes_uniprot)\n",
    "    depth_list = sorted(set([item.depth for item in selected_goea]))[::-1]\n",
    "    assign_dict = {}\n",
    "    for depth in depth_list:\n",
    "        go_terms_at_level = [item for item in selected_goea if item.depth == depth]\n",
    "        for go_term in go_terms_at_level:\n",
    "            study_item_list = list(go_term.study_items)\n",
    "            for study_item in study_item_list:\n",
    "                if study_item in all_study_items:\n",
    "                    assign_dict[study_item] = go_term.name\n",
    "                    all_study_items.remove(study_item)\n",
    "\n",
    "    for remaining_item in all_study_items:\n",
    "        assign_dict[remaining_item] = \"Unassigned\"\n",
    "\n",
    "    return assign_dict\n",
    "\n",
    "\n",
    "def plot_cluster_timeseries(\n",
    "    df,\n",
    "    cluster_label,\n",
    "    cluster_subset=None,\n",
    "    feature_vector_label=\"Feature Vector\",\n",
    "    feature_labels=[\n",
    "        \"Birth: Volume\",\n",
    "        \"Division: Volume\",\n",
    "        \"Mean Linear Growth Rate: Volume\",\n",
    "        \"Mean Exponential Growth Rate: Volume\",\n",
    "        \"Mean: minor_axis_length\",\n",
    "        \"Mean: mCherry Intensity\",\n",
    "        \"Delta t\",\n",
    "    ],\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "\n",
    "    if cluster_subset is not None:\n",
    "        df = copy.copy(df)\n",
    "        df = df[df[cluster_label].isin(cluster_subset)]\n",
    "\n",
    "    mean_cluster_timeseries = df.groupby([cluster_label]).apply(\n",
    "        lambda x: np.mean(np.array(x[feature_vector_label].tolist()), axis=0)\n",
    "    )\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(mean_cluster_timeseries), wspace=wspace)\n",
    "\n",
    "    for i, idx in enumerate(mean_cluster_timeseries.index):\n",
    "        clust_arr = np.array(mean_cluster_timeseries[idx])\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[0, 6])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(str(idx), fontsize=fontsize)\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(clust_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8.0)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(str(idx), fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id = 0\n",
    "cluster_genes = sorted(\n",
    "    gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == clust_id][\"Gene\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "goea_quiet_enriched = get_enriched_GO_terms(\n",
    "    all_genes, cluster_genes, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    ")\n",
    "filtered_go_terms = get_filtered_go_terms(\n",
    "    obodag, objanno, goea_quiet_enriched, sim_thr=0.3, info_thr=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id = 17\n",
    "cluster_genes = sorted(\n",
    "    gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == clust_id][\n",
    "        \"Gene\"\n",
    "    ].tolist()\n",
    ")\n",
    "cluster_genes, cluster_gene_counts = np.unique(cluster_genes, return_counts=True)\n",
    "cluster_genes = [\n",
    "    str(cluster_genes[i]) + \":\" + str(cluster_gene_counts[i])\n",
    "    for i in range(len(cluster_genes))\n",
    "]\n",
    "\n",
    "\n",
    "gene_names = \" \".join(sorted(cluster_genes))\n",
    "print(textwrap.fill(gene_names, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_gene_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clust_id = 7\n",
    "cluster_genes = sorted(\n",
    "    gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == clust_id][\"Gene\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "goea_quiet_enriched = get_enriched_GO_terms(\n",
    "    all_genes, cluster_genes, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    ")\n",
    "filtered_go_terms = get_filtered_go_terms(\n",
    "    obodag, objanno, goea_quiet_enriched, sim_thr=0.3, info_thr=1.0\n",
    ")\n",
    "\n",
    "cluster_graph = highlight_cluster(\n",
    "    gene_cluster_df_full,\n",
    "    global_graph,\n",
    "    global_graph_umap_layout,\n",
    "    \"Leiden Clusters\",\n",
    "    clust_id,\n",
    "    bbox=(0, 0, 500, 500),\n",
    ")\n",
    "print(\"\")\n",
    "for go_term in filtered_go_terms:\n",
    "    print(str(go_term.name) + \": P-val: \" + str(go_term.p_fdr_bh))\n",
    "    print(\n",
    "        \"Percent of this GO Term: \"\n",
    "        + str(go_term.ratio_in_study[0] / go_term.ratio_in_pop[0])\n",
    "    )\n",
    "    gene_names = \" \".join(\n",
    "        sorted([inv_gene_to_id[item] for item in go_term.study_items])\n",
    "    )\n",
    "    print(textwrap.fill(gene_names, 80))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "cluster_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_timeseries(\n",
    "    gene_cluster_df_full, \"Leiden Clusters\", cluster_subset=[0, 12, 17], figsize=(8, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = [\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta t\",\n",
    "]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(len(feature_labels)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    ax.set_title(feature_labels[i])\n",
    "    ax.plot(\n",
    "        np.array(\n",
    "            gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == 17][\n",
    "                \"Feature Vector\"\n",
    "            ].tolist()\n",
    "        )[:, i, :].T,\n",
    "        c=\"tab:blue\",\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == 17][\"sgRNA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == 17][\"Gene\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "\n",
    "pval = 0.05\n",
    "GO_type = \"BP\"\n",
    "\n",
    "clust_id = 4\n",
    "cluster_genes = sorted(\n",
    "    gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == clust_id][\"Gene\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "cluster_genes_uniprot = [\n",
    "    gene_to_id[item] for item in cluster_genes if item in gene_to_id.keys()\n",
    "]\n",
    "\n",
    "goeaobj = GOEnrichmentStudy(\n",
    "    all_genes_uniprot,  # List of mouse protein-coding genes\n",
    "    ns2assoc[GO_type],  # geneid/GO associations\n",
    "    obodag,  # Ontologies\n",
    "    propagate_counts=True,\n",
    "    alpha=pval,  # default significance cut-off\n",
    "    methods=[\"fdr_bh\"],\n",
    ")  # defult multipletest correction method\n",
    "\n",
    "goea_results_all = goeaobj.run_study(cluster_genes_uniprot, prt=None)\n",
    "goea_quiet_sig = [r for r in goea_results_all if r.p_fdr_bh < pval]\n",
    "goea_quiet_enriched = [r for r in goea_quiet_sig if r.enrichment == \"e\"]\n",
    "# #reloading corrupted items (GOEnrichmentStudy messes them up)\n",
    "# obodag = GODag(obo_fname)\n",
    "# ns2assoc = objanno.get_ns2assc()\n",
    "\n",
    "# parent_goeaobj = GOEnrichmentStudy(\n",
    "#         all_genes_uniprot, # List of mouse protein-coding genes\n",
    "#         ns2assoc[GO_type], # geneid/GO associations\n",
    "#         obodag, # Ontologies\n",
    "#         propagate_counts = False,\n",
    "#         alpha = pval, # default significance cut-off\n",
    "#         methods=[\"fdr_bh\"]) # defult multipletest correction method\n",
    "\n",
    "# parent_goea_results_all = parent_goeaobj.run_study(cluster_genes_uniprot, prt=None)\n",
    "# parent_goea_quiet_sig = [r for r in parent_goea_results_all if r.p_fdr_bh < pval]\n",
    "\n",
    "# parent_goeaobj = GOEnrichmentStudyNS(\n",
    "#         all_genes_uniprot, # List of mouse protein-coding genes\n",
    "#         ns2assoc, # geneid/GO associations\n",
    "#         obodag, # Ontologies\n",
    "#         propagate_counts = False,\n",
    "#         alpha = pval, # default significance cut-off\n",
    "#         methods=[\"fdr_bh\"]) # defult multipletest correction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"{N} of {M:,} results were significant\".format(\n",
    "        N=len(goea_quiet_sig), M=len(goea_results_all)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Significant results: {E} enriched, {P} purified\".format(\n",
    "        E=sum(1 for r in goea_quiet_sig if r.enrichment == \"e\"),\n",
    "        P=sum(1 for r in goea_quiet_sig if r.enrichment == \"p\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goatools.semantic import semantic_similarity\n",
    "from goatools.semantic import TermCounts, get_info_content\n",
    "\n",
    "termcounts = TermCounts(obodag, objanno.get_ns2assc()[GO_type])\n",
    "go_id3 = \"GO:0051301\"\n",
    "go_id4 = \"GO:0071103\"\n",
    "\n",
    "sim = semantic_similarity(go_id3, go_id4, obodag)\n",
    "print(\n",
    "    \"The semantic similarity between terms {} and {} is {}.\".format(go_id3, go_id4, sim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objanno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from goatools.semantic import semantic_similarity\n",
    "from goatools.semantic import TermCounts, get_info_content\n",
    "\n",
    "\n",
    "def pick_exemplar(go1, go2, termcounts, obodag, info_thr, pval_factor=2.0):\n",
    "\n",
    "    info_1_low = get_info_content(go1.GO, termcounts) < info_thr\n",
    "    info_2_low = get_info_content(go2.GO, termcounts) < info_thr\n",
    "    if info_1_low and not info_2_low:\n",
    "        return go2\n",
    "    elif info_2_low and not info_1_low:\n",
    "        return go1\n",
    "    elif info_2_low and info_1_low:\n",
    "        return go1\n",
    "\n",
    "    pval_ratio = go1.p_fdr_bh / go2.p_fdr_bh\n",
    "\n",
    "    if pval_ratio > pval_factor:\n",
    "        return go2\n",
    "    elif pval_ratio < (1.0 / pval_factor):\n",
    "        return go1\n",
    "\n",
    "    go1_parents = list(obodag[go1.GO].get_all_parents())\n",
    "    go2_parents = list(obodag[go2.GO].get_all_parents())\n",
    "\n",
    "    if go2.GO in go1_parents:\n",
    "        return go2\n",
    "\n",
    "    elif go1.GO in go2_parents:\n",
    "        return go1\n",
    "\n",
    "    return go1\n",
    "\n",
    "\n",
    "def get_filtered_go_terms(\n",
    "    obodag, objanno, goea_list, sim_thr=0.05, info_thr=1.0, GO_type=\"BP\"\n",
    "):\n",
    "\n",
    "    termcounts = TermCounts(obodag, objanno.get_ns2assc()[GO_type])\n",
    "\n",
    "    go_term_list = [item.GO for item in goea_list]\n",
    "    sim_arr = np.zeros((len(go_term_list), len(go_term_list)))\n",
    "    for i in range(len(go_term_list)):\n",
    "        for j in range(len(go_term_list)):\n",
    "            sim_arr[i, j] = semantic_similarity(\n",
    "                go_term_list[i], go_term_list[j], obodag\n",
    "            )\n",
    "    np.fill_diagonal(sim_arr, 0.0)\n",
    "\n",
    "    working_group_idx = 0\n",
    "    grouped_terms = {}\n",
    "    group_exemplars = {}\n",
    "    go_term_indices = list(range(len(go_term_list)))\n",
    "\n",
    "    while len(go_term_indices) > 0:\n",
    "        i = go_term_indices[0]\n",
    "        most_sim_arg = np.argmax(sim_arr[i])\n",
    "        sim_score = sim_arr[i, most_sim_arg]\n",
    "        if sim_score > sim_thr:\n",
    "            if len(grouped_terms) > 0:\n",
    "                in_other_group_keys = [\n",
    "                    key for key, val in grouped_terms.items() if most_sim_arg in val\n",
    "                ]\n",
    "                if len(in_other_group_keys) == 1:\n",
    "                    other_group_idx = in_other_group_keys[0]\n",
    "                    grouped_terms[other_group_idx] = grouped_terms[other_group_idx] + [\n",
    "                        i\n",
    "                    ]\n",
    "                    group_exemplars[other_group_idx] = pick_exemplar(\n",
    "                        group_exemplars[other_group_idx],\n",
    "                        goea_list[i],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                else:\n",
    "                    grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                    group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                        goea_list[i],\n",
    "                        goea_list[most_sim_arg],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                    working_group_idx += 1\n",
    "                    go_term_indices.remove(most_sim_arg)\n",
    "            else:\n",
    "                grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                    goea_list[i], goea_list[most_sim_arg], termcounts, obodag, info_thr\n",
    "                )\n",
    "                working_group_idx += 1\n",
    "                go_term_indices.remove(most_sim_arg)\n",
    "        go_term_indices.remove(i)\n",
    "\n",
    "    group_exemplars = list(group_exemplars.values())\n",
    "\n",
    "    return group_exemplars\n",
    "\n",
    "\n",
    "def get_GO_assign_dict(selected_goea, cluster_genes_uniprot):\n",
    "    all_study_items = copy.copy(cluster_genes_uniprot)\n",
    "    depth_list = sorted(set([item.depth for item in selected_goea]))[::-1]\n",
    "    assign_dict = {}\n",
    "    for depth in depth_list:\n",
    "        go_terms_at_level = [item for item in selected_goea if item.depth == depth]\n",
    "        for go_term in go_terms_at_level:\n",
    "            study_item_list = list(go_term.study_items)\n",
    "            for study_item in study_item_list:\n",
    "                if study_item in all_study_items:\n",
    "                    assign_dict[study_item] = go_term.name\n",
    "                    all_study_items.remove(study_item)\n",
    "\n",
    "    for remaining_item in all_study_items:\n",
    "        assign_dict[remaining_item] = \"Unassigned\"\n",
    "\n",
    "    return assign_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_go_terms = get_filtered_go_terms(\n",
    "    obodag, objanno, goea_quiet_enriched, sim_thr=0.05, info_thr=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = filtered_go_terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_go_terms = get_filtered_go_terms(\n",
    "    obodag, objanno, goea_quiet_enriched, sim_thr=0.3, info_thr=1.0\n",
    ")\n",
    "ttl = 0\n",
    "for item in filtered_go_terms:\n",
    "    print(item.name)\n",
    "    print(item.study_count)\n",
    "    ttl += item.study_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_study_items = copy.copy(cluster_genes_uniprot)\n",
    "assign_dict = {}\n",
    "for study_item in all_study_items:\n",
    "    valid_go_terms = []\n",
    "    for go_term in filtered_go_terms:\n",
    "        if study_item in go_term.study_items:\n",
    "            valid_go_terms.append(go_term)\n",
    "    if len(valid_go_terms) == 0:\n",
    "        assign_dict[study_item] = \"Unassigned\"\n",
    "    elif len(valid_go_terms) == 1:\n",
    "        assign_dict[study_item] = valid_go_terms[0].name\n",
    "    else:\n",
    "        go_ex = pick_exemplar(\n",
    "            valid_go_terms[0],\n",
    "            valid_go_terms[1],\n",
    "            termcounts,\n",
    "            obodag,\n",
    "            info_thr,\n",
    "            pval_factor=2.0,\n",
    "        )\n",
    "        for i in range(len(valid_go_terms) - 2):\n",
    "            go_ex = pick_exemplar(\n",
    "                go_ex,\n",
    "                valid_go_terms[i + 2],\n",
    "                termcounts,\n",
    "                obodag,\n",
    "                info_thr,\n",
    "                pval_factor=2.0,\n",
    "            )\n",
    "        assign_dict[study_item] = go_ex.name\n",
    "# depth_list = sorted(set([item.depth for item in selected_goea]))[::-1]\n",
    "# assign_dict = {}\n",
    "# for depth in depth_list:\n",
    "#     go_terms_at_level = [item for item in selected_goea if item.depth == depth]\n",
    "#     for go_term in go_terms_at_level:\n",
    "#         study_item_list = list(go_term.study_items)\n",
    "#         for study_item in study_item_list:\n",
    "#             if study_item in all_study_items:\n",
    "#                 assign_dict[study_item] = go_term.name\n",
    "#                 all_study_items.remove(study_item)\n",
    "\n",
    "# for remaining_item in all_study_items:\n",
    "#     assign_dict[remaining_item] = \"Unassigned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([key for key, val in assign_dict.items() if val != \"Unassigned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([item.study_count for item in goea_quiet_sig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([item.study_n for item in goea_quiet_sig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([item.pop_count for item in goea_quiet_sig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goatools.semantic import TermCounts, get_info_content\n",
    "\n",
    "# First get the counts of each GO term.\n",
    "termcounts = TermCounts(obodag, objanno.get_ns2assc()[GO_type])\n",
    "\n",
    "# Calculate the information content\n",
    "go_id = \"GO:0051301\"\n",
    "infocontent = get_info_content(go_id, termcounts)\n",
    "print(\"Information content ({}) = {}\".format(go_id, infocontent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each GO term is grouped with its most similar term if their pairwise semantic similarity score is higher than or equal to a user-adjustable threshold value. One GO term is then selected as the representative of the group following the logical reasoning outlined below. If the most similar term is already part of another group, then the GO term is added to this group and a representative term is selected between the newly added GO term or the current representative of the group. This process is repeated for all GO terms provided in the list from the enrichment analysis, resulting in groups of GO terms represented by a single term, based on their semantic similarities, with user-control over the resolution of redundancy reduction. GO terms with no other term above the semantic similarity score threshold remain as singletons on the scatterplot. Representative selection is based on the following stepwise logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_thr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goea_quiet_sig[0].GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioproc_goea = [item for item in goea_quiet_sig if item.NS == \"BP\"]\n",
    "bioproc_parent_goea = [item for item in parent_goea_quiet_sig if item.NS == \"BP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_GO_assign_dict(selected_goea, cluster_genes_uniprot):\n",
    "    all_study_items = copy.copy(cluster_genes_uniprot)\n",
    "    depth_list = sorted(set([item.depth for item in selected_goea]))[::-1]\n",
    "    assign_dict = {}\n",
    "    for depth in depth_list:\n",
    "        go_terms_at_level = [item for item in selected_goea if item.depth == depth]\n",
    "        for go_term in go_terms_at_level:\n",
    "            study_item_list = list(go_term.study_items)\n",
    "            for study_item in study_item_list:\n",
    "                if study_item in all_study_items:\n",
    "                    assign_dict[study_item] = go_term.name\n",
    "                    all_study_items.remove(study_item)\n",
    "\n",
    "    for remaining_item in all_study_items:\n",
    "        assign_dict[remaining_item] = \"Unassigned\"\n",
    "\n",
    "    return assign_dict\n",
    "\n",
    "    # signif_item_list = [list(item.study_items) for item in bioproc_goea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goea_quiet_sig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_dict = get_GO_assign_dict(bioproc_goea, cluster_genes_uniprot)\n",
    "parent_assign_dict = get_GO_assign_dict(bioproc_parent_goea, cluster_genes_uniprot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = goea_quiet_sig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_arr = np.array(\n",
    "    [item.ratio_in_pop[0] / item.ratio_in_pop[1] for item in goea_quiet_sig]\n",
    ")\n",
    "norm_ratio_arr = ratio_arr / np.sum(ratio_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ratio_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in goea_results_sig:\n",
    "    print(item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signif_item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.namespace2NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.study_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = goea_results_sig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf_fname = goatools.base.gunzip(\"./ecocyc.gaf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cluster_timeseries = gene_cluster_df_full.groupby([\"Leiden Clusters\"]).apply(\n",
    "    lambda x: np.mean(np.array(x[\"Feature Vector\"].tolist()), axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_timeseries(\n",
    "    df,\n",
    "    cluster_label,\n",
    "    feature_vector_label=\"Feature Vector\",\n",
    "    feature_labels=[\n",
    "        \"Birth: Volume\",\n",
    "        \"Division: Volume\",\n",
    "        \"Mean Linear Growth Rate: Volume\",\n",
    "        \"Mean Exponential Growth Rate: Volume\",\n",
    "        \"Mean: minor_axis_length\",\n",
    "        \"Mean: mCherry Intensity\",\n",
    "        \"Delta t\",\n",
    "    ],\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "\n",
    "    mean_cluster_timeseries = df.groupby([cluster_label]).apply(\n",
    "        lambda x: np.mean(np.array(x[feature_vector_label].tolist()), axis=0)\n",
    "    )\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(mean_cluster_timeseries), wspace=wspace)\n",
    "\n",
    "    for i in range(len(mean_cluster_timeseries)):\n",
    "        clust_arr = np.array(mean_cluster_timeseries[i])\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[0, 6])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(\"Cluster \" + str(i), fontsize=fontsize)\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(clust_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8.0)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(\"Cluster \" + str(i), fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_timeseries(gene_cluster_df_full, figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_cluster(df, graph, graph_layout, cluster_label, cluster_id):\n",
    "    binary_list = (df[cluster_label] == cluster_id).tolist()\n",
    "    color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "        graph, binary_list, graph_layout\n",
    "    )\n",
    "    graph_plot = ig.plot(color_graph, layout=layout, vertex_frame_color=outline_hex)\n",
    "    return graph_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot = highlight_cluster(\n",
    "    gene_cluster_df_full, global_graph, global_graph_umap_layout, \"Leiden Clusters\", 4\n",
    ")\n",
    "graph_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = part.subgraph(3)\n",
    "subgraph_layout = (\n",
    "    np.array(global_graph_umap_layout.coords)[np.array(part.membership) == 3]\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_df = gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_umap(df, feature_label, n_neighbors=10, n_iterations=10, edge_weight=10):\n",
    "\n",
    "    X = np.array(df[feature_label].tolist())\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "\n",
    "    norm_soft_dtw_arr = parallel_norm_soft_dtw(X)\n",
    "\n",
    "    G, ig_G, part = knn_leiden_dist(\n",
    "        norm_soft_dtw_arr, n_neighbors=n_neighbors, n_iterations=n_iterations\n",
    "    )\n",
    "\n",
    "    graph = copy.copy(part.graph)\n",
    "\n",
    "    pal = ig.drawing.colors.ClusterColoringPalette(len(part))\n",
    "    graph.vs[\"color\"] = pal.get_many(part.membership)\n",
    "    graph.vs[\"size\"] = [5 for i in graph.vs[\"color\"]]\n",
    "    graph.es[\"width\"] = [item * edge_weight for item in graph.es[\"weight\"]]\n",
    "\n",
    "    graph_umap = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=2, metric=\"precomputed\"\n",
    "    ).fit_transform(norm_soft_dtw_arr)\n",
    "    umap_layout = ig.layout.Layout(\n",
    "        [tuple(graph_umap[i]) for i in range(graph_umap.shape[0])]\n",
    "    )\n",
    "\n",
    "    return graph, umap_layout, part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = gene_cluster_df_full[gene_cluster_df_full[\"Leiden Clusters\"] == 0]\n",
    "graph, umap_layout, cluster_part = get_df_umap(cluster_df, \"Feature Vector\")\n",
    "cluster_df[\"Subgraph Clusters\"] = cluster_part.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = cluster_df[\"Gene\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes, gene_counts = np.unique(gene_list, return_counts=True)\n",
    "genes = [str(genes[i]) + \":\" + str(gene_counts[i]) for i in range(len(genes))]\n",
    "\n",
    "\n",
    "gene_names = \" \".join(sorted(genes))\n",
    "print(textwrap.fill(gene_names, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot = ig.plot(graph, layout=umap_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_timeseries(cluster_df, \"Subgraph Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot = highlight_cluster(cluster_df, graph, umap_layout, \"Subgraph Clusters\", 1)\n",
    "graph_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_list = [\n",
    "    True if \"rpoC\" in gene else False for gene in cluster_df[\"Gene\"].tolist()\n",
    "]\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    graph, binary_list, umap_layout\n",
    ")\n",
    "ig.plot(color_graph, layout=layout, vertex_frame_color=outline_hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_in_clusters = cluster_df.groupby(\"Subgraph Clusters\").apply(\n",
    "    lambda x: x[\"Gene\"].unique().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(set([val for item in genes_in_clusters[0:4] for val in item])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(genes_in_clusters[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_label = \"Kernel Trace: Mean: mCherry Intensity: z score: Mean\"\n",
    "file_label = \"Subgraph_3_mCherry_Zscore\"\n",
    "\n",
    "make_zscore_file(cluster_df, graph, umap_layout, zscore_label, file_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cluster_df[\"Kernel Trace: Birth: Volume: z score\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.array(cluster_df[\"Kernel Trace: Mean: mCherry Intensity: z score\"].tolist()).T,\n",
    "    c=\"blue\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.array(\n",
    "        cluster_df[cluster_df[\"Subgraph Clusters\"] == 4][\n",
    "            \"Kernel Trace: Birth: Volume: z score\"\n",
    "        ].tolist()[:3]\n",
    "    ).T,\n",
    "    c=\"blue\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    cluster_df[\"Kernel Trace: Birth: Volume: z score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_list = [True if \"fts\" in gene else False for gene in gene_list]\n",
    "color_graph, layout, outline_hex = umap_plot_nodes_of_interest(\n",
    "    global_graph, binary_list, global_graph_umap_layout\n",
    ")\n",
    "ig.plot(color_graph, layout=layout, vertex_frame_color=outline_hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    \"Kernel Trace: Division: Volume: z score\",\n",
    "    \"Kernel Trace: Mean Linear Growth Rate: Volume: z score\",\n",
    "    \"Kernel Trace: Mean Exponential Growth Rate: Volume: z score\",\n",
    "    \"Kernel Trace: Mean: minor_axis_length: z score\",\n",
    "    \"Kernel Trace: Mean: mCherry Intensity: z score\",\n",
    "    \"Kernel Trace: Delta t: z score\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = skl.neighbors.NearestNeighbors(\n",
    "    n_neighbors=21, metric=\"precomputed\", algorithm=\"brute\"\n",
    ").fit(\n",
    "    norm_soft_dtw_arr\n",
    ")  # plus one neighbor to account for self-loops\n",
    "\n",
    "G = knn.kneighbors_graph(norm_soft_dtw_arr, mode=\"distance\")\n",
    "G[G.nonzero()] = 1.0 / (G[G.nonzero()] + 1.0)  ##Distance normalization\n",
    "G_arr = G.toarray()\n",
    "\n",
    "ig_G = ig.Graph.Weighted_Adjacency(G_arr, mode=\"undirected\", loops=False)\n",
    "ig_G.vs[\"index\"] = [i for i in range(ig_G.vcount())]\n",
    "ig_G.vs[\"weight\"] = [1.0 for i in range(ig_G.vcount())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = leidenalg.Optimiser()\n",
    "profile = optimiser.resolution_profile(\n",
    "    ig_G, leidenalg.CPMVertexPartition, resolution_range=(0, 0.002), weights=\"weight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = [profile[i].resolution_parameter for i in range(len(profile))]\n",
    "modularities = [profile[i].modularity for i in range(len(profile))]\n",
    "n_clusts = [len(profile[i].sizes()) for i in range(len(profile))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resolutions, modularities)\n",
    "plt.show()\n",
    "plt.plot(resolutions, n_clusts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = leidenalg.find_partition(\n",
    "    ig_G, leidenalg.ModularityVertexPartition, weights=\"weight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ig.Graph.Famous(\"Zachary\")\n",
    "optimiser = la.Optimiser()\n",
    "profile = optimiser.resolution_profile(\n",
    "    G, la.CPMVertexPartition, resolution_range=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_leiden_dist(dist_arr, n_neighbors=5):\n",
    "\n",
    "    knn = skl.neighbors.NearestNeighbors(\n",
    "        n_neighbors=n_neighbors + 1, metric=\"precomputed\", algorithm=\"brute\"\n",
    "    ).fit(\n",
    "        dist_arr\n",
    "    )  # plus one neighbor to account for self-loops\n",
    "\n",
    "    G = knn.kneighbors_graph(dist_arr, mode=\"distance\")\n",
    "    G[G.nonzero()] = 1.0 / (G[G.nonzero()] + 1.0)  ##Distance normalization\n",
    "    G_arr = G.toarray()\n",
    "\n",
    "    ig_G = ig.Graph.Weighted_Adjacency(G_arr, mode=\"undirected\", loops=False)\n",
    "    ig_G.vs[\"index\"] = [i for i in range(ig_G.vcount())]\n",
    "    ig_G.vs[\"weight\"] = [1.0 for i in range(ig_G.vcount())]\n",
    "\n",
    "    part = leidenalg.find_partition(\n",
    "        ig_G, leidenalg.ModularityVertexPartition, weights=\"weight\"\n",
    "    )\n",
    "\n",
    "    return G, ig_G, part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_part = part.cluster_graph(combine_vertices=\"sum\", combine_edges=\"mean\")\n",
    "pair_arr = np.multiply.outer(clust_part.vs[\"weight\"], clust_part.vs[\"weight\"])\n",
    "edge_list = clust_part.get_edgelist()\n",
    "pair_count = [pair_arr[edge] for edge in edge_list]\n",
    "normed_edge_weight = (np.array(clust_part.es[\"weight\"]) / np.array(pair_count)).tolist()\n",
    "clust_part.es[\"normed weight\"] = normed_edge_weight\n",
    "\n",
    "clust_part.vs[\"label\"] = [i for i in range(clust_part.vcount())]\n",
    "clust_part.vs[\"size\"] = [int(weight) for weight in clust_part.vs[\"weight\"]]\n",
    "# clust_part.vs[\"color\"] = [int(weight) for weight in clust_part.vs[\"weight\"]] DO PROPERTIES OF INTEREST HERE LATER\n",
    "clust_part.es[\"width\"] = [item * 100 for item in normed_edge_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_G = ig.Graph.Weighted_Adjacency(G_arr, mode=\"undirected\")\n",
    "ig_G.vs[\"weight\"] = [1.0 for i in range(ig_G.vcount())]\n",
    "\n",
    "part = leidenalg.find_partition(ig_G, leidenalg.ModularityVertexPartition)\n",
    "ig.plot(\n",
    "    part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_weight = 0.1\n",
    "edge_weight = 0.1\n",
    "clust_part = part.cluster_graph(combine_vertices=\"sum\", combine_edges=\"sum\")\n",
    "\n",
    "clust_part.vs[\"label\"] = [i for i in range(clust_part.vcount())]\n",
    "clust_part.vs[\"size\"] = [weight * node_weight for weight in clust_part.vs[\"weight\"]]\n",
    "# clust_part.vs[\"color\"] = [int(weight) for weight in clust_part.vs[\"weight\"]] DO PROPERTIES OF INTEREST HERE LATER\n",
    "clust_part.es[\"width\"] = [item * edge_weight for item in clust_part.es[\"weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_part.personalized_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig.plot(\n",
    "    clust_part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(gene_cluster_df[\"Feature Vector\"].tolist())\n",
    "X = np.swapaxes(X, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, ig_G, part = knn_leiden(X, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df[\"Leiden Clusters\"] = part.membership\n",
    "gene_names = gene_cluster_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_part = part.cluster_graph(combine_vertices=\"sum\", combine_edges=\"sum\")\n",
    "pair_arr = np.multiply.outer(clust_part.vs[\"weight\"], clust_part.vs[\"weight\"])\n",
    "edge_list = clust_part.get_edgelist()\n",
    "pair_count = [pair_arr[edge] for edge in edge_list]\n",
    "normed_edge_weight = (np.array(clust_part.es[\"weight\"]) / np.array(pair_count)).tolist()\n",
    "clust_part.es[\"normed weight\"] = normed_edge_weight\n",
    "\n",
    "clust_part.vs[\"label\"] = [i for i in range(clust_part.vcount())]\n",
    "clust_part.vs[\"size\"] = [int(weight) for weight in clust_part.vs[\"weight\"]]\n",
    "# clust_part.vs[\"color\"] = [int(weight) for weight in clust_part.vs[\"weight\"]] DO PROPERTIES OF INTEREST HERE LATER\n",
    "clust_part.es[\"width\"] = [item * 100 for item in normed_edge_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    gene_cluster_df[gene_cluster_df[\"Leiden Clusters\"] == 4][\"Gene\"].unique().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph = part.subgraph(4)\n",
    "sub_graph.vs[\"label\"] = [gene_names[idx] for idx in sub_graph.vs[\"index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_part = part.cluster_graph(combine_vertices=\"sum\", combine_edges=\"sum\")\n",
    "pair_arr = np.multiply.outer(clust_part.vs[\"weight\"], clust_part.vs[\"weight\"])\n",
    "edge_list = clust_part.get_edgelist()\n",
    "pair_count = [pair_arr[edge] for edge in edge_list]\n",
    "normed_edge_weight = (np.array(clust_part.es[\"weight\"]) / np.array(pair_count)).tolist()\n",
    "clust_part.es[\"normed weight\"] = normed_edge_weight\n",
    "\n",
    "clust_part.vs[\"label\"] = [i for i in range(clust_part.vcount())]\n",
    "clust_part.vs[\"size\"] = [int(weight) for weight in clust_part.vs[\"weight\"]]\n",
    "# clust_part.vs[\"color\"] = [int(weight) for weight in clust_part.vs[\"weight\"]] DO PROPERTIES OF INTEREST HERE LATER\n",
    "clust_part.es[\"width\"] = [item * 100 for item in normed_edge_weight]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    final_output_df.reset_index()\n",
    "    .set_index(\"sgRNA\")\n",
    "    .groupby(\"sgRNA\")\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .compute()\n",
    ")\n",
    "df[\"phenotype trenchids\"] = (\n",
    "    final_output_df.reset_index()\n",
    "    .set_index(\"sgRNA\")\n",
    "    .groupby(\"sgRNA\")\n",
    "    .apply(lambda x: x[\"phenotype trenchid\"].unique().tolist(), meta=list)\n",
    "    .compute()\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    df, trenchids_as_list=True, trenchid_column=\"phenotype trenchids\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    df,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button  ## NEED OPTION WHETHER OR NOT TO NORM SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
