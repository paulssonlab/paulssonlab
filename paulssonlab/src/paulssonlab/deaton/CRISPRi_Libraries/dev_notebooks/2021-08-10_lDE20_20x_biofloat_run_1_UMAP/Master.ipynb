{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Timeseries Clustering Analysis of lDE20 (with lineage Dataframe ready)\n",
    "\n",
    "- This time, focusing on clustering single parameters and plugging into UMAP\n",
    "\n",
    "- Try to reformat the dataframe at some point to be by cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import scipy as sp\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_distances,\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    ")\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    masked_label_series = df.apply(\n",
    "        lambda x: np.array(x[label])[\n",
    "            (np.array(x[time_label]) >= min_timepoint)\n",
    "            * (np.array(x[time_label]) <= max_timepoint)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    if flatten_vals:\n",
    "        flattened_vals = [val for item in masked_label_series.tolist() for val in item]\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "def get_feature_stats(df, feature_label, min_timepoint, max_timepoint):\n",
    "    feature_vals = get_timepoint_values(df, feature_label, min_timepoint, max_timepoint)\n",
    "    feature_median = np.median(feature_vals)\n",
    "    feature_iqr = sp.stats.iqr(feature_vals)\n",
    "    return feature_median, feature_iqr\n",
    "\n",
    "\n",
    "def get_feature_median_bytrench(df, feature_label, min_timepoint, max_timepoint):\n",
    "    masked_label_series = get_timepoint_values(\n",
    "        final_output_df_pd_filtered,\n",
    "        feature_label,\n",
    "        min_timepoint,\n",
    "        max_timepoint,\n",
    "        flatten_vals=False,\n",
    "    )\n",
    "    trench_median_series = masked_label_series.apply(lambda x: np.nanmedian(x))\n",
    "    return trench_median_series\n",
    "\n",
    "\n",
    "def compute_score(\n",
    "    df,\n",
    "    feature_label,\n",
    "    trench_median_series,\n",
    "    feature_median,\n",
    "    feature_iqr,\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    scaling_factor = 1.35 * (feature_median / feature_iqr)\n",
    "\n",
    "    if timepoint_range == None:\n",
    "        scores = (\n",
    "            (df[feature_label].apply(lambda x: np.array(x))) / trench_median_series\n",
    "        ) - 1.0\n",
    "    else:\n",
    "        scores = (\n",
    "            (\n",
    "                df[feature_label].apply(\n",
    "                    lambda x: np.array(x)[\n",
    "                        (np.array(x[time_label]) >= timepoint_range[0])\n",
    "                        * (np.array(x[time_label]) <= timepoint_range[1])\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            / trench_median_series\n",
    "        ) - 1.0\n",
    "    scores = scaling_factor * scores\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_feature_scores(\n",
    "    df,\n",
    "    feature_label,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    feature_median, feature_iqr = get_feature_stats(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    trench_median_series = get_feature_median_bytrench(\n",
    "        df, feature_label, init_timepoint_range[0], init_timepoint_range[1]\n",
    "    )\n",
    "    scores = compute_score(\n",
    "        df,\n",
    "        feature_label,\n",
    "        trench_median_series,\n",
    "        feature_median,\n",
    "        feature_iqr,\n",
    "        time_label=time_label,\n",
    "        timepoint_range=timepoint_range,\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_scores(\n",
    "    df,\n",
    "    feature_labels,\n",
    "    init_timepoint_range=(0, 20),\n",
    "    time_label=\"final cell timepoints list\",\n",
    "    timepoint_range=None,\n",
    "):\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_scores(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "            timepoint_range=timepoint_range,\n",
    "        )\n",
    "        df[feature_label + \": score\"] = feature_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sgrnadf_from_scoredf(\n",
    "    scoredf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    scoredf_groupby = scoredf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        scoredf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": score\"] = scoredf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label + \": score\"].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = scoredf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = scoredf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = scoredf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = scoredf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = scoredf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = scoredf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "# No longer using this\n",
    "# def filter_strong_KOs(df,sampling_thr = 4, n_strongest=2):\n",
    "\n",
    "#     for i in range(sampling_thr,0,-1):\n",
    "#         sampling_mask = df[\"N Observations\"]>=sampling_thr\n",
    "#         mismatch_series = df[sampling_mask][\"N Mismatch\"]\n",
    "\n",
    "#         for n in range(n_strongest,0,-1):\n",
    "#             if len(mismatch_series)>=n:\n",
    "#                 keep_indices = np.argsort(mismatch_series)[:n]\n",
    "#                 out_df = df[sampling_mask].iloc[keep_indices]\n",
    "\n",
    "#                 return out_df\n",
    "\n",
    "\n",
    "def normalize_timeseries(feature_vector_series, lmbda=0.5):\n",
    "    timeseries_arr = np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    sigma = np.std(timeseries_arr, axis=1)\n",
    "    if lmbda > 0.0:\n",
    "        sigma_prime = ((sigma + 1) ** lmbda - 1) / lmbda  ##yeo-johnson\n",
    "    elif lmbda == 0.0:\n",
    "        sigma_prime = np.log(sigma + 1)\n",
    "    else:\n",
    "        raise ValueError(\"lmbda cannot be negative\")\n",
    "    normalizer = sigma / sigma_prime\n",
    "    normalized_timeseries = timeseries_arr / normalizer[:, np.newaxis, :]\n",
    "    return normalized_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_output_df_pd = pd.read_pickle(\"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-08_lDE20_Lineage_Analysis.pkl\")\n",
    "# final_output_df_pd = final_output_df_pd[~final_output_df_pd[\"final cell timepoints list\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-10_lDE20_Lineage_Analysis\",\n",
    "    engine=\"pyarrow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd = final_output_df_pd[\n",
    "    ~final_output_df_pd[\"final cell timepoints list\"].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "\n",
    "#### Filter for \"Normal\" Sizes at Start\n",
    "\n",
    "1) Fit a gaussian model to each of the specified feature params during the first t timepoints of the experiment (using a subsample for speed) \n",
    "2) Compute a normalized probability trenchwise for these features under the gaussian model, during the first t timepoints of the experiment\n",
    "3) Eliminate trenches that are under some p percentile value of this probability for each feature\n",
    "4) Display histograms for each property as well as the resulting theshold\n",
    "\n",
    "Note that these features should be the only features examined in the resulting analysis. For the notebook, I am looking at:\n",
    "- Birth length (Lb)\n",
    "- Division length (Ld)\n",
    "- Mean Area Increment\n",
    "- Mean Length Increment\n",
    "- Mean Width\n",
    "- Cell cycle duration (Delta t)\n",
    "- Mean mCherry Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_timepoint_cutoff = 30\n",
    "gaussian_subsample = 0.2\n",
    "percentile_threshold = 10\n",
    "\n",
    "filter_params = [\n",
    "    \"Median Linear Growth Rate: area list\",\n",
    "    \"Median Exponential Growth Rate: area list\",\n",
    "    \"Birth: area list\",\n",
    "    \"Division: area list\",\n",
    "    \"Median: minor_axis_length list\",\n",
    "    \"Median: mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]\n",
    "\n",
    "final_output_df_pd_dask = final_output_df_pd.persist()\n",
    "\n",
    "# final_output_df_pd_dask = dd.from_pandas(final_output_df_pd,npartitions=200).persist()\n",
    "# dask.distributed.wait(final_output_df_pd_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list_params = final_output_df_pd_dask.columns[29:76].tolist()\n",
    "list_params_to_nan_filter = [\"area\", \"Volume\", \"Surface Area\", \"Height\", \"Width\"]\n",
    "median_list_params_to_nan_filter = [\n",
    "    \"Median: \" + str(param) + \" list\" for param in list_params_to_nan_filter\n",
    "]\n",
    "median_gr_list_params_to_nan_filter = [\n",
    "    \"Median Linear Growth Rate: \" + str(param) + \" list\"\n",
    "    for param in list_params_to_nan_filter\n",
    "]\n",
    "birth_list_params_to_nan_filter = [\n",
    "    \"Birth: \" + str(param) + \" list\" for param in list_params_to_nan_filter\n",
    "]\n",
    "division_list_params_to_nan_filter = [\n",
    "    \"Division: \" + str(param) + \" list\" for param in list_params_to_nan_filter\n",
    "]\n",
    "\n",
    "list_params_to_nan_filter = (\n",
    "    median_list_params_to_nan_filter\n",
    "    + median_gr_list_params_to_nan_filter\n",
    "    + birth_list_params_to_nan_filter\n",
    "    + division_list_params_to_nan_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(list_params_to_nan_filter):\n",
    "    if i == 0:\n",
    "        valid_timepoint_mask = (\n",
    "            final_output_df_pd_dask[param]\n",
    "            .apply(lambda x: ~np.isnan(np.array(x)), meta=\"object\")\n",
    "            .compute()\n",
    "        )\n",
    "    else:\n",
    "        valid_timepoint_mask = valid_timepoint_mask * (\n",
    "            final_output_df_pd_dask[param]\n",
    "            .apply(lambda x: ~np.isnan(np.array(x)), meta=\"object\")\n",
    "            .compute()\n",
    "        )\n",
    "final_output_df_pd_dask[\"Valid Timepoint Mask\"] = valid_timepoint_mask\n",
    "final_output_df_pd_dask[\"Valid Timepoint Mask\"] = final_output_df_pd_dask[\n",
    "    \"Valid Timepoint Mask\"\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in all_list_params:\n",
    "    final_output_df_pd_dask[param] = final_output_df_pd_dask.apply(\n",
    "        lambda x: np.array(x[param])[x[\"Valid Timepoint Mask\"]].tolist(),\n",
    "        meta=\"object\",\n",
    "        axis=1,\n",
    "    ).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_pd_dask = final_output_df_pd_dask.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_pd_dask[\"Early Timepoint Mask\"] = final_output_df_pd_dask[\n",
    "    \"cell timepoints list\"\n",
    "].apply(\n",
    "    lambda x: np.array([item if (type(item) is int) else 10000000 for item in x])\n",
    "    < early_timepoint_cutoff,\n",
    "    meta=(None, \"object\"),\n",
    ")\n",
    "\n",
    "for filter_param in filter_params:\n",
    "    early_param_series = final_output_df_pd_dask.apply(\n",
    "        lambda x: np.array(x[filter_param])[x[\"Early Timepoint Mask\"]]\n",
    "        if type(x[filter_param]) is list\n",
    "        else np.array([]),\n",
    "        axis=1,\n",
    "        meta=(None, \"object\"),\n",
    "    )\n",
    "    all_param_values = [\n",
    "        val\n",
    "        for item in early_param_series.sample(frac=gaussian_subsample)\n",
    "        .compute()\n",
    "        .tolist()\n",
    "        for val in item\n",
    "    ]\n",
    "    gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    final_output_df_pd[filter_param + \": Probability\"] = early_param_series.apply(\n",
    "        lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "    ).persist()\n",
    "\n",
    "plt.figure(figsize=(22, 16))\n",
    "query_list = []\n",
    "for i, filter_param in enumerate(filter_params):\n",
    "    prob_threshold = np.nanpercentile(\n",
    "        final_output_df_pd[filter_param + \": Probability\"].tolist(),\n",
    "        percentile_threshold,\n",
    "    )\n",
    "    query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "    query_list.append(query)\n",
    "\n",
    "    min_v, max_v = np.min(final_output_df_pd[filter_param + \": Probability\"]), np.max(\n",
    "        final_output_df_pd[filter_param + \": Probability\"]\n",
    "    )\n",
    "\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.title(filter_param)\n",
    "    plt.hist(\n",
    "        final_output_df_pd[\n",
    "            final_output_df_pd[filter_param + \": Probability\"] < prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "    plt.hist(\n",
    "        final_output_df_pd[\n",
    "            final_output_df_pd[filter_param + \": Probability\"] >= prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "plt.show()\n",
    "\n",
    "compiled_query = \" and \".join(query_list)\n",
    "final_output_df_pd_filtered = final_output_df_pd.query(compiled_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_output_df_pd_filtered) / len(final_output_df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_output_df_pd_filtered) / len(final_output_df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\"./2021-08-10_lDE20_Lineage_Analysis_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\"./2021-08-10_lDE20_Lineage_Analysis_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_df_pd_filtered = pd.read_pickle(\"./2021-08-10_lDE20_Lineage_Analysis_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_df_pd_filtered = pd.read_pickle(\"./2021-08-10_lDE20_Lineage_Analysis_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Clustering on Individual Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def get_sgrnadf_from_df(df, feature_labels, time_label=\"final cell timepoints list\"):\n",
    "    df_groupby = df.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        df_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label] = df_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = df_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = df_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = df_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = df_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = df_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = df_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "def timeseries_lowess_reg(df, t_label, y_label, min_tpt, max_tpt, bins, frac=0.33):\n",
    "    del_tpt = max_tpt - min_tpt\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins, dtype=float)\n",
    "\n",
    "    def lowess_reg(x_arr, y_arr, start=min_tpt, end=max_tpt, bins=bins, frac=frac):\n",
    "        intervals = np.linspace(start, end, num=bins, dtype=float)\n",
    "        w = lowess(y_arr, x_arr, frac=frac, xvals=intervals, it=1)\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    lowess_result = df.apply(\n",
    "        lambda x: lowess_reg(x[t_label], x[y_label])[1], axis=1, meta=float\n",
    "    )\n",
    "\n",
    "    return lowess_result\n",
    "\n",
    "\n",
    "def get_all_lowess_regs(\n",
    "    df,\n",
    "    y_label_list,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=0.33,\n",
    "    t_label=\"final cell timepoints list\",\n",
    "    iqr_bins=8,\n",
    "    iqr_window=3,\n",
    "):\n",
    "    out_df = copy.deepcopy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        lowess_result = timeseries_lowess_reg(\n",
    "            df, t_label, y_label, min_tpt, max_tpt, bins, frac=frac\n",
    "        )\n",
    "        out_df[\"LOWESS Trace: \" + y_label] = lowess_result.persist()\n",
    "    #         out_df[\"Binned IQR: \" + y_label] = out_df.apply(lambda x: compute_binned_iqr(x[t_label], x[y_label],min_tpt,max_tpt,iqr_bins,iqr_window), axis=1, meta=\"object\").persist()\n",
    "    #         interp_series = out_df[\"Binned IQR: \" + y_label].apply(lambda x: interp1d_with_nan(np.linspace(min_tpt,max_tpt,num=iqr_bins),x), meta=\"object\").persist()\n",
    "    #         out_df[\"Binned IQR Interpolation: \" + y_label] = interp_series.apply(lambda x: x(np.linspace(min_tpt,max_tpt,num=bins)), meta=\"object\").persist()\n",
    "    #         out_df.apply(lambda x: x[\"Binned IQR Interpolation: \" + y_label](np.linspace(min_tpt,max_tpt,num=bins)), axis=1, meta=\"object\").persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def compute_binned_iqr(time_arr, val_arr, min_tpt, max_tpt, bins, window):\n",
    "    iqr_window_radius = (window - 1) // 2\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins + 1, dtype=float)\n",
    "    lower_bounds = intervals[:-1]\n",
    "    upper_bounds = intervals[1:]\n",
    "    interval_assign = np.where(\n",
    "        np.logical_and(\n",
    "            np.greater.outer(time_arr, lower_bounds),\n",
    "            np.less_equal.outer(time_arr, upper_bounds),\n",
    "        )\n",
    "    )[1]\n",
    "    iqrs = np.array(\n",
    "        [\n",
    "            iqr(\n",
    "                val_arr[\n",
    "                    (interval_assign <= i + iqr_window_radius)\n",
    "                    * (interval_assign >= (i - iqr_window_radius))\n",
    "                ],\n",
    "                nan_policy=\"omit\",\n",
    "            )\n",
    "            for i in range(len(intervals) - 1)\n",
    "        ]\n",
    "    )\n",
    "    return iqrs\n",
    "\n",
    "\n",
    "def interp1d_with_nan(timepoint_arr, timeseries_arr):\n",
    "    \"\"\"\n",
    "    interpolate to fill nan values #https://newbedev.com/interpolate-nan-values-in-a-numpy-array\n",
    "    \"\"\"\n",
    "    good_vals = np.where(np.isfinite(timeseries_arr))[0]\n",
    "    filtered_timepoint_arr, filtered_timeseries_arr = (\n",
    "        timepoint_arr[good_vals],\n",
    "        timeseries_arr[good_vals],\n",
    "    )\n",
    "    interp = sp.interpolate.interp1d(\n",
    "        filtered_timepoint_arr, filtered_timeseries_arr, kind=\"cubic\"\n",
    "    )\n",
    "    return interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_params = [\n",
    "    \"Birth: area list\",\n",
    "    \"Division: area list\",\n",
    "    \"Median Linear Growth Rate: area list\",\n",
    "    \"Median Exponential Growth Rate: area list\",\n",
    "    \"Median: Width list\",\n",
    "    \"Median: mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgrnadf = get_sgrnadf_from_df(final_output_df_pd_filtered, selected_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_Observations_thr = 4\n",
    "\n",
    "sgrnadf_wellsampled = sgrnadf[sgrnadf[\"N Observations\"] >= N_Observations_thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tpt = 0\n",
    "max_tpt = 143\n",
    "bins = 20\n",
    "# frac = (1/bins)*4\n",
    "frac = (1 / bins) * 8\n",
    "iqr_bins = 8  # optimized on minC\n",
    "iqr_window = 3\n",
    "\n",
    "sgrnadf_wellsampled_dask = dd.from_pandas(\n",
    "    sgrnadf_wellsampled, npartitions=100\n",
    ").persist()\n",
    "dask.distributed.wait(sgrnadf_wellsampled_dask)\n",
    "\n",
    "lowess_trace_df = get_all_lowess_regs(\n",
    "    sgrnadf_wellsampled_dask,\n",
    "    sgrnadf_wellsampled.columns[1:8],\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    "    iqr_bins=iqr_bins,\n",
    "    iqr_window=iqr_window,\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsz_df = lowess_trace_df[lowess_trace_df[\"Gene\"] == \"folA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ftsz_df)):\n",
    "    var = ftsz_df.iloc[i]\n",
    "    plt.plot(var[\"LOWESS Trace: Division: area list\"][4:-2])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(ftsz_df)):\n",
    "    var = ftsz_df.iloc[i]\n",
    "    plt.plot(var[\"LOWESS Trace: Delta t list\"][4:-2])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(ftsz_df)):\n",
    "    var = ftsz_df.iloc[i]\n",
    "    plt.plot(var[\"LOWESS Trace: Median Linear Growth Rate: area list\"][4:-2])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(ftsz_df)):\n",
    "    var = ftsz_df.iloc[i]\n",
    "    plt.plot(var[\"LOWESS Trace: Median Exponential Growth Rate: area list\"][4:-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Normalize Properties\n",
    "\n",
    "1) Yeo-Johnson transform the data as before (this time I am omitting the label for simplicity).\n",
    "2) Convert transformed values to time-dependent s-scores using the following formula:\n",
    "\n",
    "$$ z(i,k,t) = 1.35 \\times \\frac{median_{t\\in \\tau}(F_{i,t})}{iqr_{t\\in \\tau}(F_{i,t})}\\Bigg(\\frac{F_{i,k,t}}{median_{t\\in \\tau}(F_{i,k,t})} - 1\\Bigg) $$\n",
    "\n",
    "where $F_{i,k,t}$ are the feature values for feature i, trench k at time t. $\\tau$ are the initial pre-induction timepoints. \n",
    "\n",
    "Essentially this is a z-score using the more outlier robust median and interquartile range to define the differences from normal bahavior. The 1.35 factor scales the values such that z-scores represent number of standard deviations from the mean for a normal distribution. Finally the values are normalized by initial behaviors trenchwise by the $median_{t\\in \\tau}(F_{i,k,t})$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Lb list\",\n",
    "    \"Ld list\",\n",
    "    \"Mean Area Increment list\",\n",
    "    \"Mean Length Increment list\",\n",
    "    \"Mean Width list\",\n",
    "    \"Mean mCherry Intensity list\",\n",
    "    \"Delta t list\",\n",
    "]\n",
    "yeo_subsample = 0.1\n",
    "\n",
    "final_output_df_pd_filtered_dask = dd.from_pandas(\n",
    "    final_output_df_pd_filtered, npartitions=100\n",
    ").persist()\n",
    "dask.distributed.wait(final_output_df_pd_filtered_dask)\n",
    "\n",
    "for i, param in enumerate(params_to_transform):\n",
    "    all_param_values = [\n",
    "        float(val)\n",
    "        for item in final_output_df_pd_filtered_dask[param]\n",
    "        .sample(frac=yeo_subsample)\n",
    "        .compute()\n",
    "        .tolist()\n",
    "        for val in item\n",
    "    ]\n",
    "    l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "    final_output_df_pd_filtered_dask[param] = (\n",
    "        final_output_df_pd_filtered_dask[param]\n",
    "        .apply(\n",
    "            lambda x: sp.stats.yeojohnson(np.array(x).astype(float), lmbda=l_norm),\n",
    "            meta=\"object\",\n",
    "        )\n",
    "        .persist()\n",
    "    )\n",
    "final_output_df_pd_filtered = final_output_df_pd_filtered_dask.compute()\n",
    "\n",
    "scoredf = get_all_feature_scores(final_output_df_pd_filtered, params_to_transform)\n",
    "sgrnadf = get_sgrnadf_from_scoredf(scoredf, params_to_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### sgRNA Effect Size Filtering (within Gene groups)\n",
    "\n",
    "1) Threshold sgRNAs to include by number of observations\n",
    "2) Use LOWESS to smooth out score timeseries into 20 point timeseries\n",
    "3) For each timepoint, measure the euclidean norm of the feature vector and take the maximum over all time as a measure of effect size\n",
    "4) Thrshold sgRNAs for strong effects by applying a threshold to the euclidean norm that will be displayed with histogram\n",
    "5) Display a histogram for the sgRNA number per gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def timeseries_lowess_reg(df, t_label, y_label, min_tpt, max_tpt, bins, frac=0.33):\n",
    "    del_tpt = max_tpt - min_tpt\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins, dtype=float)\n",
    "\n",
    "    def lowess_reg(x_arr, y_arr, start=min_tpt, end=max_tpt, bins=bins, frac=frac):\n",
    "        intervals = np.linspace(start, end, num=bins, dtype=float)\n",
    "        w = lowess(y_arr, x_arr, frac=frac, xvals=intervals, it=1)\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    lowess_result = df.apply(\n",
    "        lambda x: lowess_reg(x[t_label], x[y_label])[1], axis=1, meta=float\n",
    "    )\n",
    "\n",
    "    return lowess_result\n",
    "\n",
    "\n",
    "def get_all_lowess_regs(\n",
    "    df,\n",
    "    y_label_list,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=0.33,\n",
    "    t_label=\"final cell timepoints list\",\n",
    "    iqr_bins=8,\n",
    "    iqr_window=3,\n",
    "):\n",
    "    out_df = copy.deepcopy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        lowess_result = timeseries_lowess_reg(\n",
    "            df, t_label, y_label, min_tpt, max_tpt, bins, frac=frac\n",
    "        )\n",
    "        out_df[\"LOWESS Trace: \" + y_label] = lowess_result.persist()\n",
    "        out_df[\"Binned IQR: \" + y_label] = out_df.apply(\n",
    "            lambda x: compute_binned_iqr(\n",
    "                x[t_label], x[y_label], min_tpt, max_tpt, iqr_bins, iqr_window\n",
    "            ),\n",
    "            axis=1,\n",
    "            meta=\"object\",\n",
    "        ).persist()\n",
    "        interp_series = (\n",
    "            out_df[\"Binned IQR: \" + y_label]\n",
    "            .apply(\n",
    "                lambda x: interp1d_with_nan(\n",
    "                    np.linspace(min_tpt, max_tpt, num=iqr_bins), x\n",
    "                ),\n",
    "                meta=\"object\",\n",
    "            )\n",
    "            .persist()\n",
    "        )\n",
    "        out_df[\"Binned IQR Interpolation: \" + y_label] = interp_series.apply(\n",
    "            lambda x: x(np.linspace(min_tpt, max_tpt, num=bins)), meta=\"object\"\n",
    "        ).persist()\n",
    "    #         out_df.apply(lambda x: x[\"Binned IQR Interpolation: \" + y_label](np.linspace(min_tpt,max_tpt,num=bins)), axis=1, meta=\"object\").persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# def compute_binned_std(time_arr, val_arr, min_tpt, max_tpt, bins):\n",
    "#     intervals = np.linspace(min_tpt,max_tpt,num=bins,dtype=float)\n",
    "#     lower_bounds = intervals[:-1]\n",
    "#     upper_bounds = intervals[1:]\n",
    "#     interval_assign = np.where(np.logical_and(np.greater.outer(time_arr,lower_bounds),np.less_equal.outer(time_arr,upper_bounds)))[1]\n",
    "#     std_devs = [np.nanstd(val_arr[interval_assign==i]) for i in range(len(intervals)-1)]\n",
    "#     return std_devs\n",
    "\n",
    "\n",
    "def compute_binned_iqr(time_arr, val_arr, min_tpt, max_tpt, bins, window):\n",
    "    iqr_window_radius = (window - 1) // 2\n",
    "    intervals = np.linspace(min_tpt, max_tpt, num=bins + 1, dtype=float)\n",
    "    lower_bounds = intervals[:-1]\n",
    "    upper_bounds = intervals[1:]\n",
    "    interval_assign = np.where(\n",
    "        np.logical_and(\n",
    "            np.greater.outer(time_arr, lower_bounds),\n",
    "            np.less_equal.outer(time_arr, upper_bounds),\n",
    "        )\n",
    "    )[1]\n",
    "    iqrs = np.array(\n",
    "        [\n",
    "            iqr(\n",
    "                val_arr[\n",
    "                    (interval_assign <= i + iqr_window_radius)\n",
    "                    * (interval_assign >= (i - iqr_window_radius))\n",
    "                ],\n",
    "                nan_policy=\"omit\",\n",
    "            )\n",
    "            for i in range(len(intervals) - 1)\n",
    "        ]\n",
    "    )\n",
    "    return iqrs\n",
    "\n",
    "\n",
    "def interp1d_with_nan(timepoint_arr, timeseries_arr):\n",
    "    \"\"\"\n",
    "    interpolate to fill nan values #https://newbedev.com/interpolate-nan-values-in-a-numpy-array\n",
    "    \"\"\"\n",
    "    good_vals = np.where(np.isfinite(timeseries_arr))[0]\n",
    "    filtered_timepoint_arr, filtered_timeseries_arr = (\n",
    "        timepoint_arr[good_vals],\n",
    "        timeseries_arr[good_vals],\n",
    "    )\n",
    "    interp = sp.interpolate.interp1d(\n",
    "        filtered_timepoint_arr, filtered_timeseries_arr, kind=\"cubic\"\n",
    "    )\n",
    "    return interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_Observations_thr = 4\n",
    "\n",
    "sgrnadf_wellsampled = sgrnadf[sgrnadf[\"N Observations\"] >= N_Observations_thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_tpt = 0\n",
    "max_tpt = 143\n",
    "bins = 20\n",
    "frac = (1 / bins) * 4\n",
    "iqr_bins = 8  # optimized on minC\n",
    "iqr_window = 3\n",
    "\n",
    "sgrnadf_wellsampled_dask = dd.from_pandas(\n",
    "    sgrnadf_wellsampled, npartitions=100\n",
    ").persist()\n",
    "dask.distributed.wait(sgrnadf_wellsampled_dask)\n",
    "\n",
    "lowess_trace_df = get_all_lowess_regs(\n",
    "    sgrnadf_wellsampled_dask,\n",
    "    sgrnadf_wellsampled.columns[1:8],\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    bins,\n",
    "    frac=frac,\n",
    "    iqr_bins=iqr_bins,\n",
    "    iqr_window=iqr_window,\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Clustering on Mean Behavior Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "feature_vector_series = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Feature Vector\"] = feature_vector_series\n",
    "lowess_trace_df_nan_filtered = lowess_trace_df[\n",
    "    ~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strong_effect_threshold = 35\n",
    "\n",
    "zero_vector = np.zeros(\n",
    "    (1, lowess_trace_df_nan_filtered[\"Feature Vector\"].iloc[0].shape[0])\n",
    ")\n",
    "feature_arr = np.array(lowess_trace_df_nan_filtered[\"Feature Vector\"].tolist())\n",
    "flattened_feature_arr = np.swapaxes(feature_arr, 1, 2).reshape(-1, feature_arr.shape[1])\n",
    "dist_arr = euclidean_distances(flattened_feature_arr, zero_vector).reshape(\n",
    "    feature_arr.shape[0], feature_arr.shape[2]\n",
    ")\n",
    "lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"] = sp.integrate.simpson(\n",
    "    dist_arr\n",
    ")\n",
    "# lowess_trace_df[\"Max Euclidean Norm\"] = np.max(dist_arr,axis=1)\n",
    "\n",
    "sgrnadf_strong_effect = lowess_trace_df_nan_filtered[\n",
    "    lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = (\n",
    "    np.min(lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"]),\n",
    "    np.percentile(lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"], 99),\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Euclidean Norm\")\n",
    "plt.hist(\n",
    "    lowess_trace_df_nan_filtered[\n",
    "        lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"]\n",
    "        < strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    lowess_trace_df_nan_filtered[\n",
    "        lowess_trace_df_nan_filtered[\"Integrated Euclidean Norm\"]\n",
    "        >= strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"], return_counts=True)\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_rep_example_series = (\n",
    "    sgrnadf_strong_effect.reset_index(drop=False)\n",
    "    .groupby(\"TargetID\")\n",
    "    .apply(lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])])\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(\"sgRNA\", drop=True)\n",
    ")\n",
    "\n",
    "normalized_timeseries = np.swapaxes(\n",
    "    normalize_timeseries(most_rep_example_series[\"Feature Vector\"], lmbda=0.5), 1, 2\n",
    ")\n",
    "most_rep_example_series[\"Normalized Feature Vector\"] = [\n",
    "    normalized_timeseries[i] for i in range(normalized_timeseries.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Effect Distance Metrics\n",
    "\n",
    "Now, I want to evaluate the performance of different distance metrics on the data wrt seperating it maximally while also preserving similarity within replicates\n",
    "\n",
    "- DTW (can be done with cosine similarity) \n",
    "- cosine similarity (same as pearson for z-scores)\n",
    "- cross correlation\n",
    "\n",
    "Seems like soft-DTW is a pretty good option. Going forward with that for now.\n",
    "\n",
    "<!-- In the end cosine similarity was chosen as it produced superior silhouette scores for sets of targets from genes with different phenotypes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_examples_for_distance_metric = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin([\"ftsN\", \"rplA\", \"mreB\", \"tufB\", \"tff\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import (\n",
    "    cdist_dtw,\n",
    "    cdist_soft_dtw,\n",
    "    cdist_soft_dtw_normalized,\n",
    "    dtw,\n",
    "    dtw_path_from_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_arr = np.swapaxes(\n",
    "    np.array(\n",
    "        sgrnadf_examples_for_distance_metric[\"Normalized Feature Vector\"].tolist()\n",
    "    ),\n",
    "    1,\n",
    "    2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    print(\n",
    "        \"Soft-DTW Gamma=\"\n",
    "        + str(gamma)\n",
    "        + \": \"\n",
    "        + str(\n",
    "            tslearn.clustering.silhouette_score(\n",
    "                timeseries_arr,\n",
    "                sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "                metric=\"softdtw\",\n",
    "                gamma=gamma,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"cosine\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Cosine-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"euclidean\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Euclidean-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_dtw_dist_arr = tslearn.metrics.cdist_soft_dtw(timeseries_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(soft_dtw_dist_arr.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Detecting different effects against single genes\n",
    "\n",
    "1) Plot a histogram of minimum soft-DTW similarity within groups of TargetIDs against the same genes (for genes with more than one targetID)\n",
    "2) Use affinity propagation to select the number of phenotype clusters to use per gene (preference still needs to be dialed in, not sure how to optimize on this)\n",
    "3) Among each cluster, represent the final effect as the strongest effect (integrated euc norm) of the members of the cluster\n",
    "\n",
    "~~3) Among each cluster, represent the final effect as the median of the members of the cluster~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_softdtw(feature_vector_series):\n",
    "    dist_mat = cdist_soft_dtw_normalized(\n",
    "        np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    )\n",
    "    timeseries_len = (\n",
    "        feature_vector_series[0].shape[0] * feature_vector_series[0].shape[1]\n",
    "    )\n",
    "    dist_mat = dist_mat / timeseries_len\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def get_upper_right_vals(a):\n",
    "    upper_tri = np.triu(a, k=1)\n",
    "    upper_tri[upper_tri == 0.0] = np.NaN\n",
    "    return upper_tri\n",
    "\n",
    "\n",
    "def get_sgRNA_clusters(df, preference=0.6):\n",
    "    gene_indexed_df = (\n",
    "        df.reset_index(drop=False)\n",
    "        .set_index(\"Gene\")[[\"sgRNA\", \"Normalized Feature Vector\", \"TargetID\"]]\n",
    "        .sort_index()\n",
    "    )\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = pd.Series(\n",
    "        np.zeros(len(gene_indexed_df), dtype=int), dtype=int\n",
    "    )\n",
    "    gene_df_list = []\n",
    "    for gene in gene_indexed_df.index.tolist():\n",
    "        gene_df = gene_indexed_df.loc[[gene]]\n",
    "        if len(gene_df) > 1:\n",
    "            gene_feature_vector = gene_df[\"Normalized Feature Vector\"]\n",
    "            soft_dtw_dist = get_normed_softdtw(gene_feature_vector)\n",
    "            af_labels = (\n",
    "                AffinityPropagation(\n",
    "                    affinity=\"precomputed\", preference=preference, random_state=42\n",
    "                )\n",
    "                .fit_predict(-soft_dtw_dist)\n",
    "                .astype(int)\n",
    "            )\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = af_labels\n",
    "        else:\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = 0\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = gene_indexed_df[\"sgRNA Cluster\"].astype(int)\n",
    "    return gene_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sgrna_replicate_thr = 2\n",
    "pref_factor = 3.0\n",
    "\n",
    "gene_list, counts_list = np.unique(most_rep_example_series[\"Gene\"], return_counts=True)\n",
    "genes_with_many_replicate_sgRNAs = gene_list[counts_list >= n_sgrna_replicate_thr]\n",
    "sgrnadf_many_copies_per_gene = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin(genes_with_many_replicate_sgRNAs)\n",
    "]\n",
    "\n",
    "max_distance_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: np.nanmax(\n",
    "        get_upper_right_vals(get_normed_softdtw(x[\"Normalized Feature Vector\"]))\n",
    "    )\n",
    ")\n",
    "plt.title(\"Maximum soft-DTW Distance per Gene\")\n",
    "plt.hist(max_distance_within_gene, bins=50)\n",
    "plt.show()\n",
    "\n",
    "dist_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: get_upper_right_vals(\n",
    "        get_normed_softdtw(x[\"Normalized Feature Vector\"])\n",
    "    ).flatten()\n",
    ")\n",
    "dist_within_gene = [val for item in dist_within_gene.tolist() for val in item]\n",
    "median_similarity = -np.nanmedian(dist_within_gene)\n",
    "\n",
    "gene_df = get_sgRNA_clusters(\n",
    "    most_rep_example_series, preference=pref_factor * median_similarity\n",
    ")\n",
    "\n",
    "most_rep_example_series[\"sgRNA Cluster\"] = gene_df.set_index(\"sgRNA\")[\"sgRNA Cluster\"]\n",
    "most_rep_example_series[\"sgRNA Cluster Label\"] = most_rep_example_series.apply(\n",
    "    lambda x: str(x[\"Gene\"]) + \"-\" + str(x[\"sgRNA Cluster\"]), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df = most_rep_example_series[\n",
    "    [\n",
    "        \"sgRNA Cluster Label\",\n",
    "        \"Normalized Feature Vector\",\n",
    "        \"Gene\",\n",
    "        \"Integrated Euclidean Norm\",\n",
    "    ]\n",
    "    + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "].reset_index(drop=True)\n",
    "gene_cluster_groupby = gene_cluster_df.groupby(\"sgRNA Cluster Label\")\n",
    "# median_feature_series = gene_cluster_groupby.apply(lambda x: np.median(np.stack(x[\"Feature Vector\"]).astype(float), axis=0)).to_frame().rename(columns={0:\"Feature Vector\"})\n",
    "feature_series = (\n",
    "    gene_cluster_groupby.apply(\n",
    "        lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])][\n",
    "            \"Normalized Feature Vector\"\n",
    "        ]\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Normalized Feature Vector\"})\n",
    ")\n",
    "\n",
    "gene_cluster_df = gene_cluster_groupby.apply(\n",
    "    lambda x: x.iloc[0][\n",
    "        [\"Gene\"]\n",
    "        + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "    ]\n",
    ")\n",
    "gene_cluster_df = gene_cluster_df.join(feature_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Clustering: TSNE and Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = get_normed_softdtw(gene_cluster_df[\"Normalized Feature Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(\n",
    "    n_components=2, perplexity=5.0, early_exaggeration=50.0, metric=\"precomputed\"\n",
    ").fit_transform(X_dist - np.min(X_dist))\n",
    "gene_cluster_df[\"TSNE Coords\"] = [X_embedded[i] for i in range(X_embedded.shape[0])]\n",
    "\n",
    "af_labels = (\n",
    "    AffinityPropagation(affinity=\"precomputed\", preference=-0.5)\n",
    "    .fit_predict(-X_dist)\n",
    "    .astype(int)\n",
    ")\n",
    "gene_cluster_df[\"Affinity Clusts\"] = af_labels\n",
    "\n",
    "plt.scatter(\n",
    "    X_embedded[:, 0],\n",
    "    X_embedded[:, 1],\n",
    "    s=3,\n",
    "    alpha=1,\n",
    "    c=gene_cluster_df[\"Affinity Clusts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_labels = [\n",
    "    \"Birth Length\",\n",
    "    \"Division Length\",\n",
    "    \"Area Growth Rate\",\n",
    "    \"Length Growth Rate\",\n",
    "    \"Average Width\",\n",
    "    \"mCherry Intensity\",\n",
    "    \"Cell Cycle Duration\",\n",
    "]\n",
    "\n",
    "hierarchical_labels = gene_cluster_df.index.tolist()\n",
    "\n",
    "\n",
    "def get_leaf_children(tree, leaf_id):\n",
    "    cluster_node = tree[leaf_id]\n",
    "    leaf_children = cluster_node.pre_order(lambda x: x.id)\n",
    "    return leaf_children\n",
    "\n",
    "\n",
    "def assign_dendro_clusts(df, children_labels):\n",
    "    df_out = copy.deepcopy(df)\n",
    "    df_out[\"Dendrogram Clusters\"] = pd.Series(len(df), dtype=int)\n",
    "    for clust_i, indices in enumerate(children_labels):\n",
    "        df_out[\"Dendrogram Clusters\"].iloc[indices] = clust_i\n",
    "    df_out[\"Dendrogram Clusters\"] = df_out[\"Dendrogram Clusters\"].astype(int)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "suppress_thr = 15\n",
    "min_zscore = -2\n",
    "max_zscore = 2\n",
    "\n",
    "\n",
    "def compute_and_plot_dendrogram(\n",
    "    df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "    linewidth=5,\n",
    "    fontsize=18,\n",
    "):\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    hierarchical_labels = df.index.tolist()\n",
    "    X = np.array(df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(2, suppress_thr)\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    #     Y = sch.linkage(X, method='weighted', metric='cosine',optimal_ordering=True)\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=fontsize,\n",
    "        truncate_mode=\"lastp\",\n",
    "        show_contracted=True,\n",
    "        p=suppress_thr,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "    children_labels = [get_leaf_children(cluster_tree, leaf) for leaf in Z[\"leaves\"]]\n",
    "\n",
    "    #     fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\\\n",
    "    #                  ax=dendro_ax, orientation='vertical', label='Z-score',\\\n",
    "    #                 use_gridspec=True, location='left', pad=-0.05,aspect=10)\n",
    "\n",
    "    for i, children in enumerate(children_labels):\n",
    "        children_arr = np.array(\n",
    "            df.iloc[children][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "        mean_vector = np.mean(children_arr, axis=0)  # feature,timepoint\n",
    "        # fig = plt.figure(constrained_layout=True, figsize=(20,10))\n",
    "        # gs = fig.add_gridspec(2, 10)\n",
    "        # for v in range(10):\n",
    "        #     inner_gs = gs[0,v].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0, )\n",
    "        #     inner_grid_sub = inner_gs.subplots()\n",
    "        #     for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "        #         ax.plot(mean_vector[c])\n",
    "        #         ax.set(xticks=[], yticks=[])\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                mean_vector.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[0, 6])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return children_labels\n",
    "\n",
    "\n",
    "def plot_subset(\n",
    "    df_subset,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    feature_labels=feature_labels,\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "    df_clusts = (\n",
    "        df_subset.sort_index()\n",
    "        .reset_index(drop=False)\n",
    "        .set_index(\"Dendrogram Clusters\")[\n",
    "            [\"sgRNA Cluster Label\", \"Normalized Feature Vector\"]\n",
    "        ]\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(df_clusts), wspace=wspace)\n",
    "\n",
    "    for i in range(len(df_clusts)):\n",
    "        clust_arr = np.array(\n",
    "            df_clusts[\"Normalized Feature Vector\"].iloc[i].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[0, 6])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                rotation=90,\n",
    "                fontsize=fontsize,\n",
    "            )\n",
    "\n",
    "        #             imshow_first_ax = fig.add_subplot(gs[0, i])\n",
    "        #             imshow_first_ax.imshow(df_clusts[\"Feature Vector\"].iloc[i].astype(float).reshape(-1,1),cmap=cmap,norm=norm)\n",
    "\n",
    "        #             imshow_first_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "        #             imshow_first_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "        #             imshow_first_ax.set_xlabel(df_clusts[\"sgRNA Cluster Label\"].iloc[i] + \"\\n Cluster \" + str(df_clusts.index[i]), fontsize=14)\n",
    "\n",
    "        #             imshow_first_ax.set_yticks(range(len(feature_labels)))\n",
    "        #             imshow_first_ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(clust_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8.0)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                rotation=90,\n",
    "                fontsize=fontsize,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "
\n",
    "def make_subset_dendrogram(\n",
    "    sub_df,\n",
    "    title,\n",
    "    feature_labels=feature_labels,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    figsize=(10, 10),\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "    X_dist = get_normed_softdtw(sub_df[\"Normalized Feature Vector\"])\n",
    "    X = np.array(sub_df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(2, len(sub_df))\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        show_contracted=True,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    fig.suptitle(title, fontsize=fontsize)\n",
    "\n",
    "    for i, leaf in enumerate(Z[\"leaves\"]):\n",
    "        leaf_arr = np.array(\n",
    "            sub_df.iloc[leaf][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                leaf_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[0, 6])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(leaf_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-4, 8)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "#         imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "#         imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "\n",
    "#         imshow_ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_and_plot_dendrogram(df,X_dist,feature_labels,suppress_thr,min_zscore,max_zscore,cmap=mpl.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "children_labels = compute_and_plot_dendrogram(\n",
    "    gene_cluster_df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "    linewidth=5,\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.savefig(\"./Dendrograms/Global_Dendrogram.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df = assign_dendro_clusts(gene_cluster_df, children_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### Major System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fts\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fts_subset, figsize=(20, 8))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/fts.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpl_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpl\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpl_subset, figsize=(50, 10), wspace=0.35)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/rpl.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpm\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpm_subset, figsize=(25, 10), wspace=0.25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/rpm.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rps\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rps_subset, figsize=(50, 10), wspace=0.35)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/rps.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_subset = gene_cluster_df[gene_cluster_df.apply(lambda x: \"rr\" in x[\"Gene\"], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rr_subset, figsize=(15, 10), wspace=0.35)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/rr.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tff_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"tff\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(tff_subset, figsize=(8, 10), wspace=0.25)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/tff.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpo_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpo\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpo_subset, figsize=(12, 10), wspace=0.25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/rpo.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"min\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(min_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/min.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"dna\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(dna_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/dna.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/fol.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "muk_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"muk\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(muk_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/muk.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mre\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mre_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/mre.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mur_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mur\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mur_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/mur.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nus_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"nus\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(nus_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/nus.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"sec\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(sec_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/sec.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"bam\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(bam_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/bam.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "hol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/hol.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "hda_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hda\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hda_subset, figsize=(5, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/hda.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "rodZ_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rodZ\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rodZ_subset, figsize=(5, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Gene_Groups/rodz.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "#### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_counts = np.unique(\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"], return_counts=True\n",
    ")\n",
    "singleton_clusters = clusters[cluster_counts < 3]\n",
    "small_clusters = clusters[(cluster_counts <= 40) & (cluster_counts >= 3)]\n",
    "big_clusters = clusters[cluster_counts > 40]\n",
    "print(singleton_clusters)\n",
    "print(small_clusters)\n",
    "print(big_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0to1 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([0, 1])]\n",
    "cluster_2to3 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([2, 3])]\n",
    "cluster_4to5 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([4, 5])]\n",
    "cluster_6 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([6])]\n",
    "cluster_7 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([7])]\n",
    "cluster_8to9 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([8, 9])]\n",
    "cluster_10to11 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([10, 11])]\n",
    "cluster_12 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([12])]\n",
    "cluster_13 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([13])]\n",
    "cluster_14 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_0to1,\n",
    "    \"Cluster 0 and 1 Dendrogram\",\n",
    "    figsize=(30, 20),\n",
    "    fontsize=20,\n",
    "    linewidth=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters = list(set(small_clusters) - set([3, 4, 9, 10, 11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in remaining_small_clusters:\n",
    "    cluster_df = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([i])]\n",
    "    make_subset_dendrogram(\n",
    "        cluster_df,\n",
    "        \"Cluster \" + str(i) + \" Dendrogram\",\n",
    "        figsize=(int((len(cluster_df) * 5.0) - 9.0), int((len(cluster_df) * 2.0) + 3)),\n",
    "        fontsize=4 + int(len(cluster_df) * 4.0),\n",
    "        linewidth=1 + int(len(cluster_df) * 1.0),\n",
    "    )\n",
    "    plt.show()\n",
    "#     plt.savefig(\"./Dendrograms/Cluster_\" + str(i) + \".png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_3to4,\n",
    "    \"Cluster 3 to 4 Dendrogram\",\n",
    "    figsize=(int((len(cluster_3to4) * 5.0) - 9.0), int((len(cluster_3to4) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_3to4) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_3to4) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_6to8.png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_9to11,\n",
    "    \"Cluster 9 to 11 Dendrogram\",\n",
    "    figsize=(\n",
    "        int((len(cluster_9to11) * 5.0) - 9.0),\n",
    "        int((len(cluster_9to11) * 2.0) + 3),\n",
    "    ),\n",
    "    fontsize=4 + int(len(cluster_9to11) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_9to11) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_9to10.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_2,\n",
    "    \"Cluster 2 Dendrogram\",\n",
    "    figsize=(int((len(cluster_2) * 5.0) - 9.0), int((len(cluster_2) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_2) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_2) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_12,\n",
    "    \"Cluster 12 Dendrogram\",\n",
    "    figsize=(int((len(cluster_12) * 5.0) - 9.0), int((len(cluster_12) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_12) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_12) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_13,\n",
    "    \"Cluster 13 Dendrogram\",\n",
    "    figsize=(int((len(cluster_13) * 5.0) - 9.0), int((len(cluster_13) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_13) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_13) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_14,\n",
    "    \"Cluster 14 Dendrogram\",\n",
    "    figsize=(int((len(cluster_14) * 5.0) - 9.0), int((len(cluster_14) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_14) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_14) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_cluster_df.to_csv(\"2021-07-31_Steady_State_Analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "## Clustering on Mean and Variance Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_params = [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "\n",
    "lowess_and_iqr_params = [\n",
    "    \"LOWESS Trace: \" + param + \": score\" for param in params_to_transform\n",
    "] + [\"Binned IQR Interpolation: \" + param + \": score\" for param in params_to_transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowess_trace_df[\"Feature Vector\"] = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_and_iqr_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df[\"Feature Vector OnlyLOWESS\"] = lowess_trace_df.apply(\n",
    "    lambda x: np.array(x[lowess_params].tolist()), axis=1\n",
    ")\n",
    "lowess_trace_df_nan_filtered = lowess_trace_df[\n",
    "    ~lowess_trace_df[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_effect_threshold = 35\n",
    "\n",
    "zero_vector = np.zeros(\n",
    "    (1, lowess_trace_df[\"Feature Vector OnlyLOWESS\"].iloc[0].shape[0])\n",
    ")\n",
    "feature_arr = np.array(lowess_trace_df[\"Feature Vector OnlyLOWESS\"].tolist())\n",
    "flattened_feature_arr = np.swapaxes(feature_arr, 1, 2).reshape(-1, feature_arr.shape[1])\n",
    "dist_arr = euclidean_distances(flattened_feature_arr, zero_vector).reshape(\n",
    "    feature_arr.shape[0], feature_arr.shape[2]\n",
    ")\n",
    "lowess_trace_df[\"Integrated Euclidean Norm\"] = sp.integrate.simpson(dist_arr)\n",
    "# lowess_trace_df[\"Max Euclidean Norm\"] = np.max(dist_arr,axis=1)\n",
    "\n",
    "sgrnadf_strong_effect = lowess_trace_df[\n",
    "    lowess_trace_df[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = np.min(lowess_trace_df[\"Integrated Euclidean Norm\"]), np.percentile(\n",
    "    lowess_trace_df[\"Integrated Euclidean Norm\"], 99\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Euclidean Norm\")\n",
    "plt.hist(\n",
    "    lowess_trace_df[\n",
    "        lowess_trace_df[\"Integrated Euclidean Norm\"] < strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    lowess_trace_df[\n",
    "        lowess_trace_df[\"Integrated Euclidean Norm\"] >= strong_effect_threshold\n",
    "    ][\"Integrated Euclidean Norm\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"], return_counts=True)\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_rep_example_series = (\n",
    "    sgrnadf_strong_effect.reset_index(drop=False)\n",
    "    .groupby(\"TargetID\")\n",
    "    .apply(lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])])\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(\"sgRNA\", drop=True)\n",
    ")\n",
    "\n",
    "normalized_timeseries = np.swapaxes(\n",
    "    normalize_timeseries(most_rep_example_series[\"Feature Vector\"], lmbda=0.5), 1, 2\n",
    ")\n",
    "most_rep_example_series[\"Normalized Feature Vector\"] = [\n",
    "    normalized_timeseries[i] for i in range(normalized_timeseries.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### Effect Distance Metrics\n",
    "\n",
    "Now, I want to evaluate the performance of different distance metrics on the data wrt seperating it maximally while also preserving similarity within replicates\n",
    "\n",
    "- DTW (can be done with cosine similarity) \n",
    "- cosine similarity (same as pearson for z-scores)\n",
    "- cross correlation\n",
    "\n",
    "Seems like soft-DTW is a pretty good option. Going forward with that for now.\n",
    "\n",
    "<!-- In the end cosine similarity was chosen as it produced superior silhouette scores for sets of targets from genes with different phenotypes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_examples_for_distance_metric = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin([\"ftsN\", \"rplA\", \"mreB\", \"tufB\", \"tff\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import (\n",
    "    cdist_dtw,\n",
    "    cdist_soft_dtw,\n",
    "    cdist_soft_dtw_normalized,\n",
    "    dtw,\n",
    "    dtw_path_from_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_arr = np.swapaxes(\n",
    "    np.array(\n",
    "        sgrnadf_examples_for_distance_metric[\"Normalized Feature Vector\"].tolist()\n",
    "    ),\n",
    "    1,\n",
    "    2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    print(\n",
    "        \"Soft-DTW Gamma=\"\n",
    "        + str(gamma)\n",
    "        + \": \"\n",
    "        + str(\n",
    "            tslearn.clustering.silhouette_score(\n",
    "                timeseries_arr,\n",
    "                sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "                metric=\"softdtw\",\n",
    "                gamma=gamma,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"cosine\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Cosine-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"euclidean\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Euclidean-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_dtw_dist_arr = tslearn.metrics.cdist_soft_dtw(timeseries_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(soft_dtw_dist_arr.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "### Detecting different effects against single genes\n",
    "\n",
    "1) Plot a histogram of minimum soft-DTW similarity within groups of TargetIDs against the same genes (for genes with more than one targetID)\n",
    "2) Use affinity propagation to select the number of phenotype clusters to use per gene (preference still needs to be dialed in, not sure how to optimize on this)\n",
    "3) Among each cluster, represent the final effect as the strongest effect (integrated euc norm) of the members of the cluster\n",
    "\n",
    "~~3) Among each cluster, represent the final effect as the median of the members of the cluster~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_softdtw(feature_vector_series):\n",
    "    dist_mat = cdist_soft_dtw_normalized(\n",
    "        np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    )\n",
    "    timeseries_len = (\n",
    "        feature_vector_series[0].shape[0] * feature_vector_series[0].shape[1]\n",
    "    )\n",
    "    dist_mat = dist_mat / timeseries_len\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def get_upper_right_vals(a):\n",
    "    upper_tri = np.triu(a, k=1)\n",
    "    upper_tri[upper_tri == 0.0] = np.NaN\n",
    "    return upper_tri\n",
    "\n",
    "\n",
    "def get_sgRNA_clusters(df, preference=0.6):\n",
    "    gene_indexed_df = (\n",
    "        df.reset_index(drop=False)\n",
    "        .set_index(\"Gene\")[[\"sgRNA\", \"Normalized Feature Vector\", \"TargetID\"]]\n",
    "        .sort_index()\n",
    "    )\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = pd.Series(\n",
    "        np.zeros(len(gene_indexed_df), dtype=int), dtype=int\n",
    "    )\n",
    "    gene_df_list = []\n",
    "    for gene in gene_indexed_df.index.tolist():\n",
    "        gene_df = gene_indexed_df.loc[[gene]]\n",
    "        if len(gene_df) > 1:\n",
    "            gene_feature_vector = gene_df[\"Normalized Feature Vector\"]\n",
    "            soft_dtw_dist = get_normed_softdtw(gene_feature_vector)\n",
    "            af_labels = (\n",
    "                AffinityPropagation(\n",
    "                    affinity=\"precomputed\", preference=preference, random_state=42\n",
    "                )\n",
    "                .fit_predict(-soft_dtw_dist)\n",
    "                .astype(int)\n",
    "            )\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = af_labels\n",
    "        else:\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = 0\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = gene_indexed_df[\"sgRNA Cluster\"].astype(int)\n",
    "    return gene_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sgrna_replicate_thr = 2\n",
    "pref_factor = 3.0\n",
    "\n",
    "gene_list, counts_list = np.unique(most_rep_example_series[\"Gene\"], return_counts=True)\n",
    "genes_with_many_replicate_sgRNAs = gene_list[counts_list >= n_sgrna_replicate_thr]\n",
    "sgrnadf_many_copies_per_gene = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin(genes_with_many_replicate_sgRNAs)\n",
    "]\n",
    "\n",
    "max_distance_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: np.nanmax(\n",
    "        get_upper_right_vals(get_normed_softdtw(x[\"Normalized Feature Vector\"]))\n",
    "    )\n",
    ")\n",
    "plt.title(\"Maximum soft-DTW Distance per Gene\")\n",
    "plt.hist(max_distance_within_gene, bins=50)\n",
    "plt.show()\n",
    "\n",
    "dist_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: get_upper_right_vals(\n",
    "        get_normed_softdtw(x[\"Normalized Feature Vector\"])\n",
    "    ).flatten()\n",
    ")\n",
    "dist_within_gene = [val for item in dist_within_gene.tolist() for val in item]\n",
    "median_similarity = -np.nanmedian(dist_within_gene)\n",
    "\n",
    "gene_df = get_sgRNA_clusters(\n",
    "    most_rep_example_series, preference=pref_factor * median_similarity\n",
    ")\n",
    "\n",
    "most_rep_example_series[\"sgRNA Cluster\"] = gene_df.set_index(\"sgRNA\")[\"sgRNA Cluster\"]\n",
    "most_rep_example_series[\"sgRNA Cluster Label\"] = most_rep_example_series.apply(\n",
    "    lambda x: str(x[\"Gene\"]) + \"-\" + str(x[\"sgRNA Cluster\"]), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df = most_rep_example_series[\n",
    "    [\n",
    "        \"sgRNA Cluster Label\",\n",
    "        \"Normalized Feature Vector\",\n",
    "        \"Gene\",\n",
    "        \"Integrated Euclidean Norm\",\n",
    "    ]\n",
    "    + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "].reset_index(drop=True)\n",
    "gene_cluster_groupby = gene_cluster_df.groupby(\"sgRNA Cluster Label\")\n",
    "# median_feature_series = gene_cluster_groupby.apply(lambda x: np.median(np.stack(x[\"Feature Vector\"]).astype(float), axis=0)).to_frame().rename(columns={0:\"Feature Vector\"})\n",
    "feature_series = (\n",
    "    gene_cluster_groupby.apply(\n",
    "        lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])][\n",
    "            \"Normalized Feature Vector\"\n",
    "        ]\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Normalized Feature Vector\"})\n",
    ")\n",
    "\n",
    "gene_cluster_df = gene_cluster_groupby.apply(\n",
    "    lambda x: x.iloc[0][\n",
    "        [\"Gene\"]\n",
    "        + [\"LOWESS Trace: \" + param + \": score\" for param in params_to_transform]\n",
    "    ]\n",
    ")\n",
    "gene_cluster_df = gene_cluster_df.join(feature_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "### Clustering: TSNE and Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = get_normed_softdtw(gene_cluster_df[\"Normalized Feature Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(\n",
    "    n_components=2, perplexity=5.0, early_exaggeration=50.0, metric=\"precomputed\"\n",
    ").fit_transform(X_dist - np.min(X_dist))\n",
    "gene_cluster_df[\"TSNE Coords\"] = [X_embedded[i] for i in range(X_embedded.shape[0])]\n",
    "\n",
    "af_labels = (\n",
    "    AffinityPropagation(affinity=\"precomputed\", preference=-0.5)\n",
    "    .fit_predict(-X_dist)\n",
    "    .astype(int)\n",
    ")\n",
    "gene_cluster_df[\"Affinity Clusts\"] = af_labels\n",
    "\n",
    "plt.scatter(\n",
    "    X_embedded[:, 0],\n",
    "    X_embedded[:, 1],\n",
    "    s=3,\n",
    "    alpha=1,\n",
    "    c=gene_cluster_df[\"Affinity Clusts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_labels = [\n",
    "    \"Birth Length\",\n",
    "    \"Division Length\",\n",
    "    \"Area Growth Rate\",\n",
    "    \"Length Growth Rate\",\n",
    "    \"Average Width\",\n",
    "    \"mCherry Intensity\",\n",
    "    \"Cell Cycle Duration\",\n",
    "]\n",
    "\n",
    "feature_labels = feature_labels + [label + \": IQR\" for label in feature_labels]\n",
    "\n",
    "hierarchical_labels = gene_cluster_df.index.tolist()\n",
    "\n",
    "\n",
    "def get_leaf_children(tree, leaf_id):\n",
    "    cluster_node = tree[leaf_id]\n",
    "    leaf_children = cluster_node.pre_order(lambda x: x.id)\n",
    "    return leaf_children\n",
    "\n",
    "\n",
    "def assign_dendro_clusts(df, children_labels):\n",
    "    df_out = copy.deepcopy(df)\n",
    "    df_out[\"Dendrogram Clusters\"] = pd.Series(len(df), dtype=int)\n",
    "    for clust_i, indices in enumerate(children_labels):\n",
    "        df_out[\"Dendrogram Clusters\"].iloc[indices] = clust_i\n",
    "    df_out[\"Dendrogram Clusters\"] = df_out[\"Dendrogram Clusters\"].astype(int)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "suppress_thr = 15\n",
    "min_zscore = -2\n",
    "max_zscore = 2\n",
    "\n",
    "\n",
    "def compute_and_plot_dendrogram(\n",
    "    df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "):\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    hierarchical_labels = df.index.tolist()\n",
    "    X = np.array(df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(2, suppress_thr)\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    #     Y = sch.linkage(X, method='weighted', metric='cosine',optimal_ordering=True)\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        truncate_mode=\"lastp\",\n",
    "        show_contracted=True,\n",
    "        p=suppress_thr,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "    children_labels = [get_leaf_children(cluster_tree, leaf) for leaf in Z[\"leaves\"]]\n",
    "\n",
    "    #     fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\\\n",
    "    #                  ax=dendro_ax, orientation='vertical', label='Z-score',\\\n",
    "    #                 use_gridspec=True, location='left', pad=-0.05,aspect=10)\n",
    "\n",
    "    for i, children in enumerate(children_labels):\n",
    "        children_arr = np.array(\n",
    "            df.iloc[children][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "        mean_vector = np.mean(children_arr, axis=0)  # feature,timepoint\n",
    "        # fig = plt.figure(constrained_layout=True, figsize=(20,10))\n",
    "        # gs = fig.add_gridspec(2, 10)\n",
    "        # for v in range(10):\n",
    "        #     inner_gs = gs[0,v].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0, )\n",
    "        #     inner_grid_sub = inner_gs.subplots()\n",
    "        #     for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "        #         ax.plot(mean_vector[c])\n",
    "        #         ax.set(xticks=[], yticks=[])\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                mean_vector.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 10])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=18,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(mean_vector.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(mean_vector[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(str(i), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return children_labels\n",
    "\n",
    "\n",
    "def plot_subset(\n",
    "    df_subset,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    feature_labels=feature_labels,\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "):\n",
    "    df_clusts = (\n",
    "        df_subset.sort_index()\n",
    "        .reset_index(drop=False)\n",
    "        .set_index(\"Dendrogram Clusters\")[\n",
    "            [\"sgRNA Cluster Label\", \"Normalized Feature Vector\"]\n",
    "        ]\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(df_clusts), wspace=wspace)\n",
    "\n",
    "    for i in range(len(df_clusts)):\n",
    "        clust_arr = np.array(\n",
    "            df_clusts[\"Normalized Feature Vector\"].iloc[i].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 10.0])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=18,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "        #             imshow_first_ax = fig.add_subplot(gs[0, i])\n",
    "        #             imshow_first_ax.imshow(df_clusts[\"Feature Vector\"].iloc[i].astype(float).reshape(-1,1),cmap=cmap,norm=norm)\n",
    "\n",
    "        #             imshow_first_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "        #             imshow_first_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "        #             imshow_first_ax.set_xlabel(df_clusts[\"sgRNA Cluster Label\"].iloc[i] + \"\\n Cluster \" + str(df_clusts.index[i]), fontsize=14)\n",
    "\n",
    "        #             imshow_first_ax.set_yticks(range(len(feature_labels)))\n",
    "        #             imshow_first_ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(clust_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(clust_arr[c])\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(\n",
    "                df_clusts[\"sgRNA Cluster Label\"].iloc[i]\n",
    "                + \"\\n Cluster \"\n",
    "                + str(df_clusts.index[i]),\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def make_subset_dendrogram(\n",
    "    sub_df,\n",
    "    title,\n",
    "    feature_labels=feature_labels,\n",
    "    min_zscore=min_zscore,\n",
    "    max_zscore=max_zscore,\n",
    "    figsize=(10, 10),\n",
    "    fontsize=18,\n",
    "    linewidth=5,\n",
    "):\n",
    "    X_dist = get_normed_softdtw(sub_df[\"Normalized Feature Vector\"])\n",
    "    X = np.array(sub_df[\"Normalized Feature Vector\"].tolist())\n",
    "\n",
    "    # Compute and plot dendrogram.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(2, len(sub_df))\n",
    "    dendro_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    Y = sch.linkage(\n",
    "        sp.spatial.distance.squareform(X_dist), method=\"weighted\", optimal_ordering=True\n",
    "    )\n",
    "    cluster_tree = sch.to_tree(Y, rd=True)[1]\n",
    "\n",
    "    Z = sch.dendrogram(\n",
    "        Y,\n",
    "        orientation=\"top\",\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.0,\n",
    "        leaf_font_size=12.0,\n",
    "        show_contracted=True,\n",
    "        ax=dendro_ax,\n",
    "        no_labels=True,\n",
    "    )\n",
    "\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    norm = mpl.colors.Normalize(vmin=min_zscore, vmax=max_zscore)\n",
    "\n",
    "    fig.suptitle(title, fontsize=fontsize)\n",
    "\n",
    "    for i, leaf in enumerate(Z[\"leaves\"]):\n",
    "        leaf_arr = np.array(\n",
    "            sub_df.iloc[leaf][\"Normalized Feature Vector\"].tolist(), dtype=float\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[1, i].subgridspec(\n",
    "                leaf_arr.shape[0],\n",
    "                1,\n",
    "                wspace=0,\n",
    "                hspace=0,\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[-4, 0.0, 8])\n",
    "                ax.set_ylabel(\n",
    "                    feature_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            #             imshow_first_ax = fig.add_subplot(gs[1, i])\n",
    "            #             imshow_first_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "\n",
    "            #             ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "        #             ax.set_yticks(range(len(feature_labels)))\n",
    "        #             ax.set_yticklabels(feature_labels, fontsize=18, )\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[1, i].subgridspec(leaf_arr.shape[0], 1, wspace=0, hspace=0)\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                ax.plot(leaf_arr[c], linewidth=linewidth)\n",
    "                ax.set_ylim(-6, 12)\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            #             imshow_ax = fig.add_subplot(gs[1, i], sharey=imshow_first_ax)\n",
    "            #             imshow_ax.imshow(mean_vector,cmap=cmap,norm=norm)\n",
    "            #             plt.setp(imshow_ax.get_yticklabels(), visible=False)\n",
    "\n",
    "            #             imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "            #             imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "            ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "#         imshow_ax.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n",
    "#         imshow_ax.tick_params(axis='y',which='both',left=False,right=False,labelbottom=False)\n",
    "\n",
    "#         imshow_ax.set_xlabel(sub_df.index[leaf], fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_and_plot_dendrogram(df,X_dist,feature_labels,suppress_thr,min_zscore,max_zscore,cmap=mpl.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "children_labels = compute_and_plot_dendrogram(\n",
    "    gene_cluster_df,\n",
    "    X_dist,\n",
    "    feature_labels,\n",
    "    suppress_thr,\n",
    "    min_zscore,\n",
    "    max_zscore,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    ")\n",
    "# plt.savefig(\"./Dendrograms/Global_Dendrogram.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df = assign_dendro_clusts(gene_cluster_df, children_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "#### Major System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fts\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fts_subset, figsize=(20, 8))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/fts.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpl_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpl\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpl_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpl.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpm\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpm_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpm.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rps\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rps_subset, figsize=(30, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rps.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_subset = gene_cluster_df[gene_cluster_df.apply(lambda x: \"rr\" in x[\"Gene\"], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rr_subset, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "tff_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"tff\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(tff_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/tff.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpo_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rpo\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rpo_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rpo.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"min\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(min_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/min.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"dna\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(dna_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/dna.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"fol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(fol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/fol.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "muk_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"muk\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(muk_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/muk.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mre\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mre_subset, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/mre.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "mur_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"mur\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(mur_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/mur.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "nus_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"nus\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(nus_subset, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/nus.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"sec\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(sec_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/sec.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"bam\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(bam_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/bam.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "hol_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hol\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hol_subset, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/hol.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "hda_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"hda\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(hda_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/hda.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "rodZ_subset = gene_cluster_df[\n",
    "    gene_cluster_df.apply(lambda x: \"rodZ\" in x[\"Gene\"], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(rodZ_subset, figsize=(6, 10))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Gene_Groups/rodz.png\",dpi=200,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178",
   "metadata": {},
   "source": [
    "#### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_7 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   
"id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_7,\n",
    "    \"Cluster 7 Dendrogram\",\n",
    "    figsize=(int((len(cluster_7) * 5.0) - 9.0), int((len(cluster_7) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_7) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_7) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_counts = np.unique(\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"], return_counts=True\n",
    ")\n",
    "singleton_clusters = clusters[cluster_counts < 3]\n",
    "small_clusters = clusters[(cluster_counts <= 40) & (cluster_counts >= 3)]\n",
    "big_clusters = clusters[cluster_counts > 40]\n",
    "print(singleton_clusters)\n",
    "print(small_clusters)\n",
    "print(big_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3to4 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([3, 4])]\n",
    "cluster_9to11 = gene_cluster_df[\n",
    "    gene_cluster_df[\"Dendrogram Clusters\"].isin([9, 10, 11])\n",
    "]\n",
    "cluster_2 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([2])]\n",
    "cluster_12 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([12])]\n",
    "cluster_13 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([13])]\n",
    "cluster_14 = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters = list(set(small_clusters) - set([3, 4, 9, 10, 11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_small_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in remaining_small_clusters:\n",
    "    cluster_df = gene_cluster_df[gene_cluster_df[\"Dendrogram Clusters\"].isin([i])]\n",
    "    make_subset_dendrogram(\n",
    "        cluster_df,\n",
    "        \"Cluster \" + str(i) + \" Dendrogram\",\n",
    "        figsize=(int((len(cluster_df) * 5.0) - 9.0), int((len(cluster_df) * 2.0) + 3)),\n",
    "        fontsize=4 + int(len(cluster_df) * 4.0),\n",
    "        linewidth=1 + int(len(cluster_df) * 1.0),\n",
    "    )\n",
    "    plt.show()\n",
    "#     plt.savefig(\"./Dendrograms/Cluster_\" + str(i) + \".png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_3to4,\n",
    "    \"Cluster 3 to 4 Dendrogram\",\n",
    "    figsize=(int((len(cluster_3to4) * 5.0) - 9.0), int((len(cluster_3to4) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_3to4) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_3to4) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_6to8.png\",dpi=200)\n",
    "\n",
    "make_subset_dendrogram(\n",
    "    cluster_9to11,\n",
    "    \"Cluster 9 to 11 Dendrogram\",\n",
    "    figsize=(\n",
    "        int((len(cluster_9to11) * 5.0) - 9.0),\n",
    "        int((len(cluster_9to11) * 2.0) + 3),\n",
    "    ),\n",
    "    fontsize=4 + int(len(cluster_9to11) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_9to11) * 1.0),\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig(\"./Dendrograms/Cluster_9to10.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_2,\n",
    "    \"Cluster 2 Dendrogram\",\n",
    "    figsize=(int((len(cluster_2) * 5.0) - 9.0), int((len(cluster_2) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_2) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_2) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_12,\n",
    "    \"Cluster 12 Dendrogram\",\n",
    "    figsize=(int((len(cluster_12) * 5.0) - 9.0), int((len(cluster_12) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_12) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_12) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_13,\n",
    "    \"Cluster 13 Dendrogram\",\n",
    "    figsize=(int((len(cluster_13) * 5.0) - 9.0), int((len(cluster_13) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_13) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_13) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_subset_dendrogram(\n",
    "    cluster_14,\n",
    "    \"Cluster 14 Dendrogram\",\n",
    "    figsize=(int((len(cluster_14) * 5.0) - 9.0), int((len(cluster_14) * 2.0) + 3)),\n",
    "    fontsize=4 + int(len(cluster_14) * 4.0),\n",
    "    linewidth=1 + int(len(cluster_14) * 1.0),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_cluster_df.to_csv(\"2021-07-31_Steady_State_Analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191",
   "metadata": {},
   "source": [
    "### Gene Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_output_df_pd.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "df[\"phenotype trenchids\"] = final_output_df_pd.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    df, trenchids_as_list=True, trenchid_column=\"phenotype trenchids\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    df,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button  ## NEED OPTION WHETHER OR NOT TO NORM SIGNAL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
