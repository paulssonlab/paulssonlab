{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Mapping Barcodes and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"04:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=100,\n",
    "#     death_timeout=5.,\n",
    "#     memory=\"16GB\",\n",
    "#     working_directory=\"/home/de64/scratch/de64/temp/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    death_timeout=5.0,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/temp/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/metadata.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttl_called)\n",
    "print(ttl_trenches)\n",
    "print(ttl_trenches_w_cells)\n",
    "print(percent_called)\n",
    "print(percent_called_w_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Import Lineage Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_df(df, query_list, client=False, repartition=False):\n",
    "    # filter_list must be in df.query format (see pandas docs)\n",
    "\n",
    "    # returns persisted dataframe either in cluster or local\n",
    "\n",
    "    compiled_query = \" and \".join(query_list)\n",
    "    out_df = df.query(compiled_query)\n",
    "    if client:\n",
    "        out_df = client.daskclient.persist(out_df)\n",
    "    else:\n",
    "        out_df = out_df.persist()\n",
    "\n",
    "    if repartition:\n",
    "        init_size = len(df)\n",
    "        final_size = len(out_df)\n",
    "        ratio = init_size // final_size\n",
    "        out_df = out_df.repartition(npartitions=(df.npartitions // ratio) + 1)\n",
    "\n",
    "        if client:\n",
    "            out_df = client.daskclient.persist(out_df)\n",
    "        else:\n",
    "            out_df = out_df.persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_first_cell_timepoint(df):\n",
    "    min_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmin().tolist()\n",
    "    init_cells = df.loc[min_tpts]\n",
    "    return init_cells\n",
    "\n",
    "\n",
    "def get_last_cell_timepoint(df):\n",
    "    max_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmax().tolist()\n",
    "    fin_cells = df.loc[max_tpts]\n",
    "    return fin_cells\n",
    "\n",
    "\n",
    "def get_first_last_cell_dfs(df, persist=False):\n",
    "    ### NOTE: this functions requires that the input df has partitions aligned with trenchids, so\n",
    "    ### that mother and siblings are in the same partition.\n",
    "\n",
    "    init_cells = df.map_partitions(get_first_cell_timepoint)\n",
    "    fin_cells = df.map_partitions(get_last_cell_timepoint)\n",
    "    if persist:\n",
    "        init_cells = init_cells.persist()\n",
    "        fin_cells = fin_cells.persist()\n",
    "    return init_cells, fin_cells\n",
    "\n",
    "\n",
    "def get_df_from_series_index(df, delayed_series, partition_info=None):\n",
    "    # Hack to avoid automatic partition alignment in map_partitions\n",
    "    # Allows for mismatched index lookup\n",
    "\n",
    "    n = partition_info[\"number\"]\n",
    "    list_of_indices = delayed_series[n].tolist()\n",
    "    df_out = df.loc[list_of_indices]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_relative_dfs(query_df, reference_df, persist_relatives=False):\n",
    "    init_cells, fin_cells = get_first_last_cell_dfs(query_df, persist=False)\n",
    "    cell_min_tpt_df, cell_max_tpt_df = get_first_last_cell_dfs(\n",
    "        reference_df, persist=False\n",
    "    )\n",
    "\n",
    "    init_cells = (\n",
    "        init_cells.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    fin_cells = (\n",
    "        fin_cells.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    cell_min_tpt_df = (\n",
    "        cell_min_tpt_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    cell_max_tpt_df = (\n",
    "        cell_max_tpt_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "    mother_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_max_tpt_df,\n",
    "        init_cells[\"Mother CellID\"].to_delayed(),\n",
    "        meta=cell_max_tpt_df.head()[:0],\n",
    "    )\n",
    "    sister_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_min_tpt_df,\n",
    "        init_cells[\"Sister CellID\"].to_delayed(),\n",
    "        meta=cell_min_tpt_df.head()[:0],\n",
    "    )\n",
    "    daughter_1_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_min_tpt_df,\n",
    "        fin_cells[\"Daughter CellID 1\"].to_delayed(),\n",
    "        meta=cell_min_tpt_df.head()[:0],\n",
    "    )\n",
    "    daughter_2_df = dd.map_partitions(\n",
    "        get_df_from_series_index,\n",
    "        cell_min_tpt_df,\n",
    "        fin_cells[\"Daughter CellID 2\"].to_delayed(),\n",
    "        meta=cell_min_tpt_df.head()[:0],\n",
    "    )\n",
    "\n",
    "    mother_df = mother_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "    sister_df = sister_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "    daughter_1_df = daughter_1_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "    daughter_2_df = daughter_2_df.reset_index(\n",
    "        drop=False\n",
    "    )  # .set_index(\"init_cells Index\",sorted=True)\n",
    "\n",
    "    if persist_relatives:\n",
    "        mother_df = mother_df.persist()\n",
    "        sister_df = sister_df.persist()\n",
    "        daughter_1_df = daughter_1_df.persist()\n",
    "        daughter_2_df = daughter_2_df.persist()\n",
    "\n",
    "    return init_cells, fin_cells, mother_df, sister_df, daughter_1_df, daughter_2_df\n",
    "\n",
    "\n",
    "def get_init_and_final_size(\n",
    "    query_df,\n",
    "    reference_df,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "    ##query contains cells of interest\n",
    "    ##reference contains all cells that may be retrieved (mothers,sisters,daughters)\n",
    "\n",
    "    (\n",
    "        init_cells,\n",
    "        fin_cells,\n",
    "        mother_df,\n",
    "        sister_df,\n",
    "        daughter_1_df,\n",
    "        daughter_2_df,\n",
    "    ) = get_relative_dfs(query_df, reference_df)\n",
    "\n",
    "    init_cells_noidx, fin_cells_noidx = (\n",
    "        init_cells.reset_index(drop=False).persist(),\n",
    "        fin_cells.reset_index(drop=False).persist(),\n",
    "    )\n",
    "\n",
    "    adjusted_init_size = {}\n",
    "    adjusted_final_size = {}\n",
    "    adjusted_del_size = {}\n",
    "\n",
    "    ### Ineffecient, but not sure how to avoid\n",
    "    for metric in size_metrics:\n",
    "        if metric == \"minor_axis_length\":\n",
    "            adjusted_init_size[metric] = init_cells_noidx[metric]\n",
    "\n",
    "            adjusted_final_size[metric] = fin_cells_noidx[metric]\n",
    "\n",
    "            adjusted_del_size[metric] = (\n",
    "                adjusted_final_size[metric] - adjusted_init_size[metric]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            interp_mother_final_size = (\n",
    "                (init_cells_noidx[metric] + sister_df[metric]) * mother_df[metric]\n",
    "            ) ** (1 / 2)\n",
    "            sister_frac = init_cells_noidx[metric] / (\n",
    "                sister_df[metric] + init_cells_noidx[metric]\n",
    "            )\n",
    "            adjusted_init_size[metric] = sister_frac * interp_mother_final_size\n",
    "\n",
    "            adjusted_final_size[metric] = (\n",
    "                (daughter_1_df[metric] + daughter_2_df[metric])\n",
    "                * fin_cells_noidx[metric]\n",
    "            ) ** (1 / 2)\n",
    "\n",
    "            adjusted_del_size[metric] = (\n",
    "                adjusted_final_size[metric] - adjusted_init_size[metric]\n",
    "            )\n",
    "\n",
    "    return (\n",
    "        init_cells_noidx,\n",
    "        fin_cells_noidx,\n",
    "        adjusted_init_size,\n",
    "        adjusted_final_size,\n",
    "        adjusted_del_size,\n",
    "    )\n",
    "\n",
    "\n",
    "# def get_promoter_synthesis_rate(cellid_groupby, intensity_label, size_metric_label):\n",
    "#     del_intensity_series = cellid_groupby[intensity_label].apply(lambda x: x.values[1:]-x.values[:-1]).to_frame(name=\"del intensity\")\n",
    "#     mean_intensity_series = cellid_groupby[intensity_label].apply(lambda x: (x.values[1:]+x.values[:-1])/2).to_frame(name=\"mean intensity\")\n",
    "#     del_size_series = cellid_groupby[size_metric_label].apply(lambda x: x.values[1:]-x.values[:-1]).to_frame(name=\"del size\")\n",
    "#     mean_size_series = cellid_groupby[size_metric_label].apply(lambda x: (x.values[1:]+x.values[:-1])/2).to_frame(name=\"mean size\")\n",
    "#     pro_syn_df = dd.concat([del_intensity_series,mean_intensity_series,del_size_series,mean_size_series],axis=1)\n",
    "#     promoter_activity_series = pro_syn_df.apply(lambda x: np.nanmedian(x[\"del intensity\"] + (x[\"mean intensity\"]*(x[\"del size\"]/x[\"mean size\"]))), axis=1, meta=float)\n",
    "#     return promoter_activity_series\n",
    "\n",
    "\n",
    "def get_growth_and_division_stats(\n",
    "    query_df,\n",
    "    reference_df,\n",
    "    delta_t_min=4,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "    (\n",
    "        init_cells_noidx,\n",
    "        fin_cells_noidx,\n",
    "        adjusted_init_size,\n",
    "        adjusted_final_size,\n",
    "        adjusted_del_size,\n",
    "    ) = get_init_and_final_size(query_df, reference_df, size_metrics=size_metrics)\n",
    "\n",
    "    for size_metric in size_metrics:\n",
    "        init_cells_noidx[\"Delta: \" + size_metric] = adjusted_del_size[\n",
    "            size_metric\n",
    "        ].persist()\n",
    "        init_cells_noidx[\"Birth: \" + size_metric] = adjusted_init_size[\n",
    "            size_metric\n",
    "        ].persist()\n",
    "        init_cells_noidx[\"Division: \" + size_metric] = adjusted_final_size[\n",
    "            size_metric\n",
    "        ].persist()\n",
    "\n",
    "    init_cells_noidx[\"final timepoints\"] = fin_cells_noidx[\"timepoints\"]\n",
    "    del_t = init_cells_noidx[\"final timepoints\"] - init_cells_noidx[\"timepoints\"]\n",
    "    init_cells_noidx[\"Delta t\"] = del_t\n",
    "\n",
    "    init_cells = init_cells_noidx.set_index(\"Global CellID\", sorted=True)\n",
    "\n",
    "    query_df_cellid_sorted = (\n",
    "        query_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID\", sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "    query_df[\"Global CellID-timepoints Index\"] = query_df.apply(\n",
    "        lambda x: int(f'{int(x[\"Global CellID\"]):04}{int(x[\"timepoints\"]):04}'), axis=1\n",
    "    )\n",
    "    query_df_cellid_sorted = (\n",
    "        query_df.reset_index(drop=False)\n",
    "        .set_index(\"Global CellID-timepoints Index\", sorted=False)\n",
    "        .set_index(\"Global CellID\", sorted=True)\n",
    "        .persist()\n",
    "    )\n",
    "    del query_df\n",
    "\n",
    "    for size_metric in size_metrics:  # Havn't decided between mean and median\n",
    "        mean_cell_size_metric_linear_gr = query_df_cellid_sorted.groupby(\n",
    "            \"Global CellID\"\n",
    "        )[size_metric].apply(lambda x: np.nanmean(x[1:].values - x[:-1].values))\n",
    "        mean_cell_size_metric_linear_gr = (\n",
    "            mean_cell_size_metric_linear_gr / delta_t_min\n",
    "        ) * 60  # size unit per hr\n",
    "        mean_cell_size_metric_exp_gr = query_df_cellid_sorted.groupby(\"Global CellID\")[\n",
    "            size_metric\n",
    "        ].apply(\n",
    "            lambda x: np.nanmean(\n",
    "                (2 * (x[1:].values - x[:-1].values)) / (x[1:].values + x[:-1].values)\n",
    "            )\n",
    "        )\n",
    "        mean_cell_size_metric_exp_gr = (\n",
    "            mean_cell_size_metric_exp_gr / delta_t_min\n",
    "        ) * 60  # exponential size unit per hr\n",
    "        mean_cell_size_metric = query_df_cellid_sorted.groupby(\"Global CellID\")[\n",
    "            size_metric\n",
    "        ].apply(lambda x: np.nanmean(x.values))\n",
    "\n",
    "        init_cells[\"Mean: \" + size_metric] = mean_cell_size_metric.persist()\n",
    "        init_cells[\n",
    "            \"Mean Linear Growth Rate: \" + size_metric\n",
    "        ] = mean_cell_size_metric_linear_gr.persist()\n",
    "        init_cells[\n",
    "            \"Mean Exponential Growth Rate: \" + size_metric\n",
    "        ] = mean_cell_size_metric_exp_gr.persist()\n",
    "\n",
    "    median_mchy_intensity = query_df_cellid_sorted.groupby(\"Global CellID\")[\n",
    "        \"mCherry mean_intensity\"\n",
    "    ].apply(lambda x: np.nanmean(x.values))\n",
    "    init_cells[\"Mean: mCherry Intensity\"] = median_mchy_intensity.persist()\n",
    "\n",
    "    #     volume_normed_mchy_intensity = get_promoter_synthesis_rate(query_df_cellid_sorted.groupby('Global CellID'),\"mCherry mean_intensity\",\"Volume\")\n",
    "    #     init_cells[\"Median: mCherry Promoter Activity (Volume normed)\"] = volume_normed_mchy_intensity.persist()\n",
    "\n",
    "    #     init_cells_trenchid_idx = init_cells.set_index(\"trenchid\",sorted=False).persist()\n",
    "    #     init_cells_trenchid_groupby = init_cells_trenchid_idx.groupby(\"trenchid\")\n",
    "\n",
    "    ## Filtering by cell cycle length to eliminate artifact\n",
    "    init_cells = init_cells[init_cells[\"Delta t\"] >= delta_t_min]\n",
    "    init_cells = init_cells.rename(columns={\"timepoints\": \"initial timepoints\"})\n",
    "    init_cells = init_cells.drop([\"time (s)\", \"Trenchid Timepoint Index\"], axis=1)\n",
    "\n",
    "    #     trenchid_df = query_df_cellid_sorted.reset_index(drop=False).set_index('trenchid',sorted=True).groupby('trenchid').apply(lambda x: x.iloc[0]).persist()\n",
    "\n",
    "    ## HERE\n",
    "    #     init_cells_trenchid_idx = init_cells.reset_index(drop=False).set_index(\"trenchid\",sorted=True).persist()\n",
    "\n",
    "    #     init_cells_trenchid_groupby = init_cells_trenchid_idx.groupby(\"trenchid\",sort=False)\n",
    "\n",
    "    #     for size_metric in size_metrics:\n",
    "    #         trenchid_df[\"Delta: \" + size_metric + \" list\"] = init_cells_trenchid_groupby[\"Delta: \" + size_metric].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #         trenchid_df[\"Birth: \" + size_metric + \" list\"] = init_cells_trenchid_groupby[\"Birth: \" + size_metric].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #         trenchid_df[\"Division: \" + size_metric + \" list\"] = init_cells_trenchid_groupby[\"Division: \" + size_metric].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #         trenchid_df[\"Median: \" + size_metric + \" list\"] = init_cells_trenchid_groupby[\"Median: \" + size_metric].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #         trenchid_df[\"Median Linear Growth Rate: \" + size_metric + \" list\"] = init_cells_trenchid_groupby[\"Median Linear Growth Rate: \" + size_metric].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #         trenchid_df[\"Median Exponential Growth Rate: \" + size_metric + \" list\"] = init_cells_trenchid_groupby[\"Median Exponential Growth Rate: \" + size_metric].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "\n",
    "    #     trenchid_df[\"Median: mCherry Intensity list\"] = init_cells_trenchid_groupby[\"Median: mCherry Intensity\"].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #     trenchid_df[\"Delta t list\"] = init_cells_trenchid_groupby[\"Delta t\"].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #     trenchid_df[\"Median: mCherry Promoter Activity (Volume normed) list\"] = init_cells_trenchid_groupby[\"Median: mCherry Promoter Activity (Volume normed)\"].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #     trenchid_df[\"cell timepoints list\"] = init_cells_trenchid_groupby[\"timepoints\"].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "    #     trenchid_df[\"final cell timepoints list\"] = init_cells_trenchid_groupby[\"final timepoints\"].apply(lambda x: x.tolist(), meta=list).persist()\n",
    "\n",
    "    return init_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Import Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/lineage\"\n",
    ")\n",
    "\n",
    "##temp fix\n",
    "lineage_df[\"CellID\"] = lineage_df[\"CellID\"].astype(int)\n",
    "lineage_df[\"Global CellID\"] = lineage_df[\"Global CellID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrm_find_mode(series, max_iter=1000, min_binsize=50):\n",
    "    working_series = series\n",
    "    for i in range(max_iter):\n",
    "        range_max, range_min = np.max(working_series), np.min(working_series)\n",
    "        midpoint = (range_max + range_min) / 2\n",
    "        above_middle = working_series[working_series > midpoint]\n",
    "        below_middle = working_series[working_series <= midpoint]\n",
    "\n",
    "        count_above = len(above_middle)\n",
    "        count_below = len(below_middle)\n",
    "\n",
    "        if count_above > count_below:\n",
    "            working_series = above_middle\n",
    "        else:\n",
    "            working_series = below_middle\n",
    "\n",
    "        if i > 0:\n",
    "            if (len(working_series) < min_binsize) or (last_midpoint == midpoint):\n",
    "                return np.mean(working_series)\n",
    "\n",
    "        last_midpoint = midpoint\n",
    "\n",
    "\n",
    "def bootstrap_hrm(series, n_bootstraps=100, n_per_bootstrap=100):\n",
    "    modes = []\n",
    "    for n in range(n_bootstraps):\n",
    "        modes.append(hrm_find_mode(series.sample(n=n_per_bootstrap)))\n",
    "    return np.mean(modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Variables over FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_rescale = [\n",
    "    \"mCherry mean_intensity\",\n",
    "    \"area\",\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df_subsample = (\n",
    "    lineage_df[lineage_df[\"timepoints\"] < 20].sample(frac=0.01).compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "for i, label in enumerate(values_to_rescale):\n",
    "    fov_series_groupby = lineage_df_subsample.groupby(\"fov\")[label]\n",
    "    fov_median_series = fov_series_groupby.apply(lambda x: np.median(x)).sort_index()\n",
    "    fov_correction_series = fov_median_series / np.max(fov_median_series)\n",
    "    fov_correction_dict = fov_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(fov_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"FOV #\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    label_scaling = lineage_df[\"fov\"].apply(lambda x: fov_correction_dict[x]).persist()\n",
    "    lineage_df[label + \": FOV Corrected\"] = (\n",
    "        lineage_df[label] / label_scaling\n",
    "    ).persist()\n",
    "plt.savefig(\"FOV_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Variables over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# values_to_rescale = ['mCherry mean_intensity: FOV Corrected','area: FOV Corrected', 'major_axis_length: FOV Corrected', 'minor_axis_length: FOV Corrected']\n",
    "values_to_rescale_step_2 = [value + \": FOV Corrected\" for value in values_to_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df_subsample = lineage_df.sample(frac=0.01).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "for i, label in enumerate(values_to_rescale_step_2):\n",
    "    time_series_groupby = lineage_df_subsample.groupby(\"timepoints\")[label]\n",
    "    time_mode_series = time_series_groupby.apply(\n",
    "        lambda x: bootstrap_hrm(x)\n",
    "    ).sort_index()\n",
    "    time_correction_series = time_mode_series / np.max(time_mode_series)\n",
    "    time_correction_dict = time_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(time_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Timepoint (3 min steps)\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    label_scaling = lineage_df[\"timepoints\"].apply(lambda x: time_correction_dict[x])\n",
    "    lineage_df[label + \": Time Corrected\"] = (\n",
    "        lineage_df[label] / label_scaling\n",
    "    ).persist()\n",
    "plt.savefig(\"Time_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The HSM method [2] iteratively divides the data set into samples of half the size as the original set and uses the half-sample with the minimum range, where range is defined as the difference between the maximum and the minimum value of the sample. This method terminates when the half-sample is less than three data points. An average of these three or fewer values is the mode. The HRM method [2] is similar but uses the sub-sample with the densest half-range, where range is defined as the absolute difference between the maximum and the minimum values in a sample. Of these two methods, only the HRM was used in this study because HRM has been shown to have lower bias with increasing contamination and asymmetry [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Overwrite Variables with Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in values_to_rescale:\n",
    "    lineage_df[label] = lineage_df[label + \": FOV Corrected: Time Corrected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_df = lineage_df[\n",
    "    [\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"timepoints\",\n",
    "        \"time (s)\",\n",
    "        \"lane orientation\",\n",
    "        \"y (local)\",\n",
    "        \"x (local)\",\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"trenchid\",\n",
    "        \"Trenchid Timepoint Index\",\n",
    "        \"CellID\",\n",
    "        \"Global CellID\",\n",
    "        \"Trench Score\",\n",
    "        \"Mother CellID\",\n",
    "        \"Daughter CellID 1\",\n",
    "        \"Daughter CellID 2\",\n",
    "        \"Sister CellID\",\n",
    "        \"Centroid X\",\n",
    "        \"Centroid Y\",\n",
    "        \"FOV Parquet Index\",\n",
    "    ]\n",
    "    + values_to_rescale\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Growth/Div Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_df = filter_df(\n",
    "    lineage_df, [\"`Trench Score` < -75\"], client=dask_controller, repartition=False\n",
    ").persist()\n",
    "query_df = filter_df(\n",
    "    lineage_df,\n",
    "    [\n",
    "        \"`Mother CellID` != -1\",\n",
    "        \"`Daughter CellID 1` != -1\",\n",
    "        \"`Daughter CellID 2` != -1\",\n",
    "        \"`Sister CellID` != -1\",\n",
    "        \"`Trench Score` < -75\",\n",
    "    ],\n",
    "    client=dask_controller,\n",
    "    repartition=False,\n",
    ").persist()\n",
    "init_cells = get_growth_and_division_stats(query_df, reference_df)\n",
    "\n",
    "del reference_df\n",
    "del query_df\n",
    "del lineage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_df_idx = init_cells[\"trenchid\"].unique().compute().tolist()\n",
    "valid_barcode_df = barcode_df[\n",
    "    barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "].compute()\n",
    "barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "    lambda x: trenchid_map[x]\n",
    ")\n",
    "valid_init_df_indices = barcode_df_mapped_trenchids.isin(init_df_idx)\n",
    "barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "called_df = (\n",
    "    called_df.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", drop=True, sorted=False)\n",
    "    .compute()\n",
    ")\n",
    "# called_df = called_df.repartition(npartitions=1).persist()\n",
    "init_cells = init_cells.rename(columns={\"trenchid\": \"phenotype trenchid\"})\n",
    "init_cells = (\n",
    "    init_cells.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", drop=True, sorted=False)\n",
    "    .compute()\n",
    ")\n",
    "init_cells = init_cells.merge(called_df, how=\"inner\", left_index=True, right_index=True)\n",
    "init_cells = init_cells.drop([\"Barcode Signal\"], axis=1)\n",
    "init_cells = init_cells.reset_index().set_index(\"Global CellID\")\n",
    "init_cells = init_cells.sort_index()\n",
    "final_output_df = dd.from_pandas(init_cells, npartitions=200).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-09-24_lDE20_Lineage_Analysis\",\n",
    "    engine=\"fastparquet\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_df_trenchid_idx = trenchid_df.reset_index(drop=False).set_index(\"trenchid\",sorted=True)\n",
    "\n",
    "# init_df_idx = init_cells[\"trenchid\"].unique().compute().tolist()\n",
    "# valid_barcode_df = barcode_df[barcode_df[\"trenchid\"].isin(trenchid_map.keys())].compute()\n",
    "# barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(lambda x: trenchid_map[x])\n",
    "# valid_init_df_indices = barcode_df_mapped_trenchids.isin(init_df_idx)\n",
    "# barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "# final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "# called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "# called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "# # called_df = called_df.set_index(\"phenotype trenchid\")\n",
    "# final_output_df = trenchid_df.loc[called_df.index.compute().tolist()].join(called_df)\n",
    "# final_output_df[\"phenotype trenchid\"] = final_output_df.index\n",
    "# final_output_df = final_output_df.reset_index(drop=True).set_index(\"File Parquet Index\",sorted=True)\n",
    "\n",
    "# del final_output_df[\"Barcode Signal\"]\n",
    "# final_output_df = final_output_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-09-24_lDE20_Lineage_Analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_output_df.loc[:1000000].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"major_axis_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
