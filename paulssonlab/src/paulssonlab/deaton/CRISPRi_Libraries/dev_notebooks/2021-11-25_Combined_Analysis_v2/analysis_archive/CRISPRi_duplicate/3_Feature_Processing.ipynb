{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Feature processing before main analysis\n",
    "\n",
    "- Note that there are fluctuations in the illumination intensity which may be resulting in pathological behavior from the reporter\n",
    "\n",
    "- This has been normalized out in the upstream processing, but try to fix long term\n",
    "\n",
    "- Also consider a flat field correction for the final experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import scipy as sp\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_distances,\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    ")\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgrnadf_from_scoredf(\n",
    "    scoredf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    scoredf_groupby = scoredf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        scoredf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": score\"] = scoredf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label + \": score\"].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = scoredf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = scoredf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = scoredf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = scoredf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = scoredf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = scoredf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "def normalize_timeseries(feature_vector_series, lmbda=0.5):\n",
    "    timeseries_arr = np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    sigma = np.std(timeseries_arr, axis=1)\n",
    "    if lmbda > 0.0:\n",
    "        sigma_prime = ((sigma + 1) ** lmbda - 1) / lmbda  ##yeo-johnson\n",
    "    elif lmbda == 0.0:\n",
    "        sigma_prime = np.log(sigma + 1)\n",
    "    else:\n",
    "        raise ValueError(\"lmbda cannot be negative\")\n",
    "    normalizer = sigma / sigma_prime\n",
    "    normalized_timeseries = timeseries_arr / normalizer[:, np.newaxis, :]\n",
    "    return normalized_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    n_workers_min=20,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_lineage = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Final_Lineage_df/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "final_output_df_lineage = final_output_df_lineage.dropna(\n",
    "    subset=[\n",
    "        \"Final timepoints\",\n",
    "        \"Mean Exponential Growth Rate: area\",\n",
    "        \"Birth: minor_axis_length\",\n",
    "        \"Birth: Surface Area\",\n",
    "    ]\n",
    ")\n",
    "final_output_df_lineage = (\n",
    "    final_output_df_lineage.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .repartition(npartitions=100)\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_trench_groupby = final_output_df_lineage.groupby(\n",
    "    \"phenotype trenchid\", sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "\n",
    "#### Filter for \"Normal\" Sizes at Start\n",
    "\n",
    "1) Fit a gaussian model to each of the specified feature params during the first t timepoints of the experiment (using a subsample for speed) \n",
    "2) Compute a normalized probability trenchwise for these features under the gaussian model, during the first t timepoints of the experiment\n",
    "3) Eliminate trenches that are under some p percentile value of this probability for each feature\n",
    "4) Display histograms for each property as well as the resulting theshold\n",
    "\n",
    "Note that these features should be the only features examined in the resulting analysis. For the notebook, I am looking at:\n",
    "- Birth length (Lb)\n",
    "- Division length (Ld)\n",
    "- Mean Area Increment\n",
    "- Mean Length Increment\n",
    "- Mean Width\n",
    "- Cell cycle duration (Delta t)\n",
    "- Mean mCherry Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_timepoint_cutoff = 12\n",
    "gaussian_subsample = 0.2\n",
    "percentile_threshold = 10\n",
    "\n",
    "filter_params = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta time (s)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_tpt_df = final_output_df_trench_groupby.apply(\n",
    "    lambda x: x[x[\"Final timepoints\"] < early_timepoint_cutoff].reset_index(drop=True)\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for filter_param in filter_params:\n",
    "    early_param_series = early_tpt_df[filter_param]\n",
    "    all_param_values = (\n",
    "        early_param_series.sample(frac=gaussian_subsample).compute().tolist()\n",
    "    )\n",
    "    gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    early_param_series = dd.from_pandas(\n",
    "        early_param_series.compute().droplevel(1), npartitions=50\n",
    "    )\n",
    "    trench_probability = early_param_series.groupby(\"phenotype trenchid\").apply(\n",
    "        lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "    )\n",
    "\n",
    "    final_output_df_lineage[\n",
    "        filter_param + \": Probability\"\n",
    "    ] = trench_probability.persist()\n",
    "\n",
    "final_output_df_onetrench = (\n",
    "    final_output_df_lineage.groupby(\"phenotype trenchid\")\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "values_names = [\n",
    "    \"Volume Growth Rate (linear)\",\n",
    "    \"Volume Growth Rate (ratio)\",\n",
    "    \"Division Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Interdivision Time\",\n",
    "]\n",
    "plt.figure(figsize=(22, 16))\n",
    "query_list = []\n",
    "for i, filter_param in enumerate(filter_params):\n",
    "    prob_threshold = np.nanpercentile(\n",
    "        final_output_df_onetrench[filter_param + \": Probability\"].tolist(),\n",
    "        percentile_threshold,\n",
    "    )\n",
    "    query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "    query_list.append(query)\n",
    "\n",
    "    min_v, max_v = (\n",
    "        np.min(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "        np.max(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "    )\n",
    "\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Unnormalized Likelihood\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.hist(\n",
    "        final_output_df_onetrench[\n",
    "            final_output_df_onetrench[filter_param + \": Probability\"] < prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "    plt.hist(\n",
    "        final_output_df_onetrench[\n",
    "            final_output_df_onetrench[filter_param + \": Probability\"] >= prob_threshold\n",
    "        ][filter_param + \": Probability\"].tolist(),\n",
    "        bins=50,\n",
    "        range=(min_v, max_v),\n",
    "    )\n",
    "plt.savefig(\"Prob_threshold.png\", dpi=500)\n",
    "\n",
    "compiled_query = \" and \".join(query_list)\n",
    "final_output_df_onetrench_filtered = final_output_df_onetrench.query(compiled_query)\n",
    "final_output_df_filtered = final_output_df_lineage.loc[\n",
    "    final_output_df_onetrench_filtered.index.tolist()\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(final_output_df_filtered) / len(final_output_df_lineage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"Final timepoints\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    if flatten_vals:\n",
    "        masked_label_series = df.apply(\n",
    "            lambda x: np.array(x[label])[\n",
    "                (np.array(x[time_label]) >= min_timepoint)\n",
    "                * (np.array(x[time_label]) <= max_timepoint)\n",
    "            ],\n",
    "            axis=1,\n",
    "            meta=\"object\",\n",
    "        )\n",
    "        flattened_vals = np.concatenate(masked_label_series.compute().tolist())\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        masked_label_series = (\n",
    "            df.groupby(\"phenotype trenchid\")\n",
    "            .apply(\n",
    "                lambda x: np.array(x[label])[\n",
    "                    (np.array(x[time_label]) >= min_timepoint)\n",
    "                    * (np.array(x[time_label]) <= max_timepoint)\n",
    "                ],\n",
    "                meta=\"object\",\n",
    "            )\n",
    "            .persist()\n",
    "        )\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "def get_feature_stats(\n",
    "    df, feature_label, min_timepoint, max_timepoint, time_label=\"Final timepoints\"\n",
    "):\n",
    "    feature_vals = get_timepoint_values(\n",
    "        df, feature_label, min_timepoint, max_timepoint, time_label=time_label\n",
    "    )\n",
    "    feature_median = np.median(feature_vals)\n",
    "    feature_iqr = sp.stats.iqr(feature_vals)\n",
    "    return feature_median, feature_iqr\n",
    "\n",
    "\n",
    "def compute_score(df, feature_label, feature_median, feature_iqr):\n",
    "    scores = 1.35 * ((df[feature_label] - feature_median) / feature_iqr)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_feature_scores(\n",
    "    df, feature_label, init_timepoint_range=(0, 20), time_label=\"Final timepoints\"\n",
    "):\n",
    "    feature_median, feature_iqr = get_feature_stats(\n",
    "        df,\n",
    "        feature_label,\n",
    "        init_timepoint_range[0],\n",
    "        init_timepoint_range[1],\n",
    "        time_label=time_label,\n",
    "    )\n",
    "    scores = compute_score(df, feature_label, feature_median, feature_iqr)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_scores(\n",
    "    df, feature_labels, init_timepoint_range=(0, 20), time_label=\"Final timepoints\"\n",
    "):\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_scores(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "        )\n",
    "        df[feature_label + \": z score\"] = feature_scores.persist()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Normalize Properties (maybe go back to the per trench normalization?)\n",
    "\n",
    "1) Yeo-Johnson transform the data to get a more normal-like distribution.\n",
    "2) Convert transformed values to time-dependent z-scores using the following formula:\n",
    "\n",
    "$$ z(i,k,t) = 1.35 \\times \\frac{F_{i,k,t} - median_{t\\in \\tau}(F_{i,t})}{iqr_{t\\in \\tau}(F_{i,t})} $$\n",
    "\n",
    "where $F_{i,k,t}$ are the feature values for feature i, trench k at time t. $\\tau$ are the initial pre-induction timepoints. \n",
    "\n",
    "Essentially this is a z-score using the more outlier robust median and interquartile range to define the differences from normal bahavior. The 1.35 factor scales the values such that z-scores represent number of standard deviations from the mean for a normal distribution. Finally the values are normalized by initial behaviors trenchwise by the $median_{t\\in \\tau}(F_{i,k,t})$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo_subsample = 0.1\n",
    "\n",
    "subsample_df = final_output_df_filtered.sample(frac=yeo_subsample).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_phenotypes = subsample_df[subsample_df[\"Final timepoints\"] > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param_values = (\n",
    "    subsample_df[\"Mean: mCherry Intensity\"].astype(float).compute().tolist()\n",
    ")\n",
    "all_param_values = np.array(all_param_values)\n",
    "all_param_values = all_param_values[~np.isnan(all_param_values)]\n",
    "l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "\n",
    "all_param_values = (\n",
    "    late_phenotypes[\"Mean: mCherry Intensity\"].astype(float).compute().tolist()\n",
    ")\n",
    "all_param_values = np.array(all_param_values)\n",
    "all_param_values = all_param_values[~np.isnan(all_param_values)]\n",
    "\n",
    "yj_transformed = sp.stats.yeojohnson(all_param_values, lmbda=l_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 16))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Mean mCherry Intensity\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.hist(all_param_values, bins=100)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Mean mCherry Intensity (log frequency)\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.hist(all_param_values, bins=100, log=True)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"Centered Yeo-Johnson Transformed mCherry\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.hist(yj_transformed - np.median(yj_transformed), bins=100, range=(-0.002, 0.01))\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Centered Yeo-Johnson Transformed mCherry (log frequency)\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.hist(\n",
    "    yj_transformed - np.median(yj_transformed), bins=100, range=(-0.002, 0.01), log=True\n",
    ")\n",
    "\n",
    "plt.savefig(\"YJ_transform.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta time (s)\",\n",
    "]\n",
    "\n",
    "yeo_subsample = 0.05\n",
    "\n",
    "subsample_df = final_output_df_filtered.sample(frac=yeo_subsample).persist()\n",
    "\n",
    "for i, param in enumerate(params_to_transform):\n",
    "    all_param_values = subsample_df[param].astype(float).compute().tolist()\n",
    "    all_param_values = np.array(all_param_values)\n",
    "    all_param_values = all_param_values[~np.isnan(all_param_values)]\n",
    "    l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "    final_output_df_filtered[param + \": Yeo-Johnson\"] = (\n",
    "        final_output_df_filtered[param]\n",
    "        .apply(lambda x: sp.stats.yeojohnson(x, lmbda=l_norm), meta=float)\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "final_output_df_filtered = get_all_feature_scores(\n",
    "    final_output_df_filtered,\n",
    "    [param + \": Yeo-Johnson\" for param in params_to_transform],\n",
    "    init_timepoint_range=(0, early_timepoint_cutoff),\n",
    ")\n",
    "trenchiddf = final_output_df_filtered.reset_index().set_index(\n",
    "    \"phenotype trenchid\", drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### sgRNA Effect Size Filtering (within Gene groups)\n",
    "\n",
    "1) Threshold sgRNAs to include by number of observations\n",
    "2) Use Kernel smoothing to smooth out both score and raw timeseries into 20 points\n",
    "3) For each timepoint, measure the euclidean norm of the feature vector and integrate it over all time as a measure of effect size\n",
    "4) Threshold sgRNAs for strong effects by applying a threshold to the euclidean norm that will be displayed with histogram\n",
    "5) Display a histogram for the sgRNA number per gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.nonparametric import kernel_regression\n",
    "# from scipy.stats import iqr\n",
    "# from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "# import sklearn as skl\n",
    "# from tslearn.clustering import TimeSeriesKMeans\n",
    "# from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "# def timeseries_kernel_reg(df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth):\n",
    "#     def kernel_reg(x_arr,y_arr,start=min_tpt,end=max_tpt,kernel_bins=kernel_bins,kernel_bandwidth=kernel_bandwidth):\n",
    "#         intervals = np.linspace(start, end, num=kernel_bins, dtype=float)\n",
    "#         w = kernel_regression.KernelReg(y_arr,x_arr,\"c\",reg_type=\"lc\",bw=np.array([kernel_bandwidth]),ckertype=\"gaussian\").fit(intervals)[0]\n",
    "#         reg_x, reg_y = (intervals, w)\n",
    "#         return reg_x, reg_y\n",
    "\n",
    "#     kernel_result = df.groupby(\"sgRNAid\").apply(lambda x: kernel_reg((x[\"final timepoints\"].values + x[\"initial timepoints\"].values) / 2,x[y_label].values,)[1],meta=float,)\n",
    "#     return kernel_result\n",
    "\n",
    "\n",
    "# def get_all_kernel_regs(df, y_label_list, min_tpt, max_tpt, kernel_bins=20, kernel_bandwidth=10):\n",
    "#     out_df = copy.copy(df)\n",
    "\n",
    "#     for y_label in y_label_list:\n",
    "#         kernel_result = timeseries_kernel_reg(out_df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth)\n",
    "#         out_df[\"Kernel Trace: \" + y_label] = kernel_result.persist()\n",
    "\n",
    "#     return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Trying to interpolate trenchwise instead of sgRNAwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric import kernel_regression\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def timeseries_kernel_reg(df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth):\n",
    "    def kernel_reg(\n",
    "        x_arr,\n",
    "        y_arr,\n",
    "        start=min_tpt,\n",
    "        end=max_tpt,\n",
    "        kernel_bins=kernel_bins,\n",
    "        kernel_bandwidth=kernel_bandwidth,\n",
    "    ):\n",
    "        intervals = np.linspace(start, end, num=kernel_bins, dtype=float)\n",
    "        w = kernel_regression.KernelReg(\n",
    "            y_arr,\n",
    "            x_arr,\n",
    "            \"c\",\n",
    "            reg_type=\"lc\",\n",
    "            bw=np.array([kernel_bandwidth]),\n",
    "            ckertype=\"gaussian\",\n",
    "        ).fit(intervals)[0]\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    kernel_result = df.groupby(\"phenotype trenchid\").apply(\n",
    "        lambda x: kernel_reg(\n",
    "            (x[\"Final time (s)\"].values - (x[\"Delta time (s)\"].values / 2)),\n",
    "            x[y_label].values,\n",
    "        )[1],\n",
    "        meta=float,\n",
    "    )\n",
    "    return kernel_result\n",
    "\n",
    "\n",
    "def get_all_kernel_regs(\n",
    "    df, y_label_list, min_tpt, max_tpt, kernel_bins=20, kernel_bandwidth=5\n",
    "):\n",
    "    out_df = copy.copy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        kernel_result = timeseries_kernel_reg(\n",
    "            out_df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth\n",
    "        )\n",
    "        out_df[\"Kernel Trace: \" + y_label] = kernel_result.persist()\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making an observation grid to project vals onto\n",
    "\n",
    "min_tpt = 0\n",
    "max_tpt = 36000\n",
    "\n",
    "kernel_bins = 20\n",
    "kernel_bandwidth = 2700\n",
    "\n",
    "score_params = [param + \": Yeo-Johnson: z score\" for param in params_to_transform]\n",
    "other_params = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Birth: major_axis_length\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Birth: Surface Area\",\n",
    "    \"Division: Surface Area\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta time (s)\",\n",
    "]\n",
    "\n",
    "\n",
    "def temp_kernel_reg(\n",
    "    x_arr,\n",
    "    y_arr,\n",
    "    start=min_tpt,\n",
    "    end=max_tpt,\n",
    "    kernel_bins=kernel_bins,\n",
    "    kernel_bandwidth=kernel_bandwidth,\n",
    "):\n",
    "    intervals = np.linspace(start, end, num=kernel_bins, dtype=float)\n",
    "    w = kernel_regression.KernelReg(\n",
    "        y_arr,\n",
    "        x_arr,\n",
    "        \"c\",\n",
    "        reg_type=\"lc\",\n",
    "        bw=np.array([kernel_bandwidth]),\n",
    "        ckertype=\"gaussian\",\n",
    "    ).fit(intervals)[0]\n",
    "    reg_x, reg_y = (intervals, w)\n",
    "    return reg_x, reg_y\n",
    "\n",
    "\n",
    "trenchid_list = trenchiddf.index.unique().compute().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_trenchid = np.random.choice(trenchid_list)\n",
    "\n",
    "rand_trenchid_df = trenchiddf.loc[rand_trenchid].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_arr = rand_trenchid_df[\"Final time (s)\"].values - (\n",
    "    rand_trenchid_df[\"Delta time (s)\"].values / 2\n",
    ")\n",
    "y_arr = rand_trenchid_df[\"Division: major_axis_length\"]\n",
    "reg_x, reg_y = temp_kernel_reg(x_arr, y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.title(\"Division Length Kernel Regression\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel(\"Timepoint\", fontsize=18)\n",
    "plt.ylabel(\"Division Length\", fontsize=18)\n",
    "plt.scatter(x_arr, y_arr)\n",
    "plt.plot(reg_x, reg_y)\n",
    "plt.ylim(3, 9)\n",
    "\n",
    "plt.savefig(\"Kernel_Reg_Example.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making an observation grid to project vals onto\n",
    "\n",
    "min_tpt = 0\n",
    "max_tpt = 36000\n",
    "\n",
    "kernel_bins = 20\n",
    "kernel_bandwidth = 2700\n",
    "\n",
    "score_params = [param + \": Yeo-Johnson: z score\" for param in params_to_transform]\n",
    "other_params = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Birth: major_axis_length\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Birth: Surface Area\",\n",
    "    \"Division: Surface Area\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta time (s)\",\n",
    "]\n",
    "\n",
    "trenchiddf = get_all_kernel_regs(\n",
    "    trenchiddf,\n",
    "    other_params,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    kernel_bins=kernel_bins,\n",
    "    kernel_bandwidth=kernel_bandwidth,\n",
    ")\n",
    "trenchiddf = get_all_kernel_regs(\n",
    "    trenchiddf,\n",
    "    score_params,\n",
    "    min_tpt,\n",
    "    max_tpt,\n",
    "    kernel_bins=kernel_bins,\n",
    "    kernel_bandwidth=kernel_bandwidth,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_barcodes = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Final_Barcodes_df/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .groupby(\"phenotype trenchid\", sort=False)\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .persist()\n",
    ")\n",
    "# sgRNA_dict = {sgRNA:i for i,sgRNA in enumerate(sorted(final_output_df_barcodes[\"sgRNA\"].unique().compute().tolist()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_df_barcodes[\"sgRNAid\"] = final_output_df_barcodes[\"sgRNA\"].apply(lambda x: sgRNA_dict[x], meta=int)\n",
    "# final_output_df_barcodes[\"sgRNAid\"] = final_output_df_barcodes[\"sgRNAid\"].astype(int)\n",
    "final_output_df_barcodes = final_output_df_barcodes.reset_index().set_index(\n",
    "    \"oDEPool7_id\", drop=True\n",
    ")\n",
    "final_output_df_barcodes[\"N Observations\"] = final_output_df_barcodes.groupby(\n",
    "    \"oDEPool7_id\", sort=False\n",
    ")[\"phenotype trenchid\"].apply(lambda x: len(x.unique()), meta=int)\n",
    "final_output_df_barcodes[\"N Observations\"] = final_output_df_barcodes[\n",
    "    \"N Observations\"\n",
    "].astype(int)\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.reset_index().set_index(\"phenotype trenchid\").persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trenchiddf = trenchiddf.merge(\n",
    "    final_output_df_barcodes[[\"oDEPool7_id\"]],\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").persist()\n",
    "trenchiddf_out = trenchiddf.groupby(\"phenotype trenchid\").apply(lambda x: x.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.reset_index().set_index(\"oDEPool7_id\").persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernel_traces = score_params + other_params\n",
    "all_kernel_traces = [\"Kernel Trace: \" + item for item in all_kernel_traces]\n",
    "\n",
    "trenchiddf_out_groupby = (\n",
    "    trenchiddf_out.reset_index().set_index(\"oDEPool7_id\").groupby(\"oDEPool7_id\")\n",
    ")\n",
    "\n",
    "for kernel_trace in all_kernel_traces:\n",
    "    mean_kernel = (\n",
    "        trenchiddf_out_groupby[kernel_trace]\n",
    "        .apply(lambda x: np.mean(np.array(x.tolist()), axis=0))\n",
    "        .persist()\n",
    "    )\n",
    "    final_output_df_barcodes[kernel_trace] = mean_kernel\n",
    "\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.groupby(\"oDEPool7_id\").apply(lambda x: x.iloc[0]).persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_list = final_output_df_barcodes[\"N Observations\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(obs_list, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(obs_list, bins=50, range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Observations_thr = 5\n",
    "\n",
    "sgrnadf = final_output_df_barcodes\n",
    "sgrnadf_wellsampled = final_output_df_barcodes[\n",
    "    final_output_df_barcodes[\"N Observations\"] >= N_Observations_thr\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making an observation grid to project vals onto\n",
    "\n",
    "# min_tpt = 0\n",
    "# max_tpt = 143\n",
    "\n",
    "# kernel_bins = 20\n",
    "# kernel_bandwidth = 10\n",
    "\n",
    "# score_params = [param + \": Yeo-Johnson: z score\" for param in params_to_transform]\n",
    "# other_params = [\n",
    "#     \"Mean Linear Growth Rate: Volume\",\n",
    "#     \"Mean Exponential Growth Rate: Volume\",\n",
    "#     \"Birth: major_axis_length\",\n",
    "#     \"Division: major_axis_length\",\n",
    "#     \"Birth: Volume\",\n",
    "#     \"Division: Volume\",\n",
    "#     \"Birth: Surface Area\",\n",
    "#     \"Division: Surface Area\",\n",
    "#     \"Mean: minor_axis_length\",\n",
    "#     \"Mean: mCherry Intensity\",\n",
    "#     \"Delta t\",\n",
    "# ]\n",
    "\n",
    "# trace_df_raw = get_all_kernel_regs(\n",
    "#     sgrnadf_wellsampled,\n",
    "#     other_params,\n",
    "#     min_tpt,\n",
    "#     max_tpt,\n",
    "#     kernel_bins=kernel_bins,\n",
    "#     kernel_bandwidth=kernel_bandwidth,\n",
    "# )\n",
    "# trace_df_raw = get_all_kernel_regs(\n",
    "#     trace_df_raw,\n",
    "#     score_params,\n",
    "#     min_tpt,\n",
    "#     max_tpt,\n",
    "#     kernel_bins=kernel_bins,\n",
    "#     kernel_bandwidth=kernel_bandwidth,\n",
    "# )\n",
    "# trace_df = trace_df_raw.groupby(\"sgRNAid\").apply(lambda x: x.iloc[0]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# non_group_measurements = ['Global CellID', 'phenotype trenchid', 'File Parquet Index', 'fov',\n",
    "#        'row', 'trench', 'initial timepoints', 'lane orientation', 'y (local)',\n",
    "#        'x (local)', 'File Index', 'File Trench Index', 'CellID',\n",
    "#        'Trench Score', 'Mother CellID', 'Daughter CellID 1',\n",
    "#        'Daughter CellID 2', 'Sister CellID', 'Centroid X', 'Centroid Y',\n",
    "#        'FOV Parquet Index', 'mCherry mean_intensity', 'area',\n",
    "#        'major_axis_length', 'minor_axis_length', 'Volume', 'Surface Area',\n",
    "#        'Delta: area', 'Birth: area', 'Division: area',\n",
    "#        'Delta: major_axis_length', 'Birth: major_axis_length',\n",
    "#        'Division: major_axis_length', 'Delta: minor_axis_length',\n",
    "#        'Birth: minor_axis_length', 'Division: minor_axis_length',\n",
    "#        'Delta: Volume', 'Birth: Volume', 'Division: Volume',\n",
    "#        'Delta: Surface Area', 'Birth: Surface Area', 'Division: Surface Area',\n",
    "#        'Final timepoints', 'Delta time (s)','Mean: area', 'Mean Linear Growth Rate: area',\n",
    "#        'Mean Exponential Growth Rate: area', 'Mean: major_axis_length',\n",
    "#        'Mean Linear Growth Rate: major_axis_length',\n",
    "#        'Mean Exponential Growth Rate: major_axis_length',\n",
    "#        'Mean: minor_axis_length', 'Mean Linear Growth Rate: minor_axis_length',\n",
    "#        'Mean Exponential Growth Rate: minor_axis_length', 'Mean: Volume',\n",
    "#        'Mean Linear Growth Rate: Volume',\n",
    "#        'Mean Exponential Growth Rate: Volume', 'Mean: Surface Area',\n",
    "#        'Mean Linear Growth Rate: Surface Area',\n",
    "#        'Mean Exponential Growth Rate: Surface Area', 'Mean: mCherry Intensity',\n",
    "#        'Mean Linear Growth Rate: Volume: Probability',\n",
    "#        'Mean Exponential Growth Rate: Volume: Probability',\n",
    "#        'Division: major_axis_length: Probability',\n",
    "#        'Mean: minor_axis_length: Probability',\n",
    "#        'Mean: mCherry Intensity: Probability', 'Delta time (s): Probability',\n",
    "#        'Mean Linear Growth Rate: Volume: Yeo-Johnson',\n",
    "#        'Mean Exponential Growth Rate: Volume: Yeo-Johnson',\n",
    "#        'Division: major_axis_length: Yeo-Johnson',\n",
    "#        'Mean: minor_axis_length: Yeo-Johnson',\n",
    "#        'Mean: mCherry Intensity: Yeo-Johnson', 'Delta time (s): Yeo-Johnson',\n",
    "#        'Mean Linear Growth Rate: Volume: Yeo-Johnson: z score',\n",
    "#        'Mean Exponential Growth Rate: Volume: Yeo-Johnson: z score',\n",
    "#        'Division: major_axis_length: Yeo-Johnson: z score',\n",
    "#        'Mean: minor_axis_length: Yeo-Johnson: z score',\n",
    "#        'Mean: mCherry Intensity: Yeo-Johnson: z score',\n",
    "#        'Delta time (s): Yeo-Johnson: z score','index', 'trenchid']\n",
    "\n",
    "\n",
    "non_group_measurements = [\n",
    "    \"phenotype trenchid\",\n",
    "    \"File Parquet Index\",\n",
    "    \"fov\",\n",
    "    \"row\",\n",
    "    \"trench\",\n",
    "    \"lane orientation\",\n",
    "    \"y (local)\",\n",
    "    \"x (local)\",\n",
    "    \"File Index\",\n",
    "    \"File Trench Index\",\n",
    "    \"index\",\n",
    "    \"trenchid\",\n",
    "]\n",
    "\n",
    "trenchiddf_out = sgrnadf.drop(columns=non_group_measurements).compute()\n",
    "trenchiddf_out[\"phenotype trenchids\"] = (\n",
    "    trenchiddf.reset_index()\n",
    "    .set_index(\"oDEPool7_id\")\n",
    "    .groupby(\"oDEPool7_id\", sort=False)\n",
    "    .apply(lambda x: x[\"phenotype trenchid\"].unique().tolist())\n",
    "    .compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_params = [\"Kernel Trace: \" + param for param in other_params]\n",
    "kernel_score_params = [\n",
    "    \"Kernel Trace: \" + param + \": Yeo-Johnson: z score\" for param in params_to_transform\n",
    "]\n",
    "\n",
    "feature_vector_series = trenchiddf_out.apply(\n",
    "    lambda x: np.array(x[kernel_score_params].tolist()), axis=1\n",
    ")\n",
    "trenchiddf_out[\"Feature Vector\"] = feature_vector_series\n",
    "trenchiddf_nan_filtered = trenchiddf_out[\n",
    "    ~trenchiddf_out[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strong_effect_threshold = 15\n",
    "\n",
    "zero_vector = np.zeros((1, trenchiddf_nan_filtered[\"Feature Vector\"].iloc[0].shape[0]))\n",
    "feature_arr = np.array(trenchiddf_nan_filtered[\"Feature Vector\"].tolist())\n",
    "feature_arr_abs = np.abs(feature_arr)\n",
    "trenchiddf_nan_filtered[\"Integrated Feature Vector\"] = [\n",
    "    item for item in sp.integrate.simpson(feature_arr_abs, axis=2)\n",
    "]\n",
    "trenchiddf_nan_filtered[\"Integrated Feature Max\"] = trenchiddf_nan_filtered[\n",
    "    \"Integrated Feature Vector\"\n",
    "].apply(lambda x: np.max(x))\n",
    "trenchiddf_nan_filtered[\"Integrated Euclidean Norm\"] = np.linalg.norm(\n",
    "    np.array(trenchiddf_nan_filtered[\"Integrated Feature Vector\"].tolist()), axis=1\n",
    ")\n",
    "\n",
    "sgrnadf_strong_effect = trenchiddf_nan_filtered[\n",
    "    trenchiddf_nan_filtered[\"Integrated Feature Max\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = np.min(trenchiddf_nan_filtered[\"Integrated Feature Max\"]), np.percentile(\n",
    "    trenchiddf_nan_filtered[\"Integrated Feature Max\"], 99\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Feature Max\")\n",
    "plt.hist(\n",
    "    trenchiddf_nan_filtered[\n",
    "        trenchiddf_nan_filtered[\"Integrated Feature Max\"] < strong_effect_threshold\n",
    "    ][\"Integrated Feature Max\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    trenchiddf_nan_filtered[\n",
    "        trenchiddf_nan_filtered[\"Integrated Feature Max\"] >= strong_effect_threshold\n",
    "    ][\"Integrated Feature Max\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "sgrnadf_strong_effect[\"Gene\"]\n",
    "\n",
    "unique_genes, gene_counts = np.unique(\n",
    "    sgrnadf_strong_effect[\"Gene\"][\n",
    "        sgrnadf_strong_effect[\"Gene\"].apply(lambda x: type(x) == str)\n",
    "    ].tolist(),\n",
    "    return_counts=True,\n",
    ")\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchiddf_nan_filtered.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-10_gene_cluster_df_no_filter.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_strong_effect.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-10_gene_cluster_df.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_rep_example_series = (\n",
    "    sgrnadf_strong_effect.reset_index(drop=False)\n",
    "    .groupby(\"TargetID\")\n",
    "    .apply(lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])])\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(\"sgRNA\", drop=True)\n",
    ")\n",
    "\n",
    "## THIS IS FOR A LOG TRANSFORMATION, try to do this earlier when it makes more sense....\n",
    "# normalized_timeseries = np.swapaxes(normalize_timeseries(most_rep_example_series[\"Feature Vector\"], lmbda=0.5),1,2)\n",
    "# most_rep_example_series[\"Normalized Feature Vector\"] = [normalized_timeseries[i] for i in range(normalized_timeseries.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Effect Distance Metrics\n",
    "\n",
    "Now, I want to evaluate the performance of different distance metrics on the data wrt seperating it maximally while also preserving similarity within replicates\n",
    "\n",
    "- DTW (can be done with cosine similarity) \n",
    "- cosine similarity (same as pearson for z-scores)\n",
    "- cross correlation\n",
    "\n",
    "Seems like soft-DTW is a pretty good option. Going forward with that for now.\n",
    "\n",
    "<!-- In the end cosine similarity was chosen as it produced superior silhouette scores for sets of targets from genes with different phenotypes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrnadf_examples_for_distance_metric = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin([\"ftsN\", \"rplA\", \"mreB\", \"tufB\", \"tff\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import (\n",
    "    cdist_dtw,\n",
    "    cdist_soft_dtw,\n",
    "    cdist_soft_dtw_normalized,\n",
    "    dtw,\n",
    "    dtw_path_from_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_arr = np.swapaxes(\n",
    "    np.array(sgrnadf_examples_for_distance_metric[\"Feature Vector\"].tolist()), 1, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    print(\n",
    "        \"Soft-DTW Gamma=\"\n",
    "        + str(gamma)\n",
    "        + \": \"\n",
    "        + str(\n",
    "            tslearn.clustering.silhouette_score(\n",
    "                timeseries_arr,\n",
    "                sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "                metric=\"softdtw\",\n",
    "                gamma=gamma,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"cosine\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Cosine-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dist_mat = np.zeros((timeseries_arr.shape[0], timeseries_arr.shape[0]))\n",
    "for i in range(timeseries_arr.shape[0]):\n",
    "    for j in range(i + 1, timeseries_arr.shape[0]):\n",
    "        dist = dtw_path_from_metric(\n",
    "            timeseries_arr[i],\n",
    "            timeseries_arr[j],\n",
    "            metric=\"euclidean\",\n",
    "            global_constraint=\"sakoe_chiba\",\n",
    "            sakoe_chiba_radius=3,\n",
    "        )[1]\n",
    "        dist_mat[i, j] = dist\n",
    "        dist_mat[j, i] = dist\n",
    "print(\n",
    "    \"Euclidean-DTW: \"\n",
    "    + str(\n",
    "        tslearn.clustering.silhouette_score(\n",
    "            dist_mat,\n",
    "            sgrnadf_examples_for_distance_metric[\"Gene\"].tolist(),\n",
    "            metric=\"precomputed\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_dtw_dist_arr = tslearn.metrics.cdist_soft_dtw(timeseries_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(soft_dtw_dist_arr.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Detecting different effects against single genes\n",
    "\n",
    "1) Plot a histogram of minimum soft-DTW similarity within groups of TargetIDs against the same genes (for genes with more than one targetID)\n",
    "2) Use affinity propagation to select the number of phenotype clusters to use per gene (preference still needs to be dialed in, not sure how to optimize on this)\n",
    "3) Among each cluster, represent the final effect as the strongest effect (integrated euc norm) of the members of the cluster\n",
    "\n",
    "~~3) Among each cluster, represent the final effect as the median of the members of the cluster~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_softdtw(feature_vector_series):\n",
    "    dist_mat = cdist_soft_dtw_normalized(\n",
    "        np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    )\n",
    "    timeseries_len = (\n",
    "        feature_vector_series[0].shape[0] * feature_vector_series[0].shape[1]\n",
    "    )\n",
    "    dist_mat = dist_mat / timeseries_len\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def get_upper_right_vals(a):\n",
    "    upper_tri = np.triu(a, k=1)\n",
    "    upper_tri[upper_tri == 0.0] = np.NaN\n",
    "    return upper_tri\n",
    "\n",
    "\n",
    "def get_sgRNA_clusters(df, preference=0.6):\n",
    "    gene_indexed_df = (\n",
    "        df.reset_index(drop=False)\n",
    "        .set_index(\"Gene\")[[\"sgRNA\", \"Feature Vector\", \"TargetID\"]]\n",
    "        .sort_index()\n",
    "    )\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = pd.Series(\n",
    "        np.zeros(len(gene_indexed_df), dtype=int), dtype=int\n",
    "    )\n",
    "    gene_df_list = []\n",
    "    for gene in gene_indexed_df.index.tolist():\n",
    "        gene_df = gene_indexed_df.loc[[gene]]\n",
    "        if len(gene_df) > 1:\n",
    "            gene_feature_vector = gene_df[\"Feature Vector\"]\n",
    "            soft_dtw_dist = get_normed_softdtw(gene_feature_vector)\n",
    "            af_labels = (\n",
    "                AffinityPropagation(\n",
    "                    affinity=\"precomputed\", preference=preference, random_state=42\n",
    "                )\n",
    "                .fit_predict(-soft_dtw_dist)\n",
    "                .astype(int)\n",
    "            )\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = af_labels\n",
    "        else:\n",
    "            gene_indexed_df.loc[gene, \"sgRNA Cluster\"] = 0\n",
    "    gene_indexed_df[\"sgRNA Cluster\"] = gene_indexed_df[\"sgRNA Cluster\"].astype(int)\n",
    "    return gene_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sgrna_replicate_thr = 2\n",
    "pref_factor = 3.0\n",
    "\n",
    "gene_list, counts_list = np.unique(most_rep_example_series[\"Gene\"], return_counts=True)\n",
    "genes_with_many_replicate_sgRNAs = gene_list[counts_list >= n_sgrna_replicate_thr]\n",
    "sgrnadf_many_copies_per_gene = most_rep_example_series[\n",
    "    most_rep_example_series[\"Gene\"].isin(genes_with_many_replicate_sgRNAs)\n",
    "]\n",
    "\n",
    "max_distance_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: np.nanmax(get_upper_right_vals(get_normed_softdtw(x[\"Feature Vector\"])))\n",
    ")\n",
    "plt.title(\"Maximum soft-DTW Distance per Gene\")\n",
    "plt.hist(max_distance_within_gene, bins=50)\n",
    "plt.show()\n",
    "\n",
    "dist_within_gene = sgrnadf_many_copies_per_gene.groupby(\"Gene\").apply(\n",
    "    lambda x: get_upper_right_vals(get_normed_softdtw(x[\"Feature Vector\"])).flatten()\n",
    ")\n",
    "dist_within_gene = [val for item in dist_within_gene.tolist() for val in item]\n",
    "median_similarity = -np.nanmedian(dist_within_gene)\n",
    "\n",
    "gene_df = get_sgRNA_clusters(\n",
    "    most_rep_example_series, preference=pref_factor * median_similarity\n",
    ")\n",
    "\n",
    "most_rep_example_series[\"sgRNA Cluster\"] = gene_df.set_index(\"sgRNA\")[\"sgRNA Cluster\"]\n",
    "most_rep_example_series[\"sgRNA Cluster Label\"] = most_rep_example_series.apply(\n",
    "    lambda x: str(x[\"Gene\"]) + \"-\" + str(x[\"sgRNA Cluster\"]), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df = most_rep_example_series[\n",
    "    [\"sgRNA Cluster Label\", \"Feature Vector\", \"Gene\", \"Integrated Euclidean Norm\"]\n",
    "    + kernel_params\n",
    "].reset_index(drop=True)\n",
    "gene_cluster_groupby = gene_cluster_df.groupby(\"sgRNA Cluster Label\")\n",
    "# median_feature_series = gene_cluster_groupby.apply(lambda x: np.median(np.stack(x[\"Feature Vector\"]).astype(float), axis=0)).to_frame().rename(columns={0:\"Feature Vector\"})\n",
    "feature_series = (\n",
    "    gene_cluster_groupby.apply(\n",
    "        lambda x: x.iloc[np.argmax(x[\"Integrated Euclidean Norm\"])][\"Feature Vector\"]\n",
    "    )\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"Feature Vector\"})\n",
    ")\n",
    "\n",
    "gene_cluster_df = gene_cluster_groupby.apply(\n",
    "    lambda x: x.iloc[0][[\"Gene\"] + kernel_params]\n",
    ")\n",
    "gene_cluster_df = gene_cluster_df.join(feature_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-06-14_lDE20_biofloat_fullrun_1/2021-08-16_gene_cluster_df.pkl\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
