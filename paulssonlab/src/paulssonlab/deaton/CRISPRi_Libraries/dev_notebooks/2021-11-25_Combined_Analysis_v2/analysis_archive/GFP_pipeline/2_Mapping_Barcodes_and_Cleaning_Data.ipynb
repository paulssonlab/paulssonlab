{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Mapping Barcodes and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/barcodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    death_timeout=5.0,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/temp/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/barcodes/metadata.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=1000, sort=True)\n",
    "barcode_df = barcode_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttl_called)\n",
    "print(ttl_trenches)\n",
    "print(ttl_trenches_w_cells)\n",
    "print(percent_called)\n",
    "print(percent_called_w_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Import Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/GFP/analysis\"\n",
    ")\n",
    "last_trenchid = int(analysis_df.tail(1)[\"trenchid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "\n",
    "\n",
    "def hrm_find_mode(series, max_iter=1000, min_binsize=50):\n",
    "    working_series = series\n",
    "    for i in range(max_iter):\n",
    "        range_max, range_min = np.max(working_series), np.min(working_series)\n",
    "        midpoint = (range_max + range_min) / 2\n",
    "        above_middle = working_series[working_series > midpoint]\n",
    "        below_middle = working_series[working_series <= midpoint]\n",
    "\n",
    "        count_above = len(above_middle)\n",
    "        count_below = len(below_middle)\n",
    "\n",
    "        if count_above > count_below:\n",
    "            working_series = above_middle\n",
    "        else:\n",
    "            working_series = below_middle\n",
    "\n",
    "        if i > 0:\n",
    "            if (len(working_series) < min_binsize) or (last_midpoint == midpoint):\n",
    "                return np.mean(working_series)\n",
    "\n",
    "        last_midpoint = midpoint\n",
    "\n",
    "\n",
    "def bootstrap_hrm(series, n_bootstraps=100, max_n_per_bootstrap=100):\n",
    "    modes = []\n",
    "\n",
    "    series_len = len(series)\n",
    "\n",
    "    n_per_bootstrap = min(series_len, max_n_per_bootstrap)\n",
    "\n",
    "    for n in range(n_bootstraps):\n",
    "        modes.append(hrm_find_mode(series.sample(n=n_per_bootstrap)))\n",
    "    return np.mean(modes)\n",
    "\n",
    "\n",
    "def get_GFPpos_modes(\n",
    "    GFP_series, series_groupby, frac=0.01, n_bootstraps=100, max_n_per_bootstrap=100\n",
    "):\n",
    "    gfp_vals = GFP_series.sample(frac=frac).compute()\n",
    "    tri_thr = sk.filters.threshold_triangle(gfp_vals)\n",
    "    mode_series = (\n",
    "        series_groupby.apply(\n",
    "            lambda x: bootstrap_hrm(\n",
    "                x[x > tri_thr],\n",
    "                n_bootstraps=n_bootstraps,\n",
    "                max_n_per_bootstrap=max_n_per_bootstrap,\n",
    "            )\n",
    "        )\n",
    "        .compute()\n",
    "        .sort_index()\n",
    "    )\n",
    "    return mode_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Variables over FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df_nobkd = analysis_df[analysis_df[\"Objectid\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_rescale = [\"RFP-Penta mean_intensity\", \"GFP-Penta mean_intensity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\"Median mCherry Intensity\", \"Median GFPmut2 Intensity\"]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale):\n",
    "    fov_series_groupby = analysis_df_nobkd.groupby(\"fov\")[label]\n",
    "    if label == \"RFP-Penta mean_intensity\":\n",
    "        fov_mode_series = (\n",
    "            fov_series_groupby.apply(\n",
    "                lambda x: bootstrap_hrm(x, max_n_per_bootstrap=100)\n",
    "            )\n",
    "            .compute()\n",
    "            .sort_index()\n",
    "        )\n",
    "    elif label == \"GFP-Penta mean_intensity\":\n",
    "        fov_mode_series = get_GFPpos_modes(\n",
    "            analysis_df[\"GFP-Penta mean_intensity\"],\n",
    "            fov_series_groupby,\n",
    "            max_n_per_bootstrap=100,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Weird Label\")\n",
    "\n",
    "    fov_correction_series = fov_mode_series / np.max(fov_mode_series)\n",
    "    fov_correction_dict = fov_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(fov_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"FOV #\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    label_scaling = analysis_df[\"fov\"].apply(lambda x: fov_correction_dict[x]).persist()\n",
    "    analysis_df[label + \": FOV Corrected\"] = (\n",
    "        analysis_df[label] / label_scaling\n",
    "    ).persist()\n",
    "plt.savefig(\"FOV_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Variables over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df_nobkd = analysis_df[analysis_df[\"Objectid\"] != 0]\n",
    "values_to_rescale_step_2 = [value + \": FOV Corrected\" for value in values_to_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add real time later when fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\"Median mCherry Intensity\", \"Median GFPmut2 Intensity\"]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale_step_2):\n",
    "    time_series_groupby = analysis_df_nobkd.groupby(\"timepoints\")[label]\n",
    "    if label == \"RFP-Penta mean_intensity: FOV Corrected\":\n",
    "        time_mode_series = (\n",
    "            time_series_groupby.apply(\n",
    "                lambda x: bootstrap_hrm(x, max_n_per_bootstrap=100)\n",
    "            )\n",
    "            .compute()\n",
    "            .sort_index()\n",
    "        )\n",
    "    elif label == \"GFP-Penta mean_intensity: FOV Corrected\":\n",
    "        time_mode_series = get_GFPpos_modes(\n",
    "            analysis_df[\"GFP-Penta mean_intensity\"],\n",
    "            time_series_groupby,\n",
    "            max_n_per_bootstrap=100,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Weird Label\")\n",
    "\n",
    "    time_correction_series = time_mode_series / np.max(time_mode_series)\n",
    "    time_correction_dict = time_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(time_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Timepoint (3 min steps)\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.2)\n",
    "    label_scaling = analysis_df[\"timepoints\"].apply(lambda x: time_correction_dict[x])\n",
    "    analysis_df[label + \": Time Corrected\"] = (\n",
    "        analysis_df[label] / label_scaling\n",
    "    ).persist()\n",
    "plt.savefig(\"Time_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Overwrite Variables with Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in values_to_rescale:\n",
    "    analysis_df[label] = analysis_df[label + \": FOV Corrected: Time Corrected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis_df[\n",
    "    [\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"timepoints\",\n",
    "        \"Objectid\",\n",
    "        \"centroid_y\",\n",
    "        \"centroid_x\",\n",
    "        \"area\",\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"time (s)\",\n",
    "        \"lane orientation\",\n",
    "        \"y (local)\",\n",
    "        \"x (local)\",\n",
    "        \"y (global)\",\n",
    "        \"x (global)\",\n",
    "        \"trenchid\",\n",
    "        \"Trenchid Timepoint Index\",\n",
    "    ]\n",
    "    + values_to_rescale\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### GFP Quantification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_background_subtract(series, intensity_key):\n",
    "    intensity_vals = series[intensity_key]\n",
    "    bkd_val = series[series[\"Objectid\"] == 0][intensity_key].iloc[0]\n",
    "    bkd_sub_intensity = intensity_vals - bkd_val\n",
    "    bkd_sub_intensity = bkd_sub_intensity.to_dict()\n",
    "    return bkd_sub_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis_df_trenchtimepoint_groupby = analysis_df.groupby(\"Trenchid Timepoint Index\")\n",
    "\n",
    "gfp_intensity_wo_bkd = (\n",
    "    analysis_df_trenchtimepoint_groupby.apply(\n",
    "        lambda x: local_background_subtract(x, \"GFP-Penta mean_intensity\"),\n",
    "        meta=(\"GFP-Penta mean_intensity\", float),\n",
    "    )\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    "    .to_list()\n",
    ")\n",
    "gfp_intensity_wo_bkd = {k: v for d in gfp_intensity_wo_bkd for k, v in d.items()}\n",
    "gfp_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    gfp_intensity_wo_bkd, orient=\"index\", columns=[\"GFP-Penta mean_intensity_wo_bkd\"]\n",
    ")\n",
    "\n",
    "mchy_intensity_wo_bkd = (\n",
    "    analysis_df_trenchtimepoint_groupby.apply(\n",
    "        lambda x: local_background_subtract(x, \"RFP-Penta mean_intensity\"),\n",
    "        meta=(\"RFP-Penta mean_intensity\", float),\n",
    "    )\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    "    .to_list()\n",
    ")\n",
    "mchy_intensity_wo_bkd = {k: v for d in mchy_intensity_wo_bkd for k, v in d.items()}\n",
    "mchy_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    mchy_intensity_wo_bkd, orient=\"index\", columns=[\"RFP-Penta mean_intensity_wo_bkd\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis_df.join(mchy_intensity_wo_bkd)\n",
    "analysis_df = analysis_df.join(gfp_intensity_wo_bkd)\n",
    "analysis_df_nobkd = analysis_df[analysis_df[\"Objectid\"] != 0].persist()\n",
    "\n",
    "ratio_series = (\n",
    "    analysis_df_nobkd[\"GFP-Penta mean_intensity_wo_bkd\"]\n",
    "    / analysis_df_nobkd[\"RFP-Penta mean_intensity_wo_bkd\"]\n",
    ")\n",
    "analysis_df_nobkd[\"gfp/mchy Ratio\"] = ratio_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/GFP/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### Get GFP Call Error and Recovery Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barcode_pheno_df(phenotype_df, barcode_df, trenchid_map):\n",
    "    ##phenotype_df must contain trenchids column and a File Parquet Index\n",
    "\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "    valid_barcode_df = barcode_df[\n",
    "        barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "    ].compute()\n",
    "    barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "        lambda x: trenchid_map[x]\n",
    "    )\n",
    "\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "\n",
    "    called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "    called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "    called_df = (\n",
    "        called_df.reset_index()\n",
    "        .set_index(\"phenotype trenchid\", drop=True, sorted=False)\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "    output_df = phenotype_df.rename(columns={\"trenchid\": \"phenotype trenchid\"})\n",
    "    output_df = output_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=False\n",
    "    )\n",
    "    output_df = output_df.merge(\n",
    "        called_df, how=\"inner\", left_index=True, right_index=True\n",
    "    )\n",
    "    output_df = output_df.drop([\"Barcode Signal\"], axis=1)\n",
    "    output_df = output_df.reset_index().set_index(\"File Parquet Index\").persist()\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = get_barcode_pheno_df(analysis_df_nobkd, barcode_df, trenchid_map)\n",
    "del analysis_df_nobkd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.repartition(npartitions=500)\n",
    "output_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/2021-11-17_lDE15_Analysis\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_trench_timepoint_df = (\n",
    "    output_df.groupby(\"Trenchid Timepoint Index\")\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .set_index(\"Trenchid Timepoint Index\")\n",
    ")\n",
    "single_trench_timepoint_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/2021-11-17_lDE15_Analysis_Trench-Timepoint\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
