{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "normal-saying",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PAGA Basic Analysis (Only Main Analysis)\n",
    "\n",
    "- In this notebook, I include basic leiden clustering with no downstream timeseries analysis\n",
    "\n",
    "- Mean parameter values are projected onto the space to visualize\n",
    "\n",
    "- Distributions of genes of known function should be assembled in this notebook\n",
    "\n",
    "- Finally, basic clustering and ontology enrichment are present at the end\n",
    "\n",
    "- This notebook should be used to decide on clustering parameters and the resulting paga_df saved to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-employee",
   "metadata": {},
   "source": [
    "### Note: Next Steps\n",
    "\n",
    "- Need to rerun some of the sequencing pipeline (the ipynb) to get a proper universal sgRNAid to use in the downstream stuff. For now am hacking my way out of it but this should be done soon.\n",
    "- Co-embedding doesn't totally line up:\n",
    "    - Possibly due to differences in the calculated timestep\n",
    "    - Also might be helped by revisiting the normalization scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-breakdown",
   "metadata": {},
   "source": [
    "#### Tomorrow\n",
    "- First, rerun the sequencing pipeline bit to get sgRNAids that are universal\n",
    "\n",
    "- Rerun the analysis on the old dataset to use the same time calculation since there may be a bias in that method that may compromize the co-embedding\n",
    "    - Consider doing both permutations of the timepoint calculation\n",
    "    \n",
    "- Try co-embedding again and see if the result improves\n",
    "\n",
    "- If co-embedding looks good, proceed to a distance analysis\n",
    "\n",
    "- Once this is completed, design a final joint analysis for sgRNA phenotype categories for the second library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0e42c-092a-4cff-8132-0ace361b7c61",
   "metadata": {},
   "source": [
    "#### 2/20/2022\n",
    "- Timestamps are taken from another experiment with the same grid (Final 3)\n",
    "\n",
    "- Co-embed the full dataset (post intensity filter)\n",
    "\n",
    "- Think through and perform a distance analysis\n",
    "\n",
    "- Once this is completed, design a final joint analysis for sgRNA phenotype categories for the second library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import anndata\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import scanpy as sc\n",
    "import scipy as sp\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.sparse\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import umap\n",
    "from igraph.drawing.text import TextDrawer\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from scanpy.plotting.palettes import default_20, vega_20_scanpy\n",
    "from sklearn.cluster import AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_distances,\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    ")\n",
    "from tslearn.barycenters import (\n",
    "    dtw_barycenter_averaging,\n",
    "    euclidean_barycenter,\n",
    "    softdtw_barycenter,\n",
    ")\n",
    "from tslearn.metrics import cdist_soft_dtw, cdist_soft_dtw_normalized\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-camcorder",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-trigger",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-litigation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    n_workers_min=50,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b482e6a-478c-4bfd-9a75-9ea35f22589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-sandwich",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df_full_w_control = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-26_gene_cluster_df_no_filter.pkl\"\n",
    ")\n",
    "gene_cluster_df_full = gene_cluster_df_full_w_control.dropna(\n",
    "    subset=[\"Gene\"]\n",
    ")  # no control genes\n",
    "\n",
    "gene_cluster_df_full_w_control_2 = pd.read_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-26_gene_cluster_df_no_filter.pkl\"\n",
    ")\n",
    "gene_cluster_df_full_2 = gene_cluster_df_full_w_control_2.dropna(\n",
    "    subset=[\"Gene\"]\n",
    ")  # no control genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-equation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df_full[\"Experiment #\"] = [1 for i in range(len(gene_cluster_df_full))]\n",
    "gene_cluster_df_full_2[\"Experiment #\"] = [2 for i in range(len(gene_cluster_df_full_2))]\n",
    "gene_cluster_df_full_all = pd.concat([gene_cluster_df_full, gene_cluster_df_full_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc156719-1f0a-4227-8cd9-a37603ff0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_df_full_all = gene_cluster_df_full_all.rename(\n",
    "    columns={\n",
    "        \"Kernel Trace: Division: major_axis_length: Yeo-Johnson: z score\": \"Division Length Z-score\",\n",
    "        \"Kernel Trace: major_axis_length: Yeo-Johnson: z score\": \"Length Z-score\",\n",
    "        \"Kernel Trace: minor_axis_length: Yeo-Johnson: z score\": \"Width Z-score\",\n",
    "        \"Kernel Trace: mCherry mean_intensity: Yeo-Johnson: z score\": \"mCherry Z-score\",\n",
    "        \"Kernel Trace: Exponential Growth Rate: major_axis_length: Yeo-Johnson: z score\": \"Growth Rate Z-score\",\n",
    "        \"Kernel Trace: Division: major_axis_length\": \"Division Length\",\n",
    "        \"Kernel Trace: major_axis_length\": \"Length\",\n",
    "        \"Kernel Trace: minor_axis_length\": \"Width\",\n",
    "        \"Kernel Trace: mCherry mean_intensity\": \"mCherry\",\n",
    "        \"Kernel Trace: Exponential Growth Rate: major_axis_length\": \"Growth Rate\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02aba0-cdd7-45bc-9417-25827114d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_norm_soft_dtw(X, chunk_size=200):\n",
    "    X_dask = da.from_array(X, chunks=(chunk_size, X.shape[1], X.shape[2]))\n",
    "    soft_dtw_arr = da.blockwise(\n",
    "        cdist_soft_dtw, \"ik\", X_dask, \"itd\", X_dask, \"ktd\", concatenate=True\n",
    "    ).compute()\n",
    "    d_ii = np.diag(soft_dtw_arr)\n",
    "    norm_soft_dtw_arr = soft_dtw_arr - (\n",
    "        0.5 * (d_ii.reshape((-1, 1)) + d_ii.reshape((1, -1)))\n",
    "    )\n",
    "    return norm_soft_dtw_arr\n",
    "\n",
    "\n",
    "def norm_soft_dtw(X):\n",
    "    soft_dtw_arr = cdist_soft_dtw(X)\n",
    "    d_ii = np.diag(soft_dtw_arr)\n",
    "    norm_soft_dtw_arr = soft_dtw_arr - (\n",
    "        0.5 * (d_ii.reshape((-1, 1)) + d_ii.reshape((1, -1)))\n",
    "    )\n",
    "    return norm_soft_dtw_arr\n",
    "\n",
    "\n",
    "def inter_exp_distances(x_chunk, exp_ids=[1, 2]):\n",
    "    exp_chunk_arr = []\n",
    "    mean_exp_dist_dict = {}\n",
    "\n",
    "    exp_chunk_lens = [len(x_chunk[x_chunk[\"Experiment #\"] == i]) for i in exp_ids]\n",
    "\n",
    "    if np.all([chunk_len > 1 for chunk_len in exp_chunk_lens]):\n",
    "        for i in exp_ids:\n",
    "            exp_chunk_arr.append(\n",
    "                np.swapaxes(\n",
    "                    np.stack(\n",
    "                        x_chunk[x_chunk[\"Experiment #\"] == i][\"Feature Vector\"].values\n",
    "                    ),\n",
    "                    1,\n",
    "                    2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        exp_chunk_lens = [len(exp_chunk) for exp_chunk in exp_chunk_arr]\n",
    "        exp_chunk_intervals = np.concatenate(\n",
    "            [np.array([0]), np.add.accumulate(exp_chunk_lens)]\n",
    "        )\n",
    "\n",
    "        if np.all([chunk_len > 1 for chunk_len in exp_chunk_lens]):\n",
    "            exp_chunk_arr = np.concatenate(exp_chunk_arr)\n",
    "\n",
    "            dist_out = norm_soft_dtw(exp_chunk_arr)\n",
    "\n",
    "            for i in range(len(exp_ids)):\n",
    "                for j in range(i, len(exp_ids)):\n",
    "                    dist_section = dist_out[\n",
    "                        exp_chunk_intervals[i] : exp_chunk_intervals[i + 1],\n",
    "                        exp_chunk_intervals[j] : exp_chunk_intervals[j + 1],\n",
    "                    ]\n",
    "                    if i == j:\n",
    "                        dist_section = np.trim_zeros(\n",
    "                            np.sort(np.triu(dist_section).flatten())\n",
    "                        )\n",
    "                    else:\n",
    "                        dist_section = dist_section.flatten()\n",
    "                    mean_exp_dist_dict[(exp_ids[i], exp_ids[j])] = np.mean(dist_section)\n",
    "\n",
    "    else:\n",
    "        for i in range(len(exp_ids)):\n",
    "            for j in range(i, len(exp_ids)):\n",
    "                mean_exp_dist_dict[(exp_ids[i], exp_ids[j])] = np.NaN\n",
    "\n",
    "    return mean_exp_dist_dict\n",
    "\n",
    "\n",
    "# def vector_series_to_unrolled_df(df,vector_series_name,dims=[\"Timepoint\"]):\n",
    "#     vector_series = df[vector_series_name]\n",
    "#     vector_arr = np.array(vector_series.tolist())\n",
    "#     product_index_list = [vector_series.index.tolist()] + [list(range(vector_arr.shape[i+1])) for i in range(len(dims))]\n",
    "#     index_name_list = [df.index.name] + dims\n",
    "#     index = pd.MultiIndex.from_product(product_index_list,names=index_name_list)\n",
    "#     df_out = pd.DataFrame(data=vector_arr.flatten(),index=index,columns=[vector_series_name])\n",
    "#     return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4237a-bab2-4f8c-b757-fb137f759770",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Co-embedding Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225bd29-a03c-455b-a57c-5df981b3769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = [\n",
    "    \"oDEPool7_id\",\n",
    "    \"Closest Hamming Distance\",\n",
    "    \"EcoWG1_id\",\n",
    "    \"Gene\",\n",
    "    \"N Mismatch\",\n",
    "    \"Category\",\n",
    "    \"TargetID\",\n",
    "    \"barcodeid\",\n",
    "    \"N Observations\",\n",
    "    \"Feature Vector\",\n",
    "    \"Experiment #\",\n",
    "    \"Division Length Z-score\",\n",
    "    \"Length Z-score\",\n",
    "    \"Width Z-score\",\n",
    "    \"mCherry Z-score\",\n",
    "    \"Growth Rate Z-score\",\n",
    "    \"Division Length\",\n",
    "    \"Length\",\n",
    "    \"Width\",\n",
    "    \"mCherry\",\n",
    "    \"Growth Rate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fa86a-3fcd-4c84-ac05-d7944f75b9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df_full_all_pandas = gene_cluster_df_full_all.reset_index(drop=False)\n",
    "gene_cluster_df_full_all_pandas[\n",
    "    \"oDEPool7_id-Experiment #\"\n",
    "] = gene_cluster_df_full_all_pandas.apply(\n",
    "    lambda x: int(f'{x[\"oDEPool7_id\"]:08n}{x[\"Experiment #\"]:04n}'), axis=1\n",
    ")\n",
    "gene_cluster_df_full_all_pandas = gene_cluster_df_full_all_pandas.set_index(\n",
    "    \"oDEPool7_id-Experiment #\"\n",
    ").sort_index()\n",
    "gene_cluster_df_full_all_pandas = dd.from_pandas(\n",
    "    gene_cluster_df_full_all_pandas[columns_of_interest], npartitions=100\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95060299-699c-4378-9daa-912f01d50c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Temporary Fix\n",
    "\n",
    "Growth rate distribution is very heavy tailed, even with a z-norm. Cannot think of a way to fix at the moment, so I'm just going to clip the values to a reasonable range (-5,5) as a hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16da35-970f-4842-98d2-138e6f7b389b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df_full_all_pandas[\"Growth Rate Z-score\"] = (\n",
    "    gene_cluster_df_full_all_pandas[\"Growth Rate Z-score\"]\n",
    "    .apply(lambda x: np.clip(x, -5, 5), meta=(\"Growth Rate Z-score\", object))\n",
    "    .persist()\n",
    ")\n",
    "gene_cluster_df_full_all_pandas[\"Feature Vector\"] = (\n",
    "    gene_cluster_df_full_all_pandas[\"Feature Vector\"]\n",
    "    .apply(lambda x: np.clip(x, -5, 5), meta=(\"Feature Vector\", object))\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc625e26-852a-450c-8649-ba63c569e36f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## params\n",
    "\n",
    "vectors_to_aggregate = [\n",
    "    \"Feature Vector\",\n",
    "    \"Division Length Z-score\",\n",
    "    \"Length Z-score\",\n",
    "    \"Width Z-score\",\n",
    "    \"mCherry Z-score\",\n",
    "    \"Growth Rate Z-score\",\n",
    "    \"Division Length\",\n",
    "    \"Length\",\n",
    "    \"Width\",\n",
    "    \"mCherry\",\n",
    "    \"Growth Rate\",\n",
    "]\n",
    "\n",
    "obs_thr = 10\n",
    "strong_effect_threshold = 20\n",
    "\n",
    "gene_cluster_df_full_all_pandas_merged = gene_cluster_df_full_all_pandas.set_index(\n",
    "    \"oDEPool7_id\"\n",
    ").persist()\n",
    "gene_cluster_df_full_all_pandas_merged_groupby = (\n",
    "    gene_cluster_df_full_all_pandas_merged.groupby([\"oDEPool7_id\"])\n",
    ")\n",
    "\n",
    "sgRNA_df_merged = gene_cluster_df_full_all_pandas_merged_groupby.apply(\n",
    "    lambda x: x.iloc[0]\n",
    ").persist()\n",
    "sgRNA_df_merged = sgRNA_df_merged.compute()\n",
    "\n",
    "for vector in vectors_to_aggregate:\n",
    "    mean_vectors = gene_cluster_df_full_all_pandas_merged_groupby.apply(\n",
    "        lambda x: np.mean(x[vector], axis=0) if len(x) > obs_thr else np.NaN\n",
    "    ).compute()\n",
    "    sgRNA_df_merged[vector] = mean_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42a98c-6f3c-4f6e-aa10-29b4d224e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take mean z-scores over the timeseries\n",
    "\n",
    "traces = [\"Division Length\", \"Length\", \"Width\", \"mCherry\", \"Growth Rate\"]\n",
    "\n",
    "zscore_traces = [trace + \" Z-score\" for trace in traces]\n",
    "\n",
    "for trace in traces:\n",
    "    avg = sgRNA_df_merged.apply(lambda x: np.mean(x[trace]), axis=1)\n",
    "    sgRNA_df_merged[trace + \": Mean\"] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fe6a4-b458-46a5-80c8-a59a3e73c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zscore_trace in zscore_traces:\n",
    "    avg_zscore = sgRNA_df_merged.apply(lambda x: np.mean(x[zscore_trace]), axis=1)\n",
    "    sgRNA_df_merged[zscore_trace + \": Mean\"] = avg_zscore\n",
    "\n",
    "### Filter for strong effects by taking max over integrated zscores\n",
    "sgRNA_df_merged_nonan = sgRNA_df_merged[\n",
    "    sgRNA_df_merged[\"Feature Vector\"].apply(lambda x: ~np.any(np.isnan(x)))\n",
    "]\n",
    "\n",
    "zero_vector = np.zeros((1, sgRNA_df_merged_nonan[\"Feature Vector\"].iloc[0].shape[0]))\n",
    "feature_arr = np.array(sgRNA_df_merged_nonan[\"Feature Vector\"].tolist())\n",
    "feature_arr_abs = np.abs(feature_arr)\n",
    "sgRNA_df_merged_nonan[\"Integrated Feature Vector\"] = [\n",
    "    item for item in sp.integrate.simpson(feature_arr_abs, axis=2)\n",
    "]\n",
    "sgRNA_df_merged_nonan[\"Integrated Feature Max\"] = sgRNA_df_merged_nonan[\n",
    "    \"Integrated Feature Vector\"\n",
    "].apply(lambda x: np.max(x))\n",
    "sgRNA_df_merged_nonan[\"Integrated Euclidean Norm\"] = np.linalg.norm(\n",
    "    np.array(sgRNA_df_merged_nonan[\"Integrated Feature Vector\"].tolist()), axis=1\n",
    ")\n",
    "\n",
    "gene_cluster_df_merged_filtered = sgRNA_df_merged_nonan[\n",
    "    sgRNA_df_merged_nonan[\"Integrated Feature Max\"] >= strong_effect_threshold\n",
    "]\n",
    "min_v, max_v = np.min(sgRNA_df_merged_nonan[\"Integrated Feature Max\"]), np.percentile(\n",
    "    sgRNA_df_merged_nonan[\"Integrated Feature Max\"], 99\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Integrated Feature Max\")\n",
    "plt.hist(\n",
    "    sgRNA_df_merged_nonan[\n",
    "        sgRNA_df_merged_nonan[\"Integrated Feature Max\"] < strong_effect_threshold\n",
    "    ][\"Integrated Feature Max\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.hist(\n",
    "    sgRNA_df_merged_nonan[\n",
    "        sgRNA_df_merged_nonan[\"Integrated Feature Max\"] >= strong_effect_threshold\n",
    "    ][\"Integrated Feature Max\"].tolist(),\n",
    "    bins=50,\n",
    "    range=(min_v, max_v),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "unique_genes, gene_counts = np.unique(\n",
    "    gene_cluster_df_merged_filtered[\"Gene\"][\n",
    "        gene_cluster_df_merged_filtered[\"Gene\"].apply(lambda x: type(x) == str)\n",
    "    ].tolist(),\n",
    "    return_counts=True,\n",
    ")\n",
    "plt.title(\"sgRNAs per Gene\")\n",
    "plt.xticks(range(0, 20, 2), labels=range(0, 20, 2))\n",
    "plt.hist(gene_counts, bins=np.arange(20) - 0.5)\n",
    "plt.show()\n",
    "\n",
    "### soft-DTW Calculation\n",
    "\n",
    "X = np.array(gene_cluster_df_merged_filtered[\"Feature Vector\"].tolist())\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "norm_soft_dtw_arr = parallel_norm_soft_dtw(X)\n",
    "\n",
    "### Initialize Anndata Object\n",
    "an_df_merged = anndata.AnnData(\n",
    "    X=X.reshape(X.shape[0], -1), obs=gene_cluster_df_merged_filtered\n",
    ")  # AnnData container to use scanpy functions with unwrapped time vector\n",
    "\n",
    "### Compute KNN Graph\n",
    "n_neighbors = 15\n",
    "n_pcs = 20  # This shouldn't affect anything\n",
    "\n",
    "sc.pp.neighbors(an_df_merged, n_neighbors=n_neighbors, n_pcs=n_pcs)\n",
    "knn_indices, knn_dists, forest = sc.neighbors.compute_neighbors_umap(\n",
    "    norm_soft_dtw_arr, n_neighbors=n_neighbors, metric=\"precomputed\"\n",
    ")\n",
    "(\n",
    "    an_df_merged.uns[\"neighbors\"][\"distances\"],\n",
    "    an_df_merged.uns[\"neighbors\"][\"connectivities\"],\n",
    ") = sc.neighbors._compute_connectivities_umap(\n",
    "    knn_indices,\n",
    "    knn_dists,\n",
    "    an_df_merged.shape[0],\n",
    "    n_neighbors,  # change to neighbors you plan to use\n",
    ")\n",
    "an_df_merged.obsp[\"distances\"] = an_df_merged.uns[\"neighbors\"][\"distances\"]\n",
    "an_df_merged.obsp[\"connectivities\"] = an_df_merged.uns[\"neighbors\"][\"connectivities\"]\n",
    "an_df_merged.obsp[\"soft_dtw\"] = norm_soft_dtw_arr\n",
    "\n",
    "### Computing Leiden, PAGA and UMAP\n",
    "min_dist = 0.1\n",
    "spread = 5.0\n",
    "# spread = 1.\n",
    "\n",
    "paga_df_merged_dict = {}\n",
    "for resolution in [0.15, 0.25, 1.0, 1.5, 3.0]:\n",
    "    paga_df_merged_dict[resolution] = copy.deepcopy(an_df_merged)\n",
    "    sc.tl.leiden(\n",
    "        paga_df_merged_dict[resolution], resolution=resolution, n_iterations=-1\n",
    "    )\n",
    "    sc.tl.paga(paga_df_merged_dict[resolution], groups=\"leiden\")\n",
    "    sc.pl.paga(paga_df_merged_dict[resolution], add_pos=True, show=False)\n",
    "sc.tl.umap(paga_df_merged_dict[1.0], init_pos=\"paga\", min_dist=min_dist, spread=spread)\n",
    "paga_df_merged_dict[1.0].obs[\"leiden_ultralowres\"] = paga_df_merged_dict[0.15].obs[\n",
    "    \"leiden\"\n",
    "]\n",
    "paga_df_merged_dict[1.0].obs[\"leiden_lowres\"] = paga_df_merged_dict[0.25].obs[\"leiden\"]\n",
    "paga_df_merged_dict[1.0].obs[\"leiden_highres\"] = paga_df_merged_dict[1.5].obs[\"leiden\"]\n",
    "paga_df_merged_dict[1.0].obs[\"leiden_ultrahighres\"] = paga_df_merged_dict[3.0].obs[\n",
    "    \"leiden\"\n",
    "]\n",
    "paga_df_merged = paga_df_merged_dict[1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abe21b-5d68-484c-8516-1b86d5f840c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plotting Mean Z-scores, Euclidean Norm, and N Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e81ad6-9f87-4335-9005-20caa581422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paga_df_merged.obs[\"N Match\"] = 20.0 - paga_df_merged.obs[\"N Mismatch\"]\n",
    "del_N_match_series = paga_df_merged.obs.groupby(\"TargetID\").apply(\n",
    "    lambda x: x[\"N Match\"] - np.min(x[\"N Match\"])\n",
    ")\n",
    "del_N_match_series = del_N_match_series.droplevel(\"TargetID\")\n",
    "paga_df_merged.obs[\"Delta N Match\"] = del_N_match_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e355eef-6386-4b0f-95e8-1bd1094630fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [zscore_trace + \": Mean\" for zscore_trace in zscore_traces][1:-1]\n",
    "\n",
    "fig = sc.pl.umap(\n",
    "    paga_df_merged,\n",
    "    color=labels,\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    add_outline=False,\n",
    "    size=50,\n",
    "    return_fig=True,\n",
    "    vcenter=0.0,\n",
    "    vmin=-2.5,\n",
    "    vmax=2.5,\n",
    "    cmap=\"RdBu_r\",\n",
    "    wspace=0.25,\n",
    "    ncols=2,\n",
    ")\n",
    "\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(ax.get_title(), fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "\n",
    "# fig.savefig(\"./Mean_zscores.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b53bb-ddb9-4945-9507-daaa10f04d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [trace + \": Mean\" for trace in traces][1:-1]\n",
    "\n",
    "fig = sc.pl.umap(\n",
    "    paga_df_merged,\n",
    "    color=labels,\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    add_outline=False,\n",
    "    size=50,\n",
    "    return_fig=True,\n",
    "    cmap=\"RdBu_r\",\n",
    "    wspace=0.25,\n",
    "    ncols=3,\n",
    ")\n",
    "\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(ax.get_title(), fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "\n",
    "# fig.savefig(\"./Mean_zscores.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9879e9e-8876-471a-94a6-3a1533040025",
   "metadata": {},
   "source": [
    "Growth rate is pretty messed up. I need to figure out what is going on there. It's probably dominated by outliers to be honest, median pooling at an early stage could help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf719ab-8643-4e1c-8850-e084035e2bcf",
   "metadata": {},
   "source": [
    "### Plot Fiducial Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d675c15-0e9e-46fb-9b77-53e338c3ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import goatools\n",
    "import goatools.base\n",
    "from goatools.anno.gaf_reader import GafReader\n",
    "from goatools.base import download_go_basic_obo\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "from goatools.obo_parser import GODag\n",
    "from goatools.semantic import TermCounts, get_info_content, semantic_similarity\n",
    "\n",
    "\n",
    "def search_go(ns2assoc, obodag, inv_gene_to_id, go_term):\n",
    "    namespace_abbv = {\n",
    "        \"biological_process\": \"BP\",\n",
    "        \"molecular_function\": \"MF\",\n",
    "        \"cellular_component\": \"CC\",\n",
    "    }\n",
    "\n",
    "    print(\"Searching for \" + str(obodag[go_term].name))\n",
    "    namespace = namespace_abbv[obodag[go_term].namespace]\n",
    "    child_goterms = list(obodag[go_term].get_all_children())\n",
    "    gene_list = [\n",
    "        inv_gene_to_id[key]\n",
    "        for key, val in ns2assoc[namespace].items()\n",
    "        if go_term in val\n",
    "    ]\n",
    "    for child_goterm in child_goterms:\n",
    "        gene_list += [\n",
    "            inv_gene_to_id[key]\n",
    "            for key, val in ns2assoc[namespace].items()\n",
    "            if child_goterm in val\n",
    "        ]\n",
    "    gene_list = sorted(list(set(gene_list)))\n",
    "    return gene_list\n",
    "\n",
    "\n",
    "def selection_fn(item, gene_name):\n",
    "    is_gene = item[\"Gene\"] == gene_name\n",
    "    if is_gene:\n",
    "        return item[\"TargetID\"]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def highlight_gene_group(an_df, selection_list):\n",
    "    highlight_genes_df = copy.deepcopy(an_df)\n",
    "\n",
    "    selection_list = sorted(\n",
    "        list(\n",
    "            set(highlight_genes_df.obs[\"Gene\"].unique().tolist()) & set(selection_list)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, selected_gene in enumerate(selection_list):\n",
    "        selected_series = (highlight_genes_df.obs[\"Gene\"] == selected_gene).astype(\n",
    "            \"category\"\n",
    "        )\n",
    "        selected_series = selected_series.cat.reorder_categories([True, False])\n",
    "        highlight_genes_df.obs[\"Selected Genes: \" + str(i)] = selected_series\n",
    "\n",
    "    selected_series = (highlight_genes_df.obs[\"Gene\"].isin(selection_list)).astype(\n",
    "        \"category\"\n",
    "    )\n",
    "    selected_series = selected_series.cat.reorder_categories([True, False])\n",
    "    highlight_genes_df.obs[\"All Genes\"] = selected_series\n",
    "\n",
    "    # selected_series = (paga_df.obs[\"Gene\"]==\"ftsZ\").astype(float)\n",
    "    # selected_series[selected_series==0.] = np.NaN\n",
    "    # paga_df.obs[\"Selected Genes\"] = selected_series\n",
    "\n",
    "    fig = sc.pl.umap(\n",
    "        highlight_genes_df,\n",
    "        title=selection_list + [\"All Genes\"],\n",
    "        color=[\"Selected Genes: \" + str(i) for i in range(len(selection_list))]\n",
    "        + [\"All Genes\"],\n",
    "        groups=[True],\n",
    "        show=False,\n",
    "        legend_loc=\"right margin\",\n",
    "        add_outline=False,\n",
    "        size=50,\n",
    "        return_fig=True,\n",
    "        palette={True: \"red\", False: \"lightgrey\"},\n",
    "    )  # palette ={}\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def highlight_sgrnas(an_df, selection_list):\n",
    "    highlight_genes_df = copy.deepcopy(an_df)\n",
    "    highlight_genes_df.obs[\"tempindex\"] = highlight_genes_df.obs.index\n",
    "\n",
    "    selection_list = sorted(\n",
    "        list(\n",
    "            set(highlight_genes_df.obs[\"tempindex\"].unique().tolist())\n",
    "            & set(selection_list)\n",
    "        )\n",
    "    )\n",
    "    selected_series = (highlight_genes_df.obs[\"tempindex\"].isin(selection_list)).astype(\n",
    "        \"category\"\n",
    "    )\n",
    "    selected_series = selected_series.cat.reorder_categories([True, False])\n",
    "    highlight_genes_df.obs[\"All sgRNAs\"] = selected_series\n",
    "\n",
    "    # selected_series = (paga_df.obs[\"Gene\"]==\"ftsZ\").astype(float)\n",
    "    # selected_series[selected_series==0.] = np.NaN\n",
    "    # paga_df.obs[\"Selected Genes\"] = selected_series\n",
    "\n",
    "    fig = sc.pl.umap(\n",
    "        highlight_genes_df,\n",
    "        title=\"All sgRNAs\",\n",
    "        color=\"All sgRNAs\",\n",
    "        groups=[True],\n",
    "        show=False,\n",
    "        legend_loc=\"right margin\",\n",
    "        add_outline=False,\n",
    "        size=50,\n",
    "        return_fig=True,\n",
    "        palette={True: \"red\", False: \"lightgrey\"},\n",
    "    )  # palette ={}\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae3a4d-725e-47b7-b9c5-df27bf0c8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ontologies\n",
    "obo_fname = download_go_basic_obo()\n",
    "\n",
    "# Get ecoli association file (ecocyc)\n",
    "gaf_handle = goatools.base.http_get(\n",
    "    \"http://current.geneontology.org/annotations/ecocyc.gaf.gz\", fout=\"./ecocyc.gaf.gz\"\n",
    ")\n",
    "gaf_fname = goatools.base.gunzip(\"./ecocyc.gaf.gz\")\n",
    "\n",
    "## Getting ontologies and other nonesense\n",
    "\n",
    "obodag = GODag(obo_fname)\n",
    "objanno = GafReader(gaf_fname)\n",
    "ns2assoc = objanno.get_ns2assc()\n",
    "\n",
    "gene_to_id = {assoc.DB_Symbol: assoc.DB_ID for assoc in objanno.associations}\n",
    "inv_gene_to_id = {assoc.DB_ID: assoc.DB_Symbol for assoc in objanno.associations}\n",
    "synonym_dict = {\n",
    "    synonym: assoc.DB_ID\n",
    "    for assoc in objanno.associations\n",
    "    for synonym in assoc.DB_Synonym\n",
    "}\n",
    "gene_to_id.update(synonym_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9b51e-e483-4211-bc4a-520b61f61fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = highlight_sgrnas(\n",
    "    paga_df_merged,\n",
    "    paga_df_merged.obs[\n",
    "        paga_df_merged.obs[\"Gene\"].apply(lambda x: \"fts\" in x)\n",
    "    ].index.tolist(),\n",
    ")\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(\"fts Genes\", fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "fig.savefig(\"./fts_genes.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753ba08-7d34-4cbd-a0fe-263d90380beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = highlight_sgrnas(\n",
    "    paga_df_merged,\n",
    "    paga_df_merged.obs[\n",
    "        paga_df_merged.obs[\"Gene\"].apply(\n",
    "            lambda x: (\"rps\" in x) | (\"rpm\" in x) | (\"rpl\" in x)\n",
    "        )\n",
    "    ].index.tolist(),\n",
    ")\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(\"Ribosomal Protein Genes\", fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "fig.savefig(\"./ribosomal_protein_genes.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a1467-dbdb-4ab8-8044-e916662678dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_aminoacylation_genes = search_go(ns2assoc, obodag, inv_gene_to_id, \"GO:0043039\")\n",
    "\n",
    "fig = highlight_sgrnas(\n",
    "    paga_df_merged,\n",
    "    paga_df_merged.obs[\n",
    "        paga_df_merged.obs[\"Gene\"].apply(lambda x: x in tRNA_aminoacylation_genes)\n",
    "    ].index.tolist(),\n",
    ")\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(\"tRNA Aminoacetylation Genes\", fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "fig.savefig(\"./tRNA_aminoacetylation_genes.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c1821-e1aa-4f82-be2d-8662f8a6564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = highlight_sgrnas(\n",
    "    paga_df_merged,\n",
    "    paga_df_merged.obs[\n",
    "        paga_df_merged.obs[\"Gene\"].apply(lambda x: \"murI\" in x)\n",
    "    ].index.tolist(),\n",
    ")\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(\"murI\", fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "fig.savefig(\"./murI.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481d644-8059-4b01-be24-2d4cacc7b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "rne_df = paga_df_merged.obs[paga_df_merged.obs[\"Gene\"] == \"rne\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928feff2-b55a-4f77-916e-d67e9cae1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnaa_df = paga_df_merged.obs[paga_df_merged.obs[\"Gene\"] == \"dnaA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57839680-63f9-4001-8844-6ddbb9852555",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnab_df = paga_df_merged.obs[paga_df_merged.obs[\"Gene\"] == \"dnaB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576d138-1763-4ea6-88d8-0310b1b2b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cdd05-c94d-4d4e-b1f1-d79a03bce98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_targetid = rne_df[rne_df[\"TargetID\"] == 1828]\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Length\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\", label=\"rne\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\")\n",
    "\n",
    "selected_targetid = dnaa_df[dnaa_df[\"TargetID\"] == 811]\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Length\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C1\", label=\"dnaA\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C1\")\n",
    "\n",
    "selected_targetid = dnab_df\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Length\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C2\", label=\"dnaB\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C2\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.ylabel(\"Length (um)\", size=16)\n",
    "plt.xlabel(\"Timepoint (mins)\", size=16)\n",
    "plt.xticks(size=12)\n",
    "plt.yticks(size=12)\n",
    "# plt.savefig(\"./rne_dnaa_length.png\",dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83208808-ece6-4da9-ae64-2cce14f0f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_targetid = rne_df[rne_df[\"TargetID\"] == 1828]\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Width\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\", label=\"rne\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\")\n",
    "\n",
    "selected_targetid = dnaa_df[dnaa_df[\"TargetID\"] == 811]\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Width\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C1\", label=\"dnaA\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C1\")\n",
    "\n",
    "selected_targetid = dnab_df\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Width\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C2\", label=\"dnaB\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C2\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.ylabel(\"Width (um)\", size=16)\n",
    "plt.xlabel(\"Timepoint (mins)\", size=16)\n",
    "plt.xticks(size=12)\n",
    "plt.yticks(size=12)\n",
    "plt.savefig(\"./rne_dnaa_width.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e541f6-cb47-437d-893c-26b8ef8fb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_targetid = rne_df[rne_df[\"TargetID\"] == 1828]\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"mCherry\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\", label=\"rne\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\")\n",
    "\n",
    "selected_targetid = dnaa_df[dnaa_df[\"TargetID\"] == 811]\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"mCherry\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C1\", label=\"dnaA\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C1\")\n",
    "\n",
    "selected_targetid = dnab_df\n",
    "for i in range(len(selected_targetid)):\n",
    "    row = selected_targetid.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"mCherry\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C2\", label=\"dnaB\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C2\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.ylabel(\"pRpsL-mCherry (AU)\", size=16)\n",
    "plt.xlabel(\"Timepoint (mins)\", size=16)\n",
    "plt.xticks(size=12)\n",
    "plt.yticks(size=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./rne_dnaa_mchy.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26b6a7-4468-4d1c-beae-ec3ccc2f9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "muri_df = paga_df_merged.obs[paga_df_merged.obs[\"Gene\"] == \"murI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506d786-897c-4c95-ab60-d72afb20588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "muri_df[\"Width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce40ec6-853b-4879-87d5-a01532474a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(muri_df)):\n",
    "    row = muri_df.iloc[i]\n",
    "    n_mismatch = int(row[\"N Mismatch\"])\n",
    "    trace = row[\"Width\"]\n",
    "    if i == 0:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\", label=\"murI\")\n",
    "    else:\n",
    "        plt.plot([i * 30 for i in range(20)], trace, c=\"C0\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.ylabel(\"Width (um)\", size=16)\n",
    "plt.xlabel(\"Timepoint (mins)\", size=16)\n",
    "plt.xticks(size=12)\n",
    "plt.yticks(size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567e463-3cde-4f52-990e-83a399942c11",
   "metadata": {},
   "source": [
    "### Plot Clusters\n",
    "- Couple it with lists from relevant clusters at single resolution and include in figure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c0b08-980a-4657-ad5e-396334c5d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.umap(\n",
    "    paga_df_merged,\n",
    "    color=[\"leiden_ultrahighres\", \"leiden\"],\n",
    "    title=[\"Leiden Resolution=3.\", \"Leiden Resolution=1.\"],\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    edges=False,\n",
    "    add_outline=False,\n",
    "    size=100,\n",
    "    return_fig=True,\n",
    "    palette=vega_20_scanpy,\n",
    "    ncols=2,\n",
    ")\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(ax.get_title(), fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "# fig.savefig(\"./Global_PAGA.png\",dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc955e-e2fa-4864-89c9-749a351f1e03",
   "metadata": {},
   "source": [
    "### Get Cluster Gene Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e93f0-127e-4817-8c27-272b49792f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import goatools\n",
    "import goatools.base\n",
    "from goatools.anno.gaf_reader import GafReader\n",
    "from goatools.base import download_go_basic_obo\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "from goatools.obo_parser import GODag\n",
    "from goatools.semantic import TermCounts, get_info_content, semantic_similarity\n",
    "\n",
    "\n",
    "def search_go(ns2assoc, obodag, inv_gene_to_id, go_term):\n",
    "    namespace_abbv = {\n",
    "        \"biological_process\": \"BP\",\n",
    "        \"molecular_function\": \"MF\",\n",
    "        \"cellular_component\": \"CC\",\n",
    "    }\n",
    "\n",
    "    print(\"Searching for \" + str(obodag[go_term].name))\n",
    "    namespace = namespace_abbv[obodag[go_term].namespace]\n",
    "    child_goterms = list(obodag[go_term].get_all_children())\n",
    "    gene_list = [\n",
    "        inv_gene_to_id[key]\n",
    "        for key, val in ns2assoc[namespace].items()\n",
    "        if go_term in val\n",
    "    ]\n",
    "    for child_goterm in child_goterms:\n",
    "        gene_list += [\n",
    "            inv_gene_to_id[key]\n",
    "            for key, val in ns2assoc[namespace].items()\n",
    "            if child_goterm in val\n",
    "        ]\n",
    "    gene_list = sorted(list(set(gene_list)))\n",
    "    return gene_list\n",
    "\n",
    "\n",
    "def get_enriched_GO_terms(\n",
    "    background_gene_list, gene_list, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    "):\n",
    "    gene_to_id = {assoc.DB_Symbol: assoc.DB_ID for assoc in objanno.associations}\n",
    "    synonym_dict = {\n",
    "        synonym: assoc.DB_ID\n",
    "        for assoc in objanno.associations\n",
    "        for synonym in assoc.DB_Synonym\n",
    "    }\n",
    "    gene_to_id.update(synonym_dict)\n",
    "\n",
    "    # background gene set\n",
    "\n",
    "    all_genes_uniprot = [\n",
    "        gene_to_id[item] for item in background_gene_list if item in gene_to_id.keys()\n",
    "    ]\n",
    "    selected_genes_uniprot = [\n",
    "        gene_to_id[item] for item in gene_list if item in gene_to_id.keys()\n",
    "    ]\n",
    "\n",
    "    print(len(all_genes_uniprot))\n",
    "    print(len(selected_genes_uniprot))\n",
    "\n",
    "    goeaobj = GOEnrichmentStudy(\n",
    "        all_genes_uniprot,  # List of mouse protein-coding genes\n",
    "        ns2assoc[GO_type],  # geneid/GO associations\n",
    "        obodag,  # Ontologies\n",
    "        propagate_counts=True,\n",
    "        alpha=pval,  # default significance cut-off\n",
    "        methods=[\"fdr_bh\"],\n",
    "    )\n",
    "    # defult multipletest correction method\n",
    "\n",
    "    goea_results_all = goeaobj.run_study(selected_genes_uniprot, prt=None)\n",
    "    goea_quiet_sig = [r for r in goea_results_all if r.p_fdr_bh < pval]\n",
    "    goea_quiet_enriched = [r for r in goea_quiet_sig if r.enrichment == \"e\"]\n",
    "    return goea_quiet_enriched\n",
    "\n",
    "\n",
    "def pick_exemplar(go1, go2, termcounts, obodag, info_thr, pval_factor=2.0):\n",
    "    info_1_low = get_info_content(go1.GO, termcounts) < info_thr\n",
    "    info_2_low = get_info_content(go2.GO, termcounts) < info_thr\n",
    "    if info_1_low and not info_2_low:\n",
    "        return go2\n",
    "    elif info_2_low and not info_1_low:\n",
    "        return go1\n",
    "    elif info_2_low and info_1_low:\n",
    "        return go1\n",
    "\n",
    "    pval_ratio = go1.p_fdr_bh / go2.p_fdr_bh\n",
    "\n",
    "    if pval_ratio > pval_factor:\n",
    "        return go2\n",
    "    elif pval_ratio < (1.0 / pval_factor):\n",
    "        return go1\n",
    "\n",
    "    go1_parents = list(obodag[go1.GO].get_all_parents())\n",
    "    go2_parents = list(obodag[go2.GO].get_all_parents())\n",
    "\n",
    "    if go2.GO in go1_parents:\n",
    "        return go2\n",
    "\n",
    "    elif go1.GO in go2_parents:\n",
    "        return go1\n",
    "\n",
    "    return go1\n",
    "\n",
    "\n",
    "def get_filtered_go_terms(\n",
    "    obodag, objanno, goea_list, sim_thr=0.05, info_thr=1.0, GO_type=\"BP\"\n",
    "):\n",
    "    termcounts = TermCounts(obodag, objanno.get_ns2assc()[GO_type])\n",
    "\n",
    "    go_term_list = [item.GO for item in goea_list]\n",
    "    sim_arr = np.zeros((len(go_term_list), len(go_term_list)))\n",
    "    for i in range(len(go_term_list)):\n",
    "        for j in range(len(go_term_list)):\n",
    "            sim_arr[i, j] = semantic_similarity(\n",
    "                go_term_list[i], go_term_list[j], obodag\n",
    "            )\n",
    "    np.fill_diagonal(sim_arr, 0.0)\n",
    "\n",
    "    working_group_idx = 0\n",
    "    grouped_terms = {}\n",
    "    group_exemplars = {}\n",
    "    go_term_indices = list(range(len(go_term_list)))\n",
    "\n",
    "    while len(go_term_indices) > 0:\n",
    "        i = go_term_indices[0]\n",
    "        most_sim_arg = np.argmax(sim_arr[i])\n",
    "        sim_score = sim_arr[i, most_sim_arg]\n",
    "        if sim_score > sim_thr:\n",
    "            if len(grouped_terms) > 0:\n",
    "                in_other_group_keys = [\n",
    "                    key for key, val in grouped_terms.items() if most_sim_arg in val\n",
    "                ]\n",
    "                if len(in_other_group_keys) == 1:\n",
    "                    other_group_idx = in_other_group_keys[0]\n",
    "                    grouped_terms[other_group_idx] = grouped_terms[other_group_idx] + [\n",
    "                        i\n",
    "                    ]\n",
    "                    group_exemplars[other_group_idx] = pick_exemplar(\n",
    "                        group_exemplars[other_group_idx],\n",
    "                        goea_list[i],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                else:\n",
    "                    grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                    group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                        goea_list[i],\n",
    "                        goea_list[most_sim_arg],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                    working_group_idx += 1\n",
    "                    go_term_indices.remove(most_sim_arg)\n",
    "            else:\n",
    "                grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                    goea_list[i], goea_list[most_sim_arg], termcounts, obodag, info_thr\n",
    "                )\n",
    "                working_group_idx += 1\n",
    "                go_term_indices.remove(most_sim_arg)\n",
    "        go_term_indices.remove(i)\n",
    "\n",
    "    group_exemplars = list(group_exemplars.values())\n",
    "\n",
    "    return group_exemplars\n",
    "\n",
    "\n",
    "def get_GO_assign_dict(selected_goea, cluster_genes_uniprot):\n",
    "    all_study_items = copy.copy(cluster_genes_uniprot)\n",
    "    depth_list = sorted(set([item.depth for item in selected_goea]))[::-1]\n",
    "    assign_dict = {}\n",
    "    for depth in depth_list:\n",
    "        go_terms_at_level = [item for item in selected_goea if item.depth == depth]\n",
    "        for go_term in go_terms_at_level:\n",
    "            study_item_list = list(go_term.study_items)\n",
    "            for study_item in study_item_list:\n",
    "                if study_item in all_study_items:\n",
    "                    assign_dict[study_item] = go_term.name\n",
    "                    all_study_items.remove(study_item)\n",
    "\n",
    "    for remaining_item in all_study_items:\n",
    "        assign_dict[remaining_item] = \"Unassigned\"\n",
    "\n",
    "    return assign_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad87864-adb9-4331-9ff6-452ed277552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes = paga_df_merged.obs[\"Gene\"].unique().tolist()\n",
    "\n",
    "clust_id = 8\n",
    "cluster_name = \"leiden_ultrahighres\"\n",
    "\n",
    "clust_id = str(clust_id)\n",
    "cluster_genes = sorted(\n",
    "    paga_df_merged.obs[paga_df_merged.obs[cluster_name] == clust_id][\"Gene\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "goea_quiet_enriched = get_enriched_GO_terms(\n",
    "    all_genes, cluster_genes, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    ")\n",
    "filtered_go_terms = get_filtered_go_terms(\n",
    "    obodag, objanno, goea_quiet_enriched, sim_thr=0.3, info_thr=1.0\n",
    ")\n",
    "go_term_dict = {\n",
    "    go_term.name: go_term.ratio_in_study[0] for go_term in filtered_go_terms\n",
    "}\n",
    "# ttl_terms = np.sum(list(go_term_dict.values()))\n",
    "# go_term_dict = {key:val/ttl_terms for key,val in go_term_dict.items()}\n",
    "\n",
    "print()\n",
    "for key, value in go_term_dict.items():\n",
    "    print(key, \" : \", value)\n",
    "print()\n",
    "for i in range(0, len(cluster_genes), 5):\n",
    "    print(cluster_genes[i : i + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4506d-6b45-4b10-bb33-144a6efbd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.umap(\n",
    "    paga_df_merged,\n",
    "    color=[\"leiden_ultrahighres\", \"leiden\"],\n",
    "    title=[\"Leiden Resolution=3.\", \"Leiden Resolution=1.\"],\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    edges=False,\n",
    "    add_outline=False,\n",
    "    size=100,\n",
    "    return_fig=True,\n",
    "    palette=vega_20_scanpy,\n",
    "    ncols=2,\n",
    ")\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.set_title(ax.get_title(), fontsize=18)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "# fig.savefig(\"./Global_PAGA.png\",dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431da39-6ac1-48cb-9e3a-d9e29f0d9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id = 8\n",
    "cluster_name = \"leiden_ultrahighres\"\n",
    "\n",
    "clust_id = str(clust_id)\n",
    "df_subset = paga_df_merged.obs[paga_df_merged.obs[cluster_name] == clust_id]\n",
    "prop_arr = np.array(\n",
    "    [np.array(df_subset[prop].tolist()) for prop in [\"Length\", \"Width\", \"mCherry\"]]\n",
    ")\n",
    "mean_traces = np.mean(prop_arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95deeaa5-8e8c-4418-878e-becf39663ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_subset[df_subset[\"Gene\"] == \"def\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8c5d6-e121-4b3f-b205-7ecee5f10463",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_traces[0])\n",
    "plt.show()\n",
    "plt.plot(mean_traces[1])\n",
    "plt.show()\n",
    "plt.plot(mean_traces[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b442266-41c1-45cc-a0e0-0434c815a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_arr = np.array(\n",
    "    [np.array(df_subset[prop].tolist()) for prop in [\"Length\", \"Width\", \"mCherry\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307723e8-098a-4854-a18c-d8a72cad8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab223e57-43ff-45c4-a1b3-f9cce280448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_i = 0\n",
    "sgrna_j = 93\n",
    "plt.plot(prop_arr[prop_i, sgrna_j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f11e1-c77e-49b0-92e6-48baa8138b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.iloc[93]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d98aad-c3ba-454d-bb44-958b98ab1a13",
   "metadata": {},
   "source": [
    "### Quick Transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesScalerMinMax\n",
    "\n",
    "\n",
    "def plot_cluster_timeseries(\n",
    "    df,\n",
    "    cluster_label,\n",
    "    feature_labels,\n",
    "    displayed_labels,\n",
    "    feature_range_list,\n",
    "    agg_fn=np.mean,\n",
    "    x_ticks=[0, 10, 20],\n",
    "    cluster_subset=None,\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "    hspace=0.0,\n",
    "    fontsize=14,\n",
    "    linewidth=5,\n",
    "    color_list=None,\n",
    "):\n",
    "    if cluster_subset is not None:\n",
    "        df = copy.copy(df)\n",
    "        df = df[df[cluster_label].isin(cluster_subset)]\n",
    "\n",
    "    timeseries_list = []\n",
    "    for feature_label in feature_labels:\n",
    "        agg_cluster_timeseries = (\n",
    "            df.groupby([cluster_label])\n",
    "            .apply(lambda x: agg_fn(np.array(x[feature_label].tolist()), axis=0))\n",
    "            .to_frame()\n",
    "        )\n",
    "        agg_cluster_timeseries = agg_cluster_timeseries.rename(\n",
    "            columns={0: feature_label}\n",
    "        )\n",
    "        timeseries_list.append(agg_cluster_timeseries)\n",
    "    timeseries_df = pd.concat(timeseries_list, axis=1)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(timeseries_df), wspace=wspace)\n",
    "\n",
    "    for i in range(len(timeseries_df)):\n",
    "        mean_cluster_timeseries = timeseries_df.iloc[i]\n",
    "\n",
    "        clust_arr = np.array(timeseries_df.iloc[i].tolist())\n",
    "        if color_list == None:\n",
    "            color = \"tab:blue\"\n",
    "        else:\n",
    "            color = color_list[i]\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0], 1, wspace=0, hspace=hspace\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                feature_range = feature_range_list[c[0]]\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth, color=color)\n",
    "                ax.set_ylim(feature_range[0], feature_range[1])\n",
    "                ax.set(xticks=[])\n",
    "                #                 ax.set(xticks=[], yticks=[0,6])\n",
    "                ax.set_ylabel(\n",
    "                    displayed_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(str(i), fontsize=fontsize)\n",
    "            ax.set(xticks=x_ticks)\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0], 1, wspace=0, hspace=hspace\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                feature_range = feature_range_list[c[0]]\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth, color=color)\n",
    "                ax.set_ylim(feature_range[0], feature_range[1])\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(str(i), fontsize=fontsize)\n",
    "            ax.set(xticks=x_ticks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_braycenters(df, columns=None, max_iter=50, tol=0.001):\n",
    "    df = df[columns]\n",
    "    X = np.array(df.apply(lambda x: x.tolist(), axis=1).tolist())\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "\n",
    "    Y = softdtw_barycenter(X, max_iter=max_iter, tol=tol)  # T X D\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "def plot_cluster_timeseries_braycenters(\n",
    "    df,\n",
    "    cluster_label,\n",
    "    feature_labels,\n",
    "    displayed_labels,\n",
    "    feature_range_list,\n",
    "    x_ticks=[0, 10, 20],\n",
    "    cluster_subset=None,\n",
    "    figsize=(10, 10),\n",
    "    wspace=0.0,\n",
    "    hspace=0.0,\n",
    "    fontsize=14,\n",
    "    linewidth=3,\n",
    "    color_list=None,\n",
    "):\n",
    "    if cluster_subset is not None:\n",
    "        df = copy.copy(df)\n",
    "        df = df[df[cluster_label].isin(cluster_subset)]\n",
    "\n",
    "    cluster_groupby = df.groupby([cluster_label])\n",
    "    agg_cluster_timeseries = cluster_groupby.apply(\n",
    "        lambda x: get_braycenters(x, columns=feature_labels)\n",
    "    )\n",
    "\n",
    "    timeseries_list = []\n",
    "    for i, feature_label in enumerate(feature_labels):\n",
    "        selected_feature_agg = agg_cluster_timeseries.apply(\n",
    "            lambda x: x[:, i]\n",
    "        ).to_frame()\n",
    "        selected_feature_agg = selected_feature_agg.rename(columns={0: feature_label})\n",
    "        timeseries_list.append(selected_feature_agg)\n",
    "\n",
    "    timeseries_df = pd.concat(timeseries_list, axis=1)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, len(timeseries_df), wspace=wspace)\n",
    "\n",
    "    for i in range(len(timeseries_df)):\n",
    "        mean_cluster_timeseries = timeseries_df.iloc[i]\n",
    "\n",
    "        clust_arr = np.array(timeseries_df.iloc[i].tolist())\n",
    "        if color_list == None:\n",
    "            color = \"tab:blue\"\n",
    "        else:\n",
    "            color = color_list[i]\n",
    "\n",
    "        if i == 0:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0], 1, wspace=0, hspace=hspace\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                feature_range = feature_range_list[c[0]]\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth, color=color)\n",
    "                ax.set_ylim(feature_range[0], feature_range[1])\n",
    "                ax.set(xticks=[])\n",
    "                #                 ax.set(xticks=[], yticks=[0,6])\n",
    "                ax.set_ylabel(\n",
    "                    displayed_labels[c[0]],\n",
    "                    rotation=0,\n",
    "                    labelpad=30,\n",
    "                    fontsize=fontsize,\n",
    "                    ha=\"right\",\n",
    "                )  # ,orientation=\"horizontal\")\n",
    "\n",
    "            ax.set_xlabel(str(i), fontsize=fontsize)\n",
    "            ax.set(xticks=x_ticks)\n",
    "\n",
    "        else:\n",
    "            inner_gs = gs[0, i].subgridspec(\n",
    "                clust_arr.shape[0], 1, wspace=0, hspace=hspace\n",
    "            )\n",
    "            inner_grid_sub = inner_gs.subplots(sharex=True)\n",
    "            for c, ax in np.ndenumerate(inner_grid_sub):\n",
    "                feature_range = feature_range_list[c[0]]\n",
    "                ax.plot(clust_arr[c], linewidth=linewidth, color=color)\n",
    "                ax.set_ylim(feature_range[0], feature_range[1])\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "\n",
    "            ax.set_xlabel(str(i), fontsize=fontsize)\n",
    "            ax.set(xticks=x_ticks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2bff1-69cb-49e6-8f61-78a62b6a2fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Renormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2729c91-6a89-4a20-a5f5-c8835c587567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cluster_df_merged_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_threshold = 0.5\n",
    "feature_integrated_norm = gene_cluster_df_merged_filtered[\"Length Z-score\"].apply(\n",
    "    lambda x: sp.integrate.simpson(x)\n",
    ")\n",
    "feature_max_norm = gene_cluster_df_merged_filtered[\"Length Z-score\"].apply(\n",
    "    lambda x: np.max(x)\n",
    ")\n",
    "feature_filtered_df = gene_cluster_df_merged_filtered[feature_max_norm > norm_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature = np.array(feature_filtered_df[\"Length\"].tolist())[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_max = np.max(X_feature, axis=1, keepdims=True)\n",
    "V_min = np.min(X_feature, axis=1, keepdims=True)\n",
    "V_i = X_feature[:
, 0:1]\n",
    "min_max_norm = (X_feature - V_i) / (V_max - V_min)\n",
    "feature_filtered_df[\"Length Feature Norm\"] = [item for item in min_max_norm[:, :, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(min_max_norm[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-loading",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtw_feature_norm = parallel_norm_soft_dtw(min_max_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2d331-df00-4b52-920c-e32beed6063a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize Anndata Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_df_feature = anndata.AnnData(\n",
    "    X=min_max_norm.reshape(min_max_norm.shape[0], -1), obs=feature_filtered_df\n",
    ")  # AnnData container to use scanpy functions with unwrapped time vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c177d3-2a32-45e6-89e5-8d7690be67b6",
   "metadata": {},
   "source": [
    "### Compute KNN Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "n_pcs = 20  # This shouldn't affect anything\n",
    "\n",
    "sc.pp.neighbors(an_df_feature, n_neighbors=n_neighbors, n_pcs=n_pcs)\n",
    "knn_indices, knn_dists, forest = sc.neighbors.compute_neighbors_umap(\n",
    "    dtw_feature_norm, n_neighbors=n_neighbors, metric=\"precomputed\"\n",
    ")\n",
    "(\n",
    "    an_df_feature.uns[\"neighbors\"][\"distances\"],\n",
    "    an_df_feature.uns[\"neighbors\"][\"connectivities\"],\n",
    ") = sc.neighbors._compute_connectivities_umap(\n",
    "    knn_indices,\n",
    "    knn_dists,\n",
    "    an_df_feature.shape[0],\n",
    "    n_neighbors,  # change to neighbors you plan to use\n",
    ")\n",
    "an_df_feature.obsp[\"distances\"] = an_df_feature.uns[\"neighbors\"][\"distances\"]\n",
    "an_df_feature.obsp[\"connectivities\"] = an_df_feature.uns[\"neighbors\"][\"connectivities\"]\n",
    "an_df_feature.obsp[\"soft_dtw\"] = dtw_feature_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e0161-a92c-4c2e-bc5e-c48afc8eead2",
   "metadata": {},
   "source": [
    "### Computing Leiden, PAGA and UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-graduation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_paga_df_dict = {}\n",
    "for resolution in [0.25, 1.0, 1.5]:\n",
    "    feature_paga_df_dict[resolution] = copy.deepcopy(an_df_feature)\n",
    "    sc.tl.leiden(\n",
    "        feature_paga_df_dict[resolution], resolution=resolution, n_iterations=-1\n",
    "    )\n",
    "    sc.tl.paga(feature_paga_df_dict[resolution], groups=\"leiden\")\n",
    "    sc.pl.paga(feature_paga_df_dict[resolution], add_pos=True, show=False)\n",
    "sc.tl.umap(feature_paga_df_dict[1.0], init_pos=\"paga\", min_dist=0.25, spread=5.0)\n",
    "feature_paga_df_dict[1.0].obs[\"leiden_lowres\"] = feature_paga_df_dict[0.25].obs[\n",
    "    \"leiden\"\n",
    "]\n",
    "feature_paga_df_dict[1.0].obs[\"leiden_highres\"] = feature_paga_df_dict[1.5].obs[\n",
    "    \"leiden\"\n",
    "]\n",
    "feature_paga_df = feature_paga_df_dict[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-alpha",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = sc.pl.umap(\n",
    "    feature_paga_df,\n",
    "    color=[\"leiden_lowres\", \"leiden\", \"leiden_highres\"],\n",
    "    title=[\"Leiden Resolution=0.25\", \"Leiden Resolution=1.\", \"Leiden Resolution=1.5\"],\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    edges=True,\n",
    "    add_outline=False,\n",
    "    size=50,\n",
    "    return_fig=True,\n",
    "    palette=vega_20_scanpy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_paga_df.obs[\"N Match\"] = 20.0 - feature_paga_df.obs[\"N Mismatch\"]\n",
    "feature_del_N_match_series = feature_paga_df.obs.groupby(\"TargetID\").apply(\n",
    "    lambda x: x[\"N Match\"] - np.min(x[\"N Match\"])\n",
    ")\n",
    "feature_del_N_match_series = feature_del_N_match_series.droplevel(\"TargetID\")\n",
    "feature_paga_df.obs[\"Delta N Match\"] = feature_del_N_match_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_paga_df.obs[\"Length Feature Norm Final Value\"] = feature_paga_df.obs[\n",
    "    \"Length Feature Norm\"\n",
    "].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_paga_df.obs[\"Length Feature Norm t Half Max\"] = feature_paga_df.obs[\n",
    "    \"Length Feature Norm\"\n",
    "].apply(lambda x: np.where(x >= (np.max(x) / 2.0))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.umap(\n",
    "    feature_paga_df,\n",
    "    color=[\"Length Feature Norm Final Value\", \"Length Feature Norm t Half Max\"],\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    add_outline=False,\n",
    "    size=50,\n",
    "    return_fig=True,\n",
    "    cmap=\"RdBu_r\",\n",
    "    wspace=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cluster_timeseries_braycenters(\n",
    "    feature_paga_df.obs,\n",
    "    \"leiden\",\n",
    "    [\"Length Feature Norm\", \"Length Feature Norm\"],\n",
    "    [\"Length Feature Norm\", \"Length Feature Norm\"],\n",
    "    [(-1, 1), (-1, 1)],\n",
    "    figsize=(15, 6),\n",
    "    wspace=0.25,\n",
    "    hspace=0.25,\n",
    "    color_list=vega_20_scanpy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863f54a-30e6-428b-87cf-0f930197e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(\n",
    "    feature_paga_df.obs[feature_paga_df.obs[\"leiden\"] == \"16\"][\"Gene\"],\n",
    "    return_counts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097302e7-29c2-45a6-b24d-5cc0d6be43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_paga_df.obs[feature_paga_df.obs[\"Gene\"] == \"fabG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0c2a8-4c14-4202-9b56-1dd88d134a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_paga_df.obs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69581fc-cec2-4d1a-b73a-17e91e0b7c1e",
   "metadata": {},
   "source": [
    "### GO Term Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import goatools\n",
    "import goatools.base\n",
    "from goatools.anno.gaf_reader import GafReader\n",
    "from goatools.base import download_go_basic_obo\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "from goatools.obo_parser import GODag\n",
    "from goatools.semantic import TermCounts, get_info_content, semantic_similarity\n",
    "\n",
    "\n",
    "def search_go(ns2assoc, obodag, inv_gene_to_id, go_term):\n",
    "    namespace_abbv = {\n",
    "        \"biological_process\": \"BP\",\n",
    "        \"molecular_function\": \"MF\",\n",
    "        \"cellular_component\": \"CC\",\n",
    "    }\n",
    "\n",
    "    print(\"Searching for \" + str(obodag[go_term].name))\n",
    "    namespace = namespace_abbv[obodag[go_term].namespace]\n",
    "    child_goterms = list(obodag[go_term].get_all_children())\n",
    "    gene_list = [\n",
    "        inv_gene_to_id[key]\n",
    "        for key, val in ns2assoc[namespace].items()\n",
    "        if go_term in val\n",
    "    ]\n",
    "    for child_goterm in child_goterms:\n",
    "        gene_list += [\n",
    "            inv_gene_to_id[key]\n",
    "            for key, val in ns2assoc[namespace].items()\n",
    "            if child_goterm in val\n",
    "        ]\n",
    "    gene_list = sorted(list(set(gene_list)))\n",
    "    return gene_list\n",
    "\n",
    "\n",
    "def get_enriched_GO_terms(\n",
    "    background_gene_list, gene_list, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    "):\n",
    "    gene_to_id = {assoc.DB_Symbol: assoc.DB_ID for assoc in objanno.associations}\n",
    "    synonym_dict = {\n",
    "        synonym: assoc.DB_ID\n",
    "        for assoc in objanno.associations\n",
    "        for synonym in assoc.DB_Synonym\n",
    "    }\n",
    "    gene_to_id.update(synonym_dict)\n",
    "\n",
    "    # background gene set\n",
    "\n",
    "    all_genes_uniprot = [\n",
    "        gene_to_id[item] for item in background_gene_list if item in gene_to_id.keys()\n",
    "    ]\n",
    "    selected_genes_uniprot = [\n",
    "        gene_to_id[item] for item in gene_list if item in gene_to_id.keys()\n",
    "    ]\n",
    "\n",
    "    print(len(all_genes_uniprot))\n",
    "    print(len(selected_genes_uniprot))\n",
    "\n",
    "    goeaobj = GOEnrichmentStudy(\n",
    "        all_genes_uniprot,  # List of mouse protein-coding genes\n",
    "        ns2assoc[GO_type],  # geneid/GO associations\n",
    "        obodag,  # Ontologies\n",
    "        propagate_counts=True,\n",
    "        alpha=pval,  # default significance cut-off\n",
    "        methods=[\"fdr_bh\"],\n",
    "    )\n",
    "    # defult multipletest correction method\n",
    "\n",
    "    goea_results_all = goeaobj.run_study(selected_genes_uniprot, prt=None)\n",
    "    goea_quiet_sig = [r for r in goea_results_all if r.p_fdr_bh < pval]\n",
    "    goea_quiet_enriched = [r for r in goea_quiet_sig if r.enrichment == \"e\"]\n",
    "    return goea_quiet_enriched\n",
    "\n",
    "\n",
    "def pick_exemplar(go1, go2, termcounts, obodag, info_thr, pval_factor=2.0):\n",
    "    info_1_low = get_info_content(go1.GO, termcounts) < info_thr\n",
    "    info_2_low = get_info_content(go2.GO, termcounts) < info_thr\n",
    "    if info_1_low and not info_2_low:\n",
    "        return go2\n",
    "    elif info_2_low and not info_1_low:\n",
    "        return go1\n",
    "    elif info_2_low and info_1_low:\n",
    "        return go1\n",
    "\n",
    "    pval_ratio = go1.p_fdr_bh / go2.p_fdr_bh\n",
    "\n",
    "    if pval_ratio > pval_factor:\n",
    "        return go2\n",
    "    elif pval_ratio < (1.0 / pval_factor):\n",
    "        return go1\n",
    "\n",
    "    go1_parents = list(obodag[go1.GO].get_all_parents())\n",
    "    go2_parents = list(obodag[go2.GO].get_all_parents())\n",
    "\n",
    "    if go2.GO in go1_parents:\n",
    "        return go2\n",
    "\n",
    "    elif go1.GO in go2_parents:\n",
    "        return go1\n",
    "\n",
    "    return go1\n",
    "\n",
    "\n",
    "def get_filtered_go_terms(\n",
    "    obodag, objanno, goea_list, sim_thr=0.05, info_thr=1.0, GO_type=\"BP\"\n",
    "):\n",
    "    termcounts = TermCounts(obodag, objanno.get_ns2assc()[GO_type])\n",
    "\n",
    "    go_term_list = [item.GO for item in goea_list]\n",
    "    sim_arr = np.zeros((len(go_term_list), len(go_term_list)))\n",
    "    for i in range(len(go_term_list)):\n",
    "        for j in range(len(go_term_list)):\n",
    "            sim_arr[i, j] = semantic_similarity(\n",
    "                go_term_list[i], go_term_list[j], obodag\n",
    "            )\n",
    "    np.fill_diagonal(sim_arr, 0.0)\n",
    "\n",
    "    working_group_idx = 0\n",
    "    grouped_terms = {}\n",
    "    group_exemplars = {}\n",
    "    go_term_indices = list(range(len(go_term_list)))\n",
    "\n",
    "    while len(go_term_indices) > 0:\n",
    "        i = go_term_indices[0]\n",
    "        most_sim_arg = np.argmax(sim_arr[i])\n",
    "        sim_score = sim_arr[i, most_sim_arg]\n",
    "        if sim_score > sim_thr:\n",
    "            if len(grouped_terms) > 0:\n",
    "                in_other_group_keys = [\n",
    "                    key for key, val in grouped_terms.items() if most_sim_arg in val\n",
    "                ]\n",
    "                if len(in_other_group_keys) == 1:\n",
    "                    other_group_idx = in_other_group_keys[0]\n",
    "                    grouped_terms[other_group_idx] = grouped_terms[other_group_idx] + [\n",
    "                        i\n",
    "                    ]\n",
    "                    group_exemplars[other_group_idx] = pick_exemplar(\n",
    "                        group_exemplars[other_group_idx],\n",
    "                        goea_list[i],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                else:\n",
    "                    grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                    group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                        goea_list[i],\n",
    "                        goea_list[most_sim_arg],\n",
    "                        termcounts,\n",
    "                        obodag,\n",
    "                        info_thr,\n",
    "                    )\n",
    "                    working_group_idx += 1\n",
    "                    go_term_indices.remove(most_sim_arg)\n",
    "            else:\n",
    "                grouped_terms[working_group_idx] = [i, most_sim_arg]\n",
    "                group_exemplars[working_group_idx] = pick_exemplar(\n",
    "                    goea_list[i], goea_list[most_sim_arg], termcounts, obodag, info_thr\n",
    "                )\n",
    "                working_group_idx += 1\n",
    "                go_term_indices.remove(most_sim_arg)\n",
    "        go_term_indices.remove(i)\n",
    "\n",
    "    group_exemplars = list(group_exemplars.values())\n",
    "\n",
    "    return group_exemplars\n",
    "\n",
    "\n",
    "def get_GO_assign_dict(selected_goea, cluster_genes_uniprot):\n",
    "    all_study_items = copy.copy(cluster_genes_uniprot)\n",
    "    depth_list = sorted(set([item.depth for item in selected_goea]))[::-1]\n",
    "    assign_dict = {}\n",
    "    for depth in depth_list:\n",
    "        go_terms_at_level = [item for item in selected_goea if item.depth == depth]\n",
    "        for go_term in go_terms_at_level:\n",
    "            study_item_list = list(go_term.study_items)\n",
    "            for study_item in study_item_list:\n",
    "                if study_item in all_study_items:\n",
    "                    assign_dict[study_item] = go_term.name\n",
    "                    all_study_items.remove(study_item)\n",
    "\n",
    "    for remaining_item in all_study_items:\n",
    "        assign_dict[remaining_item] = \"Unassigned\"\n",
    "\n",
    "    return assign_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ontologies\n",
    "obo_fname = download_go_basic_obo()\n",
    "\n",
    "# Get ecoli association file (ecocyc)\n",
    "gaf_handle = goatools.base.http_get(\n",
    "    \"http://current.geneontology.org/annotations/ecocyc.gaf.gz\", fout=\"./ecocyc.gaf.gz\"\n",
    ")\n",
    "gaf_fname = goatools.base.gunzip(\"./ecocyc.gaf.gz\")\n",
    "\n",
    "## Getting ontologies and other nonesense\n",
    "\n",
    "obodag = GODag(obo_fname)\n",
    "objanno = GafReader(gaf_fname)\n",
    "ns2assoc = objanno.get_ns2assc()\n",
    "\n",
    "gene_to_id = {assoc.DB_Symbol: assoc.DB_ID for assoc in objanno.associations}\n",
    "inv_gene_to_id = {assoc.DB_ID: assoc.DB_Symbol for assoc in objanno.associations}\n",
    "synonym_dict = {\n",
    "    synonym: assoc.DB_ID\n",
    "    for assoc in objanno.associations\n",
    "    for synonym in assoc.DB_Synonym\n",
    "}\n",
    "gene_to_id.update(synonym_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background gene set\n",
    "\n",
    "all_genes = feature_paga_df.obs[\"Gene\"].unique().tolist()\n",
    "\n",
    "clust_id = str(17)\n",
    "cluster_genes = sorted(\n",
    "    feature_paga_df.obs[feature_paga_df.obs[\"leiden_highres\"] == clust_id][\"Gene\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "goea_quiet_enriched = get_enriched_GO_terms(\n",
    "    all_genes, cluster_genes, obodag, objanno, ns2assoc, pval=0.05, GO_type=\"BP\"\n",
    ")\n",
    "filtered_go_terms = get_filtered_go_terms(\n",
    "    obodag, objanno, goea_quiet_enriched, sim_thr=0.3, info_thr=1.0\n",
    ")\n",
    "go_term_dict = {\n",
    "    go_term.name: go_term.ratio_in_study[0] for go_term in filtered_go_terms\n",
    "}\n",
    "# ttl_terms = np.sum(list(go_term_dict.values()))\n",
    "# go_term_dict = {key:val/ttl_terms for key,val in go_term_dict.items()}\n",
    "\n",
    "print()\n",
    "for key, value in go_term_dict.items():\n",
    "    print(key, \" : \", value)\n",
    "print()\n",
    "for i in range(0, len(cluster_genes), 5):\n",
    "    print(cluster_genes[i : i + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-surgeon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [zscore_trace + \": Mean\" for zscore_trace in zscore_traces]\n",
    "\n",
    "fig = sc.pl.umap(\n",
    "    feature_paga_df,\n",
    "    color=labels,\n",
    "    show=False,\n",
    "    legend_loc=\"on data\",\n",
    "    add_outline=False,\n",
    "    size=50,\n",
    "    return_fig=True,\n",
    "    vcenter=0.0,\n",
    "    cmap=\"RdBu_r\",\n",
    "    wspace=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cluster_timeseries_braycenters(\n",
    "    feature_paga_df.obs,\n",
    "    \"leiden\",\n",
    "    [\"Division Length Feature Norm\", \"Division Length Feature Norm\"],\n",
    "    [\"Division Length Feature Norm\", \"Division Length Feature Norm\"],\n",
    "    [(0, 1), (0, 1)],\n",
    "    figsize=(15, 6),\n",
    "    wspace=0.25,\n",
    "    hspace=0.25,\n",
    "    color_list=vega_20_scanpy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4e8da-0406-4801-9717-f1fc65f8a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-there",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "paga_df_only = paga_df.obs\n",
    "paga_df_only.to_pickle(\"./2021-12-07_paga_df_only.pkl\")\n",
    "paga_df.obs = paga_df.obs[\n",
    "    paga_df.obs.dtypes[paga_df.obs.dtypes.isin([np.int64, float])].index.tolist()\n",
    "]\n",
    "paga_df.write(\"./2021-12-07_paga_df.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
