{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Mapping Barcodes and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition of active memory manager\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"distributed.scheduler.active-memory-manager.start\": True})\n",
    "dask.config.set({\"distributed.scheduler.worker-ttl\": \"5m\"})\n",
    "dask.config.set({\"distributed.scheduler.allowed-failures\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Barcodes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"2:00:00\",\n",
    "    local=False,\n",
    "    n_workers=200,\n",
    "    n_workers_min=50,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.reset_worker_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import Lineage Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Optimizing Growth Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, query_list, client=False, repartition=False, persist=False):\n",
    "    # filter_list must be in df.query format (see pandas docs)\n",
    "\n",
    "    # returns persisted dataframe either in cluster or local\n",
    "\n",
    "    compiled_query = \" and \".join(query_list)\n",
    "    out_df = df.query(compiled_query)\n",
    "    if persist:\n",
    "        if client:\n",
    "            out_df = client.daskclient.persist(out_df)\n",
    "        else:\n",
    "            out_df = out_df.persist()\n",
    "\n",
    "    if repartition:\n",
    "        init_size = len(df)\n",
    "        final_size = len(out_df)\n",
    "        ratio = init_size // final_size\n",
    "        out_df = out_df.repartition(npartitions=(df.npartitions // ratio) + 1)\n",
    "        if persist:\n",
    "            if client:\n",
    "                out_df = client.daskclient.persist(out_df)\n",
    "            else:\n",
    "                out_df = out_df.persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_first_cell_timepoint(df):\n",
    "    min_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmin().tolist()\n",
    "    init_cells = df.loc[min_tpts]\n",
    "    return init_cells\n",
    "\n",
    "\n",
    "def get_last_cell_timepoint(df):\n",
    "    max_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmax().tolist()\n",
    "    fin_cells = df.loc[max_tpts]\n",
    "    return fin_cells\n",
    "\n",
    "\n",
    "def get_growth_and_division_stats(\n",
    "    lineage_df,\n",
    "    headpath,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "    kymo_df_path = headpath + \"/kymograph/metadata\"\n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"] * delta_t_min * 60.0\n",
    "\n",
    "    kymo_time_series = (\n",
    "        kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler=\"threads\")\n",
    "    )\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "\n",
    "    reference = filter_df(lineage_df, [\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "    query = filter_df(\n",
    "        lineage_df,\n",
    "        [\n",
    "            \"`Mother CellID` != -1\",\n",
    "            \"`Daughter CellID 1` != -1\",\n",
    "            \"`Daughter CellID 2` != -1\",\n",
    "            \"`Sister CellID` != -1\",\n",
    "            \"`Trench Score` < \" + str(trench_score_thr),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    init_cells = (\n",
    "        get_first_cell_timepoint(query)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    fin_cells = (\n",
    "        get_last_cell_timepoint(query)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    cell_min_tpt_df = (\n",
    "        get_first_cell_timepoint(reference)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    cell_max_tpt_df = (\n",
    "        get_last_cell_timepoint(reference)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    mother_df = cell_max_tpt_df.loc[init_cells[\"Mother CellID\"].tolist()]\n",
    "    sister_df = cell_min_tpt_df.loc[init_cells[\"Sister CellID\"].tolist()]\n",
    "    daughter_1_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 1\"].tolist()]\n",
    "    daughter_2_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 2\"].tolist()]\n",
    "\n",
    "    for metric in size_metrics:\n",
    "        if metric == \"minor_axis_length\":\n",
    "            init_cells[\"Birth: \" + metric] = init_cells[metric].values\n",
    "            init_cells[\"Division: \" + metric] = fin_cells[metric].values\n",
    "            init_cells[\"Delta: \" + metric] = (\n",
    "                fin_cells[metric].values - init_cells[metric].values\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            interp_mother_final_size = (\n",
    "                (init_cells[metric].values + sister_df[metric].values)\n",
    "                * mother_df[metric].values\n",
    "            ) ** (1 / 2)\n",
    "            sister_frac = init_cells[metric].values / (\n",
    "                sister_df[metric].values + init_cells[metric].values\n",
    "            )\n",
    "            init_cells[\"Birth: \" + metric] = sister_frac * interp_mother_final_size\n",
    "\n",
    "            init_cells[\"Division: \" + metric] = (\n",
    "                (daughter_1_df[metric].values + daughter_2_df[metric].values)\n",
    "                * fin_cells[metric].values\n",
    "            ) ** (1 / 2)\n",
    "\n",
    "            init_cells[\"Delta: \" + metric] = (\n",
    "                init_cells[\"Division: \" + metric].values\n",
    "                - init_cells[\"Birth: \" + metric].values\n",
    "            )\n",
    "\n",
    "    init_cells[\"Final timepoints\"] = daughter_1_df[\n",
    "        \"timepoints\"\n",
    "    ].values  # counting a timepoint in which a division occurs as a full timepoint, hacky\n",
    "    init_cells[\"Delta Timepoints\"] = (\n",
    "        init_cells[\"Final timepoints\"] - init_cells[\"timepoints\"]\n",
    "    )\n",
    "\n",
    "    # if absolute_time:\n",
    "    interpolated_final_time = (\n",
    "        fin_cells[\"time (s)\"].values + daughter_1_df[\"time (s)\"].values\n",
    "    ) / 2  # interpolating under the same assumptions as the size quantification\n",
    "    interpolated_init_time = (\n",
    "        init_cells[\"time (s)\"].values + mother_df[\"time (s)\"].values\n",
    "    ) / 2\n",
    "    init_cells[\"Final time (s)\"] = interpolated_final_time\n",
    "    init_cells[\"Delta time (s)\"] = interpolated_final_time - interpolated_init_time\n",
    "\n",
    "    query = (\n",
    "        query.reset_index()\n",
    "        .set_index([\"Global CellID\", \"timepoints\"])\n",
    "        .sort_index()\n",
    "        .reset_index(level=1)\n",
    "    )\n",
    "\n",
    "    # if absolute_time:\n",
    "\n",
    "    delta_t_series = query.groupby(level=0, sort=False)[\"time (s)\"].apply(\n",
    "        lambda x: ((x[1:].values - x[:-1].values))\n",
    "    )\n",
    "\n",
    "    init_time_gap = init_cells[\"time (s)\"].values - interpolated_init_time\n",
    "    final_time_gap = interpolated_final_time - fin_cells[\"time (s)\"].values\n",
    "\n",
    "    for size_metric in size_metrics:  # Havn't decided between mean and median\n",
    "        init_size = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: x.iloc[0]\n",
    "        )\n",
    "        final_size = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: x.iloc[-1]\n",
    "        )\n",
    "\n",
    "        init_linear_gr = init_size - (init_cells[\"Birth: \" + size_metric].values)\n",
    "        init_linear_gr = init_linear_gr / init_time_gap\n",
    "        final_linear_gr = (init_cells[\"Division: \" + size_metric].values) - final_size\n",
    "        final_linear_gr = final_linear_gr / final_time_gap\n",
    "\n",
    "        init_exp_gr = 2 * (\n",
    "            (init_size - (init_cells[\"Birth: \" + size_metric].values))\n",
    "            / (init_size + (init_cells[\"Birth: \" + size_metric].values))\n",
    "        )\n",
    "        init_exp_gr = init_exp_gr / init_time_gap\n",
    "        final_exp_gr = 2 * (\n",
    "            ((init_cells[\"Division: \" + size_metric].values) - final_size)\n",
    "            / ((init_cells[\"Division: \" + size_metric].values) + final_size)\n",
    "        )\n",
    "        final_exp_gr = final_exp_gr / final_time_gap\n",
    "\n",
    "        all_linear_gr = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: x[1:].values - x[:-1].values\n",
    "        )  ##needs to interpolate last growth rate\n",
    "        all_linear_gr = all_linear_gr / delta_t_series\n",
    "        all_linear_gr = all_linear_gr.apply(lambda x: x.tolist())\n",
    "        all_linear_gr = all_linear_gr.to_frame()\n",
    "        all_linear_gr = all_linear_gr.rename(columns={0: \"Main List\"})\n",
    "        all_linear_gr[\"Start\"] = init_linear_gr\n",
    "        all_linear_gr[\"End\"] = final_linear_gr\n",
    "        all_linear_gr[\"Appended\"] = all_linear_gr.apply(\n",
    "            lambda x: [x[\"Start\"]] + x[\"Main List\"] + [x[\"End\"]], axis=1\n",
    "        )\n",
    "        mean_linear_gr = all_linear_gr[\"Appended\"].apply(lambda x: np.nanmean(x))\n",
    "        del all_linear_gr\n",
    "        mean_linear_gr = mean_linear_gr * 3600  # size unit per hr\n",
    "\n",
    "        all_exp_gr = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: 2\n",
    "            * ((x[1:].values - x[:-1].values) / (x[1:].values + x[:-1].values))\n",
    "        )  ##needs to interpolate last growth rate\n",
    "        all_exp_gr = all_exp_gr / delta_t_series\n",
    "        all_exp_gr = all_exp_gr.apply(lambda x: x.tolist())\n",
    "        all_exp_gr = all_exp_gr.to_frame()\n",
    "        all_exp_gr = all_exp_gr.rename(columns={0: \"Main List\"})\n",
    "        all_exp_gr[\"Start\"] = init_exp_gr\n",
    "        all_exp_gr[\"End\"] = final_exp_gr\n",
    "        all_exp_gr[\"Appended\"] = all_exp_gr.apply(\n",
    "            lambda x: [x[\"Start\"]] + x[\"Main List\"] + [x[\"End\"]], axis=1\n",
    "        )\n",
    "        mean_exp_gr = all_exp_gr[\"Appended\"].apply(lambda x: np.nanmean(x))\n",
    "        del all_exp_gr\n",
    "        mean_exp_gr = mean_exp_gr * 3600  # size unit per hr\n",
    "\n",
    "        mean_cell_size_metric = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: np.nanmean(x.values)\n",
    "        )\n",
    "\n",
    "        init_cells[\"Mean: \" + size_metric] = mean_cell_size_metric\n",
    "        init_cells[\"Mean Linear Growth Rate: \" + size_metric] = mean_linear_gr\n",
    "        init_cells[\"Mean Exponential Growth Rate: \" + size_metric] = mean_exp_gr\n",
    "    #     else:\n",
    "    #         for size_metric in size_metrics: # Havn't decided between mean and median\n",
    "    #             mean_linear_gr = query.groupby(level=0,sort=False)[size_metric].apply(lambda x: np.nanmean(x[1:].values - x[:-1].values))\n",
    "    #             mean_linear_gr = (mean_linear_gr/delta_t_min)*60 #size unit per hr\n",
    "    #             mean_exp_gr = query.groupby(level=0,sort=False)[size_metric].apply(lambda x: np.nanmean((2*(x[1:].values - x[:-1].values))/(x[1:].values + x[:-1].values)))\n",
    "    #             mean_exp_gr = (mean_exp_gr/delta_t_min)*60 #exponential size unit per hr\n",
    "    #             mean_cell_size_metric = query.groupby(level=0,sort=False)[size_metric].apply(lambda x: np.nanmean(x.values))\n",
    "\n",
    "    #             init_cells[\"Mean: \" + size_metric] = mean_cell_size_metric\n",
    "    #             init_cells[\"Mean Linear Growth Rate: \" + size_metric] = mean_linear_gr\n",
    "    #             init_cells[\"Mean Exponential Growth Rate: \" + size_metric] = mean_exp_gr\n",
    "\n",
    "    median_mchy_intensity = query.groupby(\"Global CellID\")[\n",
    "        \"mCherry mean_intensity\"\n",
    "    ].apply(lambda x: np.nanmean(x.values))\n",
    "    init_cells[\"Mean: mCherry Intensity\"] = median_mchy_intensity\n",
    "\n",
    "    init_cells = init_cells.rename(columns={\"timepoints\": \"initial timepoints\"})\n",
    "\n",
    "    return init_cells\n",
    "\n",
    "\n",
    "def get_all_growth_and_division_stats(\n",
    "    lineage_df,\n",
    "    headpath,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "    test_partition = lineage_df.get_partition(0).compute()\n",
    "    test_partition = get_growth_and_division_stats(\n",
    "        test_partition,\n",
    "        headpath,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "    )\n",
    "\n",
    "    growth_div_df = dd.map_partitions(\n",
    "        get_growth_and_division_stats,\n",
    "        lineage_df,\n",
    "        headpath,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "        meta=test_partition,\n",
    "    )\n",
    "\n",
    "    return growth_div_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Import Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/lineage/\"\n",
    ")\n",
    "\n",
    "##temp fix\n",
    "lineage_df[\"CellID\"] = lineage_df[\"CellID\"].astype(int)\n",
    "lineage_df[\"Global CellID\"] = lineage_df[\"Global CellID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrm_find_mode(series, max_iter=1000, min_binsize=50):\n",
    "    working_series = series\n",
    "    for i in range(max_iter):\n",
    "        range_max, range_min = np.max(working_series), np.min(working_series)\n",
    "        midpoint = (range_max + range_min) / 2\n",
    "        above_middle = working_series[working_series > midpoint]\n",
    "        below_middle = working_series[working_series <= midpoint]\n",
    "\n",
    "        count_above = len(above_middle)\n",
    "        count_below = len(below_middle)\n",
    "\n",
    "        if count_above > count_below:\n",
    "            working_series = above_middle\n",
    "        else:\n",
    "            working_series = below_middle\n",
    "\n",
    "        if i > 0:\n",
    "            if (len(working_series) < min_binsize) or (last_midpoint == midpoint):\n",
    "                return np.mean(working_series)\n",
    "\n",
    "        last_midpoint = midpoint\n",
    "\n",
    "\n",
    "def bootstrap_hrm(series, n_bootstraps=100, max_n_per_bootstrap=100):\n",
    "    modes = []\n",
    "\n",
    "    series_len = len(series)\n",
    "\n",
    "    n_per_bootstrap = min(series_len, max_n_per_bootstrap)\n",
    "\n",
    "    for n in range(n_bootstraps):\n",
    "        modes.append(hrm_find_mode(series.sample(n=n_per_bootstrap)))\n",
    "    return np.mean(modes)\n",
    "\n",
    "\n",
    "def get_normal_fovs(fov_series, med_filter_size=5, n_stds=2):\n",
    "    median_series = sp.ndimage.median_filter(\n",
    "        fov_series, size=(med_filter_size,), mode=\"mirror\"\n",
    "    )\n",
    "\n",
    "    residuals = fov_series - median_series\n",
    "\n",
    "    gaussian_fit = sp.stats.norm.fit(residuals)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    lower, upper = (-n_stds * gaussian_fit.std(), n_stds * gaussian_fit.std())\n",
    "\n",
    "    thr_mask = (residuals > lower) & (residuals < upper)\n",
    "\n",
    "    return thr_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Variables over FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_to_rescale = [\n",
    "    \"mCherry mean_intensity\",\n",
    "    \"area\",\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "fov_sorted_lineage_df = (\n",
    "    lineage_df[[\"fov\", \"timepoints\"] + values_to_rescale]\n",
    "    .reset_index()\n",
    "    .set_index(\"fov\", sorted=True)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "fov_correction_dicts = {}\n",
    "lineage_df_subsample = fov_sorted_lineage_df[fov_sorted_lineage_df[\"timepoints\"] < 12]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale):\n",
    "    fov_series_groupby = lineage_df_subsample.groupby(\"fov\", sort=False)[label]\n",
    "    fov_median_series = (\n",
    "        fov_series_groupby.apply(lambda x: np.median(x), meta=float)\n",
    "        .compute()\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    normal_fov_series = get_normal_fovs(fov_median_series)\n",
    "    fov_median_series = fov_median_series[normal_fov_series]\n",
    "\n",
    "    fov_correction_series = fov_median_series / np.max(fov_median_series)\n",
    "    fov_correction_dicts[label] = fov_correction_series.to_dict()\n",
    "\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(fov_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"FOV #\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "\n",
    "fov_list = [set(val.keys()) for key, val in fov_correction_dicts.items()]\n",
    "filtered_fov_list = list(set.intersection(*fov_list))\n",
    "\n",
    "dask_controller.daskclient.cancel(fov_sorted_lineage_df)\n",
    "\n",
    "lineage_df_fov_correction = lineage_df[\n",
    "    [\"fov\", \"timepoints\"] + list(fov_correction_dicts.keys())\n",
    "]\n",
    "lineage_df_fov_correction = lineage_df_fov_correction[\n",
    "    lineage_df_fov_correction[\"fov\"].isin(filtered_fov_list)\n",
    "].persist()\n",
    "\n",
    "for label, fov_correction_dict in fov_correction_dicts.items():\n",
    "    fov_correction_series = lineage_df_fov_correction[\"fov\"].apply(\n",
    "        lambda x: fov_correction_dict[x], meta=float\n",
    "    )\n",
    "    lineage_df_fov_correction[label + \": FOV Corrected\"] = (\n",
    "        lineage_df_fov_correction[label] / fov_correction_series\n",
    "    ).persist()\n",
    "\n",
    "plt.savefig(\"FOV_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Variables over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_to_rescale_step_2 = [value + \": FOV Corrected\" for value in values_to_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_samples = 100000\n",
    "\n",
    "ttl_samples = len(lineage_df_fov_correction)\n",
    "frac_to_sample = target_samples / ttl_samples\n",
    "lineage_df_subsample = lineage_df_fov_correction.sample(frac=frac_to_sample).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lineage_df_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "for i, label in enumerate(values_to_rescale_step_2):\n",
    "    time_series_groupby = lineage_df_subsample.groupby(\"timepoints\")[label]\n",
    "    time_mode_series = time_series_groupby.apply(\n",
    "        lambda x: bootstrap_hrm(x)\n",
    "    ).sort_index()\n",
    "    time_correction_series = time_mode_series / np.max(time_mode_series)\n",
    "    time_correction_dict = time_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(time_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Timepoint (3 min steps)\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    lineage_df_fov_correction[\n",
    "        label + \": Time Corrected\"\n",
    "    ] = lineage_df_fov_correction.apply(\n",
    "        lambda x: x[label] / time_correction_dict[x[\"timepoints\"]], meta=float, axis=1\n",
    "    ).persist()\n",
    "plt.savefig(\"Time_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "The HSM method [2] iteratively divides the data set into samples of half the size as the original set and uses the half-sample with the minimum range, where range is defined as the difference between the maximum and the minimum value of the sample. This method terminates when the half-sample is less than three data points. An average of these three or fewer values is the mode. The HRM method [2] is similar but uses the sub-sample with the densest half-range, where range is defined as the absolute difference between the maximum and the minimum values in a sample. Of these two methods, only the HRM was used in this study because HRM has been shown to have lower bias with increasing contamination and asymmetry [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Overwrite Variables with Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_loc_from_index(df, idx_series):\n",
    "    df_out = df.loc[idx_series.tolist()]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def index_loc_lookup(df, idx_series):\n",
    "    df_out = dd.map_partitions(\n",
    "        get_aligned_loc_from_index, df, idx_series, align_dataframes=False\n",
    "    )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrected_lineage_df = index_loc_lookup(lineage_df, lineage_df_fov_correction.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##here\n",
    "for label in values_to_rescale:\n",
    "    corrected_lineage_df[label] = lineage_df_fov_correction[\n",
    "        label + \": FOV Corrected: Time Corrected\"\n",
    "    ]\n",
    "\n",
    "corrected_lineage_df = corrected_lineage_df[\n",
    "    [\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"trenchid\",\n",
    "        \"timepoints\",\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"CellID\",\n",
    "        \"Global CellID\",\n",
    "        \"Trench Score\",\n",
    "        \"Mother CellID\",\n",
    "        \"Daughter CellID 1\",\n",
    "        \"Daughter CellID 2\",\n",
    "        \"Sister CellID\",\n",
    "        \"Centroid X\",\n",
    "        \"Centroid Y\",\n",
    "        \"Kymograph File Parquet Index\",\n",
    "        \"Kymograph FOV Parquet Index\",\n",
    "        \"FOV Parquet Index\",\n",
    "    ]\n",
    "    + values_to_rescale\n",
    "]\n",
    "\n",
    "corrected_lineage_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Lineage_Analysis_with_Correction\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.reset_worker_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrected_lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Lineage_Analysis_with_Correction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_div_df = get_all_growth_and_division_stats(\n",
    "    corrected_lineage_df,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division\",\n",
    "    absolute_time=False,\n",
    "    delta_t_min=10,\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Barcodes/barcode_output_df.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()\n",
    "\n",
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells\n",
    "\n",
    "print(ttl_called)\n",
    "print(ttl_trenches)\n",
    "print(ttl_trenches_w_cells)\n",
    "print(percent_called)\n",
    "print(percent_called_w_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_barcode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Get Output Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##phenotype_df must contain trenchids column and a File Parquet Index\n",
    "# output_df = tr.get_barcode_pheno_df(growth_div_df, barcode_df, trenchid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barcode_pheno_df(\n",
    "    phenotype_df, barcode_df, trenchid_map, output_index=\"File Parquet Index\"\n",
    "):\n",
    "    ##phenotype_df must contain trenchids column and a File Parquet Index\n",
    "\n",
    "    valid_barcode_df = barcode_df[\n",
    "        barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "    ].compute()\n",
    "    barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "        lambda x: trenchid_map[x]\n",
    "    )\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    barcode_df_mapped_trenchids_list = barcode_df_mapped_trenchids.tolist()\n",
    "    final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "\n",
    "    called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "    called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "    called_df[\"phenotype trenchid\"] = called_df[\"phenotype trenchid\"].astype(int)\n",
    "    called_df = called_df.drop([\"Barcode Signal\"], axis=1)\n",
    "    called_df = called_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=False\n",
    "    )\n",
    "\n",
    "    output_df = phenotype_df.rename(columns={\"trenchid\": \"phenotype trenchid\"})\n",
    "    output_df = output_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "    output_df = output_df.loc[barcode_df_mapped_trenchids_list]\n",
    "\n",
    "    called_df = called_df.repartition(divisions=output_df.divisions).persist()\n",
    "    output_df = output_df.merge(\n",
    "        called_df, how=\"inner\", left_index=True, right_index=True\n",
    "    )\n",
    "    output_df = output_df.reset_index().set_index(output_index)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_barcode_pheno_df_split(\n",
    "    phenotype_df,\n",
    "    phenotype_kymo_df,\n",
    "    barcode_df,\n",
    "    trenchid_map,\n",
    "    output_index=\"File Parquet Index\",\n",
    "    output_kymo_index=\"FOV Parquet Index\",\n",
    "):\n",
    "    ## phenotype_df must contain trenchids column and a File Parquet Index\n",
    "    ## can be made more effecient still, with a direct output to parquet\n",
    "    ## More effecient implementation that splits outputs into two smaller parts (every cell, every trench)\n",
    "    ## Saves by elmininating some redundent entries\n",
    "    ## Still filters only for cells that made it to final \"phenotype_df\" in most cases lineage traced\n",
    "\n",
    "    valid_barcode_df = barcode_df[\n",
    "        barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "    ].compute()\n",
    "    barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "        lambda x: trenchid_map[x]\n",
    "    )\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    barcode_df_mapped_trenchids_list = barcode_df_mapped_trenchids.tolist()\n",
    "    final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "\n",
    "    called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "    called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "    called_df[\"phenotype trenchid\"] = called_df[\"phenotype trenchid\"].astype(int)\n",
    "    called_df = called_df.drop([\"Barcode Signal\"], axis=1)\n",
    "    called_df = called_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=False\n",
    "    )\n",
    "\n",
    "    output_phenotype_df = phenotype_df.rename(\n",
    "        columns={\"trenchid\": \"phenotype trenchid\"}\n",
    "    )\n",
    "    output_phenotype_df = output_phenotype_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "    output_phenotype_df = output_phenotype_df.loc[barcode_df_mapped_trenchids_list]\n",
    "    output_phenotype_df = output_phenotype_df.reset_index().set_index(output_index)\n",
    "\n",
    "    output_phenotype_kmyo_df = phenotype_kymo_df.rename(\n",
    "        columns={\"trenchid\": \"phenotype trenchid\"}\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.loc[\n",
    "        barcode_df_mapped_trenchids_list\n",
    "    ]\n",
    "    called_df = called_df.repartition(divisions=output_phenotype_kmyo_df.divisions)\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.merge(\n",
    "        called_df, how=\"inner\", left_index=True, right_index=True\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.reset_index().set_index(\n",
    "        output_kymo_index\n",
    "    )\n",
    "\n",
    "    return output_phenotype_df, output_phenotype_kmyo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_phenotype_df, output_phenotype_kmyo_df = get_barcode_pheno_df_split(\n",
    "    growth_div_df, kymo_df, barcode_df, trenchid_map, output_index=\"Global CellID\"\n",
    ")\n",
    "output_phenotype_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Final_Lineage_df/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "output_phenotype_kmyo_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Final_Barcodes_df/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "# output_df = output_df.repartition(npartitions=500).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_phenotype_kmyo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_phenotype_kmyo_df[\"FOV-Timepoint Index\"] = fov_timepoint_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division\"\n",
    ")\n",
    "# note: shutdown dask when doing this...fix bug later\n",
    "overlay_handle = tr.variant_overlay(\n",
    "    headpath,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\",\n",
    "    display_values_list=[\"Gene\", \"TargetID\", \"N Mismatch\"],\n",
    "    persist_data=False,\n",
    ")  ##fix this, was improperly made (only initial cellID timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_handle.view_overlay(vmin=0, vmax=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\"\n",
    ")\n",
    "gene_table = gene_table.reset_index().set_index(\"phenotype trenchid\", sorted=True)\n",
    "gene_table = (\n",
    "    gene_table.groupby(\"phenotype trenchid\", sort=False)\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .reset_index()\n",
    "    .set_index(\"FOV Parquet Index\")\n",
    ")\n",
    "gene_table_out = gene_table.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "gene_table_out[\"phenotype trenchids\"] = gene_table.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "gene_table_out = gene_table_out[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_out = gene_table.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "gene_table_out[\"phenotype trenchids\"] = gene_table.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "gene_table_out = gene_table_out[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division/\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    gene_table_out,\n",
    "    index_key=\"Gene\",\n",
    "    trenchids_as_list=True,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    gene_table_out,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
