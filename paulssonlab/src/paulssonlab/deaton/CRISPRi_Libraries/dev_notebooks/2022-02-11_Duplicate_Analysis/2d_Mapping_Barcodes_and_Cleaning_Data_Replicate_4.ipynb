{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Barcodes and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition of active memory manager\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"distributed.scheduler.active-memory-manager.start\": True})\n",
    "dask.config.set({\"distributed.scheduler.worker-ttl\": \"5m\"})\n",
    "dask.config.set({\"distributed.scheduler.allowed-failures\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"2:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=100,\n",
    "#     n_workers_min=100,\n",
    "#     memory=\"8GB\",\n",
    "#     working_directory=\"/home/de64/scratch/de64/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"3:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    n_workers_min=20,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.reset_worker_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lineage Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Growth Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, query_list, client=False, repartition=False, persist=False):\n",
    "    # filter_list must be in df.query format (see pandas docs)\n",
    "\n",
    "    # returns persisted dataframe either in cluster or local\n",
    "\n",
    "    compiled_query = \" and \".join(query_list)\n",
    "    out_df = df.query(compiled_query)\n",
    "    if persist:\n",
    "        if client:\n",
    "            out_df = client.daskclient.persist(out_df)\n",
    "        else:\n",
    "            out_df = out_df.persist()\n",
    "\n",
    "    if repartition:\n",
    "        init_size = len(df)\n",
    "        final_size = len(out_df)\n",
    "        ratio = init_size // final_size\n",
    "        out_df = out_df.repartition(npartitions=(df.npartitions // ratio) + 1)\n",
    "        if persist:\n",
    "            if client:\n",
    "                out_df = client.daskclient.persist(out_df)\n",
    "            else:\n",
    "                out_df = out_df.persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_first_cell_timepoint(df):\n",
    "    min_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmin().tolist()\n",
    "    init_cells = df.loc[min_tpts]\n",
    "    return init_cells\n",
    "\n",
    "\n",
    "def get_last_cell_timepoint(df):\n",
    "    max_tpts = df.groupby([\"Global CellID\"])[\"timepoints\"].idxmax().tolist()\n",
    "    fin_cells = df.loc[max_tpts]\n",
    "    return fin_cells\n",
    "\n",
    "\n",
    "def define_cell_cycle_timepoints_df(query, size_metrics):\n",
    "    ## new stuff\n",
    "    cell_cycle_timepoints_df = query[\n",
    "        [\"timepoints\", \"time (s)\", \"trenchid\"]\n",
    "        + size_metrics\n",
    "        + [\"mCherry mean_intensity\"]\n",
    "    ]\n",
    "    cell_cycle_timepoints_df[\n",
    "        \"Cell Cycle timepoints\"\n",
    "    ] = cell_cycle_timepoints_df.groupby(\"Global CellID\")[\"timepoints\"].apply(\n",
    "        lambda x: x - x.iloc[0]\n",
    "    )\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.drop(\"timepoints\", axis=1)\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.rename(\n",
    "        {\"time (s)\": \"Observation time (s)\"}, axis=1\n",
    "    )\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.reset_index(\n",
    "        drop=False\n",
    "    ).set_index([\"Global CellID\", \"Cell Cycle timepoints\"])\n",
    "\n",
    "    return cell_cycle_timepoints_df\n",
    "\n",
    "\n",
    "def define_cell_cycle_delta_timepoints_df(query, delta_t_series, size_metrics):\n",
    "    ## new stuff\n",
    "    trenchid_series = query.groupby(level=0, sort=False)[\"trenchid\"].apply(\n",
    "        lambda x: x.iloc[0]\n",
    "    )\n",
    "    cell_cycle_delta_timepoints_df = pd.concat(\n",
    "        [delta_t_series, trenchid_series], axis=1\n",
    "    )\n",
    "    cell_cycle_delta_timepoints_df[\n",
    "        \"Cell Cycle timepoints\"\n",
    "    ] = cell_cycle_delta_timepoints_df[\"time (s)\"].apply(lambda x: list(range(len(x))))\n",
    "\n",
    "    init_observed_times = query.groupby(level=0, sort=False)[\"time (s)\"].apply(\n",
    "        lambda x: x.iloc[0]\n",
    "    )\n",
    "    cell_cycle_delta_timepoints_df[\n",
    "        \"Observation time (s) from start\"\n",
    "    ] = cell_cycle_delta_timepoints_df[\"time (s)\"].apply(\n",
    "        lambda x: np.array([x[idx] / 2 + np.sum(x[:idx]) for idx in range(len(x))])\n",
    "    )\n",
    "    cell_cycle_delta_timepoints_df[\"Observation time (s)\"] = (\n",
    "        cell_cycle_delta_timepoints_df[\"Observation time (s) from start\"]\n",
    "        + init_observed_times\n",
    "    )\n",
    "\n",
    "    cell_cycle_timepoint_list = cell_cycle_delta_timepoints_df[\n",
    "        \"Cell Cycle timepoints\"\n",
    "    ].tolist()\n",
    "    cell_cycle_time_list = cell_cycle_delta_timepoints_df[\n",
    "        \"Observation time (s)\"\n",
    "    ].tolist()\n",
    "    cell_cycle_trenchid_list = cell_cycle_delta_timepoints_df[\"trenchid\"].tolist()\n",
    "    cellid_idx = cell_cycle_delta_timepoints_df.index.tolist()\n",
    "    trenchid_idx = cell_cycle_delta_timepoints_df[\"trenchid\"].tolist()\n",
    "\n",
    "    cellid_cell_cycle_idx = [\n",
    "        cellid_idx[i]\n",
    "        for i, tpts in enumerate(cell_cycle_timepoint_list)\n",
    "        for tpt in tpts\n",
    "    ]\n",
    "    unwrapped_timepoints = [tpt for tpts in cell_cycle_timepoint_list for tpt in tpts]\n",
    "    unwrapped_times = [time for times in cell_cycle_time_list for time in times]\n",
    "    trenchid_idx = [\n",
    "        trenchid_idx[i]\n",
    "        for i, tpts in enumerate(cell_cycle_timepoint_list)\n",
    "        for tpt in tpts\n",
    "    ]\n",
    "\n",
    "    cell_cycle_delta_timepoints_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Observation time (s)\": unwrapped_times,\n",
    "            \"Cell Cycle timepoints\": unwrapped_timepoints,\n",
    "            \"Global CellID\": cellid_cell_cycle_idx,\n",
    "            \"trenchid\": trenchid_idx,\n",
    "        }\n",
    "    ).set_index([\"Global CellID\", \"Cell Cycle timepoints\"])\n",
    "\n",
    "    return cell_cycle_delta_timepoints_df\n",
    "\n",
    "\n",
    "def get_growth_and_division_cell_cycle(\n",
    "    lineage_df,\n",
    "    kymo_df_path,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    ## Setting up kymograph and lineage dfs\n",
    "\n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"] * delta_t_min * 60.0\n",
    "\n",
    "    kymo_time_series = (\n",
    "        kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler=\"threads\")\n",
    "    )\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "\n",
    "    reference = filter_df(lineage_df, [\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "    query = filter_df(\n",
    "        lineage_df,\n",
    "        [\n",
    "            \"`Mother CellID` != -1\",\n",
    "            \"`Daughter CellID 1` != -1\",\n",
    "            \"`Daughter CellID 2` != -1\",\n",
    "            \"`Sister CellID` != -1\",\n",
    "            \"`Trench Score` < \" + str(trench_score_thr),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    init_cells = (\n",
    "        get_first_cell_timepoint(query)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    fin_cells = (\n",
    "        get_last_cell_timepoint(query)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    cell_min_tpt_df = (\n",
    "        get_first_cell_timepoint(reference)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    cell_max_tpt_df = (\n",
    "        get_last_cell_timepoint(reference)\n",
    "        .reset_index()\n",
    "        .set_index(\"Global CellID\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    mother_df = cell_max_tpt_df.loc[init_cells[\"Mother CellID\"].tolist()]\n",
    "    sister_df = cell_min_tpt_df.loc[init_cells[\"Sister CellID\"].tolist()]\n",
    "    daughter_1_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 1\"].tolist()]\n",
    "    daughter_2_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 2\"].tolist()]\n",
    "\n",
    "    for metric in size_metrics:\n",
    "\n",
    "        if metric == \"minor_axis_length\":\n",
    "\n",
    "            init_cells[\"Birth: \" + metric] = init_cells[metric].values\n",
    "            init_cells[\"Division: \" + metric] = fin_cells[metric].values\n",
    "            init_cells[\"Delta: \" + metric] = (\n",
    "                fin_cells[metric].values - init_cells[metric].values\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            interp_mother_final_size = (\n",
    "                (init_cells[metric].values + sister_df[metric].values)\n",
    "                * mother_df[metric].values\n",
    "            ) ** (1 / 2)\n",
    "            sister_frac = init_cells[metric].values / (\n",
    "                sister_df[metric].values + init_cells[metric].values\n",
    "            )\n",
    "            init_cells[\"Birth: \" + metric] = sister_frac * interp_mother_final_size\n",
    "\n",
    "            init_cells[\"Division: \" + metric] = (\n",
    "                (daughter_1_df[metric].values + daughter_2_df[metric].values)\n",
    "                * fin_cells[metric].values\n",
    "            ) ** (1 / 2)\n",
    "\n",
    "            init_cells[\"Delta: \" + metric] = (\n",
    "                init_cells[\"Division: \" + metric].values\n",
    "                - init_cells[\"Birth: \" + metric].values\n",
    "            )\n",
    "\n",
    "    init_cells[\"Final timepoints\"] = daughter_1_df[\n",
    "        \"timepoints\"\n",
    "    ].values  # counting a timepoint in which a division occurs as a full timepoint, hacky\n",
    "    init_cells[\"Delta Timepoints\"] = (\n",
    "        init_cells[\"Final timepoints\"] - init_cells[\"timepoints\"]\n",
    "    )\n",
    "\n",
    "    # if absolute_time:\n",
    "    interpolated_final_time = (\n",
    "        fin_cells[\"time (s)\"].values + daughter_1_df[\"time (s)\"].values\n",
    "    ) / 2  # interpolating under the same assumptions as the size quantification\n",
    "    interpolated_init_time = (\n",
    "        init_cells[\"time (s)\"].values + mother_df[\"time (s)\"].values\n",
    "    ) / 2\n",
    "    init_cells[\"Final time (s)\"] = interpolated_final_time\n",
    "    init_cells[\"Delta time (s)\"] = interpolated_final_time - interpolated_init_time\n",
    "\n",
    "    query = (\n",
    "        query.reset_index()\n",
    "        .set_index([\"Global CellID\", \"timepoints\"])\n",
    "        .sort_index()\n",
    "        .reset_index(level=1)\n",
    "    )\n",
    "\n",
    "    delta_t_series = query.groupby(level=0, sort=False)[\"time (s)\"].apply(\n",
    "        lambda x: ((x[1:].values - x[:-1].values))\n",
    "    )\n",
    "\n",
    "    init_time_gap = init_cells[\"time (s)\"].values - interpolated_init_time\n",
    "    final_time_gap = interpolated_final_time - fin_cells[\"time (s)\"].values\n",
    "\n",
    "    init_cells = init_cells.rename(columns={\"timepoints\": \"initial timepoints\"})\n",
    "\n",
    "    return init_cells\n",
    "\n",
    "\n",
    "def get_growth_and_division_cell_cycle_timepoints(\n",
    "    lineage_df,\n",
    "    kymo_df_path,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    ## Setting up kymograph and lineage dfs\n",
    "\n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"] * delta_t_min * 60.0\n",
    "\n",
    "    kymo_time_series = (\n",
    "        kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler=\"threads\")\n",
    "    )\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "\n",
    "    query = filter_df(\n",
    "        lineage_df,\n",
    "        [\n",
    "            \"`Mother CellID` != -1\",\n",
    "            \"`Daughter CellID 1` != -1\",\n",
    "            \"`Daughter CellID 2` != -1\",\n",
    "            \"`Sister CellID` != -1\",\n",
    "            \"`Trench Score` < \" + str(trench_score_thr),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    query = (\n",
    "        query.reset_index()\n",
    "        .set_index([\"Global CellID\", \"timepoints\"])\n",
    "        .sort_index()\n",
    "        .reset_index(level=1)\n",
    "    )\n",
    "\n",
    "    cell_cycle_timepoints_df = define_cell_cycle_timepoints_df(query, size_metrics)\n",
    "\n",
    "    cellid_cell_cycle_timepoint_idx = (\n",
    "        cell_cycle_timepoints_df.reset_index()\n",
    "        .apply(\n",
    "            lambda x: int(\n",
    "                f'{int(x[\"Global CellID\"]):016n}{int(x[\"Cell Cycle timepoints\"]):04n}'\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        .tolist()\n",
    "    )\n",
    "    cell_cycle_timepoints_df[\n",
    "        \"Global CellID-Cell Cycle timepoints\"\n",
    "    ] = cellid_cell_cycle_timepoint_idx\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.reset_index().set_index(\n",
    "        \"Global CellID-Cell Cycle timepoints\"\n",
    "    )\n",
    "\n",
    "    return cell_cycle_timepoints_df\n",
    "\n",
    "\n",
    "def get_growth_and_division_cell_cycle_delta_timepoints(\n",
    "    lineage_df,\n",
    "    kymo_df_path,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    ## Setting up kymograph and lineage dfs\n",
    "\n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"] * delta_t_min * 60.0\n",
    "\n",
    "    kymo_time_series = (\n",
    "        kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler=\"threads\")\n",
    "    )\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "\n",
    "    query = filter_df(\n",
    "        lineage_df,\n",
    "        [\n",
    "            \"`Mother CellID` != -1\",\n",
    "            \"`Daughter CellID 1` != -1\",\n",
    "            \"`Daughter CellID 2` != -1\",\n",
    "            \"`Sister CellID` != -1\",\n",
    "            \"`Trench Score` < \" + str(trench_score_thr),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    query = (\n",
    "        query.reset_index()\n",
    "        .set_index([\"Global CellID\", \"timepoints\"])\n",
    "        .sort_index()\n",
    "        .reset_index(level=1)\n",
    "    )\n",
    "\n",
    "    delta_t_series = query.groupby(level=0, sort=False)[\"time (s)\"].apply(\n",
    "        lambda x: ((x[1:].values - x[:-1].values))\n",
    "    )\n",
    "\n",
    "    cell_cycle_delta_timepoints_df = define_cell_cycle_delta_timepoints_df(\n",
    "        query, delta_t_series, size_metrics\n",
    "    )\n",
    "\n",
    "    for size_metric in size_metrics:  # Havn't decided between mean and median\n",
    "\n",
    "        # Initial/Final Growth Rate numbers are probably untrustworthy since noise in delta t from the division\n",
    "        # event is probably pretty high. Going to exclude.\n",
    "\n",
    "        all_linear_gr = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: x[1:].values - x[:-1].values\n",
    "        )\n",
    "        all_linear_gr = all_linear_gr / delta_t_series\n",
    "        all_linear_gr = all_linear_gr.apply(lambda x: x.tolist())\n",
    "        all_linear_gr = all_linear_gr.to_frame()\n",
    "        all_linear_gr = all_linear_gr.rename(columns={0: \"Main List\"})\n",
    "\n",
    "        all_linear_gr[\"Main List\"] = all_linear_gr[\"Main List\"].apply(\n",
    "            lambda x: np.array(x) * 3600\n",
    "        )  # size unit per hr\n",
    "\n",
    "        log_2 = np.log(2)\n",
    "        all_exp_gr = query.groupby(level=0, sort=False)[size_metric].apply(\n",
    "            lambda x: log_2 / (np.log(x[1:].values / x[:-1].values))\n",
    "        )\n",
    "        all_exp_gr = all_exp_gr * delta_t_series\n",
    "        all_exp_gr = all_exp_gr.apply(lambda x: x.tolist())\n",
    "        all_exp_gr = all_exp_gr.to_frame()\n",
    "        all_exp_gr = all_exp_gr.rename(columns={0: \"Main List\"})\n",
    "\n",
    "        all_exp_gr[\"Main List\"] = all_exp_gr[\"Main List\"].apply(\n",
    "            lambda x: np.array(x) / 3600\n",
    "        )  # size unit per hr\n",
    "\n",
    "        unwrapped_all_linear_gr = [\n",
    "            item for chunk in all_linear_gr[\"Main List\"] for item in chunk\n",
    "        ]\n",
    "        cell_cycle_delta_timepoints_df[\n",
    "            \"Linear Growth Rate: \" + size_metric\n",
    "        ] = unwrapped_all_linear_gr\n",
    "        unwrapped_all_exp_gr = [\n",
    "            item for chunk in all_exp_gr[\"Main List\"] for item in chunk\n",
    "        ]\n",
    "        cell_cycle_delta_timepoints_df[\n",
    "            \"Exponential Growth Rate: \" + size_metric\n",
    "        ] = unwrapped_all_exp_gr\n",
    "\n",
    "    cellid_cell_cycle_timepoint_idx = (\n",
    "        cell_cycle_delta_timepoints_df.reset_index()\n",
    "        .apply(\n",
    "            lambda x: int(\n",
    "                f'{int(x[\"Global CellID\"]):016n}{int(x[\"Cell Cycle timepoints\"]):04n}'\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        .tolist()\n",
    "    )\n",
    "    cell_cycle_delta_timepoints_df[\n",
    "        \"Global CellID-Cell Cycle timepoints\"\n",
    "    ] = cellid_cell_cycle_timepoint_idx\n",
    "    cell_cycle_delta_timepoints_df = (\n",
    "        cell_cycle_delta_timepoints_df.reset_index().set_index(\n",
    "            \"Global CellID-Cell Cycle timepoints\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return cell_cycle_delta_timepoints_df\n",
    "\n",
    "\n",
    "def get_all_growth_and_division_stats(\n",
    "    lineage_df,\n",
    "    kymo_df_path,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    size_metrics=[\n",
    "        \"area\",\n",
    "        \"major_axis_length\",\n",
    "        \"minor_axis_length\",\n",
    "        \"Volume\",\n",
    "        \"Surface Area\",\n",
    "    ],\n",
    "):\n",
    "    input_test_partition = lineage_df.get_partition(0).compute()\n",
    "\n",
    "    test_partition_1 = get_growth_and_division_cell_cycle(\n",
    "        input_test_partition,\n",
    "        kymo_df_path,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "    )\n",
    "    growth_div_df = dd.map_partitions(\n",
    "        get_growth_and_division_cell_cycle,\n",
    "        lineage_df,\n",
    "        kymo_df_path,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "        meta=test_partition_1,\n",
    "    )\n",
    "\n",
    "    test_partition_2 = get_growth_and_division_cell_cycle_timepoints(\n",
    "        input_test_partition,\n",
    "        kymo_df_path,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "    )\n",
    "    cell_cycle_timepoints_df = dd.map_partitions(\n",
    "        get_growth_and_division_cell_cycle_timepoints,\n",
    "        lineage_df,\n",
    "        kymo_df_path,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "        meta=test_partition_2,\n",
    "    )\n",
    "\n",
    "    test_partition_3 = get_growth_and_division_cell_cycle_delta_timepoints(\n",
    "        input_test_partition,\n",
    "        kymo_df_path,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "    )\n",
    "    cell_cycle_delta_timepoints_df = dd.map_partitions(\n",
    "        get_growth_and_division_cell_cycle_delta_timepoints,\n",
    "        lineage_df,\n",
    "        kymo_df_path,\n",
    "        trench_score_thr=trench_score_thr,\n",
    "        absolute_time=absolute_time,\n",
    "        delta_t_min=delta_t_min,\n",
    "        size_metrics=size_metrics,\n",
    "        meta=test_partition_3,\n",
    "    )\n",
    "\n",
    "    return growth_div_df, cell_cycle_timepoints_df, cell_cycle_delta_timepoints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/lineage\"\n",
    ")\n",
    "\n",
    "##temp fix\n",
    "lineage_df[\"CellID\"] = lineage_df[\"CellID\"].astype(int)\n",
    "lineage_df[\"Global CellID\"] = lineage_df[\"Global CellID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrm_find_mode(series, max_iter=1000, min_binsize=50):\n",
    "    working_series = series\n",
    "    for i in range(max_iter):\n",
    "        range_max, range_min = np.max(working_series), np.min(working_series)\n",
    "        midpoint = (range_max + range_min) / 2\n",
    "        above_middle = working_series[working_series > midpoint]\n",
    "        below_middle = working_series[working_series <= midpoint]\n",
    "\n",
    "        count_above = len(above_middle)\n",
    "        count_below = len(below_middle)\n",
    "\n",
    "        if count_above > count_below:\n",
    "            working_series = above_middle\n",
    "        else:\n",
    "            working_series = below_middle\n",
    "\n",
    "        if i > 0:\n",
    "            if (len(working_series) < min_binsize) or (last_midpoint == midpoint):\n",
    "                return np.mean(working_series)\n",
    "\n",
    "        last_midpoint = midpoint\n",
    "\n",
    "\n",
    "def bootstrap_hrm(series, n_bootstraps=100, max_n_per_bootstrap=100):\n",
    "    modes = []\n",
    "\n",
    "    series_len = len(series)\n",
    "\n",
    "    n_per_bootstrap = min(series_len, max_n_per_bootstrap)\n",
    "\n",
    "    for n in range(n_bootstraps):\n",
    "        modes.append(hrm_find_mode(series.sample(n=n_per_bootstrap)))\n",
    "    return np.mean(modes)\n",
    "\n",
    "\n",
    "def get_normal_fovs(fov_series, med_filter_size=5, n_stds=2):\n",
    "    median_series = sp.ndimage.median_filter(\n",
    "        fov_series, size=(med_filter_size,), mode=\"mirror\"\n",
    "    )\n",
    "\n",
    "    residuals = fov_series - median_series\n",
    "\n",
    "    gaussian_fit = sp.stats.norm.fit(residuals)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    lower, upper = (-n_stds * gaussian_fit.std(), n_stds * gaussian_fit.std())\n",
    "\n",
    "    thr_mask = (residuals > lower) & (residuals < upper)\n",
    "\n",
    "    return thr_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables over FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_to_rescale = [\n",
    "    \"mCherry mean_intensity\",\n",
    "    \"area\",\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "fov_sorted_lineage_df = (\n",
    "    lineage_df[[\"fov\", \"timepoints\"] + values_to_rescale]\n",
    "    .reset_index()\n",
    "    .set_index(\"fov\", sorted=True)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "fov_correction_dicts = {}\n",
    "lineage_df_subsample = fov_sorted_lineage_df[fov_sorted_lineage_df[\"timepoints\"] < 12]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale):\n",
    "    fov_series_groupby = lineage_df_subsample.groupby(\"fov\", sort=False)[label]\n",
    "    fov_median_series = (\n",
    "        fov_series_groupby.apply(lambda x: np.median(x), meta=float)\n",
    "        .compute()\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    normal_fov_series = get_normal_fovs(fov_median_series)\n",
    "    fov_median_series = fov_median_series[normal_fov_series]\n",
    "\n",
    "    fov_correction_series = fov_median_series / np.max(fov_median_series)\n",
    "    fov_correction_dicts[label] = fov_correction_series.to_dict()\n",
    "\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(fov_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"FOV #\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "\n",
    "fov_list = [set(val.keys()) for key, val in fov_correction_dicts.items()]\n",
    "filtered_fov_list = list(set.intersection(*fov_list))\n",
    "\n",
    "dask_controller.daskclient.cancel(fov_sorted_lineage_df)\n",
    "\n",
    "lineage_df_fov_correction = lineage_df[\n",
    "    [\"fov\", \"timepoints\"] + list(fov_correction_dicts.keys())\n",
    "]\n",
    "lineage_df_fov_correction = lineage_df_fov_correction[\n",
    "    lineage_df_fov_correction[\"fov\"].isin(filtered_fov_list)\n",
    "].persist()\n",
    "\n",
    "for label, fov_correction_dict in fov_correction_dicts.items():\n",
    "    fov_correction_series = lineage_df_fov_correction[\"fov\"].apply(\n",
    "        lambda x: fov_correction_dict[x], meta=float\n",
    "    )\n",
    "    lineage_df_fov_correction[label + \": FOV Corrected\"] = (\n",
    "        lineage_df_fov_correction[label] / fov_correction_series\n",
    "    ).persist()\n",
    "\n",
    "plt.savefig(\"FOV_correction_replicate_3.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_to_rescale_step_2 = [value + \": FOV Corrected\" for value in values_to_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_samples = 100000\n",
    "\n",
    "ttl_samples = len(lineage_df_fov_correction)\n",
    "frac_to_sample = target_samples / ttl_samples\n",
    "lineage_df_subsample = lineage_df_fov_correction.sample(frac=frac_to_sample).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lineage_df_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "for i, label in enumerate(values_to_rescale_step_2):\n",
    "    time_series_groupby = lineage_df_subsample.groupby(\"timepoints\")[label]\n",
    "    time_mode_series = time_series_groupby.apply(\n",
    "        lambda x: bootstrap_hrm(x)\n",
    "    ).sort_index()\n",
    "    time_correction_series = time_mode_series / np.max(time_mode_series)\n",
    "    time_correction_dict = time_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(time_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Timepoint (3 min steps)\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    lineage_df_fov_correction[\n",
    "        label + \": Time Corrected\"\n",
    "    ] = lineage_df_fov_correction.apply(\n",
    "        lambda x: x[label] / time_correction_dict[x[\"timepoints\"]], meta=float, axis=1\n",
    "    ).persist()\n",
    "plt.savefig(\"Time_correction_replicate_3.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HSM method [2] iteratively divides the data set into samples of half the size as the original set and uses the half-sample with the minimum range, where range is defined as the difference between the maximum and the minimum value of the sample. This method terminates when the half-sample is less than three data points. An average of these three or fewer values is the mode. The HRM method [2] is similar but uses the sub-sample with the densest half-range, where range is defined as the absolute difference between the maximum and the minimum values in a sample. Of these two methods, only the HRM was used in this study because HRM has been shown to have lower bias with increasing contamination and asymmetry [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwrite Variables with Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_loc_from_index(df, idx_series):\n",
    "    df_out = df.loc[idx_series.tolist()]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def index_loc_lookup(df, idx_series):\n",
    "    df_out = dd.map_partitions(\n",
    "        get_aligned_loc_from_index, df, idx_series, align_dataframes=False\n",
    "    )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrected_lineage_df = index_loc_lookup(lineage_df, lineage_df_fov_correction.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in values_to_rescale:\n",
    "    corrected_lineage_df[label] = lineage_df_fov_correction[\n",
    "        label + \": FOV Corrected: Time Corrected\"\n",
    "    ]\n",
    "\n",
    "corrected_lineage_df = corrected_lineage_df[\n",
    "    [\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"trenchid\",\n",
    "        \"timepoints\",\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"CellID\",\n",
    "        \"Global CellID\",\n",
    "        \"Trench Score\",\n",
    "        \"Mother CellID\",\n",
    "        \"Daughter CellID 1\",\n",
    "        \"Daughter CellID 2\",\n",
    "        \"Sister CellID\",\n",
    "        \"Centroid X\",\n",
    "        \"Centroid Y\",\n",
    "        \"Kymograph File Parquet Index\",\n",
    "        \"Kymograph FOV Parquet Index\",\n",
    "        \"FOV Parquet Index\",\n",
    "    ]\n",
    "    + values_to_rescale\n",
    "]\n",
    "\n",
    "corrected_lineage_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Analysis_with_Correction\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.reset_worker_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrected_lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-09_lDE20_Lineage_Analysis_with_Correction/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hack for corrupted timepoints\n",
    "\n",
    "Here, I am overwriting the corrupted timestamps of this experiment with a duplicate experiment that should have roughly the same timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata\"\n",
    ")\n",
    "global_meta_replace = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division/metadata.hdf5\"\n",
    ").read_df(\"global\", read_metadata=False)\n",
    "\n",
    "kymo_df[\"fov-timepoint_idx\"] = kymo_df.apply(\n",
    "    lambda x: int(f'{int(x[\"fov\"]):04n}{int(x[\"timepoints\"]):04n}'), axis=1\n",
    ")\n",
    "kymo_df = kymo_df.reset_index().set_index(\"fov-timepoint_idx\")\n",
    "global_meta_replace = global_meta_replace.reset_index()\n",
    "global_meta_replace[\"fov-timepoint_idx\"] = global_meta_replace.apply(\n",
    "    lambda x: int(f'{int(x[\"fov\"]):04n}{int(x[\"timepoints\"]):04n}'), axis=1\n",
    ")\n",
    "global_meta_replace = global_meta_replace[[\"fov-timepoint_idx\", \"t\"]]\n",
    "global_meta_replace = global_meta_replace.set_index(\"fov-timepoint_idx\")\n",
    "kymo_df = kymo_df.join(global_meta_replace).set_index(\"FOV Parquet Index\")\n",
    "kymo_df[\"time (s)\"] = kymo_df[\"t\"]\n",
    "kymo_df = kymo_df.drop(\"t\", axis=1)\n",
    "\n",
    "dd.to_parquet(\n",
    "    kymo_df,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata_timefix\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    growth_div_df,\n",
    "    cell_cycle_timepoints_df,\n",
    "    cell_cycle_delta_timepoints_df,\n",
    ") = get_all_growth_and_division_stats(\n",
    "    corrected_lineage_df,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata_timefix\",\n",
    "    absolute_time=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Barcodes/barcode_output_df.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()\n",
    "\n",
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells\n",
    "\n",
    "print(ttl_called)\n",
    "print(ttl_trenches)\n",
    "print(ttl_trenches_w_cells)\n",
    "print(percent_called)\n",
    "print(percent_called_w_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping\n",
    "\n",
    "#### Note the trenches are unaligned so I had to manually insert an offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(phenotype_kymopath, barcode_kymopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Output Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barcode_pheno_kymo_df(\n",
    "    phenotype_df,\n",
    "    phenotype_kymo_df,\n",
    "    barcode_df,\n",
    "    trenchid_map,\n",
    "    output_kymo_index=\"FOV Parquet Index\",\n",
    "):\n",
    "    ## phenotype_df_list must contain dfs with trenchids column\n",
    "    ## can be made more effecient still, with a direct output to parquet\n",
    "    ## More effecient implementation that splits outputs into two smaller parts (every cell, every trench)\n",
    "    ## Saves by elmininating some redundent entries\n",
    "    ## Still filters only for cells that made it to final \"phenotype_df\" in most cases lineage traced\n",
    "\n",
    "    valid_barcode_df = barcode_df[\n",
    "        barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "    ].compute()\n",
    "    barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "        lambda x: trenchid_map[x]\n",
    "    )\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    barcode_df_mapped_trenchids_list = barcode_df_mapped_trenchids.tolist()\n",
    "    final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "\n",
    "    called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "    called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "    called_df[\"phenotype trenchid\"] = called_df[\"phenotype trenchid\"].astype(int)\n",
    "    called_df = called_df.drop([\"Barcode Signal\"], axis=1)\n",
    "    called_df = called_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=False\n",
    "    )\n",
    "\n",
    "    output_phenotype_kmyo_df = phenotype_kymo_df.rename(\n",
    "        columns={\"trenchid\": \"phenotype trenchid\"}\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.loc[\n",
    "        barcode_df_mapped_trenchids_list\n",
    "    ]\n",
    "    called_df = called_df.repartition(divisions=output_phenotype_kmyo_df.divisions)\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.merge(\n",
    "        called_df, how=\"inner\", left_index=True, right_index=True\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.reset_index().set_index(\n",
    "        output_kymo_index\n",
    "    )\n",
    "\n",
    "    return output_phenotype_kmyo_df, barcode_df_mapped_trenchids\n",
    "\n",
    "\n",
    "def get_output_phenotype_df(\n",
    "    phenotype_df, barcode_df_mapped_trenchids, output_index=\"File Parquet Index\"\n",
    "):\n",
    "\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    barcode_df_mapped_trenchids_list = barcode_df_mapped_trenchids.tolist()\n",
    "\n",
    "    output_phenotype_df = phenotype_df.rename(\n",
    "        columns={\"trenchid\": \"phenotype trenchid\"}\n",
    "    )\n",
    "    output_phenotype_df = output_phenotype_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "\n",
    "    output_phenotype_df = output_phenotype_df.loc[barcode_df_mapped_trenchids_list]\n",
    "    output_phenotype_df = output_phenotype_df.reset_index().set_index(output_index)\n",
    "\n",
    "    return output_phenotype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/Growth_Division/kymograph/metadata_timefix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growth_div_df = growth_div_df.persist()\n",
    "output_phenotype_kmyo_df, barcode_df_mapped_trenchids = get_barcode_pheno_kymo_df(\n",
    "    growth_div_df, kymo_df, barcode_df, trenchid_map\n",
    ")\n",
    "output_phenotype_kmyo_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-15_lDE20_Final_Barcodes_df/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(output_phenotype_kmyo_df)\n",
    "\n",
    "growth_div_df_output = get_output_phenotype_df(\n",
    "    growth_div_df, barcode_df_mapped_trenchids, output_index=\"Global CellID\"\n",
    ")\n",
    "growth_div_df_output.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-15_lDE20_Lineage_Cell_Cycle/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(growth_div_df_output)\n",
    "dask_controller.daskclient.cancel(growth_div_df)\n",
    "\n",
    "cell_cycle_timepoints_df = cell_cycle_timepoints_df.persist()\n",
    "cell_cycle_timepoints_df_output = get_output_phenotype_df(\n",
    "    cell_cycle_timepoints_df,\n",
    "    barcode_df_mapped_trenchids,\n",
    "    output_index=\"Global CellID-Cell Cycle timepoints\",\n",
    ")\n",
    "cell_cycle_timepoints_df_output.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-15_lDE20_Lineage_Observations/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(cell_cycle_timepoints_df_output)\n",
    "dask_controller.daskclient.cancel(cell_cycle_timepoints_df)\n",
    "\n",
    "cell_cycle_delta_timepoints_df = cell_cycle_delta_timepoints_df.persist()\n",
    "cell_cycle_delta_timepoints_df_output = get_output_phenotype_df(\n",
    "    cell_cycle_delta_timepoints_df,\n",
    "    barcode_df_mapped_trenchids,\n",
    "    output_index=\"Global CellID-Cell Cycle timepoints\",\n",
    ")\n",
    "cell_cycle_delta_timepoints_df_output.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-20_lDE20_Final_6/2022-02-15_lDE20_Lineage_Delta_Observations/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(cell_cycle_delta_timepoints_df_output)\n",
    "dask_controller.daskclient.cancel(cell_cycle_delta_timepoints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Section Should be Refreshed Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_phenotype_kmyo_df = dd.read_parquet(\"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_phenotype_kmyo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Final_Barcodes_df/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_phenotype_kmyo_df[\"FOV-Timepoint Index\"] = fov_timepoint_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/\"\n",
    ")\n",
    "# note: shutdown dask when doing this...fix bug later\n",
    "overlay_handle = tr.variant_overlay(\n",
    "    headpath,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\",\n",
    "    display_values_list=[\"Gene\", \"TargetID\", \"N Mismatch\"],\n",
    "    persist_data=False,\n",
    ")  ##fix this, was improperly made (only initial cellID timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_handle.view_overlay(vmin=0, vmax=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Final_Barcodes_df\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "gene_table = gene_table.reset_index().set_index(\"phenotype trenchid\", sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table = (\n",
    "    gene_table.groupby(\"phenotype trenchid\", sort=False)\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .reset_index()\n",
    "    .set_index(\"FOV Parquet Index\")\n",
    ")\n",
    "gene_table_out = gene_table.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "gene_table_out[\"phenotype trenchids\"] = gene_table.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "gene_table_out = gene_table_out[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"TargetID\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    gene_table_out,\n",
    "    index_key=\"Gene\",\n",
    "    trenchids_as_list=True,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    gene_table_out,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
