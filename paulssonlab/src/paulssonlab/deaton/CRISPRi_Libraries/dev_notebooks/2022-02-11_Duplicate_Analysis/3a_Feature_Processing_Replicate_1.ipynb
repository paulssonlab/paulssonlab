{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing before main analysis\n",
    "\n",
    "- Note that there are fluctuations in the illumination intensity which may be resulting in pathological behavior from the reporter\n",
    "\n",
    "- This has been normalized out in the upstream processing, but try to fix long term\n",
    "\n",
    "- Also consider a flat field correction for the final experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask\n",
    "import warnings\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import (\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    "    cosine_distances,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "\n",
    "\n",
    "import pylab\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(action=\"once\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgrnadf_from_scoredf(\n",
    "    scoredf, feature_labels, time_label=\"final cell timepoints list\"\n",
    "):\n",
    "    scoredf_groupby = scoredf.groupby(\"sgRNA\")\n",
    "    sgrnadf = (\n",
    "        scoredf_groupby.apply(lambda x: x[\"phenotype trenchid\"].tolist())\n",
    "        .to_frame()\n",
    "        .rename(columns={0: \"phenotype trenchid\"})\n",
    "    )\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        sgrnadf[feature_label + \": score\"] = scoredf_groupby.apply(\n",
    "            lambda x: np.array(\n",
    "                [val for item in x[feature_label + \": score\"].tolist() for val in item]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sgrnadf[time_label] = scoredf_groupby.apply(\n",
    "        lambda x: np.array([val for item in x[time_label].tolist() for val in item])\n",
    "    )\n",
    "    sgrnadf[\"Gene\"] = scoredf_groupby.apply(lambda x: x[\"Gene\"].iloc[0])\n",
    "    sgrnadf[\"TargetID\"] = scoredf_groupby.apply(lambda x: x[\"TargetID\"].iloc[0])\n",
    "    sgrnadf[\"N Mismatch\"] = scoredf_groupby.apply(lambda x: x[\"N Mismatch\"].iloc[0])\n",
    "    sgrnadf[\"N Observations\"] = scoredf_groupby.apply(\n",
    "        lambda x: len(x[\"phenotype trenchid\"].tolist())\n",
    "    )\n",
    "    sgrnadf[\"Category\"] = scoredf_groupby.apply(lambda x: x[\"Category\"].iloc[0])\n",
    "\n",
    "    return sgrnadf\n",
    "\n",
    "\n",
    "def normalize_timeseries(feature_vector_series, lmbda=0.5):\n",
    "    timeseries_arr = np.swapaxes(np.array(feature_vector_series.tolist()), 1, 2)\n",
    "    sigma = np.std(timeseries_arr, axis=1)\n",
    "    if lmbda > 0.0:\n",
    "        sigma_prime = ((sigma + 1) ** lmbda - 1) / lmbda  ##yeo-johnson\n",
    "    elif lmbda == 0.0:\n",
    "        sigma_prime = np.log(sigma + 1)\n",
    "    else:\n",
    "        raise ValueError(\"lmbda cannot be negative\")\n",
    "    normalizer = sigma / sigma_prime\n",
    "    normalized_timeseries = timeseries_arr / normalizer[:, np.newaxis, :]\n",
    "    return normalized_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "\n",
    "Here, I am going to try and replicate (to some extant) the corrections from \"Genomewide phenotypic analysis of growth, cell morphogenesis, and cell cycle events in Escherichia coli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=100,\n",
    "    n_workers_min=20,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_lineage = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Lineage_df/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "final_output_df_lineage = final_output_df_lineage.dropna(\n",
    "    subset=[\n",
    "        \"Final timepoints\",\n",
    "        \"Mean Exponential Growth Rate: area\",\n",
    "        \"Birth: minor_axis_length\",\n",
    "        \"Birth: Surface Area\",\n",
    "    ]\n",
    ")\n",
    "final_output_df_lineage = (\n",
    "    final_output_df_lineage.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .repartition(npartitions=100)\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Filter for \"Normal\" Sizes at Start\n",
    "\n",
    "1) Fit a gaussian model to each of the specified feature params during the first t timepoints of the experiment (using a subsample for speed) \n",
    "2) Compute a normalized probability trenchwise for these features under the gaussian model, during the first t timepoints of the experiment\n",
    "3) Eliminate trenches that are under some p percentile value of this probability for each feature\n",
    "4) Display histograms for each property as well as the resulting theshold\n",
    "\n",
    "Note that these features should be the only features examined in the resulting analysis. For the notebook, I am looking at:\n",
    "- Birth length (Lb)\n",
    "- Division length (Ld)\n",
    "- Mean Area Increment\n",
    "- Mean Length Increment\n",
    "- Mean Width\n",
    "- Cell cycle duration (Delta t)\n",
    "- Mean mCherry Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_early_outliers(\n",
    "    final_output_df_lineage,\n",
    "    early_timepoint_cutoff=12,\n",
    "    gaussian_subsample=0.2,\n",
    "    percentile_threshold=10,\n",
    "    filter_params=[\n",
    "        \"Mean Linear Growth Rate: Volume\",\n",
    "        \"Mean Exponential Growth Rate: Volume\",\n",
    "        \"Division: major_axis_length\",\n",
    "        \"Mean: minor_axis_length\",\n",
    "        \"Mean: mCherry Intensity\",\n",
    "        \"Delta time (s)\",\n",
    "    ],\n",
    "    plot_values_names=[\n",
    "        \"Volume Growth Rate (linear)\",\n",
    "        \"Volume Growth Rate (ratio)\",\n",
    "        \"Division Length\",\n",
    "        \"Minor Axis Length\",\n",
    "        \"Mean mCherry Intensity\",\n",
    "        \"Interdivision Time\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    final_output_df_trench_groupby = final_output_df_lineage.groupby(\n",
    "        \"phenotype trenchid\", sort=False\n",
    "    )\n",
    "    early_tpt_df = final_output_df_trench_groupby.apply(\n",
    "        lambda x: x[x[\"Final timepoints\"] < early_timepoint_cutoff].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "    ).persist()\n",
    "    for filter_param in filter_params:\n",
    "        early_param_series = early_tpt_df[filter_param]\n",
    "        all_param_values = (\n",
    "            early_param_series.sample(frac=gaussian_subsample).compute().tolist()\n",
    "        )\n",
    "        gaussian_fit = sp.stats.norm.fit(all_param_values)\n",
    "        gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "        early_param_series = dd.from_pandas(\n",
    "            early_param_series.compute().droplevel(1), npartitions=50\n",
    "        )\n",
    "        trench_probability = early_param_series.groupby(\"phenotype trenchid\").apply(\n",
    "            lambda x: np.exp(np.sum(gaussian_fit.logpdf(x)) / len(x)), meta=float\n",
    "        )\n",
    "\n",
    "        final_output_df_lineage[\n",
    "            filter_param + \": Probability\"\n",
    "        ] = trench_probability.persist()\n",
    "\n",
    "    final_output_df_onetrench = (\n",
    "        final_output_df_lineage.groupby(\"phenotype trenchid\")\n",
    "        .apply(lambda x: x.iloc[0])\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(22, 16))\n",
    "    query_list = []\n",
    "    for i, filter_param in enumerate(filter_params):\n",
    "        prob_threshold = np.nanpercentile(\n",
    "            final_output_df_onetrench[filter_param + \": Probability\"].tolist(),\n",
    "            percentile_threshold,\n",
    "        )\n",
    "        query = \"`\" + filter_param + \": Probability` > \" + str(prob_threshold)\n",
    "        query_list.append(query)\n",
    "\n",
    "        min_v, max_v = (\n",
    "            np.min(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "            np.max(final_output_df_onetrench[filter_param + \": Probability\"]),\n",
    "        )\n",
    "\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.title(plot_values_names[i], fontsize=22)\n",
    "        plt.xlabel(\"Unnormalized Likelihood\", fontsize=18)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.hist(\n",
    "            final_output_df_onetrench[\n",
    "                final_output_df_onetrench[filter_param + \": Probability\"]\n",
    "                < prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "        plt.hist(\n",
    "            final_output_df_onetrench[\n",
    "                final_output_df_onetrench[filter_param + \": Probability\"]\n",
    "                >= prob_threshold\n",
    "            ][filter_param + \": Probability\"].tolist(),\n",
    "            bins=50,\n",
    "            range=(min_v, max_v),\n",
    "        )\n",
    "\n",
    "    compiled_query = \" and \".join(query_list)\n",
    "    final_output_df_onetrench_filtered = final_output_df_onetrench.query(compiled_query)\n",
    "    final_output_df_filtered = final_output_df_lineage.loc[\n",
    "        final_output_df_onetrench_filtered.index.tolist()\n",
    "    ].persist()\n",
    "\n",
    "    return final_output_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_df_filtered = remove_early_outliers(\n",
    "    final_output_df_lineage,\n",
    "    early_timepoint_cutoff=12,\n",
    "    gaussian_subsample=0.2,\n",
    "    percentile_threshold=10,\n",
    ")\n",
    "plt.savefig(\"Prob_threshold_Replicate_1.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(final_output_df_filtered) / len(final_output_df_lineage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_values(\n",
    "    df,\n",
    "    label,\n",
    "    min_timepoint,\n",
    "    max_timepoint,\n",
    "    time_label=\"Final timepoints\",\n",
    "    flatten_vals=True,\n",
    "):\n",
    "    if flatten_vals:\n",
    "        masked_label_series = df.apply(\n",
    "            lambda x: np.array(x[label])[\n",
    "                (np.array(x[time_label]) >= min_timepoint)\n",
    "                * (np.array(x[time_label]) <= max_timepoint)\n",
    "            ],\n",
    "            axis=1,\n",
    "            meta=\"object\",\n",
    "        )\n",
    "        flattened_vals = np.concatenate(masked_label_series.compute().tolist())\n",
    "        return flattened_vals\n",
    "    else:\n",
    "        masked_label_series = (\n",
    "            df.groupby(\"phenotype trenchid\")\n",
    "            .apply(\n",
    "                lambda x: np.array(x[label])[\n",
    "                    (np.array(x[time_label]) >= min_timepoint)\n",
    "                    * (np.array(x[time_label]) <= max_timepoint)\n",
    "                ],\n",
    "                meta=\"object\",\n",
    "            )\n",
    "            .persist()\n",
    "        )\n",
    "        return masked_label_series\n",
    "\n",
    "\n",
    "def get_feature_stats(\n",
    "    df, feature_label, min_timepoint, max_timepoint, time_label=\"Final timepoints\"\n",
    "):\n",
    "    feature_vals = get_timepoint_values(\n",
    "        df, feature_label, min_timepoint, max_timepoint, time_label=time_label\n",
    "    )\n",
    "    feature_median = np.median(feature_vals)\n",
    "    feature_iqr = sp.stats.iqr(feature_vals)\n",
    "    return feature_median, feature_iqr\n",
    "\n",
    "\n",
    "def compute_score(df, feature_label, feature_median, feature_iqr):\n",
    "    scores = 1.35 * ((df[feature_label] - feature_median) / feature_iqr)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_feature_scores(\n",
    "    df, feature_label, init_timepoint_range=(0, 20), time_label=\"Final timepoints\"\n",
    "):\n",
    "    feature_median, feature_iqr = get_feature_stats(\n",
    "        df,\n",
    "        feature_label,\n",
    "        init_timepoint_range[0],\n",
    "        init_timepoint_range[1],\n",
    "        time_label=time_label,\n",
    "    )\n",
    "    scores = compute_score(df, feature_label, feature_median, feature_iqr)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_all_feature_scores(\n",
    "    df, feature_labels, init_timepoint_range=(0, 20), time_label=\"Final timepoints\"\n",
    "):\n",
    "\n",
    "    for feature_label in feature_labels:\n",
    "        print(feature_label)\n",
    "        feature_scores = get_feature_scores(\n",
    "            df,\n",
    "            feature_label,\n",
    "            init_timepoint_range=init_timepoint_range,\n",
    "            time_label=time_label,\n",
    "        )\n",
    "        df[feature_label + \": z score\"] = feature_scores.persist()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Properties (maybe go back to the per trench normalization?)\n",
    "\n",
    "1) Yeo-Johnson transform the data to get a more normal-like distribution.\n",
    "2) Convert transformed values to time-dependent z-scores using the following formula:\n",
    "\n",
    "$$ z(i,k,t) = 1.35 \\times \\frac{F_{i,k,t} - median_{t\\in \\tau}(F_{i,t})}{iqr_{t\\in \\tau}(F_{i,t})} $$\n",
    "\n",
    "where $F_{i,k,t}$ are the feature values for feature i, trench k at time t. $\\tau$ are the initial pre-induction timepoints. \n",
    "\n",
    "Essentially this is a z-score using the more outlier robust median and interquartile range to define the differences from normal bahavior. The 1.35 factor scales the values such that z-scores represent number of standard deviations from the mean for a normal distribution. Finally the values are normalized by initial behaviors trenchwise by the $median_{t\\in \\tau}(F_{i,k,t})$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_yj_transform(\n",
    "    final_output_df_filtered,\n",
    "    yeo_subsample=0.05,\n",
    "    early_timepoint_cutoff=12,\n",
    "    params_to_transform=[\n",
    "        \"Mean Linear Growth Rate: Volume\",\n",
    "        \"Mean Exponential Growth Rate: Volume\",\n",
    "        \"Division: major_axis_length\",\n",
    "        \"Mean: minor_axis_length\",\n",
    "        \"Mean: mCherry Intensity\",\n",
    "        \"Delta time (s)\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    subsample_df = final_output_df_filtered.sample(frac=yeo_subsample).persist()\n",
    "\n",
    "    for i, param in enumerate(params_to_transform):\n",
    "        all_param_values = subsample_df[param].astype(float).compute().tolist()\n",
    "        all_param_values = np.array(all_param_values)\n",
    "        all_param_values = all_param_values[~np.isnan(all_param_values)]\n",
    "        l_norm = sp.stats.yeojohnson_normmax(all_param_values)\n",
    "        final_output_df_filtered[param + \": Yeo-Johnson\"] = (\n",
    "            final_output_df_filtered[param]\n",
    "            .apply(lambda x: sp.stats.yeojohnson(x, lmbda=l_norm), meta=float)\n",
    "            .persist()\n",
    "        )\n",
    "\n",
    "    final_output_df_filtered = get_all_feature_scores(\n",
    "        final_output_df_filtered,\n",
    "        [param + \": Yeo-Johnson\" for param in params_to_transform],\n",
    "        init_timepoint_range=(0, early_timepoint_cutoff),\n",
    "    )\n",
    "    trenchiddf = final_output_df_filtered.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True\n",
    "    )\n",
    "\n",
    "    return final_output_df_filtered, trenchiddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_filtered, trenchiddf = apply_yj_transform(\n",
    "    final_output_df_filtered, yeo_subsample=0.05, early_timepoint_cutoff=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to interpolate trenchwise instead of sgRNAwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric import kernel_regression\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import sklearn as skl\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "def timeseries_kernel_reg(df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth):\n",
    "    def kernel_reg(\n",
    "        x_arr,\n",
    "        y_arr,\n",
    "        start=min_tpt,\n",
    "        end=max_tpt,\n",
    "        kernel_bins=kernel_bins,\n",
    "        kernel_bandwidth=kernel_bandwidth,\n",
    "    ):\n",
    "        intervals = np.linspace(start, end, num=kernel_bins, dtype=float)\n",
    "        w = kernel_regression.KernelReg(\n",
    "            y_arr,\n",
    "            x_arr,\n",
    "            \"c\",\n",
    "            reg_type=\"lc\",\n",
    "            bw=np.array([kernel_bandwidth]),\n",
    "            ckertype=\"gaussian\",\n",
    "        ).fit(intervals)[0]\n",
    "        reg_x, reg_y = (intervals, w)\n",
    "        return reg_x, reg_y\n",
    "\n",
    "    kernel_result = df.groupby(\"phenotype trenchid\").apply(\n",
    "        lambda x: kernel_reg(\n",
    "            (x[\"Final time (s)\"].values - (x[\"Delta time (s)\"].values / 2)),\n",
    "            x[y_label].values,\n",
    "        )[1],\n",
    "        meta=float,\n",
    "    )\n",
    "    return kernel_result\n",
    "\n",
    "\n",
    "def get_all_kernel_regs(\n",
    "    df, y_label_list, min_tpt=0, max_tpt=36000, kernel_bins=20, kernel_bandwidth=2700\n",
    "):\n",
    "    out_df = copy.copy(df)\n",
    "\n",
    "    for y_label in y_label_list:\n",
    "        kernel_result = timeseries_kernel_reg(\n",
    "            out_df, y_label, min_tpt, max_tpt, kernel_bins, kernel_bandwidth\n",
    "        )\n",
    "        out_df[\"Kernel Trace: \" + y_label] = kernel_result.persist()\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_transform = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta time (s)\",\n",
    "]\n",
    "score_params = [param + \": Yeo-Johnson: z score\" for param in params_to_transform]\n",
    "other_params = [\n",
    "    \"Mean Linear Growth Rate: Volume\",\n",
    "    \"Mean Exponential Growth Rate: Volume\",\n",
    "    \"Birth: major_axis_length\",\n",
    "    \"Division: major_axis_length\",\n",
    "    \"Birth: Volume\",\n",
    "    \"Division: Volume\",\n",
    "    \"Birth: Surface Area\",\n",
    "    \"Division: Surface Area\",\n",
    "    \"Mean: minor_axis_length\",\n",
    "    \"Mean: mCherry Intensity\",\n",
    "    \"Delta time (s)\",\n",
    "]\n",
    "\n",
    "params_to_trace = score_params + other_params\n",
    "\n",
    "# making an observation grid to project vals onto\n",
    "\n",
    "trenchiddf = get_all_kernel_regs(\n",
    "    trenchiddf,\n",
    "    params_to_trace,\n",
    "    min_tpt=0,\n",
    "    max_tpt=36000,\n",
    "    kernel_bins=20,\n",
    "    kernel_bandwidth=2700,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_df_barcodes = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.set_index(\"phenotype trenchid\", sorted=True)\n",
    "    .groupby(\"phenotype trenchid\", sort=False)\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .persist()\n",
    ")\n",
    "final_output_df_barcodes = final_output_df_barcodes.reset_index().set_index(\n",
    "    \"oDEPool7_id\", drop=True\n",
    ")\n",
    "# final_output_df_barcodes[\"N Observations\"] = final_output_df_barcodes.groupby(\"oDEPool7_id\",sort=False)[\"phenotype trenchid\"].apply(lambda x: len(x.unique()), meta=int)\n",
    "# final_output_df_barcodes[\"N Observations\"] = final_output_df_barcodes[\"N Observations\"].astype(int)\n",
    "final_output_df_barcodes = (\n",
    "    final_output_df_barcodes.reset_index().set_index(\"phenotype trenchid\").persist()\n",
    ")\n",
    "\n",
    "trenchiddf = trenchiddf.merge(\n",
    "    final_output_df_barcodes[\n",
    "        [\n",
    "            \"oDEPool7_id\",\n",
    "            \"Barcode\",\n",
    "            \"sgRNA\",\n",
    "            \"Closest Hamming Distance\",\n",
    "            \"EcoWG1_id\",\n",
    "            \"Gene\",\n",
    "            \"N Mismatch\",\n",
    "            \"Category\",\n",
    "            \"TargetID\",\n",
    "            \"barcodeid\",\n",
    "            \"N Observations\",\n",
    "        ]\n",
    "    ],\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").persist()\n",
    "trenchiddf_out = trenchiddf.groupby(\"phenotype trenchid\").apply(lambda x: x.iloc[0])\n",
    "trenchiddf_out[\"N Observations\"] = trenchiddf_out.groupby(\"oDEPool7_id\", sort=False)[\n",
    "    \"phenotype trenchid\"\n",
    "].apply(lambda x: len(x.unique()), meta=int)\n",
    "trenchiddf_out[\"N Observations\"] = trenchiddf_out[\"N Observations\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_params = [\"Kernel Trace: \" + param for param in other_params]\n",
    "kernel_score_params = [\n",
    "    \"Kernel Trace: \" + param + \": Yeo-Johnson: z score\" for param in params_to_transform\n",
    "]\n",
    "\n",
    "feature_vector_series = trenchiddf_out.apply(\n",
    "    lambda x: np.array(x[kernel_score_params].tolist()), axis=1\n",
    ")\n",
    "trenchiddf_out[\"Feature Vector\"] = feature_vector_series\n",
    "trenchiddf_nan_filtered = trenchiddf_out[\n",
    "    ~trenchiddf_out[\"Feature Vector\"].apply(lambda x: np.any(np.isnan(x)), meta=bool)\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strong_effect_threshold = 15\n",
    "\n",
    "# zero_vector = np.zeros((1,trenchiddf_nan_filtered[\"Feature Vector\"].iloc[0].shape[0]))\n",
    "# feature_arr = np.array(trenchiddf_nan_filtered[\"Feature Vector\"].tolist())\n",
    "# feature_arr_abs = np.abs(feature_arr)\n",
    "# trenchiddf_nan_filtered[\"Integrated Feature Vector\"] = [item for item in sp.integrate.simpson(feature_arr_abs,axis=2)]\n",
    "# trenchiddf_nan_filtered[\"Integrated Feature Max\"] = trenchiddf_nan_filtered[\"Integrated Feature Vector\"].apply(lambda x: np.max(x))\n",
    "# trenchiddf_nan_filtered[\"Integrated Euclidean Norm\"] = np.linalg.norm(np.array(trenchiddf_nan_filtered[\"Integrated Feature Vector\"].tolist()), axis=1)\n",
    "\n",
    "# sgrnadf_strong_effect = trenchiddf_nan_filtered[trenchiddf_nan_filtered[\"Integrated Feature Max\"]>=strong_effect_threshold]\n",
    "# min_v,max_v = np.min(trenchiddf_nan_filtered[\"Integrated Feature Max\"]),np.percentile(trenchiddf_nan_filtered[\"Integrated Feature Max\"],99)\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.title(\"Integrated Feature Max\")\n",
    "# plt.hist(trenchiddf_nan_filtered[trenchiddf_nan_filtered[\"Integrated Feature Max\"]<strong_effect_threshold][\"Integrated Feature Max\"].tolist(),bins=50,range=(min_v,max_v))\n",
    "# plt.hist(trenchiddf_nan_filtered[trenchiddf_nan_filtered[\"Integrated Feature Max\"]>=strong_effect_threshold][\"Integrated Feature Max\"].tolist(),bins=50,range=(min_v,max_v))\n",
    "# plt.show()\n",
    "\n",
    "# unique_genes, gene_counts = np.unique(sgrnadf_strong_effect[\"Gene\"][sgrnadf_strong_effect[\"Gene\"].apply(lambda x: type(x)==str)].tolist(), return_counts=True)\n",
    "# plt.title(\"sgRNAs per Gene\")\n",
    "# plt.xticks(range(0,20,2),labels=range(0,20,2))\n",
    "# plt.hist(gene_counts,bins=np.arange(20)-0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Representative Effect per TargetID\n",
    "Note this may need to be revisited later to resolve transients that are only resolvable at intermediate KO\n",
    "\n",
    "1) For each target, pick the sgRNA that has the strongest phenotype (highest integrated euclidean norm)\n",
    "2) Additionally identify any targets with titration information by saving a dataframe with targetIDs that posess at least N sgRNAs\n",
    "    - this is in a preliminary form; transfer to a full notebook later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchiddf_nan_filtered.to_pickle(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2022-02-10_gene_cluster_df_no_filter.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
