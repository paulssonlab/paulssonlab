{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Important Note\n",
    "\n",
    "Due to the current policy of SLURM schedulers to charge the full allocated time to the account fairshare (regardless of timeout), it is a good idea to refurbish the slurm scheduler to adopt a \"take and hold\" approach to O2 resources since you get penalized for time that you request but do not use making allocation of resources later in the day (after running one or two steps) more difficult.\n",
    "\n",
    "The primary piece of code that would need to be written for a take and hold model would need to include the following:\n",
    "\n",
    "- More sophisticated memory clearing of the dask_controller object (total memory clearing without a restart that leads to crashes) THIS FIRST \n",
    "- Rule-based adjustments of chunk sizes to limit per-node memory use to make sure a flat memory request can be made at cluster initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains the entire `TrenchRipper` pipline, divided into simple steps. This pipline is ideal for Mother <br>Machine image data where cells possess fluorescent segmentation markers. Segmentation on phase or brightfield data <br>is being developed, but is still an experimental feature.\n",
    "\n",
    "The steps in this pipeline are as follows:\n",
    "1. Extracting your Mother Machine data (.nd2) into hdf5 format\n",
    "2. Identifying and cropping individual trenches into kymographs\n",
    "3. Segmenting cells with a fluorescent marker\n",
    "4. Determining lineages and object properties\n",
    "\n",
    "In each step, the user will dynamically specify parameters using a series of interactive diagnostics on their dataset. <br>Following this, a parameter file will be written to disk and then used to deploy a parallel computation on the <br>dataset, either locally or on a SLURM cluster.\n",
    "\n",
    "\n",
    "This is intended as an end-to-end solution to analyzing Mother Machine data. As such, **it is not trivial to plug data <br>directly into intermediate steps**, as it will lack the correct formatting and associated metadata. A notable <br>exception to this is using another program to segment data. The library references binary segmentation masks using <br>only metadata derived from their associated kymographs. As such, it is possible to generate segmentations on these <br>kymographs elsewhere and place them into the segmentation data path to have `TrenchRipper` act on those <br>segmentations instead. More on this in the segmentation section..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Run this section to import all relavent packages and libraries used in this notebook. You must run this everytime you open a new python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition of active memory manager\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"distributed.scheduler.active-memory-manager.start\": True})\n",
    "dask.config.set({\"distributed.scheduler.worker-ttl\": \"5m\"})\n",
    "dask.config.set({\"distributed.scheduler.allowed-failures\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Growth/Division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/\"\n",
    ")\n",
    "# hdf5inputpath = \"/home/de64/scratch/de64/sync_folder/2021-01-28_lDE14/run/\"\n",
    "nd2file = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Experiment.nd2\"\n",
    "\n",
    "# headpath = \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Small_Dataset/\"\n",
    "# # hdf5inputpath = \"/home/de64/scratch/de64/sync_folder/2021-01-28_lDE14/run/\"\n",
    "# nd2file = \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Experiment.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"1:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=200,\n",
    "#     n_workers_min=20,\n",
    "#     memory=\"6GB\",\n",
    "#     working_directory=\"/home/de64/scratch/de64/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"2:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    n_workers_min=50,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.ndextract.hdf5_fov_extractor(\n",
    "    nd2file,\n",
    "    headpath,\n",
    "    tpts_per_file=50,\n",
    "    ignore_fovmetadata=False,\n",
    "    nd2reader_override={\"z_levels\": [], \"z_coordinates\": []},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/home/de64/scratch/de64/sync_folder/2021-10-21_Flat_Fields/MCHERRY_20x_Ph2_flatfield.tiff\"\n",
    "\"/home/de64/scratch/de64/sync_folder/2021-10-21_Flat_Fields/fake_dark_img.tiff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_flatfieldpaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "[ '/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002',\n",
    " '/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/ezrdm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/mbm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/Sb7_L35',\n",
    " '/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9',\n",
    " '/n/scratch3/users/d/de64/Vibrio_2_1_TOP',\n",
    " '/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms',\n",
    " '/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X',\n",
    " '/n/scratch3/users/d/de64/Main_Experiment',\n",
    " '/n/scratch3/users/d/de64/bde17_gotime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath, persist_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y Percentile 95\n",
    "Y Foreground Percentile 65\n",
    "Y Smoothing Kernel 51\n",
    "Y Percentile Threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_consensus_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimum Trench Length 50\n",
    "Midpoint Distance Tolerance 50\n",
    "Y Padding 20\n",
    "Trench Length 130\n",
    "Orientation Detection Method 1\n",
    "Expected Number of Rows (Manual Orientation Detection) 10\n",
    "Alternate Orientation True\n",
    "Alternate Orientation Over Rows? False\n",
    "Use Median Drift? False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X Percentile 85\n",
    "X Background Kernel 51\n",
    "X Smoothing Kernel 5\n",
    "Otsu Threshold Scaling 0.0\n",
    "Minimum X Threshold 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trench Width 20\n",
    "Trench Presence Threshold 1.0\n",
    "All Channels ['mCherry']\n",
    "Filter Channel None\n",
    "Invert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Notes on improved scheduling\n",
    "\n",
    "Make a utility that will be able to run analysis steps without intervention from the ipynb, so things can be run as \"fire and forget\"\n",
    "\n",
    "I will start by adapting this to the simple single step case of the lineage trace. In order to fire off an independent job that will deploy, maintain and close the cluster dynamically for a single application I need the following information from the process:\n",
    "\n",
    "- The estimated time for the process to complete so that the scheduler head knows how long it needs to be queued up\n",
    "- The estimated minimum memory in the pool to execute the task successfully\n",
    "\n",
    "I should be able to get this information as a method from the process.\n",
    "\n",
    "In addition, this scheduler should attempt to minimally impact my fairshare. Features that would promote this would be:\n",
    "\n",
    "- Don't overschedule the time usage of the process. Queue up workers with short wall times (30 mins) and just maintain a constant target size. Currently implementing this...\n",
    "- Setting some kind of minimum worker number to ensure progression requires the proper amount of memory to be in place. Try setting the adaptive minimum based on memory considerations. Forget this for now, the adaptive scheduler might be able to handle this...\n",
    "\n",
    "Implement headless scheduler with fixed resources and task before integrating with the requirement of the job itself...\n",
    "\n",
    "Headless scheduler doesn't seem worth it....continue using with the adaptive scheduling adjustment and see if that helps...\n",
    "\n",
    "Couldnt find an easy way to dynamically adapt the memory per worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(headpath=headpath, paramfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ff.plot_histograms(intensity_range=(0, 1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller, trench_timepoints_per_file=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Fluorescence Segmentation\n",
    "\n",
    "Now that you have copped your data into kymographs, we will now perform segmentation/cell detection <br>\n",
    "on your kymographs. Currently, this pipeline only supports segmentation of fluorescence images; however, <br>\n",
    "segmentation of transmitted light imaging techniques is in development.\n",
    "\n",
    "The output of this step will be a set of `segmentation_[File #].hdf5` files stored in `headpath/fluorsegmentation`.<br>\n",
    "The image data stored in these files takes the exact same form as the kymograph data, `(K,T,Y,X)` arrays <br>\n",
    "where K is the trench index, T is time, and Y,X are the crop dimensions. These arrays are accessible using <br>\n",
    "keys of the form `\"[Trench Row Number]\"`.\n",
    "\n",
    "Since no metadata is generated by this step, it is possible to use another segmentation algorithm on the kymograph <br>\n",
    "data. The output of segmentation must be split into `segmentation_[File #].hdf5` files, where `[File #]` agrees with the<br>\n",
    "corresponding `kymograph_[File #].hdf5` file. Additionally, the `(K,T,Y,X)` arrays must be of the same shape as the <br>\n",
    "kymograph arrays and accessible at the corresponding `\"[Trench Row Number]\"` key. These files must be placed into <br>\n",
    "their own folder at `headpath/foldername`. This folder may then be used in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "##### Initialize the interactive segmentation class\n",
    "\n",
    "As a first step, initialize the `tr.fluo_segmentation_interactive` class that will be handling all steps of generating a segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_segmentation = tr.fluo_segmentation_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "##### Choose channel to segment on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.choose_seg_channel_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "#### Import data\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**fov_idx (int)** :\n",
    "\n",
    "**n_trenches (int)** :\n",
    "\n",
    "**t_range (tuple)** :\n",
    "\n",
    "**t_subsample_step (int)** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.import_array_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "8 Bit Maximum: 3000\n",
    "Scale Fluorescence? False\n",
    "Scaling Percentile: 90\n",
    "Image Scaling Factor: 1.0\n",
    "Gaussian Kernel Sigma: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_processed_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### Determine Cell Mask Envelope\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**cell_mask_method (str)** : Thresholding method, can be a local or global Otsu threshold.\n",
    "\n",
    "**cell_otsu_scaling (float)** : Scaling factor applied to determined threshold.\n",
    "\n",
    "**local_otsu_r (int)** : Radius of thresholding kernel used in the local otsu thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Local Threshold Method:': 'otsu',\n",
    " 'Background Threshold Method:': 'triangle',\n",
    " 'Global Threshold:': 18,\n",
    " 'Local Window Size:': 11,\n",
    " 'Otsu Scaling:': 1.0,\n",
    " 'Niblack K:': 0.2,\n",
    " 'Background Threshold Scaling:': 1.0,\n",
    " 'Minimum Object Size:': 20,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_cell_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    " 'Hessian Blur Sigma:': 1.75,\n",
    " 'Hessian Rolling Ball Size:': 20,\n",
    " 'Hessian Local Threshold Method:': 'niblack',\n",
    " 'Hessian Otsu Scaling:': 1.0,\n",
    " 'Hessian Niblack K:': 0.05,\n",
    " 'Hessian Local Window Size:': 7,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_eig_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " 'Distance Threshold:': 1,\n",
    " 'Border Buffer:': 1,\n",
    " 'Horizontal Border Only:': False,\n",
    " 'Segmentation Channel:': 'mCherry'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_dist_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_marker_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Generate Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"1:00:00\",\n",
    "    local=False,\n",
    "    n_workers=400,\n",
    "    n_workers_min=20,\n",
    "    memory=\"1GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = tr.segment.fluo_segmentation_cluster(headpath, paramfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segment.dask_segment(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Stop Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## Lineage Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = tr.tracking.scorefn(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    u_size=0.22,\n",
    "    sig_size=0.08,\n",
    "    u_pos=0.21,\n",
    "    sig_pos=0.1,\n",
    "    w_merge=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_function.interactive_scorefn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver = tr.tracking.tracking_solver(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    ScoreFn=score_function,\n",
    "    edge_limit=2,\n",
    ")\n",
    "data, orientation = score_function.output.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver.interactive_tracking(data, orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tracking_Solver.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate Lineage Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"1:00:00\",\n",
    "    local=False,\n",
    "    n_workers=400,\n",
    "    n_workers_min=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver = tr.tracking.tracking_solver(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    paramfile=True,\n",
    "    volume_estimation=True,\n",
    "    props_list=[\"area\", \"major_axis_length\", \"minor_axis_length\"],\n",
    "    props_to_unpack={},\n",
    "    pixel_scaling_factors={\"area\": 2, \"major_axis_length\": 1, \"minor_axis_length\": 1},\n",
    "    intensity_props_list=[\"mean_intensity\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tracking_Solver.compute_all_lineages(dask_controller, entries_per_partition=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "## Examine Results\n",
    "\n",
    " - Finalize a simple viewer that works without downstream association of barcodes\n",
    " - I have some old code for this, but it is inferior to the viewer I built for linked graphs\n",
    " - Rework the second viewer to fill this role and delete the old viewer from the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "import h5py\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import skimage as sk\n",
    "import xarray as xr\n",
    "from distributed.client import futures_of\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "omg = {0: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "{**{0: 0}, **{1: 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm, colors\n",
    "\n",
    "\n",
    "def get_lineage_df_lookup(lineage_df, trenchid, segment_index_offset=1):\n",
    "    # segment_index_offset to correct differences from background label, check if this is the case\n",
    "    init_idx = int(f\"{int(trenchid):08n}\" + \"0000\")\n",
    "    fin_idx = int(f\"{int(trenchid):08n}\" + \"9999\")\n",
    "    selection = lineage_df.loc[init_idx:fin_idx]  # .compute()\n",
    "    selection[\"Label\"] = (\n",
    "        selection[\"CellID\"] % (12 - segment_index_offset)\n",
    "    ) + segment_index_offset\n",
    "    selection[\"Segment Index\"] = selection[\"Segment Index\"] + segment_index_offset\n",
    "    selection = selection[[\"timepoints\", \"Segment Index\", \"Label\"]].compute()\n",
    "    selection = selection.set_index([\"timepoints\", \"Segment Index\"]).sort_index()\n",
    "    return selection\n",
    "\n",
    "\n",
    "def unlinked_kymograph(\n",
    "    xrstack, seg_xrstack=None, lineage_df_path=None, y_scale=3, x_window_size=300\n",
    "):\n",
    "    #### stream must return trenchid value\n",
    "    #### df must have trenchid lookups\n",
    "    width, height = xrstack.shape[3], int(xrstack.shape[2] * y_scale)\n",
    "    x_window_scale = x_window_size / xrstack.shape[3]\n",
    "    x_size = int(xrstack.shape[3] * (y_scale * x_window_scale))\n",
    "\n",
    "    if lineage_df_path != None:\n",
    "        if len(seg_xrstack.shape) == 4:\n",
    "            lineage_df = dd.read_parquet(lineage_df_path).set_index(\n",
    "                \"Trenchid Timepoint Index\", sorted=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Segmentation xarray must be dim(k,t,y,x) to support lineage display!\"\n",
    "            )\n",
    "\n",
    "    def select_pt_load_image(channel, trenchid, width=width, height=height):\n",
    "        arr = xrstack.loc[channel, trenchid].values\n",
    "        img = hv.Image(arr, bounds=(0, 0, width, height))\n",
    "        return img\n",
    "\n",
    "    def select_pt_load_seg(trenchid, lineage_df=lineage_df, width=width, height=height):\n",
    "        seg_arr = seg_xrstack.loc[trenchid].values\n",
    "        if type(lineage_df) is dd.core.DataFrame:\n",
    "            lineage_df_lookup = get_lineage_df_lookup(lineage_df, trenchid)\n",
    "            for t in range(seg_arr.shape[0]):\n",
    "                working_lookup = {\n",
    "                    **lineage_df_lookup.loc[t].to_dict()[\"Label\"],\n",
    "                    **{0: 0},\n",
    "                }\n",
    "                seg_arr[t] = np.vectorize(working_lookup.get)(seg_arr[t])\n",
    "            seg_arr = (\n",
    "                seg_arr.swapaxes(1, 2).reshape(-1, seg_arr.shape[1]).swapaxes(0, 1)\n",
    "            )\n",
    "        seg_img = hv.Image(seg_arr, bounds=(0, 0, width, height))\n",
    "        return seg_img\n",
    "\n",
    "    def print_trenchid(trenchid):\n",
    "        return hv.Text(3.0, 20.0, str(trenchid), fontsize=30)\n",
    "\n",
    "    def set_bounds(\n",
    "        fig, element, y_dim=height, x_dim=width, x_window_size=x_window_size\n",
    "    ):\n",
    "        sy = y_dim - 0.5\n",
    "        sx = x_dim - 0.5\n",
    "\n",
    "        fig.state.y_range.bounds = (-0.5, sy)\n",
    "        fig.state.x_range.bounds = (0, sx)\n",
    "        fig.state.x_range.start = 0\n",
    "        fig.state.x_range.reset_start = 0\n",
    "        fig.state.x_range.end = x_window_size\n",
    "        fig.state.x_range.reset_end = x_window_size\n",
    "\n",
    "        fig.state.y_range.start = 0\n",
    "        fig.state.y_range.reset_start = 0\n",
    "        fig.state.y_range.end = y_dim\n",
    "        fig.state.y_range.reset_end = y_dim\n",
    "\n",
    "    image_stack = hv.DynamicMap(select_pt_load_image, kdims=[\"Channel\", \"trenchid\"])\n",
    "\n",
    "    kymograph_display = image_stack.opts(\n",
    "        plot={\n",
    "            \"Image\": dict(\n",
    "                colorbar=True, tools=[\"hover\"], hooks=[set_bounds], aspect=\"equal\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    kymograph_display = kymograph_display.opts(\n",
    "        cmap=\"Greys_r\", height=height, width=x_size\n",
    "    )\n",
    "\n",
    "    kymograph_display = kymograph_display.redim.range(trenchid=(0, xrstack.shape[1]))\n",
    "    kymograph_display = kymograph_display.redim.values(\n",
    "        Channel=xrstack.coords[\"Channel\"].values.tolist(),\n",
    "        trenchid=xrstack.coords[\"trenchid\"].values.tolist(),\n",
    "    )\n",
    "\n",
    "    trenchid_display = hv.DynamicMap(print_trenchid, kdims=[\"trenchid\"])\n",
    "    trenchid_display = trenchid_display.opts(text_align=\"left\", text_color=\"white\")\n",
    "\n",
    "    if type(seg_xarray) == xr.core.dataarray.DataArray:\n",
    "        if lineage_df_path != None:\n",
    "            seg_stack = hv.DynamicMap(select_pt_load_seg, kdims=[\"trenchid\"])\n",
    "        else:\n",
    "            seg_stack = hv.DynamicMap(select_pt_load_seg, kdims=[\"trenchid\"])\n",
    "\n",
    "        seg_display = seg_stack.opts(\n",
    "            plot={\n",
    "                \"Image\": dict(\n",
    "                    colorbar=True, tools=[\"hover\"], hooks=[set_bounds], aspect=\"equal\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # cmap = cm.get_cmap('Set3')\n",
    "        # bounds=np.linspace(0.5,12.5,num=13)\n",
    "        # norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        seg_display = seg_display.opts(\n",
    "            cmap=\"Set3\", height=height, width=x_size, clim=(-0.5, 11.5)\n",
    "        )\n",
    "\n",
    "        seg_display = seg_display.redim.range(trenchid=(0, seg_xrstack.shape[0]))\n",
    "        seg_display = seg_display.redim.values(\n",
    "            trenchid=seg_xrstack.coords[\"trenchid\"].values.tolist()\n",
    "        )\n",
    "\n",
    "        output_display = hv.Layout(\n",
    "            [kymograph_display * trenchid_display, seg_display]\n",
    "        ).cols(1)\n",
    "\n",
    "    else:\n",
    "        output_display = kymograph_display * trenchid_display\n",
    "\n",
    "    return output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "ree = get_lineage_df_lookup(lineage_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "ree.loc[13].to_dict()[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(headpath + \"/lineage/output\").set_index(\n",
    "    \"Trenchid Timepoint Index\", sorted=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df.loc[:18260000].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hdf5(filename, channel):\n",
    "    with h5py.File(filename, \"r\") as infile:\n",
    "        data = infile[channel][:]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_xarr(\n",
    "    headpath,\n",
    "    segname=\"fluorsegmentation\",\n",
    "    subset=None,\n",
    "    in_memory=False,\n",
    "    in_distributed_memory=False,\n",
    "    unwrap=True,\n",
    "):\n",
    "    data_parquet = dd.read_parquet(headpath + \"/kymograph/metadata\")\n",
    "    file_indices = data_parquet[\"File Index\"].unique().compute().to_list()\n",
    "\n",
    "    delayed_fetch_hdf5 = delayed(fetch_hdf5)\n",
    "    filenames = [\n",
    "        headpath + \"/\" + segname + \"/segmentation_\" + str(file_idx) + \".hdf5\"\n",
    "        for file_idx in file_indices\n",
    "    ]\n",
    "\n",
    "    sample = fetch_hdf5(filenames[0], \"data\")\n",
    "\n",
    "    channel_arr = []\n",
    "    filenames = [\n",
    "        headpath + \"/\" + segname + \"/segmentation_\" + str(file_idx) + \".hdf5\"\n",
    "        for file_idx in file_indices\n",
    "    ]\n",
    "    delayed_arrays = [delayed_fetch_hdf5(fn, \"data\") for fn in filenames]\n",
    "    da_file_arrays = [\n",
    "        da.from_delayed(delayed_reader, shape=sample.shape, dtype=sample.dtype)\n",
    "        for delayed_reader in delayed_arrays\n",
    "    ]\n",
    "    da_file_index_arr = da.concatenate(da_file_arrays, axis=0)\n",
    "\n",
    "    if unwrap:\n",
    "        da_file_index_arr = (\n",
    "            da_file_index_arr.swapaxes(2, 3)\n",
    "            .reshape(da_file_index_arr.shape[0], -1, da_file_index_arr.shape[2])\n",
    "            .swapaxes(1, 2)\n",
    "        )\n",
    "        if subset != None:\n",
    "            da_file_index_arr = da_file_index_arr[subset]\n",
    "        if in_memory:\n",
    "            da_file_index_arr = da_file_index_arr.compute()\n",
    "        elif in_distributed_memory:\n",
    "            da_file_index_arr = da_file_index_arr.persist()\n",
    "\n",
    "        # defining xarr\n",
    "        dims = [\"trenchid\", \"y\", \"xt\"]\n",
    "        coords = {d: np.arange(s) for d, s in zip(dims, da_file_index_arr.shape)}\n",
    "        kymo_xarr = xr.DataArray(\n",
    "            da_file_index_arr, dims=dims, coords=coords, name=\"Data\"\n",
    "        ).astype(\"uint16\")\n",
    "\n",
    "        return kymo_xarr\n",
    "\n",
    "    else:\n",
    "        if subset != None:\n",
    "            da_file_index_arr = da_file_index_arr[subset]\n",
    "        if in_memory:\n",
    "            da_file_index_arr = da_file_index_arr.compute()\n",
    "        elif in_distributed_memory:\n",
    "            da_file_index_arr = da_file_index_arr.persist()\n",
    "\n",
    "        # defining xarr\n",
    "        dims = [\"trenchid\", \"t\", \"y\", \"x\"]\n",
    "        coords = {d: np.arange(s) for d, s in zip(dims, da_file_index_arr.shape)}\n",
    "        kymo_xarr = xr.DataArray(\n",
    "            da_file_index_arr, dims=dims, coords=coords, name=\"Data\"\n",
    "        ).astype(\"uint16\")\n",
    "\n",
    "        return kymo_xarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_lineage_df_lookup(lineage_df,k_range):\n",
    "#     init_idx = int(f'{int(k_range[0]):08n}' + '0000')\n",
    "#     fin_idx = int(f'{int(k_range[1]):08n}' + '9999')\n",
    "#     selection = lineage_df.loc[init_idx:fin_idx]\n",
    "#     selection[\"Label\"] = selection[\"CellID\"]%12\n",
    "#     selection = selection[[\"trenchid\",\"timepoints\",\"Segment Index\",\"Label\"]].compute()\n",
    "#     selection = selection.set_index([\"trenchid\",\"timepoints\",\"Segment Index\"]).sort_index()\n",
    "#     return selection\n",
    "\n",
    "# def lazy_lineage_map(da_block,lineage_df=None,block_info=None):\n",
    "#     if block_info != None:\n",
    "#         loc = block_info[None]['array-location']\n",
    "#         k_range,t_range=(loc[0],loc[1])\n",
    "#         print(k_range)\n",
    "#         cell_label_lookup = get_lineage_df_lookup(lineage_df,k_range)\n",
    "\n",
    "#         for k in range(k_range[0],k_range[1]):\n",
    "#             for t in range(t_range[0],t_range[1]):\n",
    "#                 working_lookup = cell_label_lookup.loc[k,t].to_dict()\n",
    "#                 da_block[k,t] = np.vectorize(working_lookup.get)(da_block[k,t])\n",
    "#         return da_block\n",
    "#     else:\n",
    "#         return da_block\n",
    "#     # lineage_lookup = get_lineage_df_lookup(lineage_df,trenchid)\n",
    "#     # test_dict = {i:i+1 for i in range(1000)}\n",
    "#     # for k in range(x.shape[0]):\n",
    "#     #     for t in range(x.shape[1]):\n",
    "#     #         x[k,t] = np.vectorize(test_dict.get)(x[k,t])\n",
    "#     # return x\n",
    "# # def func(da_block,block_info=None):\n",
    "# #     if block_info != None:\n",
    "# #         print(block_info)\n",
    "# #     else:\n",
    "# #         return da_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_xarray = seg_xarr(headpath, unwrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_xarray[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lineage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_xarray = tr.kymo_xarr(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unlinked_kymo = unlinked_kymograph(\n",
    "    kymo_xarray, seg_xrstack=seg_xarray, lineage_df_path=headpath + \"/lineage/output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlinked_kymo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE LATER\n",
    "\n",
    "from ipywidgets import IntSlider, IntText, interactive\n",
    "\n",
    "kyview = tr.analysis.kymograph_viewer(headpath, \"mCherry\", \"fluorsegmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": 
[
    "%matplotlib widget\n",
    "kyviewer = interactive(\n",
    "    kyview.inspect_trench,\n",
    "    {\"manual\": True},\n",
    "    file_idx=IntText(value=0, description=\"File Index:\", disabled=False),\n",
    "    trench_idx=IntText(value=0, description=\"Trench Index:\", disabled=False),\n",
    "    x_size=IntSlider(\n",
    "        value=20, description=\"X Size:\", min=0, max=50, step=1, disabled=False\n",
    "    ),\n",
    "    y_size=IntSlider(\n",
    "        value=6, description=\"Y Size:\", min=0, max=30, step=1, disabled=False\n",
    "    ),\n",
    ")\n",
    "display(kyviewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2: Barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Barcodes\"\n",
    "hdf5inputpath = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/run/\"\n",
    "# nd2file = \"/home/de64/scratch/de64/sync_folder/2021-05-27_lDE18_20x_run_1/run_100ms_mchry100_yfp50.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.marlin_extractor(\n",
    "    hdf5inputpath,\n",
    "    headpath,\n",
    "    pixel_microns=0.2125,\n",
    "    metaparsestr=\"metadata_{timepoint:d}.hdf5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/home/de64/scratch/de64/sync_folder/2021-10-21_Flat_Fields/RFP-Penta_20x_Ph2_flatfield.tiff\"\n",
    "\"/home/de64/scratch/de64/sync_folder/2021-10-21_Flat_Fields/Cy5-Penta_20x_Ph2_flatfield.tiff\"\n",
    "\"/home/de64/scratch/de64/sync_folder/2021-10-21_Flat_Fields/Cy7-Penta_20x_Ph2_flatfield.tiff\"\n",
    "\"/home/de64/scratch/de64/sync_folder/2021-10-21_Flat_Fields/fake_dark_img.tiff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_flatfieldpaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath, persist_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y Percentile 99\n",
    "Y Foreground Percentile 65\n",
    "Y Smoothing Kernel 51\n",
    "Y Percentile Threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_consensus_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimum Trench Length 80\n",
    "Midpoint Distance Tolerance 50\n",
    "Y Padding 20\n",
    "Trench Length 130\n",
    "Orientation Detection Method 1\n",
    "Expected Number of Rows (Manual Orientation Detection) 10\n",
    "Alternate Orientation True\n",
    "Alternate Orientation Over Rows? False\n",
    "Use Median Drift? False\n",
    "Consensus Orientations [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "X Percentile 98\n",
    "X Background Kernel 61\n",
    "X Smoothing Kernel 5\n",
    "Otsu Threshold Scaling 0.\n",
    "Minimum X Threshold 500\n",
    "Trench Width 10\n",
    "Trench Presence Threshold 1.0\n",
    "All Channels ['BF', 'Cy5', 'Cy7', 'RFP']\n",
    "Filter Channel None\n",
    "Invert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(headpath=headpath, paramfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller, trench_timepoints_per_file=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "## FISH Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.reset_worker_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191",
   "metadata": {},
   "source": [
    "#### Get Barcode Signal (Percentile Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tr.get_all_image_measurements(\n",
    "    dask_controller,\n",
    "    headpath,\n",
    "    headpath + \"/percentiles\",\n",
    "    [\"RFP\", \"Cy5\", \"Cy7\"],\n",
    "    \"98th Percentile\",\n",
    "    np.percentile,\n",
    "    98,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "#### Determine Barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test = tr.fish_analysis(\n",
    "    headpath,\n",
    "    \"../2021-11-25_Combined_Analysis_v2/analysis_archive/CRISPRi_pipeline/lDE20_final_df.tsv\",\n",
    "    remove_bit_list=[],\n",
    "    hamming_thr=2,\n",
    "    channel_names=[\"RFP 98th Percentile\", \"Cy5 98th Percentile\", \"Cy7 98th Percentile\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.plot_signal_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.get_bit_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.bit_threshold_list = [\n",
    "    1000,\n",
    "    800,\n",
    "    1400,\n",
    "    1000,\n",
    "    1000,\n",
    "    1000,\n",
    "    1000,\n",
    "    1000,\n",
    "    1000,\n",
    "    1100,\n",
    "    10000,\n",
    "    5000,\n",
    "    5000,\n",
    "    5500,\n",
    "    4000,\n",
    "    3000,\n",
    "    3200,\n",
    "    4000,\n",
    "    3000,\n",
    "    4500,\n",
    "    800,\n",
    "    1000,\n",
    "    750,\n",
    "    800,\n",
    "    1000,\n",
    "    800,\n",
    "    800,\n",
    "    800,\n",
    "    900,\n",
    "    800,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.plot_bit_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.export_bit_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fish_test.output_barcode_df(\n",
    "    dask_controller,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Barcodes/barcode_output_df.hdf5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
