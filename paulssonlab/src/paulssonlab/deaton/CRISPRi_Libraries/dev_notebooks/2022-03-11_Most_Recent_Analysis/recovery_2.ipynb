{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Mapping Barcodes and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from dask.distributed import wait\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition of active memory manager\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"distributed.scheduler.active-memory-manager.start\": True})\n",
    "dask.config.set({\"distributed.scheduler.worker-ttl\": \"5m\"})\n",
    "dask.config.set({\"distributed.scheduler.allowed-failures\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Barcodes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dask_controller = tr.trcluster.dask_controller(\n",
    "#     walltime=\"2:00:00\",\n",
    "#     local=False,\n",
    "#     n_workers=200,\n",
    "#     n_workers_min=200,\n",
    "#     memory=\"8GB\",\n",
    "#     working_directory=\"/home/de64/scratch/de64/dask\",\n",
    "# )\n",
    "# dask_controller.startdask()\n",
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"3:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    n_workers_min=20,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Import Lineage Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Optimizing Growth Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,query_list,client=False,repartition=False,persist=False):\n",
    "    #filter_list must be in df.query format (see pandas docs)\n",
    "    \n",
    "    #returns persisted dataframe either in cluster or local\n",
    "    \n",
    "    compiled_query = ' and '.join(query_list)\n",
    "    out_df = df.query(compiled_query)\n",
    "    if persist:\n",
    "        if client:\n",
    "            out_df = client.daskclient.persist(out_df)\n",
    "        else:\n",
    "            out_df = out_df.persist()\n",
    "        \n",
    "    if repartition:\n",
    "        init_size = len(df)\n",
    "        final_size = len(out_df)\n",
    "        ratio = init_size//final_size\n",
    "        out_df = out_df.repartition(npartitions=(df.npartitions // ratio) + 1)\n",
    "        if persist:\n",
    "            if client:\n",
    "                out_df = client.daskclient.persist(out_df)\n",
    "            else:\n",
    "                out_df = out_df.persist()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "def get_first_cell_timepoint(df):\n",
    "    min_tpts = df.groupby(['Global CellID'])['timepoints'].idxmin().tolist()\n",
    "    init_cells = df.loc[min_tpts]\n",
    "    return init_cells\n",
    "\n",
    "def get_last_cell_timepoint(df):\n",
    "    max_tpts = df.groupby(['Global CellID'])['timepoints'].idxmax().tolist()\n",
    "    fin_cells = df.loc[max_tpts]\n",
    "    return fin_cells\n",
    "\n",
    "def define_cell_cycle_timepoints_df(query,size_metrics):\n",
    "    ## new stuff\n",
    "    cell_cycle_timepoints_df = query[[\"timepoints\",\"time (s)\",\"trenchid\"] + size_metrics + [\"mCherry mean_intensity\"]]\n",
    "    cell_cycle_timepoints_df[\"Cell Cycle timepoints\"] = cell_cycle_timepoints_df.groupby(\"Global CellID\")[\"timepoints\"].apply(lambda x: x - x.iloc[0])\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.drop(\"timepoints\", axis=1)\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.rename({\"time (s)\":\"Observation time (s)\"}, axis=1)\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.reset_index(drop=False).set_index([\"Global CellID\",\"Cell Cycle timepoints\"])\n",
    "    \n",
    "    return cell_cycle_timepoints_df\n",
    "\n",
    "def compile_ols_arrs(kymo_df,min_fov,max_fov):\n",
    "    min_fov_idx = int(f'{int(min_fov):08n}' + '000000000000')\n",
    "    max_fov_idx = int(f'{int(max_fov):08n}' + '999999999999')\n",
    "    selected_kymo_df = kymo_df.loc[min_fov_idx:max_fov_idx]\n",
    "    timepoint_df = selected_kymo_df.groupby([\"fov\",\"timepoints\"])[\"time (s)\"].apply(lambda x: x.iloc[0]).compute(scheduler='threads')\n",
    "    timepoint_df = pd.DataFrame(timepoint_df).sort_index()\n",
    "    timepoint_df[\"time (s)^2\"] = timepoint_df[\"time (s)\"]**2\n",
    "    fov_list = timepoint_df.index.get_level_values(0).unique().tolist()\n",
    "    time_arr = np.array([timepoint_df[\"time (s)\"].loc[f].values for f in fov_list])\n",
    "    time_arr_accum = np.add.accumulate(time_arr,axis=1)\n",
    "    time_sq_arr = np.array([timepoint_df[\"time (s)^2\"].loc[f].values for f in fov_list])\n",
    "    time_sq_arr_accum = np.add.accumulate(time_sq_arr,axis=1)\n",
    "    return fov_list,time_arr,time_arr_accum,time_sq_arr_accum\n",
    "\n",
    "def fit_least_squares(log_size_arr,fov_idx_in_arr,time_i,time_f,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr):\n",
    "    # everything assumes an inclusive time_f\n",
    "    if time_i>0:\n",
    "        t_sum = time_arr_accum[fov_idx_in_arr,time_f]-time_arr_accum[fov_idx_in_arr,time_i-1]\n",
    "        t_sq_sum = time_sq_arr_accum[fov_idx_in_arr,time_f]-time_sq_arr_accum[fov_idx_in_arr,time_i-1]\n",
    "    else:\n",
    "        t_sum = time_arr_accum[fov_idx_in_arr,time_f]\n",
    "        t_sq_sum = time_sq_arr_accum[fov_idx_in_arr,time_f]\n",
    "        \n",
    "    t_sum = t_sum+interp_time_arr[fov_idx_in_arr,time_i]+interp_time_arr[fov_idx_in_arr,time_f+1]\n",
    "    t_sq_sum = t_sq_sum+interp_time_sq_arr[fov_idx_in_arr,time_i]+interp_time_sq_arr[fov_idx_in_arr,time_f+1]\n",
    "    \n",
    "    n_obs = time_f-time_i+1\n",
    "    n_obs = n_obs+2 # for interpolated values\n",
    "    \n",
    "    size_sum = np.sum(log_size_arr)\n",
    "    \n",
    "    time_arr_with_interpolation = time_arr[fov_idx_in_arr,time_i:time_f+1].tolist()\n",
    "    time_arr_with_interpolation = [interp_time_arr[fov_idx_in_arr,time_i]] + time_arr_with_interpolation + [interp_time_arr[fov_idx_in_arr,time_f+1]]\n",
    "    time_arr_with_interpolation = np.array(time_arr_with_interpolation)\n",
    "    \n",
    "    time_size_sum = np.sum(log_size_arr*time_arr_with_interpolation)\n",
    "    \n",
    "    denom = (n_obs*t_sq_sum) - (t_sum**2)\n",
    "    num = (n_obs*time_size_sum) - (t_sum*size_sum)\n",
    "    \n",
    "    beta = num/denom\n",
    "    alpha = np.mean(log_size_arr)-beta*(t_sum/n_obs)\n",
    "    return beta,alpha\n",
    "\n",
    "def fit_least_squares_windowed(timepoint_arr,log_size_arr,fov_idx_in_arr,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr,ols_window_size=3):\n",
    "    timepoint_arr_len = len(timepoint_arr)\n",
    "    timepoint_window_arr = [timepoint_arr[i:i+ols_window_size] for i in range(0,timepoint_arr_len-ols_window_size+1,1)]\n",
    "    timepoint_window_arr = np.array([[item[0],item[-1]] for item in timepoint_window_arr])\n",
    "    beta_output = []\n",
    "    time_output = []\n",
    "    for timepoint_idx,timepoint_window in enumerate(timepoint_window_arr):\n",
    "        time_i = timepoint_window[0]\n",
    "        time_f = timepoint_window[1]\n",
    "        windowed_log_size_arr = log_size_arr[timepoint_idx:timepoint_idx+ols_window_size]\n",
    "        beta,_ = fit_least_squares(windowed_log_size_arr,fov_idx_in_arr,time_i,time_f,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr)\n",
    "        beta_output.append(beta)\n",
    "        time_output.append(np.mean(time_arr[fov_idx_in_arr,time_i:time_f+1]))\n",
    "    return beta_output,time_output\n",
    "\n",
    "def fit_least_squares_windowed_cellgroupby(cell_group,growth_metric,fov_list,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr,ols_window_size=3):\n",
    "    timepoint_arr = cell_group[\"timepoints\"].values\n",
    "    log_size_arr = np.log(cell_group[growth_metric].values)\n",
    "    fov_idx = cell_group[\"fov\"].iloc[0]\n",
    "    fov_idx_in_arr = np.where(np.array(fov_list)==fov_idx)[0][0]\n",
    "    \n",
    "    beta_output,time_output = fit_least_squares_windowed(timepoint_arr,log_size_arr,fov_idx_in_arr,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr,ols_window_size=ols_window_size)\n",
    "    growth_rate_output = np.array([beta_output,time_output]).T\n",
    "    growth_rate_output = growth_rate_output.tolist()\n",
    "    return growth_rate_output\n",
    "\n",
    "def get_growth_rate_df(query,growth_metric,fov_list,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr,get_measurement_time=False):\n",
    "    all_gr = query.groupby(level=0,sort=False).apply(lambda x: fit_least_squares_windowed_cellgroupby(x,growth_metric,fov_list,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr))\n",
    "    trenchid_series = query.groupby(level=0,sort=False).apply(lambda x: x[\"trenchid\"].iloc[0])\n",
    "    \n",
    "    if get_measurement_time:\n",
    "        all_gr = pd.concat([all_gr,trenchid_series],axis=1).rename({0:\"Growth Measurment\",1:\"trenchid\"},axis=1)\n",
    "    else:\n",
    "        all_gr = pd.DataFrame(all_gr, columns=[\"Growth Measurment\"])\n",
    "    all_gr = all_gr.explode(\"Growth Measurment\").dropna(subset=[\"Growth Measurment\"])\n",
    "    \n",
    "    all_gr[\"Growth Rate Measurement Index\"] = all_gr.groupby(\"Global CellID\").cumcount()\n",
    "    all_gr[\"Growth Rate: \" + growth_metric] = all_gr.apply(lambda x: x[\"Growth Measurment\"][0], axis=1)*3600 #size unit per hr\n",
    "    if get_measurement_time:\n",
    "        all_gr[\"Measurement time (s)\"] = all_gr.apply(lambda x: x[\"Growth Measurment\"][1], axis=1)\n",
    "    all_gr = all_gr.drop(\"Growth Measurment\", axis=1)\n",
    "    return all_gr\n",
    "\n",
    "def get_growth_and_division_cell_cycle(lineage_df,kymo_df_path,trench_score_thr=-75,absolute_time=True,delta_t_min=None,\\\n",
    "                                  size_metrics=[\"area\",\"major_axis_length\",\"minor_axis_length\",\"Volume\",\"Surface Area\"]):\n",
    "    \n",
    "    ## Setting up kymograph and lineage dfs\n",
    "    \n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "    \n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"]*delta_t_min*60.\n",
    "    \n",
    "    kymo_time_series = kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler='threads')\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "            \n",
    "    reference = filter_df(lineage_df,[\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "    query = filter_df(lineage_df,[\"`Mother CellID` != -1\",\"`Daughter CellID 1` != -1\",\"`Daughter CellID 2` != -1\",\\\n",
    "                                                  \"`Sister CellID` != -1\",\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "    \n",
    "    init_cells = get_first_cell_timepoint(query).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "    fin_cells = get_last_cell_timepoint(query).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "\n",
    "    cell_min_tpt_df = get_first_cell_timepoint(reference).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "    cell_max_tpt_df = get_last_cell_timepoint(reference).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "\n",
    "    mother_df = cell_max_tpt_df.loc[init_cells[\"Mother CellID\"].tolist()]\n",
    "    sister_df = cell_min_tpt_df.loc[init_cells[\"Sister CellID\"].tolist()]\n",
    "    daughter_1_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 1\"].tolist()]\n",
    "    daughter_2_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 2\"].tolist()]\n",
    "\n",
    "    for metric in size_metrics:\n",
    "\n",
    "        if metric == \"minor_axis_length\":\n",
    "\n",
    "            init_cells[\"Birth: \" + metric] = init_cells[metric].values\n",
    "            init_cells[\"Division: \" + metric] = fin_cells[metric].values\n",
    "            init_cells[\"Delta: \" + metric] = fin_cells[metric].values-init_cells[metric].values\n",
    "\n",
    "        else:\n",
    "\n",
    "            interp_mother_final_size = ((init_cells[metric].values + sister_df[metric].values) * mother_df[metric].values)**(1/2)\n",
    "            sister_frac = init_cells[metric].values/(sister_df[metric].values+init_cells[metric].values)\n",
    "            init_cells[\"Birth: \" + metric] = (sister_frac*interp_mother_final_size)\n",
    "\n",
    "            init_cells[\"Division: \" + metric] = (((daughter_1_df[metric].values + daughter_2_df[metric].values) * fin_cells[metric].values)**(1/2))\n",
    "\n",
    "            init_cells[\"Delta: \" + metric] = init_cells[\"Division: \" + metric].values-init_cells[\"Birth: \" + metric].values\n",
    "\n",
    "    init_cells[\"Final timepoints\"] = daughter_1_df[\"timepoints\"].values #counting a timepoint in which a division occurs as a full timepoint, hacky\n",
    "    init_cells[\"Delta Timepoints\"] = init_cells[\"Final timepoints\"]-init_cells[\"timepoints\"]\n",
    "\n",
    "    # if absolute_time:\n",
    "    interpolated_final_time = (fin_cells[\"time (s)\"].values+daughter_1_df[\"time (s)\"].values)/2 #interpolating under the same assumptions as the size quantification\n",
    "    interpolated_init_time = ((init_cells[\"time (s)\"].values + mother_df[\"time (s)\"].values)/2)\n",
    "    init_cells[\"Final time (s)\"] = interpolated_final_time\n",
    "    init_cells[\"Delta time (s)\"] = interpolated_final_time-interpolated_init_time\n",
    "\n",
    "    query = query.reset_index().set_index([\"Global CellID\",\"timepoints\"]).sort_index().reset_index(level=1)\n",
    "    \n",
    "    delta_t_series = query.groupby(level=0,sort=False)[\"time (s)\"].apply(lambda x: ((x[1:].values - x[:-1].values)))\n",
    "\n",
    "    init_time_gap = init_cells[\"time (s)\"].values - interpolated_init_time\n",
    "    final_time_gap = interpolated_final_time - fin_cells[\"time (s)\"].values\n",
    "    \n",
    "    init_cells = init_cells.rename(columns={\"timepoints\":\"initial timepoints\"})\n",
    "    \n",
    "    return init_cells\n",
    "\n",
    "def get_growth_and_division_cell_cycle_timepoints(lineage_df,kymo_df_path,trench_score_thr=-75,absolute_time=True,delta_t_min=None,\\\n",
    "                                  size_metrics=[\"area\",\"major_axis_length\",\"minor_axis_length\",\"Volume\",\"Surface Area\"]):\n",
    "    \n",
    "    ## Setting up kymograph and lineage dfs\n",
    "    \n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "    \n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"]*delta_t_min*60.\n",
    "    \n",
    "    kymo_time_series = kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler='threads')\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "            \n",
    "    query = filter_df(lineage_df,[\"`Mother CellID` != -1\",\"`Daughter CellID 1` != -1\",\"`Daughter CellID 2` != -1\",\\\n",
    "                                                  \"`Sister CellID` != -1\",\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "\n",
    "    query = query.reset_index().set_index([\"Global CellID\",\"timepoints\"]).sort_index().reset_index(level=1)\n",
    "    \n",
    "    cell_cycle_timepoints_df = define_cell_cycle_timepoints_df(query,size_metrics)\n",
    "    \n",
    "    cellid_cell_cycle_timepoint_idx = cell_cycle_timepoints_df.reset_index().apply(lambda x: int(f'{int(x[\"Global CellID\"]):016n}{int(x[\"Cell Cycle timepoints\"]):04n}'), axis=1).tolist()\n",
    "    cell_cycle_timepoints_df[\"Global CellID-Cell Cycle timepoints\"] = cellid_cell_cycle_timepoint_idx\n",
    "    cell_cycle_timepoints_df = cell_cycle_timepoints_df.reset_index().set_index(\"Global CellID-Cell Cycle timepoints\")\n",
    "    \n",
    "    return cell_cycle_timepoints_df\n",
    "\n",
    "def get_growth_and_division_growth_rate(lineage_df,kymo_df_path,trench_score_thr=-75,absolute_time=True,delta_t_min=None,\\\n",
    "                                  growth_metrics=[\"area\",\"major_axis_length\",\"Volume\",\"Surface Area\"]):\n",
    "\n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"]*delta_t_min*60.\n",
    "\n",
    "    kymo_time_series = kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler='threads')\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "    \n",
    "    reference = filter_df(lineage_df,[\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "    query = filter_df(lineage_df,[\"`Mother CellID` != -1\",\"`Daughter CellID 1` != -1\",\"`Daughter CellID 2` != -1\",\\\n",
    "                                                  \"`Sister CellID` != -1\",\"`Trench Score` < \" + str(trench_score_thr)])\n",
    "    \n",
    "    init_cells = get_first_cell_timepoint(query).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "    fin_cells = get_last_cell_timepoint(query).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "\n",
    "    cell_min_tpt_df = get_first_cell_timepoint(reference).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "    cell_max_tpt_df = get_last_cell_timepoint(reference).reset_index().set_index(\"Global CellID\").sort_index()\n",
    "\n",
    "    mother_df = cell_max_tpt_df.loc[init_cells[\"Mother CellID\"].tolist()]\n",
    "    sister_df = cell_min_tpt_df.loc[init_cells[\"Sister CellID\"].tolist()]\n",
    "    daughter_1_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 1\"].tolist()]\n",
    "    daughter_2_df = cell_min_tpt_df.loc[fin_cells[\"Daughter CellID 2\"].tolist()]\n",
    "\n",
    "    query = query.reset_index().set_index([\"Global CellID\",\"timepoints\"]).sort_index().reset_index(level=1)\n",
    "    \n",
    "\n",
    "    fov_list = query[\"fov\"].unique().tolist()\n",
    "    min_fov = fov_list[0]\n",
    "    max_fov = fov_list[-1]\n",
    "\n",
    "    fov_list,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr = compile_ols_arrs(kymo_df,min_fov,max_fov)\n",
    "\n",
    "    gr_df_list = []\n",
    "    for growth_idx,growth_metric in enumerate(growth_metrics): # Havn't decided between mean and median\n",
    "        \n",
    "        interp_mother_final_size = ((init_cells[metric].values + sister_df[metric].values) * mother_df[metric].values)**(1/2)\n",
    "            sister_frac = init_cells[metric].values/(sister_df[metric].values+init_cells[metric].values)\n",
    "            init_cells[\"Birth: \" + metric] = (sister_frac*interp_mother_final_size)\n",
    "\n",
    "            init_cells[\"Division: \" + metric] = (((daughter_1_df[metric].values + daughter_2_df[metric].values) * fin_cells[metric].values)**(1/2))\n",
    "        \n",
    "        \n",
    "        if growth_idx == 0:\n",
    "            gr_df = get_growth_rate_df(query,growth_metric,fov_list,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr,get_measurement_time=True)\n",
    "        else:\n",
    "            gr_df = get_growth_rate_df(query,growth_metric,fov_list,time_arr,time_arr_accum,time_sq_arr_accum,interp_time_arr,interp_time_sq_arr,get_measurement_time=False)\n",
    "\n",
    "        gr_df[\"Global CellID-Growth Rate Measurement Index\"] = gr_df.reset_index().apply(lambda x: int(f'{int(x[\"Global CellID\"]):016n}{int(x[\"Growth Rate Measurement Index\"]):04n}'), axis=1).tolist()\n",
    "        gr_df = gr_df.reset_index()\n",
    "        if growth_idx != 0:\n",
    "            gr_df = gr_df.drop([\"Global CellID\",\"Growth Rate Measurement Index\"],axis=1)\n",
    "        gr_df = gr_df.set_index(\"Global CellID-Growth Rate Measurement Index\")\n",
    "        gr_df_list.append(gr_df)\n",
    "\n",
    "        # Initial/Final Growth Rate numbers are probably untrustworthy since noise in delta t from the division \n",
    "        # event is probably pretty high. Going to exclude.\n",
    "    gr_df_out = pd.concat(gr_df_list,axis=1)\n",
    "    \n",
    "    return gr_df_out\n",
    "\n",
    "def get_all_growth_and_division_stats(lineage_df,kymo_df_path,trench_score_thr=-75,absolute_time=True,delta_t_min=None,\\\n",
    "                                      size_metrics=[\"area\",\"major_axis_length\",\"minor_axis_length\",\"Volume\",\"Surface Area\"],\\\n",
    "                                     growth_metrics=[\"area\",\"major_axis_length\",\"Volume\",\"Surface Area\"]):\n",
    "    input_test_partition = lineage_df.get_partition(0).compute()\n",
    "    \n",
    "    test_partition_1 = get_growth_and_division_cell_cycle(input_test_partition,kymo_df_path,\\\n",
    "                                    trench_score_thr=trench_score_thr,absolute_time=absolute_time,\\\n",
    "                                    delta_t_min=delta_t_min,size_metrics=size_metrics)\n",
    "    growth_div_df = dd.map_partitions(get_growth_and_division_cell_cycle,lineage_df,kymo_df_path,\\\n",
    "                                      trench_score_thr=trench_score_thr,absolute_time=absolute_time,delta_t_min=delta_t_min,\\\n",
    "                                      size_metrics=size_metrics,meta=test_partition_1)\n",
    "    \n",
    "    test_partition_2 = get_growth_and_division_cell_cycle_timepoints(input_test_partition,kymo_df_path,\\\n",
    "                                    trench_score_thr=trench_score_thr,absolute_time=absolute_time,\\\n",
    "                                    delta_t_min=delta_t_min,size_metrics=size_metrics)\n",
    "    cell_cycle_timepoints_df = dd.map_partitions(get_growth_and_division_cell_cycle_timepoints,lineage_df,kymo_df_path,\\\n",
    "                                      trench_score_thr=trench_score_thr,absolute_time=absolute_time,delta_t_min=delta_t_min,\\\n",
    "                                      size_metrics=size_metrics,meta=test_partition_2)\n",
    "    \n",
    "    test_partition_3 = get_growth_and_division_growth_rate(input_test_partition,kymo_df_path,\\\n",
    "                                    trench_score_thr=trench_score_thr,absolute_time=absolute_time,\\\n",
    "                                    delta_t_min=delta_t_min,growth_metrics=growth_metrics)\n",
    "    cell_cycle_growth_df = dd.map_partitions(get_growth_and_division_growth_rate,lineage_df,kymo_df_path,\\\n",
    "                                      trench_score_thr=trench_score_thr,absolute_time=absolute_time,delta_t_min=delta_t_min,\\\n",
    "                                      growth_metrics=growth_metrics,meta=test_partition_3)\n",
    "        \n",
    "    return growth_div_df,cell_cycle_timepoints_df,cell_cycle_growth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[[2, 3, 4], [1, 2, 3]], [[2, 3, 4], [1, 2, 3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reshape(test.shape[0], -1, order=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((np.zeros((2, 1)), test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "tags": []
   },
   "source": [
    "test### Import Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/lineage/\"\n",
    ")\n",
    "\n",
    "##temp fix\n",
    "lineage_df[\"CellID\"] = lineage_df[\"CellID\"].astype(int)\n",
    "lineage_df[\"Global CellID\"] = lineage_df[\"Global CellID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrm_find_mode(series, max_iter=1000, min_binsize=50):\n",
    "    working_series = series\n",
    "    for i in range(max_iter):\n",
    "        range_max, range_min = np.max(working_series), np.min(working_series)\n",
    "        midpoint = (range_max + range_min) / 2\n",
    "        above_middle = working_series[working_series > midpoint]\n",
    "        below_middle = working_series[working_series <= midpoint]\n",
    "\n",
    "        count_above = len(above_middle)\n",
    "        count_below = len(below_middle)\n",
    "\n",
    "        if count_above > count_below:\n",
    "            working_series = above_middle\n",
    "        else:\n",
    "            working_series = below_middle\n",
    "\n",
    "        if i > 0:\n",
    "            if (len(working_series) < min_binsize) or (last_midpoint == midpoint):\n",
    "                return np.mean(working_series)\n",
    "\n",
    "        last_midpoint = midpoint\n",
    "\n",
    "\n",
    "def bootstrap_hrm(series, n_bootstraps=100, max_n_per_bootstrap=100):\n",
    "    modes = []\n",
    "\n",
    "    series_len = len(series)\n",
    "\n",
    "    n_per_bootstrap = min(series_len, max_n_per_bootstrap)\n",
    "\n",
    "    for n in range(n_bootstraps):\n",
    "        modes.append(hrm_find_mode(series.sample(n=n_per_bootstrap)))\n",
    "    return np.mean(modes)\n",
    "\n",
    "\n",
    "def get_normal_fovs(fov_series, med_filter_size=5, n_stds=2):\n",
    "    median_series = sp.ndimage.median_filter(\n",
    "        fov_series, size=(med_filter_size,), mode=\"mirror\"\n",
    "    )\n",
    "\n",
    "    residuals = fov_series - median_series\n",
    "\n",
    "    gaussian_fit = sp.stats.norm.fit(residuals)\n",
    "    gaussian_fit = sp.stats.norm(loc=gaussian_fit[0], scale=gaussian_fit[1])\n",
    "\n",
    "    lower, upper = (-n_stds * gaussian_fit.std(), n_stds * gaussian_fit.std())\n",
    "\n",
    "    thr_mask = (residuals > lower) & (residuals < upper)\n",
    "\n",
    "    return thr_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Variables over FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_to_rescale = [\n",
    "    \"mCherry mean_intensity\",\n",
    "    \"area\",\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "fov_sorted_lineage_df = (\n",
    "    lineage_df[[\"fov\", \"timepoints\"] + values_to_rescale]\n",
    "    .reset_index()\n",
    "    .set_index(\"fov\", sorted=True)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "fov_correction_dicts = {}\n",
    "lineage_df_subsample = fov_sorted_lineage_df[fov_sorted_lineage_df[\"timepoints\"] < 12]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale):\n",
    "    fov_series_groupby = lineage_df_subsample.groupby(\"fov\", sort=False)[label]\n",
    "    fov_median_series = (\n",
    "        fov_series_groupby.apply(lambda x: np.median(x), meta=float)\n",
    "        .compute()\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    normal_fov_series = get_normal_fovs(fov_median_series)\n",
    "    fov_median_series = fov_median_series[normal_fov_series]\n",
    "\n",
    "    fov_correction_series = fov_median_series / np.max(fov_median_series)\n",
    "    fov_correction_dicts[label] = fov_correction_series.to_dict()\n",
    "\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(fov_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"FOV #\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "\n",
    "fov_list = [set(val.keys()) for key, val in fov_correction_dicts.items()]\n",
    "filtered_fov_list = list(set.intersection(*fov_list))\n",
    "\n",
    "dask_controller.daskclient.cancel(fov_sorted_lineage_df)\n",
    "\n",
    "lineage_df_fov_correction = lineage_df[\n",
    "    [\"fov\", \"timepoints\"] + list(fov_correction_dicts.keys())\n",
    "]\n",
    "lineage_df_fov_correction = lineage_df_fov_correction[\n",
    "    lineage_df_fov_correction[\"fov\"].isin(filtered_fov_list)\n",
    "].persist()\n",
    "\n",
    "for label, fov_correction_dict in fov_correction_dicts.items():\n",
    "    fov_correction_series = lineage_df_fov_correction[\"fov\"].apply(\n",
    "        lambda x: fov_correction_dict[x], meta=float\n",
    "    )\n",
    "    lineage_df_fov_correction[label + \": FOV Corrected\"] = (\n",
    "        lineage_df_fov_correction[label] / fov_correction_series\n",
    "    ).persist()\n",
    "\n",
    "plt.savefig(\"FOV_correction_replicate_3.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Variables over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_to_rescale_step_2 = [value + \": FOV Corrected\" for value in values_to_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_samples = 100000\n",
    "\n",
    "ttl_samples = len(lineage_df_fov_correction)\n",
    "frac_to_sample = target_samples / ttl_samples\n",
    "lineage_df_subsample = lineage_df_fov_correction.sample(frac=frac_to_sample).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lineage_df_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\n",
    "    \"Mean mCherry Intensity\",\n",
    "    \"Area\",\n",
    "    \"Major Axis Length\",\n",
    "    \"Minor Axis Length\",\n",
    "    \"Volume\",\n",
    "    \"Surface Area\",\n",
    "]\n",
    "for i, label in enumerate(values_to_rescale_step_2):\n",
    "    time_series_groupby = lineage_df_subsample.groupby(\"timepoints\")[label]\n",
    "    time_mode_series = time_series_groupby.apply(\n",
    "        lambda x: bootstrap_hrm(x)\n",
    "    ).sort_index()\n",
    "    time_correction_series = time_mode_series / np.max(time_mode_series)\n",
    "    time_correction_dict = time_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(time_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Timepoint (3 min steps)\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    lineage_df_fov_correction[\n",
    "        label + \": Time Corrected\"\n",
    "    ] = lineage_df_fov_correction.apply(\n",
    "        lambda x: x[label] / time_correction_dict[x[\"timepoints\"]], meta=float, axis=1\n",
    "    ).persist()\n",
    "plt.savefig(\"Time_correction_replicate_3.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The HSM method [2] iteratively divides the data set into samples of half the size as the original set and uses the half-sample with the minimum range, where range is defined as the difference between the maximum and the minimum value of the sample. This method terminates when the half-sample is less than three data points. An average of these three or fewer values is the mode. The HRM method [2] is similar but uses the sub-sample with the densest half-range, where range is defined as the absolute difference between the maximum and the minimum values in a sample. Of these two methods, only the HRM was used in this study because HRM has been shown to have lower bias with increasing contamination and asymmetry [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Overwrite Variables with Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_loc_from_index(df, idx_series):\n",
    "    df_out = df.loc[idx_series.tolist()]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def index_loc_lookup(df, idx_series):\n",
    "    df_out = dd.map_partitions(\n",
    "        get_aligned_loc_from_index, df, idx_series, align_dataframes=False\n",
    "    )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrected_lineage_df = index_loc_lookup(lineage_df, lineage_df_fov_correction.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##here\n",
    "for label in values_to_rescale:\n",
    "    corrected_lineage_df[label] = lineage_df_fov_correction[\n",
    "        label + \": FOV Corrected: Time Corrected\"\n",
    "    ]\n",
    "\n",
    "corrected_lineage_df = corrected_lineage_df[\n",
    "    [\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"trenchid\",\n",
    "        \"timepoints\",\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"CellID\",\n",
    "        \"Global CellID\",\n",
    "        \"Trench Score\",\n",
    "        \"Mother CellID\",\n",
    "        \"Daughter CellID 1\",\n",
    "        \"Daughter CellID 2\",\n",
    "        \"Sister CellID\",\n",
    "        \"Centroid X\",\n",
    "        \"Centroid Y\",\n",
    "        \"Kymograph File Parquet Index\",\n",
    "        \"Kymograph FOV Parquet Index\",\n",
    "        \"FOV Parquet Index\",\n",
    "    ]\n",
    "    + values_to_rescale\n",
    "]\n",
    "\n",
    "corrected_lineage_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Analysis_with_Correction\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrected_lineage_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Analysis_with_Correction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Hack for corrupted timepoints\n",
    "\n",
    "Here, I am overwriting the corrupted timestamps of this experiment with a duplicate experiment that should have roughly the same timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata\"\n",
    ")\n",
    "global_meta_replace = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division/metadata.hdf5\"\n",
    ").read_df(\"global\", read_metadata=False)\n",
    "\n",
    "kymo_df[\"fov-timepoint_idx\"] = kymo_df.apply(\n",
    "    lambda x: int(f'{int(x[\"fov\"]):04n}{int(x[\"timepoints\"]):04n}'), axis=1\n",
    ")\n",
    "kymo_df = kymo_df.reset_index().set_index(\"fov-timepoint_idx\")\n",
    "global_meta_replace = global_meta_replace.reset_index()\n",
    "global_meta_replace[\"fov-timepoint_idx\"] = global_meta_replace.apply(\n",
    "    lambda x: int(f'{int(x[\"fov\"]):04n}{int(x[\"timepoints\"]):04n}'), axis=1\n",
    ")\n",
    "global_meta_replace = global_meta_replace[[\"fov-timepoint_idx\", \"t\"]]\n",
    "global_meta_replace = global_meta_replace.set_index(\"fov-timepoint_idx\")\n",
    "kymo_df = kymo_df.join(global_meta_replace).set_index(\"FOV Parquet Index\")\n",
    "kymo_df[\"time (s)\"] = kymo_df[\"t\"]\n",
    "kymo_df = kymo_df.drop(\"t\", axis=1)\n",
    "\n",
    "dd.to_parquet(\n",
    "    kymo_df,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata_timefix\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Troubleshooting Growth Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_lineage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_partition = corrected_lineage_df.get_partition(0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = get_growth_and_division_growth_rate(\n",
    "    input_test_partition,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata_timefix\",\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_test_output = test_output[test_output[\"Measurement time (s)\"] < 5000]\n",
    "early_input_test_partition = input_test_partition[\n",
    "    input_test_partition[\"time (s)\"] < 5000\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cell = test_output[\"Global CellID\"].sample(n=1).iloc[0]\n",
    "output_cell = test_output[test_output[\"Global CellID\"] == random_cell]\n",
    "input_cell = input_test_partition[input_test_partition[\"Global CellID\"] == random_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gr = np.mean(output_cell[\"Growth Rate: Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cell = early_test_output[\"Global CellID\"].sample(n=1).iloc[0]\n",
    "output_cell = early_test_output[early_test_output[\"Global CellID\"] == random_cell]\n",
    "input_cell = early_input_test_partition[\n",
    "    early_input_test_partition[\"Global CellID\"] == random_cell\n",
    "]\n",
    "mean_gr = np.mean(output_cell[\"Growth Rate: Volume\"])\n",
    "print(mean_gr)\n",
    "scatter_x = (input_cell[\"time (s)\"] / 3600).values\n",
    "scatter_y = (np.log(input_cell[\"Volume\"])).values\n",
    "\n",
    "intercept = scatter_y[0] - (scatter_x[0] * mean_gr)\n",
    "gr_timepoints = np.linspace(scatter_x[0], scatter_x[-1])\n",
    "gr_line = (mean_gr * gr_timepoints) + intercept\n",
    "\n",
    "plt.scatter(scatter_x, scatter_y)\n",
    "plt.plot(gr_timepoints, gr_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(early_test_output[\"Growth Rate: Volume\"], bins=50, range=(0.2, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cell = early_test_output[\"Global CellID\"].sample(n=1).iloc[0]\n",
    "output_cell = early_test_output[early_test_output[\"Global CellID\"] == random_cell]\n",
    "input_cell = early_input_test_partition[\n",
    "    early_input_test_partition[\"Global CellID\"] == random_cell\n",
    "]\n",
    "mean_gr = np.mean(output_cell[\"Growth Rate: Volume\"])\n",
    "print(mean_gr)\n",
    "print()\n",
    "scatter_x = (input_cell[\"time (s)\"] / 3600).values\n",
    "scatter_y = (input_cell[\"Volume\"]).values\n",
    "\n",
    "plt.scatter(scatter_x, scatter_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = get_growth_and_division_growth_rate(\n",
    "    input_test_partition,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata_timefix\",\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_df_path = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata_timefix\"\n",
    "trench_score_thr = -75\n",
    "absolute_time = True\n",
    "delta_t_min = None\n",
    "\n",
    "kymo_df = dd.read_parquet(kymo_df_path)\n",
    "kymo_idx_list = input_test_partition[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "if not absolute_time:\n",
    "    kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"] * delta_t_min * 60.0\n",
    "\n",
    "kymo_time_series = kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler=\"threads\")\n",
    "kymo_time_series.index = input_test_partition.index\n",
    "input_test_partition[\"time (s)\"] = kymo_time_series\n",
    "\n",
    "query = filter_df(\n",
    "    input_test_partition,\n",
    "    [\n",
    "        \"`Mother CellID` != -1\",\n",
    "        \"`Daughter CellID 1` != -1\",\n",
    "        \"`Daughter CellID 2` != -1\",\n",
    "        \"`Sister CellID` != -1\",\n",
    "        \"`Trench Score` < \" + str(trench_score_thr),\n",
    "    ],\n",
    ")\n",
    "\n",
    "query = (\n",
    "    query.reset_index()\n",
    "    .set_index([\"Global CellID\", \"timepoints\"])\n",
    "    .sort_index()\n",
    "    .reset_index(level=1)\n",
    ")\n",
    "\n",
    "fov_list = query[\"fov\"].unique().tolist()\n",
    "min_fov = fov_list[0]\n",
    "max_fov = fov_list[-1]\n",
    "\n",
    "fov_list, time_arr, time_arr_accum, time_sq_arr_accum = compile_ols_arrs(\n",
    "    kymo_df, min_fov, max_fov\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_least_squares_windowed_cellgroupby(\n",
    "    input_cell,\n",
    "    \"Volume\",\n",
    "    fov_list,\n",
    "    time_arr,\n",
    "    time_arr_accum,\n",
    "    time_sq_arr_accum,\n",
    "    ols_window_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_query = query.loc[301100018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gr = query.groupby(level=0, sort=False).apply(\n",
    "    lambda x: fit_least_squares_windowed_cellgroupby(\n",
    "        x, \"Volume\", fov_list, time_arr, time_arr_accum, time_sq_arr_accum\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.loc[301100018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_ols_arrs(kymo_df, min_fov, max_fov):\n",
    "    min_fov_idx = int(f\"{int(min_fov):08n}\" + \"000000000000\")\n",
    "    max_fov_idx = int(f\"{int(max_fov):08n}\" + \"999999999999\")\n",
    "    selected_kymo_df = kymo_df.loc[min_fov_idx:max_fov_idx]\n",
    "    timepoint_df = (\n",
    "        selected_kymo_df.groupby([\"fov\", \"timepoints\"])[\"time (s)\"]\n",
    "        .apply(lambda x: x.iloc[0])\n",
    "        .compute(scheduler=\"threads\")\n",
    "    )\n",
    "    timepoint_df = pd.DataFrame(timepoint_df).sort_index()\n",
    "    timepoint_df[\"time (s)^2\"] = timepoint_df[\"time (s)\"] ** 2\n",
    "    fov_list = timepoint_df.index.get_level_values(0).unique().tolist()\n",
    "    time_arr = np.array([timepoint_df[\"time (s)\"].loc[f].values for f in fov_list])\n",
    "    time_arr_accum = np.add.accumulate(time_arr, axis=1)\n",
    "    time_sq_arr = np.array([timepoint_df[\"time (s)^2\"].loc[f].values for f in fov_list])\n",
    "    time_sq_arr_accum = np.add.accumulate(time_sq_arr, axis=1)\n",
    "    return fov_list, time_arr, time_arr_accum, time_sq_arr_accum\n",
    "\n",
    "\n",
    "def fit_least_squares(\n",
    "    log_size_arr,\n",
    "    fov_idx_in_arr,\n",
    "    time_i,\n",
    "    time_f,\n",
    "    time_arr,\n",
    "    time_arr_accum,\n",
    "    time_sq_arr_accum,\n",
    "):\n",
    "    # everything assumes an inclusive time_f\n",
    "    if time_i > 0:\n",
    "        t_sum = (\n",
    "            time_arr_accum[fov_idx_in_arr, time_f]\n",
    "            - time_arr_accum[fov_idx_in_arr, time_i - 1]\n",
    "        )\n",
    "        t_sq_sum = (\n",
    "            time_sq_arr_accum[fov_idx_in_arr, time_f]\n",
    "            - time_sq_arr_accum[fov_idx_in_arr, time_i - 1]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        t_sum = time_arr_accum[fov_idx_in_arr, time_f]\n",
    "        t_sq_sum = time_sq_arr_accum[fov_idx_in_arr, time_f]\n",
    "\n",
    "    n_obs = time_f - time_i + 1\n",
    "\n",
    "    size_sum = np.sum(log_size_arr)\n",
    "    time_size_sum = np.sum(log_size_arr * time_arr[fov_idx_in_arr, time_i : time_f + 1])\n",
    "\n",
    "    denom = (n_obs * t_sq_sum) - (t_sum**2)\n",
    "    num = (n_obs * time_size_sum) - (t_sum * size_sum)\n",
    "\n",
    "    beta = num / denom\n",
    "    alpha = np.mean(log_size_arr) - beta * (t_sum / n_obs)\n",
    "    return beta, alpha\n",
    "\n",
    "\n",
    "def fit_least_squares_windowed(\n",
    "    timepoint_arr,\n",
    "    log_size_arr,\n",
    "    fov_idx_in_arr,\n",
    "    time_arr,\n",
    "    time_arr_accum,\n",
    "    time_sq_arr_accum,\n",
    "    ols_window_size=3,\n",
    "):\n",
    "    timepoint_arr_len = len(timepoint_arr)\n",
    "    timepoint_window_arr = [\n",
    "        timepoint_arr[i : i + ols_window_size]\n",
    "        for i in range(0, timepoint_arr_len - ols_window_size + 1, 1)\n",
    "    ]\n",
    "    timepoint_window_arr = np.array(\n",
    "        [[item[0], item[-1]] for item in timepoint_window_arr]\n",
    "    )\n",
    "    beta_output = []\n",
    "    time_output = []\n",
    "    for timepoint_idx, timepoint_window in enumerate(timepoint_window_arr):\n",
    "        time_i = timepoint_window[0]\n",
    "        time_f = timepoint_window[1]\n",
    "        windowed_log_size_arr = log_size_arr[\n",
    "            timepoint_idx : timepoint_idx + ols_window_size\n",
    "        ]\n",
    "        beta, _ = fit_least_squares(\n",
    "            windowed_log_size_arr,\n",
    "            fov_idx_in_arr,\n",
    "            time_i,\n",
    "            time_f,\n",
    "            time_arr,\n",
    "            time_arr_accum,\n",
    "            time_sq_arr_accum,\n",
    "        )\n",
    "        beta_output.append(beta)\n",
    "        time_output.append(np.mean(time_arr[fov_idx_in_arr, time_i : time_f + 1]))\n",
    "    return beta_output, time_output\n",
    "\n",
    "\n",
    "def fit_least_squares_windowed_cellgroupby(\n",
    "    cell_group,\n",
    "    growth_metric,\n",
    "    fov_list,\n",
    "    time_arr,\n",
    "    time_arr_accum,\n",
    "    time_sq_arr_accum,\n",
    "    ols_window_size=3,\n",
    "):\n",
    "    timepoint_arr = cell_group[\"timepoints\"].values\n",
    "    log_size_arr = np.log(cell_group[growth_metric].values)\n",
    "    fov_idx = cell_group[\"fov\"].iloc[0]\n",
    "    fov_idx_in_arr = np.where(np.array(fov_list) == fov_idx)[0][0]\n",
    "\n",
    "    beta_output, time_output = fit_least_squares_windowed(\n",
    "        timepoint_arr,\n",
    "        log_size_arr,\n",
    "        fov_idx_in_arr,\n",
    "        time_arr,\n",
    "        time_arr_accum,\n",
    "        time_sq_arr_accum,\n",
    "        ols_window_size=ols_window_size,\n",
    "    )\n",
    "    growth_rate_output = np.array([beta_output, time_output]).T\n",
    "    growth_rate_output = growth_rate_output.tolist()\n",
    "    return growth_rate_output\n",
    "\n",
    "\n",
    "def get_growth_rate_df(\n",
    "    query,\n",
    "    growth_metric,\n",
    "    fov_list,\n",
    "    time_arr,\n",
    "    time_arr_accum,\n",
    "    time_sq_arr_accum,\n",
    "    get_measurement_time=False,\n",
    "):\n",
    "    all_gr = query.groupby(level=0, sort=False).apply(\n",
    "        lambda x: fit_least_squares_windowed_cellgroupby(\n",
    "            x, growth_metric, fov_list, time_arr, time_arr_accum, time_sq_arr_accum\n",
    "        )\n",
    "    )\n",
    "    trenchid_series = query.groupby(level=0, sort=False).apply(\n",
    "        lambda x: x[\"trenchid\"].iloc[0]\n",
    "    )\n",
    "\n",
    "    if get_measurement_time:\n",
    "        all_gr = pd.concat([all_gr, trenchid_series], axis=1).rename(\n",
    "            {0: \"Growth Measurment\", 1: \"trenchid\"}, axis=1\n",
    "        )\n",
    "    else:\n",
    "        all_gr = pd.DataFrame(all_gr, columns=[\"Growth Measurment\"])\n",
    "    all_gr = all_gr.explode(\"Growth Measurment\").dropna(subset=[\"Growth Measurment\"])\n",
    "\n",
    "    all_gr[\"Growth Rate Measurement Index\"] = all_gr.groupby(\"Global CellID\").cumcount()\n",
    "    all_gr[\"Growth Rate: \" + growth_metric] = (\n",
    "        all_gr.apply(lambda x: x[\"Growth Measurment\"][0], axis=1) * 3600\n",
    "    )  # size unit per hr\n",
    "    if get_measurement_time:\n",
    "        all_gr[\"Measurement time (s)\"] = all_gr.apply(\n",
    "            lambda x: x[\"Growth Measurment\"][1], axis=1\n",
    "        )\n",
    "    all_gr = all_gr.drop(\"Growth Measurment\", axis=1)\n",
    "    return all_gr\n",
    "\n",
    "\n",
    "def get_growth_and_division_growth_rate(\n",
    "    lineage_df,\n",
    "    kymo_df_path,\n",
    "    trench_score_thr=-75,\n",
    "    absolute_time=True,\n",
    "    delta_t_min=None,\n",
    "    growth_metrics=[\"area\", \"major_axis_length\", \"Volume\", \"Surface Area\"],\n",
    "):\n",
    "    kymo_df = dd.read_parquet(kymo_df_path)\n",
    "    kymo_idx_list = lineage_df[\"Kymograph FOV Parquet Index\"].tolist()\n",
    "\n",
    "    if not absolute_time:\n",
    "        kymo_df[\"time (s)\"] = kymo_df[\"timepoints\"] * delta_t_min * 60.0\n",
    "\n",
    "    kymo_time_series = (\n",
    "        kymo_df[\"time (s)\"].loc[kymo_idx_list].compute(scheduler=\"threads\")\n",
    "    )\n",
    "    kymo_time_series.index = lineage_df.index\n",
    "    lineage_df[\"time (s)\"] = kymo_time_series\n",
    "\n",
    "    query = filter_df(\n",
    "        lineage_df,\n",
    "        [\n",
    "            \"`Mother CellID` != -1\",\n",
    "            \"`Daughter CellID 1` != -1\",\n",
    "            \"`Daughter CellID 2` != -1\",\n",
    "            \"`Sister CellID` != -1\",\n",
    "            \"`Trench Score` < \" + str(trench_score_thr),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    query = (\n",
    "        query.reset_index()\n",
    "        .set_index([\"Global CellID\", \"timepoints\"])\n",
    "        .sort_index()\n",
    "        .reset_index(level=1)\n",
    "    )\n",
    "\n",
    "    fov_list = query[\"fov\"].unique().tolist()\n",
    "    min_fov = fov_list[0]\n",
    "    max_fov = fov_list[-1]\n",
    "\n",
    "    fov_list, time_arr, time_arr_accum, time_sq_arr_accum = compile_ols_arrs(\n",
    "        kymo_df, min_fov, max_fov\n",
    "    )\n",
    "\n",
    "    gr_df_list = []\n",
    "    for growth_idx, growth_metric in enumerate(\n",
    "        growth_metrics\n",
    "    ):  # Havn't decided between mean and median\n",
    "        if growth_idx == 0:\n",
    "            gr_df = get_growth_rate_df(\n",
    "                query,\n",
    "                growth_metric,\n",
    "                fov_list,\n",
    "                time_arr,\n",
    "                time_arr_accum,\n",
    "                time_sq_arr_accum,\n",
    "                get_measurement_time=True,\n",
    "            )\n",
    "        else:\n",
    "            gr_df = get_growth_rate_df(\n",
    "                query,\n",
    "                growth_metric,\n",
    "                fov_list,\n",
    "                time_arr,\n",
    "                time_arr_accum,\n",
    "                time_sq_arr_accum,\n",
    "                get_measurement_time=False,\n",
    "            )\n",
    "\n",
    "        gr_df[\"Global CellID-Growth Rate Measurement Index\"] = (\n",
    "            gr_df.reset_index()\n",
    "            .apply(\n",
    "                lambda x: int(\n",
    "                    f'{int(x[\"Global CellID\"]):016n}{int(x[\"Growth Rate Measurement Index\"]):04n}'\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            .tolist()\n",
    "        )\n",
    "        gr_df = gr_df.reset_index()\n",
    "        if growth_idx != 0:\n",
    "            gr_df = gr_df.drop(\n",
    "                [\"Global CellID\", \"Growth Rate Measurement Index\"], axis=1\n",
    "            )\n",
    "        gr_df = gr_df.set_index(\"Global CellID-Growth Rate Measurement Index\")\n",
    "        gr_df_list.append(gr_df)\n",
    "\n",
    "        # Initial/Final Growth Rate numbers are probably untrustworthy since noise in delta t from the division\n",
    "        # event is probably pretty high. Going to exclude.\n",
    "    gr_df_out = pd.concat(gr_df_list, axis=1)\n",
    "\n",
    "    return gr_df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    growth_div_df,\n",
    "    cell_cycle_timepoints_df,\n",
    "    cell_cycle_growth_df,\n",
    ") = get_all_growth_and_division_stats(\n",
    "    corrected_lineage_df,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata_timefix\",\n",
    "    absolute_time=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Barcodes/barcode_output_df.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()\n",
    "\n",
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells\n",
    "\n",
    "print(ttl_called)\n",
    "print(ttl_trenches)\n",
    "print(ttl_trenches_w_cells)\n",
    "print(percent_called)\n",
    "print(percent_called_w_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping\n",
    "\n",
    "#### Note the trenches are unaligned so I had to manually insert an offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phenotype_kymopath = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata\"\n",
    "barcode_kymopath = \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Barcodes/kymograph/metadata\"\n",
    "\n",
    "trenchid_map = tr.files_to_trenchid_map(\n",
    "    phenotype_kymopath, barcode_kymopath, offset_x=(-0.2125 * 12)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Get Output Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barcode_pheno_kymo_df(\n",
    "    phenotype_df,\n",
    "    phenotype_kymo_df,\n",
    "    barcode_df,\n",
    "    trenchid_map,\n",
    "    output_kymo_index=\"FOV Parquet Index\",\n",
    "):\n",
    "    ## phenotype_df_list must contain dfs with trenchids column\n",
    "    ## can be made more effecient still, with a direct output to parquet\n",
    "    ## More effecient implementation that splits outputs into two smaller parts (every cell, every trench)\n",
    "    ## Saves by elmininating some redundent entries\n",
    "    ## Still filters only for cells that made it to final \"phenotype_df\" in most cases lineage traced\n",
    "\n",
    "    valid_barcode_df = barcode_df[\n",
    "        barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "    ].compute()\n",
    "    barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "        lambda x: trenchid_map[x]\n",
    "    )\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    barcode_df_mapped_trenchids_list = barcode_df_mapped_trenchids.tolist()\n",
    "    final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "\n",
    "    called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "    called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "    called_df[\"phenotype trenchid\"] = called_df[\"phenotype trenchid\"].astype(int)\n",
    "    called_df = called_df.drop([\"Barcode Signal\"], axis=1)\n",
    "    called_df = called_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=False\n",
    "    )\n",
    "\n",
    "    output_phenotype_kmyo_df = phenotype_kymo_df.rename(\n",
    "        columns={\"trenchid\": \"phenotype trenchid\"}\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.loc[\n",
    "        barcode_df_mapped_trenchids_list\n",
    "    ]\n",
    "    called_df = called_df.repartition(\n",
    "        divisions=output_phenotype_kmyo_df.divisions\n",
    "    ).persist()\n",
    "    wait(called_df)\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.merge(\n",
    "        called_df, how=\"inner\", left_index=True, right_index=True\n",
    "    )\n",
    "    output_phenotype_kmyo_df = output_phenotype_kmyo_df.reset_index().set_index(\n",
    "        output_kymo_index\n",
    "    )\n",
    "\n",
    "    return output_phenotype_kmyo_df, barcode_df_mapped_trenchids\n",
    "\n",
    "\n",
    "def get_output_phenotype_df(\n",
    "    phenotype_df, barcode_df_mapped_trenchids, output_index=\"File Parquet Index\"\n",
    "):\n",
    "    phenotype_df_idx = phenotype_df[\"trenchid\"].unique().compute().tolist()\n",
    "    valid_init_df_indices = barcode_df_mapped_trenchids.isin(phenotype_df_idx)\n",
    "    barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "    barcode_df_mapped_trenchids_list = barcode_df_mapped_trenchids.tolist()\n",
    "\n",
    "    output_phenotype_df = phenotype_df.rename(\n",
    "        columns={\"trenchid\": \"phenotype trenchid\"}\n",
    "    )\n",
    "    output_phenotype_df = output_phenotype_df.reset_index().set_index(\n",
    "        \"phenotype trenchid\", drop=True, sorted=True\n",
    "    )\n",
    "\n",
    "    output_phenotype_df = output_phenotype_df.loc[barcode_df_mapped_trenchids_list]\n",
    "    output_phenotype_df = output_phenotype_df.reset_index().set_index(output_index)\n",
    "\n",
    "    return output_phenotype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/Growth_Division/kymograph/metadata_timefix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_div_df = growth_div_df.persist()\n",
    "wait(growth_div_df)\n",
    "output_phenotype_kmyo_df, barcode_df_mapped_trenchids = get_barcode_pheno_kymo_df(\n",
    "    growth_div_df, kymo_df, barcode_df, trenchid_map\n",
    ")\n",
    "output_phenotype_kmyo_df.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Final_Barcodes_df/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(output_phenotype_kmyo_df)\n",
    "\n",
    "growth_div_df_output = get_output_phenotype_df(\n",
    "    growth_div_df, barcode_df_mapped_trenchids, output_index=\"Global CellID\"\n",
    ")\n",
    "growth_div_df_output.to_parquet(\n",
    "    \"/home/de64/scratch/de64
/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Cell_Cycle/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(growth_div_df_output)\n",
    "dask_controller.daskclient.cancel(growth_div_df)\n",
    "\n",
    "cell_cycle_timepoints_df = cell_cycle_timepoints_df.persist()\n",
    "wait(cell_cycle_timepoints_df)\n",
    "cell_cycle_timepoints_df_output = get_output_phenotype_df(\n",
    "    cell_cycle_timepoints_df,\n",
    "    barcode_df_mapped_trenchids,\n",
    "    output_index=\"Global CellID-Cell Cycle timepoints\",\n",
    ")\n",
    "cell_cycle_timepoints_df_output.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Observations/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(cell_cycle_timepoints_df_output)\n",
    "dask_controller.daskclient.cancel(cell_cycle_timepoints_df)\n",
    "\n",
    "cell_cycle_growth_df = cell_cycle_growth_df.persist()\n",
    "wait(cell_cycle_growth_df)\n",
    "cell_cycle_growth_df_output = get_output_phenotype_df(\n",
    "    cell_cycle_growth_df,\n",
    "    barcode_df_mapped_trenchids,\n",
    "    output_index=\"Global CellID-Growth Rate Measurement Index\",\n",
    ")\n",
    "cell_cycle_growth_df_output.to_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2022-01-18_lDE20_Final_5/2022-02-15_lDE20_Lineage_Growth_Observations/\",\n",
    "    engine=\"pyarrow\",\n",
    "    overwrite=True,\n",
    ")\n",
    "dask_controller.daskclient.cancel(cell_cycle_growth_df_output)\n",
    "dask_controller.daskclient.cancel(cell_cycle_growth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### Interactive Section Should be Refreshed Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_phenotype_kmyo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_phenotype_kmyo_df[\"FOV-Timepoint Index\"] = fov_timepoint_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headpath = (\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division\"\n",
    ")\n",
    "# note: shutdown dask when doing this...fix bug later\n",
    "overlay_handle = tr.variant_overlay(\n",
    "    headpath,\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\",\n",
    "    display_values_list=[\"Gene\", \"TargetID\", \"N Mismatch\"],\n",
    "    persist_data=False,\n",
    ")  ##fix this, was improperly made (only initial cellID timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_handle.view_overlay(vmin=0, vmax=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_table = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/2021-12-01_lDE20_Final_Barcodes_df/\"\n",
    ")\n",
    "gene_table = gene_table.reset_index().set_index(\"phenotype trenchid\", sorted=True)\n",
    "gene_table = (\n",
    "    gene_table.groupby(\"phenotype trenchid\", sort=False)\n",
    "    .apply(lambda x: x.iloc[0])\n",
    "    .reset_index()\n",
    "    .set_index(\"FOV Parquet Index\")\n",
    ")\n",
    "gene_table_out = gene_table.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "gene_table_out[\"phenotype trenchids\"] = gene_table.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "gene_table_out = gene_table_out[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_out = gene_table.groupby(\"sgRNA\").apply(lambda x: x.iloc[0])\n",
    "gene_table_out[\"phenotype trenchids\"] = gene_table.groupby(\"sgRNA\").apply(\n",
    "    lambda x: x[\"phenotype trenchid\"].tolist()\n",
    ")\n",
    "gene_table_out = gene_table_out[\n",
    "    [\n",
    "        \"Gene\",\n",
    "        \"Target Sequence\",\n",
    "        \"phenotype trenchids\",\n",
    "        \"N Mismatch\",\n",
    "        \"N Target Sites\",\n",
    "        \"Category\",\n",
    "        \"Strand\",\n",
    "    ]\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division/\"\n",
    ")\n",
    "wrapped_kymo_xarr = tr.kymo_xarr(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-11-08_lDE20_Final_3/Growth_Division\",\n",
    "    unwrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    gene_table_layout,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid,\n",
    ") = tr.linked_gene_table(\n",
    "    gene_table_out,\n",
    "    index_key=\"Gene\",\n",
    "    trenchids_as_list=True,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display, save_button = tr.linked_kymograph_for_gene_table(\n",
    "    kymo_xarr,\n",
    "    wrapped_kymo_xarr,\n",
    "    gene_table_out,\n",
    "    select_gene,\n",
    "    select_trenchid,\n",
    "    select_unpacked_trenchid=select_unpacked_trenchid,\n",
    "    trenchid_column=\"phenotype trenchids\",\n",
    "    y_scale=3,\n",
    "    x_window_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_button"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
