{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### LATER MERGE WORKFLOW WITH OLDER GFP PLUS NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_read(querystr, cigarstr, pattern=re.compile(\"[0-9]{0,10}[MDI]\")):\n",
    "    result = pattern.finditer(cigarstr)\n",
    "    cigar_seq = [(item.group(0)[-1], int(item.group(0)[:-1])) for item in result]\n",
    "    #     output_str = \"\".join([\"-\" for i in range(cigar[1])])\n",
    "    output_str = \"\"\n",
    "    current_idx = 0\n",
    "    for item in cigar_seq:\n",
    "        if item[0] == \"M\":\n",
    "            added_str = querystr[current_idx : current_idx + item[1]]\n",
    "            output_str += added_str\n",
    "            current_idx += item[1]\n",
    "        elif item[0] == \"D\":\n",
    "            added_str = \"\".join([\"-\" for i in range(item[1])])\n",
    "            output_str += added_str\n",
    "        elif item[0] == \"I\":\n",
    "            current_idx += item[1]\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def cigarsfromsam(samfilepath):\n",
    "    cigars = {}\n",
    "    with open(samfilepath, \"r\") as samfile:\n",
    "        for line in samfile:\n",
    "            if line[0] == \"@\":\n",
    "                next(samfile)\n",
    "            else:\n",
    "                splitline = line.split(\"\\t\")\n",
    "                cigars[splitline[0]] = splitline[5]\n",
    "    return cigars\n",
    "\n",
    "\n",
    "def strsfromfasta(fastafilepath):\n",
    "    queries = SeqIO.to_dict(SeqIO.parse(fastafilepath, \"fasta\"))\n",
    "    queries = {key: str(val.seq) for key, val in queries.items()}\n",
    "    return queries\n",
    "\n",
    "\n",
    "def make_seg_dict(gfafile):\n",
    "    segment_dict = {}\n",
    "    with open(gfafile, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            if line[0] == \"S\":\n",
    "                splitline = line.split(\"\\t\")\n",
    "                segment_dict[splitline[1]] = splitline[2][:-1]\n",
    "    return segment_dict\n",
    "\n",
    "\n",
    "def get_ref_intervals(gfafile):\n",
    "    segment_dict = {}\n",
    "    current_idx = 0\n",
    "    with open(gfafile, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            if line[0] == \"S\":\n",
    "                splitline = line.split(\"\\t\")\n",
    "                if \"OFF\" not in splitline[1]:\n",
    "                    refstr = splitline[2][:-1]\n",
    "                    strlen = len(refstr)\n",
    "                    name = splitline[1]\n",
    "                    if \"ON\" in name:\n",
    "                        name = name[:-2]\n",
    "                    segment_dict[name] = tuple((current_idx, current_idx + strlen))\n",
    "                    current_idx += strlen\n",
    "    return segment_dict\n",
    "\n",
    "\n",
    "def align_read(\n",
    "    querystr, refstr, cigarstr, startpos=1, pattern=re.compile(\"[0-9]{0,10}[MDI]\")\n",
    "):\n",
    "    start_pos = startpos - 1  ##comes as 1 indexed from minimap\n",
    "    result = pattern.finditer(cigarstr)\n",
    "    cigar_seq = [(item.group(0)[-1], int(item.group(0)[:-1])) for item in result]\n",
    "    #     output_str = \"\".join([\"-\" for i in range(cigar[1])])\n",
    "    output_str = \"\"\n",
    "    if start_pos > 0:\n",
    "        output_str += \"\".join([\"-\" for i in range(start_pos)])\n",
    "    current_idx = 0\n",
    "    for item in cigar_seq:\n",
    "        if item[0] == \"M\":\n",
    "            added_str = querystr[current_idx : current_idx + item[1]]\n",
    "            output_str += added_str\n",
    "            current_idx += item[1]\n",
    "        elif item[0] == \"D\":\n",
    "            added_str = \"\".join([\"-\" for i in range(item[1])])\n",
    "            output_str += added_str\n",
    "        elif item[0] == \"I\":\n",
    "            current_idx += item[1]\n",
    "    remaining_len = len(refstr) - len(output_str)\n",
    "    if remaining_len > 0:\n",
    "        output_str += \"\".join([\"-\" for i in range(remaining_len)])\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def splitstr(instr, ref_intervals):\n",
    "    strassign = {key: instr[val[0] : val[1]] for key, val in ref_intervals.items()}\n",
    "    return strassign\n",
    "\n",
    "\n",
    "def slow_hamming_distance(s1, s2):\n",
    "    if len(s1) != len(s2):\n",
    "        print(s1, s2)\n",
    "        raise ValueError(\"Strand lengths are not equal!\")\n",
    "    term_list = []\n",
    "    for ch1, ch2 in zip(s1, s2):\n",
    "        if ch1 == \"N\" or ch2 == \"N\":\n",
    "            term_list.append(False)\n",
    "        else:\n",
    "            term_list.append(ch1 != ch2)\n",
    "    result = sum(term_list)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_dict_dist(dict1, dict2):\n",
    "    hamming_dict = {\n",
    "        key: slow_hamming_distance(dict1[key], dict2[key]) for key in dict1.keys()\n",
    "    }\n",
    "    return hamming_dict\n",
    "\n",
    "\n",
    "def remove_bits_from_barcode(barcode, removed_bits=[]):\n",
    "    # Will convert removed bits to 0 values\n",
    "\n",
    "    barcode = np.array(list(barcode))\n",
    "    barcode[removed_bits] = 0\n",
    "    barcode = \"\".join(barcode.tolist())\n",
    "\n",
    "    return barcode\n",
    "\n",
    "\n",
    "def remove_bits(df, removed_bits):\n",
    "    # Will convert removed bits to 0 values\n",
    "\n",
    "    in_df = copy.deepcopy(df)\n",
    "    #     removed_bits = (removed_bits)\n",
    "\n",
    "    #     all_bits = set(range(len(in_df[\"barcode\"].iloc[0])))\n",
    "    #     kept_bits = list(all_bits-removed_bits)\n",
    "\n",
    "    subsampled_barcode = in_df[\"barcode\"].apply(\n",
    "        remove_bits_from_barcode, removed_bits=removed_bits\n",
    "    )\n",
    "    in_df[\"subsampled_barcode\"] = subsampled_barcode\n",
    "    unique_barcodes, n_occurances = np.unique(subsampled_barcode, return_counts=True)\n",
    "    redundant_barcodes = unique_barcodes[n_occurances > 1]\n",
    "    redundant_indices = subsampled_barcode[\n",
    "        subsampled_barcode.isin(redundant_barcodes.tolist())\n",
    "    ].index\n",
    "\n",
    "    in_df = (\n",
    "        in_df.drop(redundant_indices)\n",
    "        .reset_index(drop=True)\n",
    "        .drop([\"Unnamed: 0\", \"barcodeid\", \"barcode\"], axis=1)\n",
    "    )\n",
    "    in_df = in_df.rename({\"subsampled_barcode\": \"barcode\"}, axis=1)\n",
    "    in_df[\"barcodeid\"] = in_df.index\n",
    "    #     in_df[\"removed_bits\"] = removed_bits\n",
    "    #     in_df.insert(0, 'removed_bits', [removed_bits for i in range(len(in_df))])\n",
    "\n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metadata = {\"removed_bits\": [1]}\n",
    "data = pd.read_csv(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-02_snakemake_lDE15/output.tsv\",\n",
    "    delimiter=\"\\t\",\n",
    ")\n",
    "## Removing the problem bit for RFP, second cycle\n",
    "data = remove_bits(data, output_metadata[\"removed_bits\"])\n",
    "ref_intervals = get_ref_intervals(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-02_snakemake_lDE15/ref.gfa\"\n",
    ")\n",
    "barcodes = set(data[\"barcode\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_arr = np.array([list(item) for item in barcodes]).astype(\"uint8\")\n",
    "bit_freq = np.mean(bit_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bit_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "sns.barplot(x=list(range(len(bit_freq))), y=bit_freq, color=\"grey\")\n",
    "plt.xlabel(\"Bit Number\", fontsize=20)\n",
    "plt.ylabel(\"Percent Positive\", fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.savefig(\"./figure_1.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import h5py\n",
    "from dask.distributed import Client, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "\n",
    "def writedir(directory, overwrite=False):\n",
    "    if overwrite:\n",
    "        if os.path.exists(directory):\n",
    "            shutil.rmtree(directory)\n",
    "        os.makedirs(directory)\n",
    "    else:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "class dask_controller:  # adapted from Charles' code\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_workers=6,\n",
    "        local=True,\n",
    "        queue=\"short\",\n",
    "        death_timeout=3.0,\n",
    "        walltime=\"01:30:00\",\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        memory=\"6GB\",\n",
    "        working_directory=\"./\",\n",
    "        job_extra=[],\n",
    "    ):\n",
    "        self.local = local\n",
    "        self.n_workers = n_workers\n",
    "        self.walltime = walltime\n",
    "        self.queue = queue\n",
    "        self.death_timeout = death_timeout\n",
    "        self.processes = processes\n",
    "        self.memory = memory\n",
    "        self.cores = cores\n",
    "        self.working_directory = working_directory\n",
    "        self.job_extra = job_extra\n",
    "\n",
    "        writedir(working_directory, overwrite=False)\n",
    "\n",
    "    def startdask(self):\n",
    "        if self.local:\n",
    "            self.daskclient = Client()\n",
    "            self.daskclient.cluster.scale(self.n_workers)\n",
    "        else:\n",
    "            self.daskcluster = SLURMCluster(\n",
    "                n_workers=self.n_workers,\n",
    "                queue=self.queue,\n",
    "                death_timeout=self.death_timeout,\n",
    "                walltime=self.walltime,\n",
    "                processes=self.processes,\n",
    "                memory=self.memory,\n",
    "                cores=self.cores,\n",
    "                local_directory=self.working_directory,\n",
    "                log_directory=self.working_directory,\n",
    "                job_extra=self.job_extra,\n",
    "            )\n",
    "            #             self.workers = self.daskcluster.start_workers(self.n_workers)\n",
    "            self.daskclient = Client(self.daskcluster)\n",
    "\n",
    "    def shutdown(self):\n",
    "        self.daskclient.restart()\n",
    "        if not self.local:\n",
    "            self.daskcluster.close()\n",
    "        for item in os.listdir(self.working_directory):\n",
    "            if \"worker-\" in item or \"slurm-\" in item or \".lock\" in item:\n",
    "                path = \"./\" + item\n",
    "                if os.path.isfile(path):\n",
    "                    os.remove(path)\n",
    "                elif os.path.isdir(path):\n",
    "                    shutil.rmtree(path)\n",
    "\n",
    "    def printprogress(self):\n",
    "        complete = len([item for item in self.futures if item.status == \"finished\"])\n",
    "        print(str(complete) + \"/\" + str(len(self.futures)))\n",
    "\n",
    "    def displaydashboard(self):\n",
    "        link = self.daskcluster.dashboard_link\n",
    "        display(HTML('<a href=\"' + link + '\">Dashboard</a>'))\n",
    "\n",
    "    def mapfovs(self, function, fov_list, retries=0):\n",
    "        self.function = function\n",
    "        self.retries = retries\n",
    "\n",
    "        def mapallfovs(fov_number, function=function):\n",
    "            function(fov_number)\n",
    "\n",
    "        self.futures = {}\n",
    "        for fov in fov_list:\n",
    "            future = self.daskclient.submit(mapallfovs, fov, retries=retries)\n",
    "            self.futures[fov] = future\n",
    "\n",
    "    def retry_failed(self):\n",
    "        self.failed_fovs = [\n",
    "            fov for fov, future in self.futures.items() if future.status != \"finished\"\n",
    "        ]\n",
    "        out = self.daskclient.restart()\n",
    "        self.mapfovs(self.function, self.failed_fovs, retries=self.retries)\n",
    "\n",
    "    def retry_processing(self):\n",
    "        self.proc_fovs = [\n",
    "            fov for fov, future in self.futures.items() if future.status == \"pending\"\n",
    "        ]\n",
    "        out = self.daskclient.restart()\n",
    "        self.mapfovs(self.function, self.proc_fovs, retries=self.retries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller = dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/sync_folder/2021-03-02_snakemake_lDE15/dask/\",\n",
    ")\n",
    "\n",
    "# dask_controller = dask_controller(\n",
    "#     local=True,\n",
    "#     n_workers=1,\n",
    "#     memory=\"16GB\",\n",
    "#     working_directory= \"/home/de64/scratch/de64/sync_folder/2021-01-28_snakemake_lDE14/dask\",\n",
    "# )\n",
    "\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_bit_arr = da.from_array(bit_arr, chunks=(10000, 30)).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_bit_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 5\n",
    "step = dask_bit_arr.chunksize[0] * n_chunks\n",
    "\n",
    "closest_match_list = []\n",
    "\n",
    "for i in range(0, dask_bit_arr.shape[0], step):\n",
    "    both_on = (dask_bit_arr[i : i + step] @ dask_bit_arr.T).astype(\"uint8\")\n",
    "    both_off = ((-dask_bit_arr + 1)[i : i + step] @ (-dask_bit_arr.T + 1)).astype(\n",
    "        \"uint8\"\n",
    "    )\n",
    "    ttl_match = both_on + both_off\n",
    "    hamming_dist = dask_bit_arr.shape[1] - ttl_match\n",
    "\n",
    "    N = hamming_dist.shape[0]\n",
    "    M = hamming_dist.shape[1]\n",
    "    K = max(0, dask_bit_arr.shape[0] - (i + step))\n",
    "    zeros_arr_left = da.zeros((N, i)).astype(bool)\n",
    "    zeros_arr_right = da.zeros((N, K)).astype(bool)\n",
    "    diagonal_mask = da.eye(N).astype(bool)\n",
    "    padded_diagonal_mask = da.concatenate(\n",
    "        [zeros_arr_left, diagonal_mask, zeros_arr_right], axis=1\n",
    "    )\n",
    "\n",
    "    hamming_dist[padded_diagonal_mask] = 100\n",
    "    closest_match = da.min(hamming_dist, axis=1)\n",
    "    closest_match_list.append(closest_match.compute())\n",
    "\n",
    "closest_match_list = np.concatenate(closest_match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./closest_match.npy\", \"wb\") as f:\n",
    "    np.save(f, closest_match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./closest_match.npy\", \"rb\") as f:\n",
    "    closest_match_list = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_match_dict = {\n",
    "    barcode: closest_match_list[k] for k, barcode in enumerate(barcodes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(closest_match_list, bins=range(0, 9))\n",
    "plt.xlabel(\"Closest Hamming Distance\", fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.savefig(\"./figure_2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(closest_match_list, range=(0, 1), bins=2)\n",
    "plt.xlabel(\"Closest Hamming Distance\", fontsize=20)\n",
    "plt.xticks([0.25, 0.75], [0, 1], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.savefig(\"./figure_3.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(closest_match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = dd.from_pandas(data, npartitions=100).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Align consensus sequence to reference using cigar string\n",
    "aligned_cons = data_df.apply(\n",
    "    lambda x: align_read(\n",
    "        x[\"consensus\"], x[\"reference\"], x[\"cigar\"], startpos=x[\"alignmentstart\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ").persist()\n",
    "## Use GFA reference to determine intervals for each annotation\n",
    "ref_intervals = get_ref_intervals(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-01-28_snakemake_lDE14/ref.gfa\"\n",
    ")\n",
    "## Split sequences based on annotated intervals\n",
    "split_ref = data_df.apply(\n",
    "    lambda x: splitstr(x[\"reference\"], ref_intervals), axis=1\n",
    ").persist()\n",
    "split_align = aligned_cons.apply(lambda x: splitstr(x, ref_intervals)).persist()\n",
    "\n",
    "data_df[\"aligned_cons\"] = aligned_cons\n",
    "data_df[\"split_ref\"] = split_ref\n",
    "data_df[\"split_align\"] = split_align\n",
    "\n",
    "## Compute hamming distance from reference, by annotated element\n",
    "hamm_ref = data_df.apply(\n",
    "    lambda x: get_dict_dist(x[\"split_align\"], x[\"split_ref\"]), axis=1, meta=dict\n",
    ").persist()\n",
    "## Get hamming distance from reference of the nucleotides which vary in the library, to determine GFP vs DarkGFP\n",
    "dark_gfp = (\n",
    "    data_df.apply(\n",
    "        lambda x: slow_hamming_distance(\n",
    "            x[\"split_align\"][\"GFP\"][623:625], x[\"split_ref\"][\"GFP\"][623:625]\n",
    "        ),\n",
    "        axis=1,\n",
    "        meta=int,\n",
    "    )\n",
    "    > 0\n",
    ").persist()\n",
    "## Assign closest match to each barcode (for costructing output df later)\n",
    "closest_hamming_dist = (\n",
    "    data_df[\"barcode\"].compute().apply(lambda x: closest_match_dict[x])\n",
    ")\n",
    "\n",
    "data_df[\"hamm_ref\"] = hamm_ref\n",
    "data_df[\"dark_gfp\"] = dark_gfp\n",
    "data_df[\"Closest Hamming Distance\"] = closest_hamming_dist\n",
    "data_df = data_df.persist()\n",
    "\n",
    "del hamm_ref\n",
    "del dark_gfp\n",
    "del closest_hamming_dist\n",
    "del aligned_cons\n",
    "del split_align\n",
    "del split_ref\n",
    "\n",
    "data_df = data_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFP_freq = (\n",
    "    np.unique(data_df[\"dark_gfp\"], return_counts=True)[1] / data_df[\"dark_gfp\"].size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFP_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Metadata stored as a json for later\n",
    "import json\n",
    "\n",
    "with open(\"./lDE15_final_df.json\", \"w\") as f:\n",
    "    json.dump(output_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing dataframe as a csv\n",
    "data_df.to_csv(\"./lDE15_final_df.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
