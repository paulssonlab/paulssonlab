{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrenchRipper Master Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook contains the entire `TrenchRipper` pipline, divided into simple steps. This pipline is ideal for Mother <br>Machine image data where cells possess fluorescent segmentation markers. Segmentation on phase or brightfield data <br>is being developed, but is still an experimental feature.\n",
    "\n",
    "The steps in this pipeline are as follows:\n",
    "1. Extracting your Mother Machine data (.nd2) into hdf5 format\n",
    "2. Identifying and cropping individual trenches into kymographs\n",
    "3. Segmenting cells with a fluorescent marker\n",
    "4. Determining lineages and object properties\n",
    "\n",
    "In each step, the user will dynamically specify parameters using a series of interactive diagnostics on their dataset. <br>Following this, a parameter file will be written to disk and then used to deploy a parallel computation on the <br>dataset, either locally or on a SLURM cluster.\n",
    "\n",
    "\n",
    "This is intended as an end-to-end solution to analyzing Mother Machine data. As such, **it is not trivial to plug data <br>directly into intermediate steps**, as it will lack the correct formatting and associated metadata. A notable <br>exception to this is using another program to segment data. The library references binary segmentation masks using <br>only metadata derived from their associated kymographs. As such, it is possible to generate segmentations on these <br>kymographs elsewhere and place them into the segmentation data path to have `TrenchRipper` act on those <br>segmentations instead. More on this in the segmentation section..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Run this section to import all relavent packages and libraries used in this notebook. You must run this everytime you open a new python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/n/scratch2/de64/2019-11-09_CN_Growth_Curve/\"\n",
    "nd2file = \"/n/scratch2/de64/2019-11-09_CN_Growth_Curve/CN_Limited_GC_restart.nd2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath)\n",
    "viewer.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.ndextract.hdf5_fov_extractor(\n",
    "    nd2file,\n",
    "    headpath,\n",
    "    tpts_per_file=50,\n",
    "    ignore_fovmetadata=False,\n",
    "    nd2reader_override={\"z_levels\": [], \"z_coordinates\": []},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.view_image_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Napari Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5\n",
    "# Note that this Magics command needs to be run in a cell\n",
    "# before any of the Napari objects are instantiated to\n",
    "# ensure it has time to finish executing before they are\n",
    "# called\n",
    "\n",
    "from skimage import data\n",
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue\n",
    "\n",
    "Napari does not function in line in the Jupyter notebook. May be annoying to integrate with the remote framework the rest of trenchripper is built on. For now, just try working with holoviews/datashader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holoviews Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "import h5py\n",
    "\n",
    "from holoviews.operation.datashader import regrid\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdf5_viewer:\n",
    "    def __init__(\n",
    "        self, headpath, compute_data=False, persist_data=False, select_fovs=[]\n",
    "    ):\n",
    "        meta_handle = tr.pandas_hdf5_handler(headpath + \"metadata.hdf5\")\n",
    "        hdf5_df = meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        metadata = hdf5_df.metadata\n",
    "        index_df = pd.DataFrame(range(len(hdf5_df)), columns=[\"lookup index\"])\n",
    "        index_df.index = hdf5_df.index\n",
    "        hdf5_df = hdf5_df.join(index_df)\n",
    "        self.channels = metadata[\"channels\"]\n",
    "        if len(select_fovs) > 0:\n",
    "            fov_indices = select_fovs\n",
    "        else:\n",
    "            fov_indices = hdf5_df.index.get_level_values(\"fov\").unique().tolist()\n",
    "        file_indices = hdf5_df[\"File Index\"].unique().tolist()\n",
    "\n",
    "        dask_arrays = []\n",
    "\n",
    "        for fov_idx in fov_indices:\n",
    "            fov_arrays = []\n",
    "            fov_df = hdf5_df.loc[fov_idx:fov_idx]\n",
    "            file_indices = fov_df[\"File Index\"].unique().tolist()\n",
    "            for channel in self.channels:\n",
    "                channel_arrays = []\n",
    "                for file_idx in file_indices:\n",
    "                    infile = h5py.File(\n",
    "                        headpath + \"/hdf5/hdf5_\" + str(file_idx) + \".hdf5\", \"r\"\n",
    "                    )\n",
    "                    data = infile[channel]\n",
    "                    array = da.from_array(\n",
    "                        data, chunks=(1, data.shape[1], data.shape[2])\n",
    "                    )\n",
    "                    channel_arrays.append(array)\n",
    "                da_channel_arrays = da.concatenate(channel_arrays, axis=0)\n",
    "                fov_arrays.append(da_channel_arrays)\n",
    "            da_fov_arrays = da.stack(fov_arrays, axis=0)\n",
    "            dask_arrays.append(da_fov_arrays)\n",
    "        self.main_array = da.stack(dask_arrays, axis=0)\n",
    "        if compute_data:\n",
    "            self.main_array = self.main_array.compute()\n",
    "        elif persist_data:\n",
    "            self.main_array = self.main_array.persist()\n",
    "\n",
    "    def view(\n",
    "        self, width=1000, height=1000, cmap=\"Greys_r\", hist_on=False, hist_color=\"grey\"\n",
    "    ):\n",
    "        # Wrap in xarray DataArray and label coordinates\n",
    "        dims = [\n",
    "            \"FOV\",\n",
    "            \"Channel\",\n",
    "            \"time\",\n",
    "            \"y\",\n",
    "            \"x\",\n",
    "        ]\n",
    "        coords = {d: np.arange(s) for d, s in zip(dims, self.main_array.shape)}\n",
    "        coords[\"Channel\"] = np.array(self.channels)\n",
    "        xrstack = xr.DataArray(\n",
    "            self.main_array, dims=dims, coords=coords, name=\"Data\"\n",
    "        ).astype(\"uint16\")\n",
    "\n",
    "        # Wrap in HoloViews Dataset\n",
    "        ds = hv.Dataset(xrstack)\n",
    "\n",
    "        # # Convert to stack of images with x/y-coordinates along axes\n",
    "        image_stack = ds.to(hv.Image, [\"x\", \"y\"], dynamic=True)\n",
    "\n",
    "        # # Apply regridding if each image is large\n",
    "        regridded = regrid(image_stack)\n",
    "\n",
    "        # # Set a global Intensity range\n",
    "        # regridded = regridded.redim.range(Intensity=(0, 1000))\n",
    "\n",
    "        # # Set plot options\n",
    "        display_obj = regridded.opts(\n",
    "            plot={\n",
    "                \"Image\": dict(\n",
    "                    colorbar=True, width=width, height=height, tools=[\"hover\"]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        display_obj = display_obj.opts(cmap=cmap)\n",
    "\n",
    "        if hist_on:\n",
    "            hist = hv.operation.histogram(image_stack, num_bins=30)\n",
    "            hist = hist.opts(line_width=0, color=hist_color, width=200, height=height)\n",
    "            return display_obj << hist\n",
    "        else:\n",
    "            return display_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [\n",
    "    \"FOV\",\n",
    "    \"Channel\",\n",
    "    \"time\",\n",
    "    \"y\",\n",
    "    \"x\",\n",
    "]\n",
    "coords = {d: np.arange(s) for d, s in zip(dims, viewer.main_array.shape)}\n",
    "coords[\"Channel\"] = np.array(viewer.channels)\n",
    "xrstack = xr.DataArray(viewer.main_array, dims=dims, coords=coords, name=\"Data\").astype(\n",
    "    \"uint16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = hv.Dataset(xrstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = hdf5_viewer(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(hist_on=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.operation.datashader.regrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_indices = hdf5_df.index.get_level_values(\"fov\").unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_indices = hdf5_df[\"File Index\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_arrays = []\n",
    "\n",
    "for fov_idx in fov_indices:\n",
    "    fov_arrays = []\n",
    "    fov_df = hdf5_df.loc[fov_idx:fov_idx]\n",
    "    file_indices = fov_df[\"File Index\"].unique().tolist()\n",
    "    for channel in channels:\n",
    "        channel_arrays = []\n",
    "        for file_idx in file_indices:\n",
    "            infile = h5py.File(headpath + \"/hdf5/hdf5_\" + str(file_idx) + \".hdf5\", \"r\")\n",
    "            data = infile[channel]\n",
    "            array = da.from_array(data, chunks=(1, data.shape[1], data.shape[2]))\n",
    "            channel_arrays.append(array)\n",
    "        da_channel_arrays = da.concatenate(channel_arrays, axis=0)\n",
    "        fov_arrays.append(da_channel_arrays)\n",
    "    da_fov_arrays = da.stack(fov_arrays, axis=0)\n",
    "    dask_arrays.append(da_fov_arrays)\n",
    "x = da.stack(dask_arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.help(image_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdims = [\n",
    "    hv.Dimension(\"phase\", range=(0, np.pi)),\n",
    "    hv.Dimension(\"frequency\", values=[0.1, 1, 2, 5, 10]),\n",
    "    hv.Dimension(\"amplitude\", values=[0.5, 5, 10]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_stack.opts(kdims=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in xarray DataArray and label coordinates\n",
    "dims = [\n",
    "    \"FOV\",\n",
    "    \"Channel\",\n",
    "    \"time\",\n",
    "    \"y\",\n",
    "    \"x\",\n",
    "]\n",
    "coords = {d: np.arange(s) for d, s in zip(dims, x.shape)}\n",
    "coords[\"Channel\"] = np.array(channels)\n",
    "xrstack = xr.DataArray(\n",
    "    x,\n",
    "    dims=dims,\n",
    "    coords=coords,\n",
    ").astype(\"uint16\")\n",
    "\n",
    "# Wrap in HoloViews Dataset\n",
    "ds = hv.Dataset(xrstack)\n",
    "\n",
    "# # Convert to stack of images with x/y-coordinates along axes\n",
    "image_stack = ds.to(hv.Image, [\"x\", \"y\"], dynamic=True)\n",
    "\n",
    "# # Apply regridding if each image is large\n",
    "regridded = regrid(image_stack)\n",
    "\n",
    "# # Set a global Intensity range\n",
    "# regridded = regridded.redim.range(Intensity=(0, 1000))\n",
    "\n",
    "# # Set plot options\n",
    "display_obj = regridded.opts(\n",
    "    plot={\"Image\": dict(colorbar=True, width=1000, height=1000, tools=[\"hover\"])}\n",
    ")\n",
    "display_obj = display_obj.opts(cmap=\"Greys_r\")\n",
    "\n",
    "# hist = histogram(image_stack,num_bins=30)\n",
    "# hist = hist.opts(line_width=0,color=\"grey\", width=200,height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_obj  # << hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_hist(x_range, y_range):\n",
    "    # Apply current ranges\n",
    "    obj = img.select(x=x_range, y=y_range) if x_range and y_range else img\n",
    "\n",
    "    # Compute histogram\n",
    "    return hv.operation.histogram(obj)\n",
    "\n",
    "\n",
    "rangexy = hv.streams.RangeXY(source=display_obj)\n",
    "\n",
    "display_obj << hv.DynamicMap(selected_hist, streams=[rangexy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_hist(x_range, y_range):\n",
    "    # Apply current ranges\n",
    "    obj = img.select(x=x_range, y=y_range) if x_range and y_range else img\n",
    "\n",
    "    # Compute histogram\n",
    "    return hv.operation.histogram(obj)\n",
    "\n",
    "\n",
    "# Define a RangeXY stream linked to the image\n",
    "rangexy = hv.streams.RangeXY(source=display_obj)\n",
    "\n",
    "# Adjoin the dynamic histogram computed based on the current ranges\n",
    "img << hv.DynamicMap(selected_hist, streams=[rangexy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_obj + hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot and style options\n",
    "opts.defaults(\n",
    "    opts.Curve(\n",
    "        xaxis=None,\n",
    "        yaxis=None,\n",
    "        show_grid=False,\n",
    "        show_frame=False,\n",
    "        color=\"orangered\",\n",
    "        framewise=True,\n",
    "        width=100,\n",
    "    ),\n",
    "    opts.Image(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        shared_axes=False,\n",
    "        logz=True,\n",
    "        xaxis=None,\n",
    "        yaxis=None,\n",
    "        axiswise=True,\n",
    "    ),\n",
    "    opts.HLine(color=\"white\", line_width=1),\n",
    "    opts.Layout(shared_axes=False),\n",
    "    opts.VLine(color=\"white\", line_width=1),\n",
    ")\n",
    "\n",
    "# Read the parquet file\n",
    "df = dd.read_parquet(\"./data/nyc_taxi_wide.parq\").persist()\n",
    "\n",
    "# Declare points\n",
    "points = hv.Points(df, kdims=[\"pickup_x\", \"pickup_y\"], vdims=[])\n",
    "\n",
    "# Use datashader to rasterize and linked streams for interactivity\n",
    "agg = aggregate(points, link_inputs=True, x_sampling=0.0001, y_sampling=0.0001)\n",
    "pointerx = hv.streams.PointerX(x=np.mean(points.range(\"pickup_x\")), source=points)\n",
    "pointery = hv.streams.PointerY(y=np.mean(points.range(\"pickup_y\")), source=points)\n",
    "vline = hv.DynamicMap(lambda x: hv.VLine(x), streams=[pointerx])\n",
    "hline = hv.DynamicMap(lambda y: hv.HLine(y), streams=[pointery])\n",
    "\n",
    "sampled = hv.util.Dynamic(\n",
    "    agg,\n",
    "    operation=lambda obj, x: obj.sample(pickup_x=x),\n",
    "    streams=[pointerx],\n",
    "    link_inputs=False,\n",
    ")\n",
    "\n",
    "hvobj = (agg * hline * vline) << sampled\n",
    "\n",
    "# Obtain Bokeh document and set the title\n",
    "doc = renderer.server_doc(hvobj)\n",
    "doc.title = \"NYC Taxi Crosshair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
