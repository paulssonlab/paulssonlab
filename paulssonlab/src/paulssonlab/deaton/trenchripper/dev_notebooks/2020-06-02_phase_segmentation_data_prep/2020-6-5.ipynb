{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage as sk\n",
    "from skimage.util import img_as_ubyte\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "import copy\n",
    "import scipy\n",
    "from skimage.morphology import square\n",
    "from skimage.filters import rank\n",
    "from scipy.ndimage import convolve1d\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_border(labeled):\n",
    "    mask = sk.segmentation.find_boundaries(labeled)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask(labeled):\n",
    "    mask = np.zeros(labeled.shape, dtype=bool)\n",
    "    mask[labeled > 0] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_background(labeled):\n",
    "    mask = np.zeros(labeled.shape, dtype=bool)\n",
    "    mask[labeled == 0] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_masknoborder(labeled):\n",
    "    mask = get_mask(labeled)\n",
    "    border = get_border(labeled)\n",
    "    mask[border] = False\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_segmentation(labeled, mode_list=[\"background\", \"mask\", \"border\"]):\n",
    "    segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "    for i, mode in enumerate(mode_list):\n",
    "        if mode == \"background\":\n",
    "            segmentation[get_background(labeled)] = i\n",
    "        elif mode == \"mask\":\n",
    "            segmentation[get_mask(labeled)] = i\n",
    "        elif mode == \"border\":\n",
    "            segmentation[get_border(labeled)] = i\n",
    "        elif mode == \"masknoborder\":\n",
    "            segmentation[get_masknoborder(labeled)] = i\n",
    "        else:\n",
    "            raise\n",
    "    return segmentation\n",
    "\n",
    "\n",
    "def get_standard_weightmap(segmentation):\n",
    "    num_labels = len(np.unique(segmentation))\n",
    "    label_count = []\n",
    "    label_masks = []\n",
    "    for label in range(num_labels):\n",
    "        label_mask = segmentation == label\n",
    "        num_label = np.sum(label_mask)\n",
    "        label_masks.append(label_mask)\n",
    "        label_count.append(num_label)\n",
    "\n",
    "    ttl_count = np.sum(label_count)\n",
    "    class_weight = ttl_count / (np.array(label_count) + 1)\n",
    "    class_weight = class_weight / np.sum(class_weight)\n",
    "    weight_map = np.zeros(segmentation.shape, dtype=np.float32)\n",
    "    for i, label_mask in enumerate(label_masks):\n",
    "        weight_map[label_mask] = class_weight[i]\n",
    "\n",
    "    return weight_map\n",
    "\n",
    "\n",
    "def get_unet_weightmap(labeled, W0=5.0, Wsigma=2.0):\n",
    "    mask = labeled > 0\n",
    "\n",
    "    ttl_count = mask.size\n",
    "    mask_count = np.sum(mask)\n",
    "    background_count = ttl_count - mask_count\n",
    "\n",
    "    class_weight = np.array(\n",
    "        [ttl_count / (background_count + 1), ttl_count / (mask_count + 1)]\n",
    "    )\n",
    "    class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "    labels = np.unique(labeled)[1:]\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    if num_labels == 0:\n",
    "        weight_map = np.ones(labeled.shape) * class_weight[0]\n",
    "    elif num_labels == 1:\n",
    "        weight_map = np.ones(labeled.shape) * class_weight[0]\n",
    "        weight_map[mask] += class_weight[1]\n",
    "    else:\n",
    "        dist_maps = []\n",
    "        borders = []\n",
    "        for i in labels:\n",
    "            cell = labeled == i\n",
    "            eroded = sk.morphology.binary_dilation(cell)\n",
    "            border = eroded ^ cell\n",
    "            borders.append(border)\n",
    "            dist_map = scipy.ndimage.morphology.distance_transform_edt(~border)\n",
    "            dist_maps.append(dist_map)\n",
    "        dist_maps = np.array(dist_maps)\n",
    "        borders = np.array(borders)\n",
    "        borders = np.max(borders, axis=0)\n",
    "        dist_maps = np.sort(dist_maps, axis=0)\n",
    "        weight_map = W0 * np.exp(\n",
    "            -((dist_maps[0] + dist_maps[1]) ** 2) / (2 * (Wsigma**2))\n",
    "        )\n",
    "        weight_map[mask] += class_weight[1]\n",
    "        weight_map[~mask] += class_weight[0]\n",
    "\n",
    "    return weight_map\n",
    "\n",
    "\n",
    "def get_flows(labeled):\n",
    "    rps = sk.measure.regionprops(labeled)\n",
    "    centers = np.array([np.round(rp.centroid).astype(\"uint16\") for rp in rps])\n",
    "    y_lens = np.array([rp.bbox[2] - rp.bbox[0] for rp in rps])\n",
    "    x_lens = np.array([rp.bbox[3] - rp.bbox[1] for rp in rps])\n",
    "    N_arr = 2 * (y_lens + x_lens)\n",
    "    kernel = np.ones(3, float) / 3.0\n",
    "\n",
    "    x_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "    y_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "\n",
    "    for cell_idx in range(1, len(rps) + 1):\n",
    "        cell_mask = labeled == cell_idx\n",
    "        cell_center = centers[cell_idx - 1]\n",
    "        diffusion_arr = np.zeros(cell_mask.shape, dtype=np.float32)\n",
    "        for i in range(N_arr[cell_idx - 1]):\n",
    "            diffusion_arr[cell_center] += 1.0\n",
    "            diffusion_arr = convolve1d(\n",
    "                convolve1d(diffusion_arr, kernel, axis=0), kernel, axis=1\n",
    "            )\n",
    "            diffusion_arr[~cell_mask] = 0.0\n",
    "        y_grad, x_grad = np.gradient(diffusion_arr)\n",
    "        norm = np.sqrt(y_grad**2 + x_grad**2)\n",
    "        y_grad, x_grad = (y_grad / norm), (x_grad / norm)\n",
    "        y_grad[~cell_mask] = 0.0\n",
    "        x_grad[~cell_mask] = 0.0\n",
    "\n",
    "        y_grad_arr += y_grad\n",
    "        x_grad_arr += x_grad\n",
    "\n",
    "    return y_grad_arr, x_grad_arr\n",
    "\n",
    "\n",
    "def get_two_class(labeled):\n",
    "    segmentation = get_segmentation(labeled, mode_list=[\"background\", \"mask\", \"border\"])\n",
    "    weightmap = get_standard_weightmap(segmentation)\n",
    "    if np.any(np.isnan(segmentation)) or np.any(np.isnan(weightmap)):\n",
    "        segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "        weightmap = np.zeros(segmentation.shape, dtype=np.float32)\n",
    "    return segmentation, weightmap\n",
    "\n",
    "\n",
    "def get_one_class(labeled, W0=5.0, Wsigma=2.0):\n",
    "    segmentation = get_segmentation(\n",
    "        labeled, mode_list=[\"background\", \"masknoborder\"]\n",
    "    ).astype(bool)\n",
    "    weightmap = get_unet_weightmap(labeled, W0=W0, Wsigma=Wsigma)\n",
    "    if np.any(np.isnan(segmentation)) or np.any(np.isnan(weightmap)):\n",
    "        segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "        weightmap = np.zeros(segmentation.shape, dtype=np.float32)\n",
    "    return segmentation, weightmap\n",
    "\n",
    "\n",
    "def get_cellpose(labeled):\n",
    "    segmentation = get_segmentation(labeled, mode_list=[\"background\", \"mask\"]).astype(\n",
    "        bool\n",
    "    )\n",
    "    y_grad_arr, x_grad_arr = get_flows(labeled)\n",
    "    if (\n",
    "        np.any(np.isnan(segmentation))\n",
    "        or np.any(np.isnan(y_grad_arr))\n",
    "        or np.any(np.isnan(x_grad_arr))\n",
    "    ):\n",
    "        segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "        x_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "        y_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "    return segmentation, y_grad_arr, x_grad_arr\n",
    "\n",
    "\n",
    "## next make flow vector generation\n",
    "## package into data preprocessing class that generates requested arrays accessible on keys (look at prior work)\n",
    "## test generation at scale\n",
    "## augment on the fly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import copy\n",
    "import ipywidgets as ipyw\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import itertools\n",
    "import qgrid\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from random import shuffle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy import interpolate, ndimage\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import skimage as sk\n",
    "import pickle as pkl\n",
    "import skimage.morphology\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch._six import container_abcs, string_classes, int_classes\n",
    "import numpy as np\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import (\n",
    "    pandas_hdf5_handler,\n",
    "    kymo_handle,\n",
    "    writedir,\n",
    ")\n",
    "from imgaug.augmentables.heatmaps import HeatmapsOnImage\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import hdf5lock\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import object_f_scores\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_Training_DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nndatapath=\"\",\n",
    "        experimentname=\"\",\n",
    "        output_names=[\"train\", \"test\", \"val\"],\n",
    "        output_modes=[\"class\", \"multiclass\", \"cellpose\"],\n",
    "        input_paths=[],\n",
    "        W0_list=[5.0],\n",
    "        Wsigma_list=[2.0],\n",
    "    ):\n",
    "        self.nndatapath = nndatapath\n",
    "        self.metapath = nndatapath + \"/metadata.hdf5\"\n",
    "        self.experimentname = experimentname\n",
    "        self.output_names = output_names\n",
    "        self.output_modes = output_modes\n",
    "        self.input_paths = input_paths\n",
    "        self.W0_list = W0_list\n",
    "        self.Wsigma_list = Wsigma_list\n",
    "\n",
    "    def get_metadata(self, headpath):\n",
    "        meta_handle = pandas_hdf5_handler(headpath + \"/metadata.hdf5\")\n",
    "        global_handle = meta_handle.read_df(\"global\", read_metadata=True)\n",
    "\n",
    "        kymodf = dd.read_parquet(headpath + \"/kymograph/metadata\").persist()\n",
    "        kymodf = kymodf.set_index(\"fov\").persist()\n",
    "\n",
    "        channel_list = global_handle.metadata[\"channels\"]\n",
    "        fov_list = kymodf.index.compute().get_level_values(\"fov\").unique().tolist()\n",
    "        t_len = len(kymodf.loc[fov_list[0]][\"timepoints\"].unique().compute())\n",
    "        ttl_trenches = len(kymodf[\"trenchid\"].unique().compute())\n",
    "\n",
    "        trench_dict = {\n",
    "            fov: len(kymodf.loc[fov][\"trenchid\"].unique().compute()) for fov in fov_list\n",
    "        }\n",
    "        with open(headpath + \"/kymograph/metadata.pkl\", \"rb\") as handle:\n",
    "            ky_metadata = pkl.load(handle)\n",
    "\n",
    "        shape_y = ky_metadata[\"kymograph_params\"][\"ttl_len_y\"]\n",
    "        shape_x = ky_metadata[\"kymograph_params\"][\"trench_width_x\"]\n",
    "        kymograph_img_shape = tuple((shape_y, shape_x))\n",
    "        return (\n",
    "            channel_list,\n",
    "            fov_list,\n",
    "            t_len,\n",
    "            trench_dict,\n",
    "            ttl_trenches,\n",
    "            kymograph_img_shape,\n",
    "        )\n",
    "\n",
    "    def inter_get_selection(self):\n",
    "        output_tabs = []\n",
    "        for i in range(len(self.output_names)):\n",
    "            dset_tabs = []\n",
    "            for j in range(len(self.input_paths)):\n",
    "                (\n",
    "                    channel_list,\n",
    "                    fov_list,\n",
    "                    t_len,\n",
    "                    trench_dict,\n",
    "                    ttl_trenches,\n",
    "                    kymograph_img_shape,\n",
    "                ) = self.get_metadata(self.input_paths[j])\n",
    "\n",
    "                feature_dropdown = ipyw.Dropdown(\n",
    "                    options=channel_list,\n",
    "                    value=channel_list[0],\n",
    "                    description=\"Feature Channel:\",\n",
    "                    disabled=False,\n",
    "                )\n",
    "                max_samples = ipyw.IntText(\n",
    "                    value=0, description=\"Maximum Samples per Dataset:\", disabled=False\n",
    "                )\n",
    "                t_range = ipyw.IntRangeSlider(\n",
    "                    value=[0, t_len - 1],\n",
    "                    description=\"Timepoint Range:\",\n",
    "                    min=0,\n",
    "                    max=t_len - 1,\n",
    "                    step=1,\n",
    "                    disabled=False,\n",
    "                    continuous_update=False,\n",
    "                )\n",
    "\n",
    "                working_tab = ipyw.VBox(\n",
    "                    children=[feature_dropdown, max_samples, t_range]\n",
    "                )\n",
    "                dset_tabs.append(working_tab)\n",
    "\n",
    "            dset_ipy_tabs = ipyw.Tab(children=dset_tabs)\n",
    "            for j in range(len(self.input_paths)):\n",
    "                dset_ipy_tabs.set_title(j, self.input_paths[j].split(\"/\")[-1])\n",
    "            output_tabs.append(dset_ipy_tabs)\n",
    "        output_ipy_tabs = ipyw.Tab(children=output_tabs)\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            output_ipy_tabs.set_title(i, output_name)\n",
    "        self.tab = output_ipy_tabs\n",
    "\n",
    "        return self.tab\n",
    "\n",
    "    def get_import_params(self):\n",
    "        self.import_param_dict = {}\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            self.import_param_dict[output_name] = {}\n",
    "            for j, input_path in enumerate(self.input_paths):\n",
    "                working_vbox = self.tab.children[i].children[j]\n",
    "                self.import_param_dict[output_name][input_path] = {\n",
    "                    child.description: child.value for child in working_vbox.children\n",
    "                }\n",
    "\n",
    "        print(\"======== Import Params ========\")\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            print(str(output_name))\n",
    "            for j, input_path in enumerate(self.input_paths):\n",
    "                (\n",
    "                    channel_list,\n",
    "                    fov_list,\n",
    "                    t_len,\n",
    "                    trench_dict,\n",
    "                    ttl_trenches,\n",
    "                    kymograph_img_shape,\n",
    "                ) = self.get_metadata(input_path)\n",
    "                ttl_possible_samples = t_len * ttl_trenches\n",
    "                param_dict = self.import_param_dict[output_name][input_path]\n",
    "                requested_samples = param_dict[\"Maximum Samples per Dataset:\"]\n",
    "                if requested_samples > 0:\n",
    "                    print(str(input_path))\n",
    "                    for key, val in param_dict.items():\n",
    "                        print(key + \" \" + str(val))\n",
    "                    print(\n",
    "                        \"Requested Samples / Total Samples: \"\n",
    "                        + str(requested_samples)\n",
    "                        + \"/\"\n",
    "                        + str(ttl_possible_samples)\n",
    "                    )\n",
    "\n",
    "        del self.tab\n",
    "\n",
    "    def export_chunk(self, output_name, init_idx, chunk_size, chunk_idx):\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        output_df = output_meta_handle.read_df(output_name)\n",
    "        working_df = output_df[init_idx : init_idx + chunk_size]\n",
    "        nndatapath = (\n",
    "            self.nndatapath + \"/\" + output_name + \"_\" + str(chunk_idx) + \".hdf5\"\n",
    "        )\n",
    "\n",
    "        dset_paths = working_df.index.get_level_values(0).unique().tolist()\n",
    "        for dset_path in dset_paths:\n",
    "            dset_path_key = dset_path.split(\"/\")[-1]\n",
    "            dset_df = working_df.loc[dset_path]\n",
    "\n",
    "            if isinstance(dset_df, pd.Series):\n",
    "                dset_df = dset_df.to_frame()\n",
    "\n",
    "            param_dict = self.import_param_dict[output_name][dset_path]\n",
    "            feature_channel = param_dict[\"Feature Channel:\"]\n",
    "\n",
    "            img_arr_list = []\n",
    "            seg_arr_list = []\n",
    "\n",
    "            dset_df = dset_df.set_index(\"File Index\")\n",
    "            dset_df = dset_df.sort_index()\n",
    "            file_indices = dset_df.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "            for file_idx in file_indices:\n",
    "                file_df = dset_df.loc[file_idx:file_idx]\n",
    "\n",
    "                img_path = dset_path + \"/kymograph/kymograph_\" + str(file_idx) + \".hdf5\"\n",
    "                seg_path = (\n",
    "                    dset_path\n",
    "                    + \"/fluorsegmentation/segmentation_\"\n",
    "                    + str(file_idx)\n",
    "                    + \".hdf5\"\n",
    "                )\n",
    "\n",
    "                with h5py.File(img_path, \"r\") as imgfile:\n",
    "                    working_arr = imgfile[feature_channel][:]\n",
    "\n",
    "                trench_df = file_df.set_index(\"File Trench Index\")\n",
    "                trench_df = trench_df.sort_index()\n",
    "\n",
    "                for trench_idx, row in trench_df.iterrows():\n",
    "                    img_arr = working_arr[trench_idx, row[\"timepoints\"]][\n",
    "                        np.newaxis, np.newaxis, :, :\n",
    "                    ]  # 1,1,y,x img\n",
    "                    img_arr = img_arr.astype(\"uint16\")\n",
    "                    img_arr_list.append(img_arr)\n",
    "\n",
    "                with h5py.File(seg_path, \"r\") as segfile:\n",
    "                    working_arr = segfile[\"data\"][:]\n",
    "\n",
    "                for trench_idx, row in trench_df.iterrows():\n",
    "                    seg_arr = working_arr[trench_idx, row[\"timepoints\"]][\n",
    "                        np.newaxis, np.newaxis, :, :\n",
    "                    ]\n",
    "                    seg_arr = seg_arr.astype(\"int8\")\n",
    "                    seg_arr_list.append(seg_arr)\n",
    "\n",
    "            output_img_arr = np.concatenate(img_arr_list, axis=0)\n",
    "            output_seg_arr = np.concatenate(seg_arr_list, axis=0)  # N,1,y,x\n",
    "            chunk_shape = (1, 1, output_img_arr.shape[2], output_img_arr.shape[3])\n",
    "\n",
    "            with h5py.File(nndatapath, \"w\") as outfile:\n",
    "                img_handle = outfile.create_dataset(\n",
    "                    dset_path_key + \"/img\",\n",
    "                    data=output_img_arr,\n",
    "                    chunks=chunk_shape,\n",
    "                    dtype=\"uint16\",\n",
    "                )\n",
    "                seg_handle = outfile.create_dataset(\n",
    "                    dset_path_key + \"/seg\",\n",
    "                    data=output_seg_arr,\n",
    "                    chunks=chunk_shape,\n",
    "                    dtype=\"int8\",\n",
    "                )\n",
    "\n",
    "                for output_mode in self.output_modes:\n",
    "\n",
    "                    if output_mode == \"class\":\n",
    "                        output_seg_arr_class = []\n",
    "                        for i, W0 in enumerate(self.W0_list):\n",
    "                            for j, Wsigma in enumerate(self.Wsigma_list):\n",
    "                                output_weight_arr = []\n",
    "                                for l in range(output_seg_arr.shape[0]):\n",
    "                                    labeled = output_seg_arr[l, 0]\n",
    "                                    segmentation, weightmap = get_one_class(\n",
    "                                        labeled, W0=W0, Wsigma=Wsigma\n",
    "                                    )\n",
    "                                    output_weight_arr.append(\n",
    "                                        weightmap[np.newaxis, np.newaxis, :, :]\n",
    "                                    )\n",
    "                                    if i + j == 0:\n",
    "                                        output_seg_arr_class.append(\n",
    "                                            segmentation[np.newaxis, np.newaxis, :, :]\n",
    "                                        )\n",
    "                                output_weight_arr = np.concatenate(\n",
    "                                    output_weight_arr, axis=0\n",
    "                                )\n",
    "                                weight_handle = outfile.create_dataset(\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/W0=\"\n",
    "                                    + str(W0)\n",
    "                                    + \"_Wsigma=\"\n",
    "                                    + str(Wsigma)\n",
    "                                    + \"/weight\",\n",
    "                                    data=output_weight_arr,\n",
    "                                    chunks=chunk_shape,\n",
    "                                    dtype=np.float32,\n",
    "                                )\n",
    "\n",
    "                        output_seg_arr_class = np.concatenate(\n",
    "                            output_seg_arr_class, axis=0\n",
    "                        )\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/seg\",\n",
    "                            data=output_seg_arr_class,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "                    elif output_mode == \"multiclass\":\n",
    "                        output_seg_arr_multiclass = []\n",
    "                        output_weight_arr = []\n",
    "                        for l in range(output_seg_arr.shape[0]):\n",
    "                            labeled = output_seg_arr[l, 0]\n",
    "                            segmentation, weightmap = get_two_class(labeled)\n",
    "                            output_seg_arr_multiclass.append(\n",
    "                                segmentation[np.newaxis, np.newaxis, :, :]\n",
    "                            )\n",
    "                            output_weight_arr.append(\n",
    "                                weightmap[np.newaxis, np.newaxis, :, :]\n",
    "                            )\n",
    "                        output_seg_arr_multiclass = np.concatenate(\n",
    "                            output_seg_arr_multiclass, axis=0\n",
    "                        )\n",
    "                        output_weight_arr = np.concatenate(output_weight_arr, axis=0)\n",
    "\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/seg\",\n",
    "                            data=output_seg_arr_multiclass,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "                        weight_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/weight\",\n",
    "                            data=output_weight_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "\n",
    "                    elif output_mode == \"cellpose\":\n",
    "                        output_seg_arr_cellpose = []\n",
    "                        output_y_grad_arr = []\n",
    "                        output_x_grad_arr = []\n",
    "                        for l in range(output_seg_arr.shape[0]):\n",
    "                            labeled = output_seg_arr[l, 0]\n",
    "                            segmentation, y_grad_arr, x_grad_arr = get_cellpose(labeled)\n",
    "                            output_seg_arr_cellpose.append(\n",
    "                                segmentation[np.newaxis, np.newaxis, :, :]\n",
    "                            )\n",
    "                            output_y_grad_arr.append(\n",
    "                                y_grad_arr[np.newaxis, np.newaxis, :, :]\n",
    "                            )\n",
    "                            output_x_grad_arr.append(\n",
    "                                x_grad_arr[np.newaxis, np.newaxis, :, :]\n",
    "                            )\n",
    "                        output_seg_arr_cellpose = np.concatenate(\n",
    "                            output_seg_arr_cellpose, axis=0\n",
    "                        )\n",
    "                        output_y_grad_arr = np.concatenate(output_y_grad_arr, axis=0)\n",
    "                        output_x_grad_arr = np.concatenate(output_x_grad_arr, axis=0)\n",
    "\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/seg\",\n",
    "                            data=output_seg_arr_cellpose,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "                        y_grad_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/y_grad\",\n",
    "                            data=output_y_grad_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "\n",
    "                        x_grad_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/x_grad\",\n",
    "                            data=output_x_grad_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "        return init_idx\n",
    "\n",
    "    def gather_chunks(self, output_name, init_idx_list, chunk_idx_list, chunk_size):\n",
    "\n",
    "        #                       outputdf,output_metadata,selectionname,file_idx_list,weight_grid_list):\n",
    "        nnoutputpath = self.nndatapath + \"/\" + output_name + \".hdf5\"\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        output_df = output_meta_handle.read_df(output_name)\n",
    "\n",
    "        dset_paths = output_df.index.get_level_values(0).unique().tolist()\n",
    "        for dset_path in dset_paths:\n",
    "            dset_path_key = dset_path.split(\"/\")[-1]\n",
    "            dset_df = output_df.loc[dset_path]\n",
    "\n",
    "            tempdatapath = self.nndatapath + \"/\" + output_name + \"_0.hdf5\"\n",
    "            with h5py.File(tempdatapath, \"r\") as infile:\n",
    "                img_shape = infile[dset_path_key + \"/img\"].shape\n",
    "            output_shape = (len(dset_df.index), 1, img_shape[2], img_shape[3])\n",
    "            chunk_shape = (1, 1, img_shape[2], img_shape[3])\n",
    "\n",
    "            with h5py.File(nnoutputpath, \"w\") as outfile:\n",
    "                img_handle = outfile.create_dataset(\n",
    "                    dset_path_key + \"/img\",\n",
    "                    output_shape,\n",
    "                    chunks=chunk_shape,\n",
    "                    dtype=\"uint16\",\n",
    "                )\n",
    "                seg_handle = outfile.create_dataset(\n",
    "                    dset_path_key + \"/seg\",\n",
    "                    output_shape,\n",
    "                    chunks=chunk_shape,\n",
    "                    dtype=\"int8\",\n",
    "                )\n",
    "                for output_mode in self.output_modes:\n",
    "                    if output_mode == \"class\":\n",
    "                        for i, W0 in enumerate(self.W0_list):\n",
    "                            for j, Wsigma in enumerate(self.Wsigma_list):\n",
    "                                weight_handle = outfile.create_dataset(\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/W0=\"\n",
    "                                    + str(W0)\n",
    "                                    + \"_Wsigma=\"\n",
    "                                    + str(Wsigma)\n",
    "                                    + \"/weight\",\n",
    "                                    output_shape,\n",
    "                                    chunks=chunk_shape,\n",
    "                                    dtype=np.float32,\n",
    "                                )\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/seg\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "                    elif output_mode == \"multiclass\":\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/seg\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "                        weight_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/weight\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                    elif output_mode == \"cellpose\":\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/seg\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "                        y_grad_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/y_grad\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                        x_grad_handle = outfile.create_dataset(\n",
    "                            dset_path_key + \"/\" + output_mode + \"/x_grad\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "        current_dset_path = \"\"\n",
    "        for i, init_idx in enumerate(init_idx_list):\n",
    "            chunk_idx = chunk_idx_list[i]\n",
    "            nndatapath = (\n",
    "                self.nndatapath + \"/\" + output_name + \"_\" + str(chunk_idx) + \".hdf5\"\n",
    "            )\n",
    "            working_df = output_df[init_idx : init_idx + chunk_size]\n",
    "            dset_paths = working_df.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "            with h5py.File(nndatapath, \"r\") as infile:\n",
    "\n",
    "                for dset_path in dset_paths:\n",
    "                    if dset_path != current_dset_path:\n",
    "                        current_idx = 0\n",
    "                        current_dset_path = dset_path\n",
    "\n",
    "                    dset_path_key = dset_path.split(\"/\")[-1]\n",
    "                    dset_df = output_df.loc[dset_path]\n",
    "\n",
    "                    with h5py.File(nnoutputpath, \"a\") as outfile:\n",
    "                        img_arr = infile[dset_path_key + \"/img\"][:]\n",
    "                        num_indices = img_arr.shape[0]\n",
    "                        outfile[dset_path_key + \"/img\"][\n",
    "                            current_idx : current_idx + num_indices\n",
    "                        ] = img_arr\n",
    "\n",
    "                        seg_arr = infile[dset_path_key + \"/seg\"][:]\n",
    "                        outfile[dset_path_key + \"/seg\"][\n",
    "                            current_idx : current_idx + num_indices\n",
    "                        ] = seg_arr\n",
    "\n",
    "                        for output_mode in self.output_modes:\n",
    "                            if output_mode == \"class\":\n",
    "                                seg_arr = infile[\n",
    "                                    dset_path_key + \"/\" + output_mode + \"/seg\"\n",
    "                                ][:]\n",
    "                                outfile[dset_path_key + \"/\" + output_mode + \"/seg\"][\n",
    "                                    current_idx : current_idx + num_indices\n",
    "                                ] = seg_arr\n",
    "                                for i, W0 in enumerate(self.W0_list):\n",
    "                                    for j, Wsigma in enumerate(self.Wsigma_list):\n",
    "                                        weight_arr = infile[\n",
    "                                            dset_path_key\n",
    "                                            + \"/\"\n",
    "                                            + output_mode\n",
    "                                            + \"/W0=\"\n",
    "                                            + str(W0)\n",
    "                                            + \"_Wsigma=\"\n",
    "                                            + str(Wsigma)\n",
    "                                            + \"/weight\"\n",
    "                                        ][:]\n",
    "                                        outfile[\n",
    "                                            dset_path_key\n",
    "                                            + \"/\"\n",
    "                                            + output_mode\n",
    "                                            + \"/W0=\"\n",
    "                                            + str(W0)\n",
    "                                            + \"_Wsigma=\"\n",
    "                                            + str(Wsigma)\n",
    "                                            + \"/weight\"\n",
    "                                        ][\n",
    "                                            current_idx : current_idx + num_indices\n",
    "                                        ] = weight_arr\n",
    "\n",
    "                            elif output_mode == \"multiclass\":\n",
    "                                seg_arr = infile[\n",
    "                                    dset_path_key + \"/\" + output_mode + \"/seg\"\n",
    "                                ][:]\n",
    "                                outfile[dset_path_key + \"/\" + output_mode + \"/seg\"][\n",
    "                                    current_idx : current_idx + num_indices\n",
    "                                ] = seg_arr\n",
    "                                weight_arr = infile[\n",
    "                                    dset_path_key + \"/\" + output_mode + \"/weight\"\n",
    "                                ][:]\n",
    "                                outfile[dset_path_key + \"/\" + output_mode + \"/weight\"][\n",
    "                                    current_idx : current_idx + num_indices\n",
    "                                ] = weight_arr\n",
    "\n",
    "                            elif output_mode == \"cellpose\":\n",
    "                                seg_arr = infile[\n",
    "                                    dset_path_key + \"/\" + output_mode + \"/seg\"\n",
    "                                ][:]\n",
    "                                outfile[dset_path_key + \"/\" + output_mode + \"/seg\"][\n",
    "                                    current_idx : current_idx + num_indices\n",
    "                                ] = seg_arr\n",
    "                                y_grad_arr = infile[\n",
    "                                    dset_path_key + \"/\" + output_mode + \"/y_grad\"\n",
    "                                ][:]\n",
    "                                outfile[dset_path_key + \"/\" + output_mode + \"/y_grad\"][\n",
    "                                    current_idx : current_idx + num_indices\n",
    "                                ] = y_grad_arr\n",
    "                                x_grad_arr = infile[\n",
    "                                    dset_path_key + \"/\" + output_mode + \"/x_grad\"\n",
    "                                ][:]\n",
    "                                outfile[dset_path_key + \"/\" + output_mode + \"/x_grad\"][\n",
    "                                    current_idx : current_idx + num_indices\n",
    "                                ] = x_grad_arr\n",
    "\n",
    "                            else:\n",
    "                                raise\n",
    "\n",
    "                    current_idx += num_indices\n",
    "\n",
    "            os.remove(nndatapath)\n",
    "\n",
    "    def export_data(self, dask_controller, chunk_size=250):\n",
    "\n",
    "        dask_controller.futures = {}\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        all_output_dfs = {}\n",
    "\n",
    "        for output_name, _ in self.import_param_dict.items():\n",
    "            output_df = []\n",
    "\n",
    "            for input_path, param_dict in self.import_param_dict[output_name].items():\n",
    "\n",
    "                kymodf = dd.read_parquet(input_path + \"/kymograph/metadata\").persist()\n",
    "\n",
    "                num_samples = param_dict[\"Maximum Samples per Dataset:\"]\n",
    "                feature_channel = param_dict[\"Feature Channel:\"]\n",
    "                t_range = param_dict[\"Timepoint Range:\"]\n",
    "\n",
    "                kymodf[\"filepath\"] = input_path\n",
    "                kymodf = kymodf.reset_index()\n",
    "                timedf = kymodf.set_index(\"timepoints\").persist()\n",
    "                timedf = timedf.loc[t_range[0] : t_range[1]].persist()\n",
    "                frac = num_samples / len(timedf)\n",
    "\n",
    "                timedf_subset = timedf.sample(frac=frac).persist()\n",
    "                timedf_subset = timedf_subset.reset_index()\n",
    "                filedf_subset = timedf_subset.set_index(\"filepath\").persist()\n",
    "                output_df.append(filedf_subset.compute()[:num_samples])\n",
    "            output_df = pd.concat(output_df)\n",
    "            output_meta_handle.write_df(output_name, output_df)\n",
    "            all_output_dfs[output_name] = output_df\n",
    "\n",
    "        for output_name in all_output_dfs.keys():\n",
    "\n",
    "            output_df = all_output_dfs[output_name]\n",
    "\n",
    "            ## split into equal computation chunks here\n",
    "\n",
    "            chunk_idx_list = []\n",
    "            for chunk_idx, init_idx in enumerate(range(0, len(output_df), chunk_size)):\n",
    "                future = dask_controller.daskclient.submit(\n",
    "                    self.export_chunk,\n",
    "                    output_name,\n",
    "                    init_idx,\n",
    "                    chunk_size,\n",
    "                    chunk_idx,\n",
    "                    retries=1,\n",
    "                )\n",
    "                dask_controller.futures[\"Chunk Number: \" + str(chunk_idx)] = future\n",
    "                chunk_idx_list.append(chunk_idx)\n",
    "\n",
    "            init_idx_list = dask_controller.daskclient.gather(\n",
    "                [\n",
    "                    dask_controller.futures[\"Chunk Number: \" + str(chunk_idx)]\n",
    "                    for chunk_idx in chunk_idx_list\n",
    "                ]\n",
    "            )\n",
    "            self.gather_chunks(output_name, init_idx_list, chunk_idx_list, chunk_size)\n",
    "\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "\n",
    "        for output_name in self.import_param_dict.keys():\n",
    "            for input_path in self.import_param_dict[output_name].keys():\n",
    "                input_meta_handle = pandas_hdf5_handler(input_path + \"/metadata.hdf5\")\n",
    "                indf = input_meta_handle.read_df(\"global\", read_metadata=True)\n",
    "                global_meta = indf.metadata\n",
    "                del indf\n",
    "                self.import_param_dict[output_name][input_path][\"global\"] = global_meta\n",
    "\n",
    "                kymo_meta_path = input_path + \"/kymograph/metadata.pkl\"\n",
    "                with open(kymo_meta_path, \"rb\") as infile:\n",
    "                    kymo_meta = pkl.load(infile)\n",
    "                self.import_param_dict[output_name][input_path][\"kymograph\"] = kymo_meta\n",
    "\n",
    "                segparampath = input_path + \"/fluorescent_segmentation.par\"\n",
    "                with open(segparampath, \"rb\") as infile:\n",
    "                    seg_param_dict = pkl.load(infile)\n",
    "                self.import_param_dict[output_name][input_path][\n",
    "                    \"segmentation\"\n",
    "                ] = seg_param_dict\n",
    "\n",
    "            output_metadata = {\n",
    "                \"nndataset\": {\n",
    "                    \"experimentname\": self.experimentname,\n",
    "                    \"output_names\": self.output_names,\n",
    "                    \"output_modes\": self.output_modes,\n",
    "                    \"input_paths\": self.input_paths,\n",
    "                    \"W0_list\": self.W0_list,\n",
    "                    \"Wsigma_list\": self.Wsigma_list,\n",
    "                }\n",
    "            }\n",
    "            output_metadata = {**output_metadata, **self.import_param_dict[output_name]}\n",
    "\n",
    "            output_meta_handle.write_df(\n",
    "                output_name, all_output_dfs[output_name], metadata=output_metadata\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_meta_handle = pandas_hdf5_handler(dataloader.metapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.DataFrame([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = output_meta_handle.read_df(\"train\", read_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.index.get_level_values(0).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = output_meta_handle.read_df(\"train\", read_metadata=True).metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta[\"/n/scratch2/de64/2019-05-31_validation_data\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = UNet_Training_DataLoader(\n",
    "    nndatapath=\"/n/scratch2/de64/nntest7\",\n",
    "    experimentname=\"First NN\",\n",
    "    input_paths=[\"/n/scratch2/de64/2019-05-31_validation_data\"],\n",
    "    output_modes=[\"class\", \"multiclass\", \"cellpose\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inter_get_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.get_import_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.export_data(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.futures['Chunk Number: 40']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=\"/n/scratch2/de64/nntest7\" + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, filepath, mode=\"run\", chunksize=1000, W0=5.0, Wsigma=2.0):\n",
    "        self.filepath = filepath\n",
    "        self.mode = mode\n",
    "        self.chunksize = chunksize\n",
    "        self.W0 = W0\n",
    "        self.Wsigma = Wsigma\n",
    "        self.dset_shapes = {}\n",
    "        with h5py.File(self.filepath, \"r\") as infile:\n",
    "            for dset_name in infile.keys():\n",
    "                shape = infile[dset_name + \"/img\"].shape\n",
    "                self.dset_shapes[dset_name] = shape\n",
    "        index_ranges = [0] + np.add.accumulate(\n",
    "            [value[0] for value in self.dset_shapes.values()]\n",
    "        ).tolist()\n",
    "        self.chunk_index_ranges = self.fill_index_gaps(index_ranges)\n",
    "        start_chunks = (\n",
    "            np.array(\n",
    "                [0]\n",
    "                + np.add.accumulate(\n",
    "                    [value[0] for value in self.dset_shapes.values()]\n",
    "                ).tolist()\n",
    "            )\n",
    "            // chunksize\n",
    "        ).tolist()\n",
    "        self.chunk_ranges = [\n",
    "            range(start_chunks[i], start_chunks[i + 1])\n",
    "            for i in range(len(start_chunks) - 1)\n",
    "        ]\n",
    "\n",
    "        self.chunk_dsets = {}\n",
    "        for i, item in enumerate(self.dset_shapes.keys()):\n",
    "            chunk_range = self.chunk_ranges[i]\n",
    "            for chunk in chunk_range:\n",
    "                self.chunk_dsets[chunk] = item\n",
    "\n",
    "        self.current_chunk = 0\n",
    "        self.load_chunk(self.current_chunk)\n",
    "\n",
    "        self.size = 0\n",
    "        with h5py.File(self.filepath, \"r\") as infile:\n",
    "            for dset_name in infile.keys():\n",
    "                self.size += np.prod(infile[dset_name + \"/img\"].shape)\n",
    "\n",
    "    def load_chunk(self, chunk_idx):\n",
    "        self.current_dset = self.chunk_dsets[self.current_chunk]\n",
    "\n",
    "        with h5py.File(self.filepath, \"r\") as infile:\n",
    "            self.img_data = infile[self.current_dset + \"/img\"][\n",
    "                chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "            ]\n",
    "            if self.mode == \"run\":\n",
    "                pass\n",
    "            elif self.mode == \"class\":\n",
    "                self.gt_data = infile[self.current_dset + \"/seg\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.seg_data = infile[self.current_dset + \"/class/seg\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.weight_data = infile[\n",
    "                    self.current_dset\n",
    "                    + \"/class/W0=\"\n",
    "                    + str(self.W0)\n",
    "                    + \"_Wsigma=\"\n",
    "                    + str(self.Wsigma)\n",
    "                    + \"/weight\"\n",
    "                ][chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize]\n",
    "            elif self.mode == \"multiclass\":\n",
    "                self.gt_data = infile[self.current_dset + \"/seg\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.seg_data = infile[self.current_dset + \"/multiclass/seg\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.weight_data = infile[self.current_dset + \"/multiclass/weight\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "            elif self.mode == \"cellpose\":\n",
    "                self.gt_data = infile[self.current_dset + \"/seg\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.seg_data = infile[self.current_dset + \"/cellpose/seg\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.y_grad_data = infile[self.current_dset + \"/cellpose/y_grad\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "                self.x_grad_data = infile[self.current_dset + \"/cellpose/x_grad\"][\n",
    "                    chunk_idx * self.chunksize : (chunk_idx + 1) * self.chunksize\n",
    "                ]\n",
    "            else:\n",
    "                raise\n",
    "        self.current_chunk = chunk_idx\n",
    "\n",
    "    def fill_index_gaps(self, index_ranges):\n",
    "        chunk_index_ranges = [index_ranges[0]]\n",
    "        for i in range(len(index_ranges) - 1):\n",
    "            last_index = index_ranges[i]\n",
    "            not_gap_filled = True\n",
    "            while not_gap_filled:\n",
    "                del_index = index_ranges[i + 1] - last_index\n",
    "                if del_index > self.chunksize:\n",
    "                    chunk_index_ranges.append(last_index + self.chunksize)\n",
    "                    last_index = last_index + self.chunksize\n",
    "                else:\n",
    "                    chunk_index_ranges.append(index_ranges[i + 1])\n",
    "                    not_gap_filled = False\n",
    "        chunk_index_ranges = [\n",
    "            range(chunk_index_ranges[i], chunk_index_ranges[i + 1])\n",
    "            for i in range(len(chunk_index_ranges) - 1)\n",
    "        ]\n",
    "        return chunk_index_ranges\n",
    "\n",
    "    def __len__(self):\n",
    "        out_len = 0\n",
    "        with h5py.File(self.filepath, \"r\") as infile:\n",
    "            for dset_name in infile.keys():\n",
    "                out_len += infile[dset_name + \"/img\"].shape[0]\n",
    "        return out_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_chunk = [\n",
    "            i for i, interval in enumerate(self.chunk_index_ranges) if idx in interval\n",
    "        ][0]\n",
    "        subidx = idx % self.chunksize\n",
    "        if idx_chunk != self.current_chunk:\n",
    "            self.load_chunk(idx_chunk)\n",
    "\n",
    "        sample = {\"img\": self.img_data[subidx]}\n",
    "        if self.mode == \"run\":\n",
    "            pass\n",
    "        elif self.mode == \"class\" or self.mode == \"multiclass\":\n",
    "            sample[\"gt\"] = self.gt_data[subidx]\n",
    "            sample[\"seg\"] = self.seg_data[subidx]\n",
    "            sample[\"weight\"] = self.weight_data[subidx]\n",
    "        elif self.mode == \"cellpose\":\n",
    "            sample[\"gt\"] = self.gt_data[subidx]\n",
    "            sample[\"seg\"] = self.seg_data[subidx]\n",
    "            sample[\"y_grad\"] = self.y_grad_data[subidx]\n",
    "            sample[\"x_grad\"] = self.x_grad_data[subidx]\n",
    "        else:\n",
    "            raise\n",
    "        return sample\n",
    "\n",
    "\n",
    "def numpy_collate(batch):  # modified version of torch default\n",
    "    r\"\"\"Puts each data field into a numpy array with outer dimension batch size\"\"\"\n",
    "\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, np.ndarray):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return np.stack(batch, 0, out=out)\n",
    "    elif (\n",
    "        elem_type.__module__ == \"numpy\"\n",
    "        and elem_type.__name__ != \"str_\"\n",
    "        and elem_type.__name__ != \"string_\"\n",
    "    ):\n",
    "        elem = batch[0]\n",
    "        if elem_type.__name__ == \"ndarray\":\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise\n",
    "\n",
    "            return numpy_collate([np.array(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return np.array(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return np.array(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int_classes):\n",
    "        return np.array(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, container_abcs.Mapping):\n",
    "        return {key: numpy_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, \"_fields\"):  # namedtuple\n",
    "        return elem_type(*(numpy_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, container_abcs.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError(\"each element in list of batch should be of equal size\")\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moo = SegmentationDataset(\n",
    "    \"/n/scratch2/de64/nntest7/train.hdf5\", mode=\"cellpose\", chunksize=47\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(moo[50][\"seg\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        iaa.PadToFixedSize(width=50, height=300),\n",
    "        iaa.CropToFixedSize(width=50, height=300),\n",
    "        iaa.Crop(\n",
    "            percent=(0, 0.1)\n",
    "        ),  # crop images from each side by 0 to 16px (randomly chosen)\n",
    "        iaa.Fliplr(0.5),  # vertically flip 50% of the images\n",
    "        iaa.Flipud(0.5),  # horizontally flip 50% of the images\n",
    "        # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "        # But we only blur about 50% of all images.\n",
    "        iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),\n",
    "        # Strengthen or weaken the contrast in each image.\n",
    "        iaa.LinearContrast(alpha=(0.8, 1.2)),\n",
    "        # Add gaussian noise.\n",
    "        # For 50% of all images, we sample the noise once per pixel.\n",
    "        # For the other 50% of all images, we sample the noise per pixel AND\n",
    "        # channel. This can change the color (not only brightness) of the\n",
    "        # pixels.\n",
    "        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255)),\n",
    "        # Make some images brighter and some darker.\n",
    "        # In 20% of all cases, we sample the multiplier once per channel,\n",
    "        # which can end up changing the color of the images.\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "        iaa.Affine(\n",
    "            scale={\"x\": (0.7, 1.3), \"y\": (0.7, 1.3)},\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)},\n",
    "            rotate=(-15, 15),\n",
    "            shear={\"x\": (-10, 10), \"y\": (-5, 5)},\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(moo, batch_size=100, shuffle=False, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in data:\n",
    "    img = b[\"img\"]\n",
    "    seg = b[\"seg\"]\n",
    "    y_grad = b[\"y_grad\"]\n",
    "    x_grad = b[\"x_grad\"]\n",
    "    #     weight = b[\"weight\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = iaa.PadToFixedSize(width=50, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = np.stack(pad.augment_images(img[:, 0, :, :]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_arr = np.stack([y_grad[1, 0], x_grad[1, 0]], axis=2)\n",
    "grad_map = HeatmapsOnImage(grad_arr, shape=img[1, 0].shape, min_value=-1, max_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.aug_seq.augment_heatmaps(grad_map).get_arr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grad_arr[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = (\n",
    "    iaa.PadToFixedSize(\n",
    "        width=pad_width,\n",
    "        height=pad_height,\n",
    "    ),\n",
    ")\n",
    "iaa.CropToFixedSize(width=pad_width, height=pad_height),\n",
    "iaa.Crop(\n",
    "    percent=(0.0, crop_perc)\n",
    "),  # crop images from each side by 0 to 16px (randomly chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg[15, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg[15, 0])\n",
    "get_class_labels(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_grad[5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_grad[5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "\n",
    "\n",
    "def get_cellpose_labels(fx, step_size=1.0, n_iter=10):  # N, (y,x,mask), y, x\n",
    "    labeled = np.zeros((fx.shape[0], fx.shape[2], fx.shape[3]), dtype=\"uint8\")\n",
    "    for n in range(fx.shape[0]):\n",
    "        y_grad_arr = fx[n, 0]\n",
    "        x_grad_arr = fx[n, 1]\n",
    "        mask = fx[n, 2] > 0.5\n",
    "        pixel_arr = np.array(np.where(mask)).T\n",
    "        final_pixels = []\n",
    "        for pixel_idx in range(pixel_arr.shape[0]):\n",
    "            pixel_coord = pixel_arr[pixel_idx].astype(np.float32)\n",
    "            for N in range(n_iter):\n",
    "                near_coord = pixel_coord.astype(int)\n",
    "                y_grad = y_grad_arr[near_coord[0], near_coord[1]]\n",
    "                x_grad = x_grad_arr[near_coord[0], near_coord[1]]\n",
    "                pixel_coord += np.array([y_grad, x_grad]) * step_size\n",
    "            final_pixels.append(pixel_coord)\n",
    "\n",
    "        if len(final_pixels) > 0:\n",
    "            dbsc = skl.cluster.DBSCAN(eps=2.0)\n",
    "            cluster_assign = dbsc.fit_predict(final_pixels)\n",
    "\n",
    "            labeled[n, pixel_arr[:, 0], pixel_arr[:, 1]] = cluster_assign + 1\n",
    "    return labeled\n",
    "\n",
    "\n",
    "def get_class_labels(segmentation, mask_label_dim=1):\n",
    "    segmentation = np.argmax(segmentation, axis=1)\n",
    "    mask = segmentation == mask_label_dim\n",
    "    labeled = np.zeros((segmentation.shape), dtype=\"uint8\")\n",
    "    for n in range(segmentation.shape[0]):\n",
    "        labeled[n] = sk.measure.label(mask[n])\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(0.55).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f get_cellpose_labels get_cellpose_labels(seg[61,0],y_grad[61,0],x_grad[61,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = get_cellpose_labels(seg[40, 0], y_grad[40, 0], x_grad[40, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    \"\"\"(Conv => BatchNorm =>ReLU) twice.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.downconv = nn.Sequential(nn.MaxPool2d(2), double_conv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        # for padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels,\n",
    "        n_classes,\n",
    "        layers=3,\n",
    "        hidden_size=64,\n",
    "        dropout=0.0,\n",
    "        withsoftmax=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.inc = inconv(n_channels, hidden_size)\n",
    "        self.downlist = nn.ModuleList(\n",
    "            [\n",
    "                down(hidden_size * (2**i), hidden_size * (2 ** (i + 1)))\n",
    "                for i in range(0, layers - 1)\n",
    "            ]\n",
    "            + [\n",
    "                down(\n",
    "                    hidden_size * (2 ** (layers - 1)), hidden_size * (2 ** (layers - 1))\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self.uplist = nn.ModuleList(\n",
    "            [\n",
    "                up(hidden_size * (2**i), hidden_size * (2 ** (i - 2)))\n",
    "                for i in reversed(range(2, layers + 1))\n",
    "            ]\n",
    "            + [up(hidden_size * 2, hidden_size)]\n",
    "        )\n",
    "        self.outc = outconv(hidden_size, n_classes)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.withsoftmax = withsoftmax\n",
    "\n",
    "    def uniforminit(self):\n",
    "        for param in self.named_parameters():\n",
    "            param[1].data.uniform_(-0.05, 0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xlist = [self.inc(x)]\n",
    "        for item in self.downlist:\n",
    "            xlist.append(item(xlist[-1]))\n",
    "        x = xlist[-1]\n",
    "        x = self.drop(x)\n",
    "        for i, item in enumerate(self.uplist):\n",
    "            x = item(x, xlist[-(i + 2)])\n",
    "        x = self.outc(x)\n",
    "        if self.withsoftmax:\n",
    "            x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nndatapath,\n",
    "        model_number,\n",
    "        mode,\n",
    "        numepochs=10,\n",
    "        batch_size=100,\n",
    "        layers=3,\n",
    "        hidden_size=64,\n",
    "        lr=0.005,\n",
    "        momentum=0.95,\n",
    "        weight_decay=0.0005,\n",
    "        dropout=0.0,\n",
    "        W0=5.0,\n",
    "        Wsigma=2.0,\n",
    "        gpuon=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.nndatapath = nndatapath\n",
    "        self.model_number = model_number\n",
    "        self.mode = mode\n",
    "        self.aug_seq = self.define_aug_seq(**kwargs)\n",
    "\n",
    "        self.numepochs = numepochs\n",
    "        self.batch_size = batch_size\n",
    "        self.gpuon = gpuon\n",
    "\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.W0 = W0\n",
    "        self.Wsigma = Wsigma\n",
    "\n",
    "        if self.mode == \"class\":\n",
    "            self.model = UNet(\n",
    "                1,\n",
    "                2,\n",
    "                layers=layers,\n",
    "                hidden_size=hidden_size,\n",
    "                dropout=dropout,\n",
    "                withsoftmax=True,\n",
    "            )\n",
    "        elif self.mode == \"multiclass\":\n",
    "            self.model = UNet(\n",
    "                1,\n",
    "                3,\n",
    "                layers=layers,\n",
    "                hidden_size=hidden_size,\n",
    "                dropout=dropout,\n",
    "                withsoftmax=True,\n",
    "            )\n",
    "        elif self.mode == \"cellpose\":\n",
    "            self.model = UNet(\n",
    "                1,\n",
    "                3,\n",
    "                layers=layers,\n",
    "                hidden_size=hidden_size,\n",
    "                dropout=dropout,\n",
    "                withsoftmax=False,\n",
    "            )\n",
    "\n",
    "        self.model.uniforminit()\n",
    "        if gpuon:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=self.momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "    def define_aug_seq(\n",
    "        self,\n",
    "        pad_width=50,\n",
    "        pad_height=300,\n",
    "        crop_perc=0.1,\n",
    "        flip_perc=0.5,\n",
    "        blur_freq=0.5,\n",
    "        blur_sigma=0.5,\n",
    "        contrast_range=(0.8, 1.2),\n",
    "        noise=0.05,\n",
    "        mult_freq=0.2,\n",
    "        mult_range=(0.8, 1.2),\n",
    "        scale_range=(0.7, 1.3),\n",
    "        translate_range=(-0.15, 0.15),\n",
    "        rotate_range=(-15, 15),\n",
    "        x_shear_range=(-10, 10),\n",
    "        y_shear_range=(-5, 5),\n",
    "    ):\n",
    "        seq = iaa.Sequential(\n",
    "            [\n",
    "                iaa.PadToFixedSize(width=pad_width, height=pad_height),\n",
    "                iaa.CropToFixedSize(width=pad_width, height=pad_height),\n",
    "                iaa.Crop(\n",
    "                    percent=(0.0, crop_perc)\n",
    "                ),  # crop images from each side by 0 to 16px (randomly chosen)\n",
    "                iaa.Fliplr(flip_perc),  # vertically flip 50% of the images\n",
    "                iaa.Flipud(flip_perc),  # horizontally flip 50% of the images\n",
    "                # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                # But we only blur about 50% of all images.\n",
    "                iaa.Sometimes(blur_freq, iaa.GaussianBlur(sigma=(0, blur_sigma))),\n",
    "                # Strengthen or weaken the contrast in each image.\n",
    "                iaa.LinearContrast(alpha=contrast_range),\n",
    "                # Add gaussian noise.\n",
    "                # For 50% of all images, we sample the noise once per pixel.\n",
    "                # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                # channel. This can change the color (not only brightness) of the\n",
    "                # pixels.\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, noise * 255)),\n",
    "                # Make some images brighter and some darker.\n",
    "                # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                # which can end up changing the color of the images.\n",
    "                iaa.Multiply(mult_range, per_channel=mult_freq),\n",
    "                iaa.Affine(\n",
    "                    scale={\"x\": scale_range, \"y\": scale_range},\n",
    "                    translate_percent={\"x\": translate_range, \"y\": translate_range},\n",
    "                    rotate=rotate_range,\n",
    "                    shear={\"x\": x_shear_range, \"y\": y_shear_range},\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return seq\n",
    "\n",
    "    def removefile(self, path):\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "\n",
    "    def load_model(self, paramspath):\n",
    "        if self.gpuon:\n",
    "            device = torch.device(\"cuda\")\n",
    "            self.model.load_state_dict(torch.load(paramspath))\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            self.model.load_state_dict(torch.load(paramspath, map_location=device))\n",
    "\n",
    "    def class_aug(self, img_arr, seg_arr, weight_arr):  # N,1,y,x\n",
    "        in_bounds_arr = np.ones(img_arr.shape[2:], dtype=\"uint8\")\n",
    "\n",
    "        img_aug_arr = []\n",
    "        seg_aug_arr = []\n",
    "        weight_aug_arr = []\n",
    "        for n in range(img_arr.shape[0]):\n",
    "            segmap = SegmentationMapsOnImage(seg_arr[n, 0], shape=img_arr[n, 0].shape)\n",
    "            heatmap = HeatmapsOnImage(\n",
    "                weight_arr[n, 0],\n",
    "                shape=img_arr[n, 0].shape,\n",
    "                min_value=min(0.0, np.min(weight_arr[n, 0])),\n",
    "                max_value=max(1.0, np.max(weight_arr[n, 0])),\n",
    "            )\n",
    "            img_aug, seg_aug, weight_aug = self.aug_seq(\n",
    "                image=img_arr[n, 0], segmentation_maps=segmap, heatmaps=heatmap\n",
    "            )\n",
    "\n",
    "            img_aug_arr.append(img_aug)\n",
    "            seg_aug_arr.append(seg_aug.get_arr())\n",
    "            weight_aug_arr.append(weight_aug.get_arr()[:, :, 0])\n",
    "\n",
    "        img_aug_arr, seg_aug_arr, weight_aug_arr = (\n",
    "            np.stack(img_aug_arr, axis=0),\n",
    "            np.stack(seg_aug_arr, axis=0),\n",
    "            np.stack(weight_aug_arr, axis=0),\n",
    "        )\n",
    "        img_aug_arr, seg_aug_arr, weight_aug_arr = (\n",
    "            img_aug_arr[:, np.newaxis],\n",
    "            seg_aug_arr[:, np.newaxis],\n",
    "            weight_aug_arr[:, np.newaxis],\n",
    "        )\n",
    "\n",
    "        return img_aug_arr, seg_aug_arr, weight_aug_arr\n",
    "\n",
    "    def cellpose_aug(self, img_arr, seg_arr, y_grad_arr, x_grad_arr):\n",
    "        in_bounds_arr = np.ones(img_arr.shape[2:], dtype=bool)\n",
    "        in_bounds_arr = sk.morphology.binary_erosion(in_bounds_arr).astype(\"uint8\")\n",
    "\n",
    "        img_aug_arr = []\n",
    "        seg_aug_arr = []\n",
    "        y_grad_aug_arr = []\n",
    "        x_grad_aug_arr = []\n",
    "        for n in range(img_arr.shape[0]):\n",
    "            segmap = SegmentationMapsOnImage(\n",
    "                np.stack([seg_arr[n, 0], in_bounds_arr], axis=2),\n",
    "                shape=(img_arr.shape[2], img_arr.shape[3], 2),\n",
    "            )\n",
    "            #             segmap = SegmentationMapsOnImage(seg_arr[n,0], shape=img_arr[n,0].shape)\n",
    "            grad_arr = np.stack([y_grad_arr[n, 0], x_grad_arr[n, 0]], axis=2)\n",
    "            grad_map = HeatmapsOnImage(\n",
    "                grad_arr, shape=img_arr[n, 0].shape, min_value=-1, max_value=1\n",
    "            )\n",
    "            img_aug, seg_aug, grad_aug = self.aug_seq(\n",
    "                image=img_arr[n, 0], segmentation_maps=segmap, heatmaps=grad_map\n",
    "            )\n",
    "\n",
    "            img_aug_arr.append(img_aug)\n",
    "\n",
    "            seg_aug = seg_aug.get_arr()\n",
    "            grad_aug = grad_aug.get_arr()\n",
    "            seg_out, in_bounds, y_grad_out, x_grad_out = (\n",
    "                seg_aug[:, :, 0],\n",
    "                seg_aug[:, :, 1],\n",
    "                grad_aug[:, :, 0],\n",
    "                grad_aug[:, :, 1],\n",
    "            )\n",
    "\n",
    "            seg_out[in_bounds != 1] = 0\n",
    "            y_grad_out[in_bounds != 1] = 0.0\n",
    "            x_grad_out[in_bounds != 1] = 0.0\n",
    "\n",
    "            seg_aug_arr.append(seg_out)\n",
    "            y_grad_aug_arr.append(y_grad_out)\n",
    "            x_grad_aug_arr.append(x_grad_out)\n",
    "\n",
    "        img_aug_arr, seg_aug_arr, y_grad_aug_arr, x_grad_aug_arr = (\n",
    "            np.stack(img_aug_arr, axis=0),\n",
    "            np.stack(seg_aug_arr, axis=0),\n",
    "            np.stack(y_grad_aug_arr, axis=0),\n",
    "            np.stack(x_grad_aug_arr, axis=0),\n",
    "        )\n",
    "        img_aug_arr, seg_aug_arr, y_grad_aug_arr, x_grad_aug_arr = (\n",
    "            img_aug_arr[:, np.newaxis],\n",
    "            seg_aug_arr[:, np.newaxis],\n",
    "            y_grad_aug_arr[:, np.newaxis],\n",
    "            x_grad_aug_arr[:, np.newaxis],\n",
    "        )\n",
    "\n",
    "        return img_aug_arr, seg_aug_arr, y_grad_aug_arr, x_grad_aug_arr\n",
    "\n",
    "    def train(self, x, y, weightmaps):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        fx = self.model.forward(x)\n",
    "        labeled = get_class_labels(fx.detach().cpu().numpy()[0])\n",
    "        fx = torch.log(fx)\n",
    "\n",
    "        loss = F.nll_loss(fx, y, reduction=\"none\") * weightmaps\n",
    "        mean_loss = torch.mean(loss)\n",
    "        mean_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "    def test(self, x, y, weightmaps):\n",
    "        fx = self.model.forward(x)\n",
    "        fx = torch.log(fx)\n",
    "        loss = F.nll_loss(fx, y, reduction=\"none\") * weightmaps\n",
    "        loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "    def cellpose_train(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        fx = self.model.forward(x)\n",
    "        mask_pred = F.sigmoid(fx[:, 2])\n",
    "\n",
    "        mse = F.mse_loss(fx[:, :2], y[:, :2], reduction=\"none\")  ## N,* to N,*\n",
    "        cross_entropy = F.binary_cross_entropy(mask_pred, y[:, 2], reduction=\"none\")\n",
    "\n",
    "        loss = cross_entropy + 5.0 * mse[:, 0] + 5.0 * mse[:, 1]\n",
    "\n",
    "        mean_loss = torch.mean(loss)\n",
    "        mean_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "    def cellpose_test(self, x, y):\n",
    "        fx = self.model.forward(x)\n",
    "        mse = F.mse_loss(fx[:, :2], y[:, :2], reduction=\"none\")  ## N,* to N,*\n",
    "        mask_pred = F.sigmoid(fx[:, 2])\n",
    "        cross_entropy = F.binary_cross_entropy(mask_pred, y[:, 2], reduction=\"none\")\n",
    "        loss = cross_entropy + 5.0 * mse[:, 0] + 5.0 * mse[:, 1]\n",
    "\n",
    "        loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "    def perepoch(\n",
    "        self,\n",
    "        e,\n",
    "        train_iter,\n",
    "        test_iter,\n",
    "        val_iter,\n",
    "        train_data_size,\n",
    "        test_data_size,\n",
    "        val_data_size,\n",
    "    ):\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        print(\"=======epoch \" + str(e) + \"=======\")\n",
    "        self.model.train()\n",
    "        total_train_loss = 0.0\n",
    "        num_train_batches = len(train_iter)\n",
    "        for i, b in enumerate(train_iter):\n",
    "            if self.mode == \"class\" or self.mode == \"multiclass\":\n",
    "                img_arr, seg_arr, weight_arr = (b[\"img\"], b[\"seg\"], b[\"weight\"])\n",
    "                img_arr, seg_arr, weight_arr = self.class_aug(\n",
    "                    img_arr, seg_arr, weight_arr\n",
    "                )\n",
    "\n",
    "                seg_arr, weight_arr = seg_arr[:, 0], weight_arr[:, 0]\n",
    "                x = torch.Tensor(img_arr.astype(float))\n",
    "                y = torch.LongTensor(seg_arr)\n",
    "                weight_arr = torch.Tensor(weight_arr)\n",
    "                if self.gpuon:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                    weight_arr = weight_arr.cuda()\n",
    "                loss = self.train(x, y, weight_arr)\n",
    "                total_train_loss += loss.detach().cpu().numpy()\n",
    "                del weight_arr\n",
    "\n",
    "            elif self.mode == \"cellpose\":\n",
    "                img_arr, seg_arr, y_grad_arr, x_grad_arr = (\n",
    "                    b[\"img\"],\n",
    "                    b[\"seg\"],\n",
    "                    b[\"y_grad\"],\n",
    "                    b[\"x_grad\"],\n",
    "                )\n",
    "                img_arr, seg_arr, y_grad_arr, x_grad_arr = self.cellpose_aug(\n",
    "                    img_arr, seg_arr, y_grad_arr, x_grad_arr\n",
    "                )\n",
    "\n",
    "                y_grad_arr, x_grad_arr, seg_arr = (\n",
    "                    y_grad_arr[:, 0],\n",
    "                    x_grad_arr[:, 0],\n",
    "                    seg_arr[:, 0],\n",
    "                )\n",
    "                x = torch.Tensor(img_arr.astype(float))\n",
    "                y = np.stack([y_grad_arr, x_grad_arr, seg_arr], axis=1)\n",
    "                y = torch.Tensor(y)\n",
    "\n",
    "                if self.gpuon:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                loss = self.cellpose_train(x, y)\n",
    "                total_train_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            del x\n",
    "            del y\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        avgtrainnll = total_train_loss / train_data_size\n",
    "        print(\"Mean Train NLL: \" + str(avgtrainnll))\n",
    "        self.model.eval()\n",
    "\n",
    "        total_val_loss = 0.0\n",
    "        for i, b in enumerate(val_iter):\n",
    "            if self.mode == \"class\" or self.mode == \"multiclass\":\n",
    "                img_arr, seg_arr, weight_arr = (b[\"img\"], b[\"seg\"], b[\"weight\"])\n",
    "                seg_arr, weight_arr = seg_arr[:, 0], weight_arr[:, 0]\n",
    "                x = torch.Tensor(img_arr.astype(float))\n",
    "                y = torch.LongTensor(seg_arr)\n",
    "                weight_arr = torch.Tensor(weight_arr)\n",
    "                if self.gpuon:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                    weight_arr = weight_arr.cuda()\n",
    "                loss = self.test(x, y, weight_arr)\n",
    "                total_val_loss += loss.detach().cpu().numpy()\n",
    "                del weight_arr\n",
    "\n",
    "            elif self.mode == \"cellpose\":\n",
    "                img_arr, seg_arr, y_grad_arr, x_grad_arr = (\n",
    "                    b[\"img\"],\n",
    "                    b[\"seg\"],\n",
    "                    b[\"y_grad\"],\n",
    "                    b[\"x_grad\"],\n",
    "                )\n",
    "                y_grad_arr, x_grad_arr, seg_arr = (\n",
    "                    y_grad_arr[:, 0],\n",
    "                    x_grad_arr[:, 0],\n",
    "                    seg_arr[:, 0],\n",
    "                )\n",
    "                x = torch.Tensor(img_arr.astype(float))\n",
    "                y = np.stack([y_grad_arr, x_grad_arr, seg_arr], axis=1)\n",
    "                y = torch.Tensor(y)\n",
    "                if self.gpuon:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                loss = self.cellpose_test(x, y)\n",
    "                total_val_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            del x\n",
    "            del y\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        avgvalnll = total_val_loss / val_data_size\n",
    "        print(\"Mean Val NLL: \" + str(avgvalnll))\n",
    "\n",
    "        total_test_loss = 0.0\n",
    "        for i, b in enumerate(test_iter):\n",
    "            if self.mode == \"class\" or self.mode == \"multiclass\":\n",
    "                img_arr, seg_arr, weight_arr = (b[\"img\"], b[\"seg\"], b[\"weight\"])\n",
    "                seg_arr, weight_arr = seg_arr[:, 0], weight_arr[:, 0]\n",
    "                x = torch.Tensor(img_arr.astype(float))\n",
    "                y = torch.LongTensor(seg_arr)\n",
    "                weight_arr = torch.Tensor(weight_arr)\n",
    "                if self.gpuon:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                    weight_arr = weight_arr.cuda()\n",
    "                loss = self.test(x, y, weight_arr)\n",
    "                total_test_loss += loss.detach().cpu().numpy()\n",
    "                del weight_arr\n",
    "\n",
    "            elif self.mode == \"cellpose\":\n",
    "                img_arr, seg_arr, y_grad_arr, x_grad_arr = (\n",
    "                    b[\"img\"],\n",
    "                    b[\"seg\"],\n",
    "                    b[\"y_grad\"],\n",
    "                    b[\"x_grad\"],\n",
    "                )\n",
    "                y_grad_arr, x_grad_arr, seg_arr = (\n",
    "                    y_grad_arr[:, 0],\n",
    "                    x_grad_arr[:, 0],\n",
    "                    seg_arr[:, 0],\n",
    "                )\n",
    "                x = torch.Tensor(img_arr.astype(float))\n",
    "                y = np.stack([y_grad_arr, x_grad_arr, seg_arr], axis=1)\n",
    "                y = torch.Tensor(y)\n",
    "                if self.gpuon:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                loss = self.cellpose_test(x, y)\n",
    "                total_test_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            del x\n",
    "            del y\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        avgtestnll = total_test_loss / test_data_size\n",
    "        print(\"Mean Test NLL: \" + str(avgtestnll))\n",
    "\n",
    "        entry = [\n",
    "            [\n",
    "                self.model_number,\n",
    "                self.mode,\n",
    "                self.batch_size,\n",
    "                self.layers,\n",
    "                self.hidden_size,\n",
    "                self.lr,\n",
    "                self.momentum,\n",
    "                self.weight_decay,\n",
    "                self.dropout,\n",
    "                self.W0,\n",
    "                self.Wsigma,\n",
    "                e,\n",
    "                avgtrainnll,\n",
    "                avgvalnll,\n",
    "                avgtestnll,\n",
    "                str(now),\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        df_out = pd.DataFrame(\n",
    "            data=entry,\n",
    "            columns=[\n",
    "                \"Model #\",\n",
    "                \"Mode\",\n",
    "                \"Batch Size\",\n",
    "                \"Layers\",\n",
    "                \"Hidden Size\",\n",
    "                \"Learning Rate\",\n",
    "                \"Momentum\",\n",
    "                \"Weight Decay\",\n",
    "                \"Dropout\",\n",
    "                \"W0\",\n",
    "                \"Wsigma\",\n",
    "                \"Epoch\",\n",
    "                \"Train Loss\",\n",
    "                \"Val Loss\",\n",
    "                \"Test Loss\",\n",
    "                \"Date/Time\",\n",
    "            ],\n",
    "        )\n",
    "        df_out = df_out.set_index(\n",
    "            [\"Model #\", \"Epoch\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        df_out = df_out.sort_index()\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    def write_metadata(self, filepath, iomode, df_out):\n",
    "        meta_handle = pandas_hdf5_handler(filepath)\n",
    "        if os.path.exists(filepath):\n",
    "            ind = df_out.index[0]\n",
    "            df_in = meta_handle.read_df(\"data\")\n",
    "            df_mask = ~df_in.index.isin([ind])\n",
    "            df_in = df_in[df_mask]\n",
    "            df_out = pd.concat([df_in, df_out])\n",
    "        meta_handle.write_df(\"data\", df_out)\n",
    "\n",
    "    def get_class_fscore(self, iterator, mask_label_dim=1):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i, b in enumerate(iterator):\n",
    "            img_arr, y = (b[\"img\"], b[\"gt\"])\n",
    "            x = torch.Tensor(img_arr.astype(float))\n",
    "            if self.gpuon:\n",
    "                x = x.cuda()\n",
    "            fx = self.model.forward(x).detach().cpu().numpy()\n",
    "\n",
    "            y_true.append(y[:, 0])  # N,H,W\n",
    "            y_pred.append(get_class_labels(fx, mask_label_dim=mask_label_dim))  # N,H,W\n",
    "\n",
    "            del x\n",
    "            del y\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        all_f_scores = []\n",
    "        for i in range(y_true.shape[0]):\n",
    "            _, _, f_score = object_f_scores(y_true[i], y_pred[i])\n",
    "            all_f_scores += f_score.tolist()\n",
    "        all_f_scores = np.array(all_f_scores)\n",
    "        all_f_scores = all_f_scores[~np.isnan(all_f_scores)]\n",
    "\n",
    "        return all_f_scores\n",
    "\n",
    "    def get_cellpose_fscore(self, iterator, mask_label_dim=1):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i, b in enumerate(iterator):\n",
    "            img_arr, y = (b[\"img\"], b[\"gt\"])\n",
    "            x = torch.Tensor(img_arr.astype(float))\n",
    "            if self.gpuon:\n",
    "                x = x.cuda()\n",
    "            fx = self.model.forward(x).detach().cpu().numpy()\n",
    "\n",
    "            y_true.append(y[:, 0])  # N,H,W\n",
    "            y_pred.append(get_cellpose_labels(fx, step_size=1.0, n_iter=10))\n",
    "\n",
    "            del x\n",
    "            del y\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        all_f_scores = []\n",
    "        for i in range(y_true.shape[0]):\n",
    "            _, _, f_score = object_f_scores(y_true[i], y_pred[i])\n",
    "            all_f_scores += f_score.tolist()\n",
    "        all_f_scores = np.array(all_f_scores)\n",
    "        all_f_scores = all_f_scores[~np.isnan(all_f_scores)]\n",
    "\n",
    "        return all_f_scores\n",
    "\n",
    "    def train_model(self):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        start = time.time()\n",
    "        writedir(self.nndatapath + \"/models\", overwrite=False)\n",
    "        self.removefile(\n",
    "            self.nndatapath\n",
    "            + \"/models/training_metadata_\"\n",
    "            + str(self.model_number)\n",
    "            + \".hdf5\"\n",
    "        )\n",
    "\n",
    "        train_data = SegmentationDataset(\n",
    "            self.nndatapath + \"train.hdf5\",\n",
    "            mode=self.mode,\n",
    "            W0=self.W0,\n",
    "            Wsigma=self.Wsigma,\n",
    "        )\n",
    "        test_data = SegmentationDataset(\n",
    "            self.nndatapath + \"test.hdf5\",\n",
    "            mode=self.mode,\n",
    "            W0=self.W0,\n",
    "            Wsigma=self.Wsigma,\n",
    "        )\n",
    "        val_data = SegmentationDataset(\n",
    "            self.nndatapath + \"val.hdf5\", mode=self.mode, W0=self.W0, Wsigma=self.Wsigma\n",
    "        )\n",
    "\n",
    "        train_data_size = train_data.size\n",
    "        test_data_size = test_data.size\n",
    "        val_data_size = val_data.size\n",
    "\n",
    "        for e in range(0, self.numepochs):\n",
    "            train_iter = DataLoader(\n",
    "                train_data,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=numpy_collate,\n",
    "            )\n",
    "            test_iter = DataLoader(\n",
    "                test_data,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=numpy_collate,\n",
    "            )\n",
    "            val_iter = DataLoader(\n",
    "                val_data,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=numpy_collate,\n",
    "            )\n",
    "            df_out = self.perepoch(\n",
    "                e,\n",
    "                train_iter,\n",
    "                test_iter,\n",
    "                val_iter,\n",
    "                train_data_size,\n",
    "                test_data_size,\n",
    "                val_data_size,\n",
    "            )\n",
    "\n",
    "            self.write_metadata(\n",
    "                self.nndatapath\n",
    "                + \"/models/training_metadata_\"\n",
    "                + str(self.model_number)\n",
    "                + \".hdf5\",\n",
    "                \"w\",\n",
    "                df_out,\n",
    "            )\n",
    "        end = time.time()\n",
    "        time_elapsed = (end - start) / 60.0\n",
    "        torch.save(\n",
    "            self.model.state_dict(),\n",
    "            self.nndatapath + \"/models/\" + str(self.model_number) + \".pt\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            if self.mode == \"class\" or self.mode == \"multiclass\":\n",
    "                val_f = self.get_class_fscore(val_iter)\n",
    "                test_f = self.get_class_fscore(test_iter)\n",
    "            elif self.mode == \"cellpose\":\n",
    "                val_f = self.get_cellpose_fscore(val_iter)\n",
    "                test_f = self.get_cellpose_fscore(test_iter)\n",
    "        except:\n",
    "            print(\"Failed to compute F-scores\")\n",
    "            val_f = [np.NaN]\n",
    "            test_f = [np.NaN]\n",
    "\n",
    "        meta_handle = pandas_hdf5_handler(self.nndatapath + \"/metadata.hdf5\")\n",
    "        traindf = meta_handle.read_df(\"train\", read_metadata=True)\n",
    "        valdf = meta_handle.read_df(\"val\", read_metadata=True)\n",
    "        testdf = meta_handle.read_df(\"test\", read_metadata=True)\n",
    "        trainmeta = traindf.metadata\n",
    "        valmeta = valdf.metadata\n",
    "        testmeta = testdf.metadata\n",
    "        experiment_name = trainmeta[\"nndataset\"][\"experimentname\"]\n",
    "\n",
    "        train_data_list = traindf.index.get_level_values(0).unique().tolist()\n",
    "        val_data_list = valdf.index.get_level_values(0).unique().tolist()\n",
    "        test_data_list = testdf.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "        train_orgs = [\n",
    "            trainmeta[data_name][\"global\"][\"Organism\"] for data_name in train_data_list\n",
    "        ]\n",
    "        train_micros = [\n",
    "            trainmeta[data_name][\"global\"][\"Microscope\"]\n",
    "            for data_name in train_data_list\n",
    "        ]\n",
    "        val_orgs = [\n",
    "            trainmeta[data_name][\"global\"][\"Organism\"] for data_name in val_data_list\n",
    "        ]\n",
    "        val_micros = [\n",
    "            trainmeta[data_name][\"global\"][\"Microscope\"] for data_name in val_data_list\n",
    "        ]\n",
    "        test_orgs = [\n",
    "            trainmeta[data_name][\"global\"][\"Organism\"] for data_name in test_data_list\n",
    "        ]\n",
    "        test_micros = [\n",
    "            trainmeta[data_name][\"global\"][\"Microscope\"] for data_name in test_data_list\n",
    "        ]\n",
    "\n",
    "        train_ttl_img = len(traindf)\n",
    "        val_ttl_img = len(valdf)\n",
    "        test_ttl_img = len(testdf)\n",
    "\n",
    "        train_loss, val_loss, test_loss = (\n",
    "            df_out[\"Train Loss\"].tolist()[0],\n",
    "            df_out[\"Val Loss\"].tolist()[0],\n",
    "            df_out[\"Test Loss\"].tolist()[0],\n",
    "        )\n",
    "\n",
    "        entry = [\n",
    "            [\n",
    "                experiment_name,\n",
    "                self.model_number,\n",
    "                self.mode,\n",
    "                train_data_list,\n",
    "                train_orgs,\n",
    "                train_micros,\n",
    "                train_ttl_img,\n",
    "                val_data_list,\n",
    "                val_orgs,\n",
    "                val_micros,\n",
    "                val_ttl_img,\n",
    "                test_data_list,\n",
    "                test_orgs,\n",
    "                test_micros,\n",
    "                test_ttl_img,\n",
    "                self.batch_size,\n",
    "                self.layers,\n",
    "                self.hidden_size,\n",
    "                self.lr,\n",
    "                self.momentum,\n",
    "                self.weight_decay,\n",
    "                self.dropout,\n",
    "                self.W0,\n",
    "                self.Wsigma,\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                val_f,\n",
    "                test_loss,\n",
    "                test_f,\n",
    "                str(timestamp),\n",
    "                self.numepochs,\n",
    "                time_elapsed,\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        df_out = pd.DataFrame(\n",
    "            data=entry,\n",
    "            columns=[\n",
    "                \"Experiment Name\",\n",
    "                \"Model #\",\n",
    "                \"NN Mode\",\n",
    "                \"Train Datasets\",\n",
    "                \"Train Organisms\",\n",
    "                \"Train Microscopes\",\n",
    "                \"Train # Images\",\n",
    "                \"Val Datasets\",\n",
    "                \"Val Organisms\",\n",
    "                \"Val Microscopes\",\n",
    "                \"Val # Images\",\n",
    "                \"Test Datasets\",\n",
    "                \"Test Organisms\",\n",
    "                \"Test Microscopes\",\n",
    "                \"Test # Images\",\n",
    "                \"Batch Size\",\n",
    "                \"Layers\",\n",
    "                \"Hidden Size\",\n",
    "                \"Learning Rate\",\n",
    "                \"Momentum\",\n",
    "                \"Weight Decay\",\n",
    "                \"Dropout\",\n",
    "                \"W0 Weight (if applicable)\",\n",
    "                \"W Sigma (if applicable)\",\n",
    "                \"Train Loss\",\n",
    "                \"Val Loss\",\n",
    "                \"Val F1 Cell Scores\",\n",
    "                \"Test Loss\",\n",
    "                \"Test F1 Cell Scores\",\n",
    "                \"Date/Time\",\n",
    "                \"# Epochs\",\n",
    "                \"Training Time (mins)\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        df_out = df_out.set_index(\n",
    "            [\"Experiment Name\", \"Model #\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        df_out = df_out.sort_index()\n",
    "\n",
    "        metalock = hdf5lock(self.nndatapath + \"/model_metadata.hdf5\", updateperiod=5.0)\n",
    "        metalock.lockedfn(self.write_metadata, \"w\", df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = UNet_Trainer(\n",
    "    \"/n/scratch2/de64/nntest7/\",\n",
    "    0,\n",
    "    \"cellpose\",\n",
    "    numepochs=10,\n",
    "    batch_size=100,\n",
    "    layers=4,\n",
    "    hidden_size=64,\n",
    "    lr=0.005,\n",
    "    momentum=0.95,\n",
    "    weight_decay=0.0005,\n",
    "    dropout=0.0,\n",
    "    gpuon=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pandas_hdf5_handler(\"/n/scratch2/de64/nntest7/model_metadata.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = check.read_df(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(df[\"Test F1 Cell Scores\"][0], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pandas_hdf5_handler(\"/n/scratch2/de64/2019-05-31_validation_data/metadata.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "    def __init__(self, nndatapath, numepochs=50):\n",
    "        self.nndatapath = nndatapath\n",
    "        self.numepochs = numepochs\n",
    "\n",
    "    def display_grid(self):\n",
    "        meta_handle = pandas_hdf5_handler(self.nndatapath + \"/metadata.hdf5\")\n",
    "        trainmeta = meta_handle.read_df(\"train\", read_metadata=True).metadata[\n",
    "            \"nndataset\"\n",
    "        ]\n",
    "        W0_list, Wsigma_list = trainmeta[\"W0_list\"], trainmeta[\"Wsigma_list\"]\n",
    "\n",
    "        self.tab_dict = {\n",
    "            \"Mode\": [\"class\", \"multiclass\", \"cellpose\"],\n",
    "            \"Batch Size:\": [5, 10, 25, 50],\n",
    "            \"Layers:\": [2, 3, 4],\n",
    "            \"Hidden Size:\": [16, 32, 64],\n",
    "            \"Learning Rate:\": [0.001, 0.005, 0.01, 0.05],\n",
    "            \"Momentum:\": [0.9, 0.95, 0.99],\n",
    "            \"Weight Decay:\": [0.0001, 0.0005, 0.001],\n",
    "            \"Dropout:\": [0.0, 0.3, 0.5, 0.7],\n",
    "            \"W0:\": W0_list,\n",
    "            \"Wsigma\": Wsigma_list,\n",
    "        }\n",
    "\n",
    "        children = [\n",
    "            ipyw.SelectMultiple(\n",
    "                options=val, value=(val[0],), description=key, disabled=False\n",
    "            )\n",
    "            for key, val in self.tab_dict.items()\n",
    "        ]\n",
    "        self.tab = ipyw.Tab()\n",
    "        self.tab.children = children\n",
    "        for i, key in enumerate(self.tab_dict.keys()):\n",
    "            self.tab.set_title(i, key[:-1])\n",
    "        return self.tab\n",
    "\n",
    "    def get_grid_params(self):\n",
    "        self.grid_dict = {child.description: child.value for child in self.tab.children}\n",
    "        print(\"======== Grid Params ========\")\n",
    "        for key, val in self.grid_dict.items():\n",
    "            print(key + \" \" + str(val))\n",
    "\n",
    "    def generate_pyscript(self, run_idx, grid_params):\n",
    "        import_line = \"import trenchripper as tr\"\n",
    "        trainer_line = (\n",
    "            'nntrainer = tr.unet.UNet_Trainer(\"'\n",
    "            + self.nndatapath\n",
    "            + '\",'\n",
    "            + str(run_idx)\n",
    "            + ',\"'\n",
    "            + str(grid_params[0])\n",
    "            + '\",gpuon=True,numepochs='\n",
    "            + str(self.numepochs)\n",
    "            + \",batch_size=\"\n",
    "            + str(grid_params[1])\n",
    "            + \",layers=\"\n",
    "            + str(grid_params[2])\n",
    "            + \",hidden_size=\"\n",
    "            + str(grid_params[3])\n",
    "            + \",lr=\"\n",
    "            + str(grid_params[4])\n",
    "            + \",momentum=\"\n",
    "            + str(grid_params[5])\n",
    "            + \",weight_decay=\"\n",
    "            + str(grid_params[6])\n",
    "            + \",dropout=\"\n",
    "            + str(grid_params[7])\n",
    "            + \",W0=\"\n",
    "            + str(grid_params[8])\n",
    "            + \",Wsigma=\"\n",
    "            + str(grid_params[9])\n",
    "            + \")\"\n",
    "        )\n",
    "        train_line = \"nntrainer.train_model()\"\n",
    "        pyscript = \"\\n\".join([import_line, trainer_line, train_line])\n",
    "        with open(\n",
    "            self.nndatapath + \"/models/scripts/\" + str(run_idx) + \".py\", \"w\"\n",
    "        ) as scriptfile:\n",
    "            scriptfile.write(pyscript)\n",
    "\n",
    "    def generate_sbatchscript(self, run_idx, hours, cores, mem, gres):\n",
    "        shebang = \"#!/bin/bash\"\n",
    "        core_line = \"#SBATCH -c \" + str(cores)\n",
    "        hour_line = \"#SBATCH -t \" + str(hours) + \":00:00\"\n",
    "        gpu_lines = \"#SBATCH -p gpu\\n#SBATCH --gres=\" + gres\n",
    "        mem_line = \"#SBATCH --mem=\" + mem\n",
    "        report_lines = (\n",
    "            \"#SBATCH -o \"\n",
    "            + self.nndatapath\n",
    "            + \"/models/scripts/\"\n",
    "            + str(run_idx)\n",
    "            + \".out\\n#SBATCH -e \"\n",
    "            + self.nndatapath\n",
    "            + \"/models/scripts/\"\n",
    "            + str(run_idx)\n",
    "            + \".err\\n\"\n",
    "        )\n",
    "\n",
    "        run_line = (\n",
    "            \"python -u \" + self.nndatapath + \"/models/scripts/\" + str(run_idx) + \".py\"\n",
    "        )\n",
    "\n",
    "        sbatchscript = \"\\n\".join(\n",
    "            [shebang, core_line, hour_line, gpu_lines, mem_line, report_lines, run_line]\n",
    "        )\n",
    "        with open(\n",
    "            self.nndatapath + \"/models/scripts/\" + str(run_idx) + \".sh\", \"w\"\n",
    "        ) as scriptfile:\n",
    "            scriptfile.write(sbatchscript)\n",
    "\n",
    "    def run_sbatchscript(self, run_idx):\n",
    "        cmd = [\"sbatch\", self.nndatapath + \"/models/scripts/\" + str(run_idx) + \".sh\"]\n",
    "        subprocess.run(cmd)\n",
    "\n",
    "    def run_grid_search(self, hours=12, cores=2, mem=\"8G\", gres=\"gpu:1\"):\n",
    "\n",
    "        grid_keys = self.grid_dict.keys()\n",
    "        grid_combinations = list(itertools.product(*list(self.grid_dict.values())))\n",
    "        writedir(self.nndatapath + \"/models\", overwrite=True)\n",
    "        writedir(self.nndatapath + \"/models/scripts\", overwrite=True)\n",
    "\n",
    "        self.run_indices = []\n",
    "\n",
    "        for run_idx, grid_params in enumerate(grid_combinations):\n",
    "            self.generate_pyscript(run_idx, grid_params)\n",
    "            self.generate_sbatchscript(run_idx, hours, cores, mem, gres)\n",
    "            self.run_sbatchscript(run_idx)\n",
    "            self.run_indices.append(run_idx)\n",
    "\n",
    "    def cancel_all_runs(self, username):\n",
    "        for run_idx in self.run_indices:\n",
    "            cmd = [\"scancel\", \"-p\", \"gpu\", \"--user=\" + username]\n",
    "            subprocess.Popen(\n",
    "                cmd, shell=True, stdin=None, stdout=None, stderr=None, close_fds=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearch(\"/n/scratch2/de64/nntest7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.display_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_grid_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.run_grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_handle = pandas_hdf5_handler(grid.nndatapath + \"/metadata.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmeta = meta_handle.read_df(\"train\", read_metadata=True).metadata[\"nndataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
