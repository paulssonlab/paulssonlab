{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing from multiple sources\n",
    "\n",
    "Intermediate goal: multiple inputs - > single train, test, val files\n",
    "(no augmentation yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import copy\n",
    "import ipywidgets as ipyw\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import itertools\n",
    "import qgrid\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from random import shuffle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy import interpolate, ndimage\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import skimage as sk\n",
    "import pickle as pkl\n",
    "import skimage.morphology\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import (\n",
    "    pandas_hdf5_handler,\n",
    "    kymo_handle,\n",
    "    writedi    def post_process(self,dask_controller):\n",
    "        dask_controller.futures = {}\n",
    "\n",
    "        df = dd.read_parquet(self.kymographpath + \"/metadata\").persist()\n",
    "#         df = self.add_trenchids(df).persist() #NEW\n",
    "        \n",
    "        trenchid_list = df[\"trenchid\"].unique().compute().tolist()\n",
    "        file_list = df[\"File Index\"].unique().compute().tolist()\n",
    "        outputdf = df.drop(columns = [\"File Index\",\"Image Index\"]).persist()\n",
    "        trenchiddf = df.set_index(\"trenchid\").persist()\n",
    "\n",
    "#         with open(self.kymographpath + \"/metadata.pkl\", 'rb') as handle:\n",
    "#             metadata = pickle.load(handle)\n",
    "\n",
    "        num_tpts = len(trenchiddf[\"timepoints\"].unique().compute().tolist())\n",
    "        chunk_size = self.trenches_per_file*num_tpts\n",
    "        if len(trenchid_list)%self.trenches_per_file == 0:\n",
    "            num_files = (len(trenchid_list)//self.trenches_per_file)\n",
    "        else:\n",
    "            num_files = (len(trenchid_list)//self.trenches_per_file) + 1\n",
    "\n",
    "        file_indices = np.repeat(np.array(range(num_files)),chunk_size)[:len(outputdf)]\n",
    "        file_trenchid = np.repeat(np.array(range(self.trenches_per_file)),num_tpts)\n",
    "        file_trenchid = np.repeat(file_trenchid[:,np.newaxis],num_files,axis=1).T.flatten()[:len(outputdf)]\n",
    "        file_indices = pd.DataFrame(file_indices)\n",
    "        file_trenchid = pd.DataFrame(file_trenchid)\n",
    "        file_indices.index = outputdf.index\n",
    "        file_trenchid.index = outputdf.index\n",
    "\n",
    "        outputdf[\"File Index\"] = file_indices[0]\n",
    "        outputdf[\"File Trench Index\"] = file_trenchid[0]\n",
    "        parq_file_idx = outputdf.apply(lambda x: int(f'{int(x[\"File Index\"]):04}{int(x[\"File Trench Index\"]):04}{int(x[\"timepoints\"]):04}'), axis=1, meta=int)\n",
    "        outputdf[\"File Parquet Index\"] = parq_file_idx\n",
    "        outputdf = outputdf.astype({\"File Index\":int,\"File Trench Index\":int,\"File Parquet Index\":int})\n",
    "\n",
    "        random_priorities = np.random.uniform(size=(num_files,))\n",
    "        for k in range(0,num_files):\n",
    "            priority = random_priorities[k]\n",
    "            future = dask_controller.daskclient.submit(self.reorg_kymograph,k,df,trenchid_list,trenchiddf,retries=1,priority=priority)\n",
    "            dask_controller.futures[\"Kymograph Reorganized: \" + str(k)] = future\n",
    "\n",
    "        reorg_futures = [dask_controller.futures[\"Kymograph Reorganized: \" + str(k)] for k in range(num_files)]\n",
    "        future = dask_controller.daskclient.submit(self.cleanup_kymographs,reorg_futures,file_list,retries=1,priority=priority)\n",
    "        dask_controller.futures[\"Kymographs Cleaned Up\"] = future\n",
    "        dask_controller.daskclient.gather([future])\n",
    "\n",
    "        dd.to_parquet(outputdf, self.kymographpath + \"/metadata\",engine='fastparquet',compression='gzip',write_metadata_file=True)r,\n",
    ")\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import hdf5lock\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import object_f_scores\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=40,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=\"/n/scratch2/de64/nntest7/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage as sk\n",
    "from skimage.util import img_as_ubyte\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "import copy\n",
    "import scipy\n",
    "from skimage.morphology import square\n",
    "from skimage.filters import rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\n",
    "    \"/n/scratch2/de64/2019-05-31_validation_data/kymograph/kymograph_50.hdf5\", \"r\"\n",
    ") as infile:\n",
    "    data = infile[\"Phase\"][:]\n",
    "\n",
    "with h5py.File(\n",
    "    \"/n/scratch2/de64/2019-05-31_validation_data/fluorsegmentation/segmentation_50.hdf5\",\n",
    "    \"r\",\n",
    ") as infile:\n",
    "    seg_data = infile[\"data\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(seg_data[10, 3]))\n",
    "relab, fw, inv = sk.segmentation.relabel_sequential(seg_data[10, 3])\n",
    "print(np.unique(relab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg_data[11, 0, 25:125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_segmentation(seg_data[10, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    get_border(\n",
    "        seg_data[10, 0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sk.segmentation.find_boundaries(seg_data[10, 0], mode=\"inner\")\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_border(labeled):\n",
    "    mask = sk.segmentation.find_boundaries(labeled)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask(labeled):\n",
    "    mask = np.zeros(labeled.shape, dtype=bool)\n",
    "    mask[labeled > 0] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_background(labeled):\n",
    "    mask = np.zeros(labeled.shape, dtype=bool)\n",
    "    mask[labeled == 0] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_masknoborder(labeled):\n",
    "    mask = get_mask(labeled)\n",
    "    border = get_border(labeled)\n",
    "    mask[border] = False\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_segmentation(labeled, mode_list=[\"background\", \"mask\", \"border\"]):\n",
    "    segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "    for i, mode in enumerate(mode_list):\n",
    "        if mode == \"background\":\n",
    "            segmentation[get_background(labeled)] = i\n",
    "        elif mode == \"mask\":\n",
    "            segmentation[get_mask(labeled)] = i\n",
    "        elif mode == \"border\":\n",
    "            segmentation[get_border(labeled)] = i\n",
    "        elif mode == \"masknoborder\":\n",
    "            segmentation[get_masknoborder(labeled)] = i\n",
    "        else:\n",
    "            raise\n",
    "    return segmentation\n",
    "\n",
    "\n",
    "def get_standard_weightmap(segmentation):\n",
    "    num_labels = len(np.unique(segmentation))\n",
    "    label_count = []\n",
    "    label_masks = []\n",
    "    for label in range(num_labels):\n",
    "        label_mask = segmentation == label\n",
    "        num_label = np.sum(label_mask)\n",
    "        label_masks.append(label_mask)\n",
    "        label_count.append(num_label)\n",
    "\n",
    "    ttl_count = np.sum(label_count)\n",
    "    class_weight = ttl_count / (np.array(label_count) + 1)\n",
    "    class_weight = class_weight / np.sum(class_weight)\n",
    "    weight_map = np.zeros(segmentation.shape, dtype=float)\n",
    "    for i, label_mask in enumerate(label_masks):\n",
    "        weight_map[label_mask] = class_weight[i]\n",
    "\n",
    "    return weight_map\n",
    "\n",
    "\n",
    "def get_unet_weightmap(labeled, W0=5.0, Wsigma=2.0):\n",
    "    mask = labeled > 0\n",
    "\n",
    "    ttl_count = mask.size\n",
    "    mask_count = np.sum(mask)\n",
    "    background_count = ttl_count - mask_count\n",
    "\n",
    "    class_weight = np.array(\n",
    "        [ttl_count / (background_count + 1), ttl_count / (mask_count + 1)]\n",
    "    )\n",
    "    class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "    labels = np.unique(labeled)[1:]\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    if num_labels == 0:\n",
    "        weight_map = np.ones(labeled.shape) * class_weight[0]\n",
    "    elif num_labels == 1:\n",
    "        weight_map = np.ones(labeled.shape) * class_weight[0]\n",
    "        weight_map[mask] += class_weight[1]\n",
    "    else:\n",
    "        dist_maps = []\n",
    "        borders = []\n",
    "        for i in labels:\n",
    "            cell = labeled == i\n",
    "            eroded = sk.morphology.binary_dilation(cell)\n",
    "            border = eroded ^ cell\n",
    "            borders.append(border)\n",
    "            dist_map = scipy.ndimage.morphology.distance_transform_edt(~border)\n",
    "            dist_maps.append(dist_map)\n",
    "        dist_maps = np.array(dist_maps)\n",
    "        borders = np.array(borders)\n",
    "        borders = np.max(borders, axis=0)\n",
    "        dist_maps = np.sort(dist_maps, axis=0)\n",
    "        weight_map = W0 * np.exp(\n",
    "            -((dist_maps[0] + dist_maps[1]) ** 2) / (2 * (Wsigma**2))\n",
    "        )\n",
    "        weight_map[mask] += class_weight[1]\n",
    "        weight_map[~mask] += class_weight[0]\n",
    "\n",
    "    return weight_map\n",
    "\n",
    "\n",
    "def get_flows(labeled):\n",
    "    rps = sk.measure.regionprops(labeled)\n",
    "    centers = np.array([np.round(rp.centroid).astype(\"uint16\") for rp in rps])\n",
    "    y_lens = np.array([rp.bbox[2] - rp.bbox[0] for rp in rps])\n",
    "    x_lens = np.array([rp.bbox[3] - rp.bbox[1] for rp in rps])\n",
    "    N_arr = 2 * (y_lens + x_lens)\n",
    "    kernel = np.ones(3, float) / 3.0\n",
    "\n",
    "    x_grad_arr = np.zeros(labeled.shape)\n",
    "    y_grad_arr = np.zeros(labeled.shape)\n",
    "\n",
    "    for cell_idx in range(1, len(rps) + 1):\n",
    "        cell_mask = labeled == cell_idx\n",
    "        cell_center = centers[cell_idx - 1]\n",
    "        diffusion_arr = np.zeros(cell_mask.shape, dtype=float)\n",
    "        for i in range(N_arr[cell_idx - 1]):\n",
    "            diffusion_arr[cell_center] += 1.0\n",
    "            diffusion_arr = convolve1d(\n",
    "                convolve1d(diffusion_arr, kernel, axis=0), kernel, axis=1\n",
    "            )\n",
    "            diffusion_arr[~cell_mask] = 0.0\n",
    "        y_grad, x_grad = np.gradient(diffusion_arr)\n",
    "        y_grad, x_grad = (y_grad * cell_mask, x_grad * cell_mask)\n",
    "\n",
    "        y_grad_arr += y_grad\n",
    "        x_grad_arr += x_grad\n",
    "\n",
    "    return y_grad_arr, x_grad_arr\n",
    "\n",
    "\n",
    "def get_two_class(labeled):\n",
    "    segmentation = get_segmentation(labeled, mode_list=[\"background\", \"mask\", \"border\"])\n",
    "    weightmap = get_standard_weightmap(segmentation)\n",
    "    return segmentation, weightmap\n",
    "\n",
    "\n",
    "def get_one_class(labeled, W0=5.0, Wsigma=2.0):\n",
    "    segmentation = get_segmentation(labeled, mode_list=[\"background\", \"mask\"]).astype(\n",
    "        bool\n",
    "    )\n",
    "    weightmap = get_unet_weightmap(labeled, W0=W0, Wsigma=Wsigma)\n",
    "    return segmentation, weightmap\n",
    "\n",
    "\n",
    "def cellpose(labeled):\n",
    "    segmentation = get_segmentation(labeled, mode_list=[\"background\", \"mask\"]).astype(\n",
    "        bool\n",
    "    )\n",
    "    y_grad_arr, x_grad_arr = get_flows(labeled)\n",
    "    return segmentation, y_grad_arr, x_grad_arr\n",
    "\n",
    "\n",
    "## next make flow vector generation\n",
    "## package into data preprocessing class that generates requested arrays accessible on keys (look at prior work)\n",
    "## test generation at scale\n",
    "## augment on the fly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellpose(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad_arr, x_grad_arr = get_flows(seg_data[10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_len = rps[0].bbox[2] - rps[0].bbox[0]\n",
    "x_len = rps[0].bbox[3] - rps[0].bbox[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps = sk.measure.regionprops(seg_data[10, 0])\n",
    "centers = np.array([np.round(rp.centroid).astype(\"uint16\") for rp in rps])\n",
    "y_lens = np.array([rp.bbox[2] - rp.bbox[0] for rp in rps])\n",
    "x_lens = np.array([rp.bbox[3] - rp.bbox[1] for rp in rps])\n",
    "N_arr = 2 * (y_lens + x_lens)\n",
    "kernel = np.ones(3, float) / 3.0\n",
    "\n",
    "x_grad_arr = np.zeros(seg_data[10, 0].shape)\n",
    "y_grad_arr = np.zeros(seg_data[10, 0].shape)\n",
    "\n",
    "for cell_idx in range(1, len(rps) + 1):\n",
    "    cell_mask = seg_data[10, 0] == cell_idx\n",
    "    cell_center = centers[cell_idx - 1]\n",
    "    diffusion_arr = np.zeros(cell_mask.shape, dtype=float)\n",
    "    for i in range(N_arr[cell_idx - 1]):\n",
    "        diffusion_arr[cell_center] += 1.0\n",
    "        diffusion_arr = convolve1d(\n",
    "            convolve1d(diffusion_arr, kernel, axis=0), kernel, axis=1\n",
    "        )\n",
    "        diffusion_arr[~cell_mask] = 0.0\n",
    "    y_grad, x_grad = np.gradient(diffusion_arr)\n",
    "    y_grad, x_grad = (y_grad * cell_mask, x_grad * cell_mask)\n",
    "    x_grad_arr += x_grad\n",
    "    y_grad_arr += y_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_grad_arr, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad_arr[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = seg_data[10, 0] > 0\n",
    "pixel_arr = np.array(np.where(test_mask)).T\n",
    "step_size = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pixels = []\n",
    "for pixel_idx in range(pixel_arr.shape[0]):\n",
    "    pixel_coord = pixel_arr[pixel_idx].astype(float)\n",
    "    for N in range(200):\n",
    "        near_coord = np.array(np.round(pixel_coord)).astype(int)\n",
    "        y_grad = y_grad_arr[near_coord[0], near_coord[1]]\n",
    "        x_grad = x_grad_arr[near_coord[0], near_coord[1]]\n",
    "        pixel_coord += np.array([y_grad, x_grad]) * step_size\n",
    "    final_pixels.append(pixel_coord)\n",
    "\n",
    "dbsc = skl.cluster.DBSCAN(eps=2.0)\n",
    "cluster_assign = dbsc.fit_predict(final_pixels)\n",
    "\n",
    "out_labels = np.zeros(test_mask.shape, dtype=int)\n",
    "out_labels[pixel_arr[:, 0], pixel_arr[:, 1]] = cluster_assign + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsc = skl.cluster.DBSCAN(eps=2.0)\n",
    "cluster_assign = dbsc.fit_predict(final_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assign + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_labels = np.zeros(test_mask.shape, dtype=int)\n",
    "out_labels[pixel_arr[:, 0], pixel_arr[:, 1]] = cluster_assign + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_grad_arr, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.gradient(diffusion_arr[150:])[1], cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad[1][cell_mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = diffusion_arr\n",
    "grad = np.gradient(arr)\n",
    "\n",
    "plt.figure()\n",
    "plt.quiver(grad[1] * cell_mask, grad[0] * cell_mask, scale=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve1d\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones(3, float) / 3.0\n",
    "diffusion_arr = np.zeros(cell_mask.shape, dtype=float)\n",
    "N = 2 * (cell_mask.shape[0] + cell_mask.shape[1])\n",
    "for i in range(N_arr[0]):\n",
    "    print(i)\n",
    "    diffusion_arr[cell_center] += 1.0\n",
    "    diffusion_arr = convolve1d(\n",
    "        convolve1d(diffusion_arr, kernel, axis=0), kernel, axis=1\n",
    "    )\n",
    "    diffusion_arr[~cell_mask] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(diffusion_arr[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import data\n",
    "from skimage.morphology import square\n",
    "from skimage.filters import rank\n",
    "\n",
    "\n",
    "image = data.coins()\n",
    "selem = disk(20)\n",
    "\n",
    "percentile_result = rank.mean_percentile(image, selem=selem, p0=0.1, p1=0.9)\n",
    "bilateral_result = rank.mean_bilateral(image, selem=selem, s0=500, s1=500)\n",
    "normal_result = rank.mean(image, selem=selem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.array([np.round(rp.centroid).astype(\"uint16\") for rp in rps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = get_segmentation(seg_data[10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap = get_standard_weightmap(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(weightmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = sk.segmentation.find_boundaries(seg_data[11, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_arr = data.reshape(-1, data.shape[2], data.shape[3])[:, :, :, np.newaxis]\n",
    "reshaped_arr = img_as_ubyte(reshaped_arr)\n",
    "\n",
    "reshaped_seg_arr = seg_data.reshape(-1, seg_data.shape[2], seg_data.shape[3])[\n",
    "    :, :, :, np.newaxis\n",
    "]\n",
    "mask_arr = (reshaped_seg_arr > 0).astype(\"uint8\")\n",
    "\n",
    "img_list = [reshaped_arr[i] for i in range(reshaped_arr.shape[0])]\n",
    "seg_list = [mask_arr[i] for i in range(mask_arr.shape[0])]\n",
    "bound_list = [sk.segmentation.find_boundaries(seg) for seg in seg_list]\n",
    "\n",
    "segmap_list = [SegmentationMapsOnImage(seg, shape=seg.shape) for seg in seg_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        iaa.PadToFixedSize(width=50, height=300),\n",
    "        iaa.CropToFixedSize(width=50, height=300),\n",
    "        iaa.Crop(\n",
    "            percent=(0, 0.1)\n",
    "        ),  # crop images from each side by 0 to 16px (randomly chosen)\n",
    "        iaa.Fliplr(0.5),  # vertically flip 50% of the images\n",
    "        iaa.Flipud(0.5),  # horizontally flip 50% of the images\n",
    "        # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "        # But we only blur about 50% of all images.\n",
    "        iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),\n",
    "        # Strengthen or weaken the contrast in each image.\n",
    "        iaa.LinearContrast(alpha=(0.7, 1.3)),\n",
    "        # Add gaussian noise.\n",
    "        # For 50% of all images, we sample the noise once per pixel.\n",
    "        # For the other 50% of all images, we sample the noise per pixel AND\n",
    "        # channel. This can change the color (not only brightness) of the\n",
    "        # pixels.\n",
    "        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255)),\n",
    "        # Make some images brighter and some darker.\n",
    "        # In 20% of all cases, we sample the multiplier once per channel,\n",
    "        # which can end up changing the color of the images.\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "        iaa.Affine(\n",
    "            scale={\"x\": (0.7, 1.3), \"y\": (0.7, 1.3)},\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)},\n",
    "            rotate=(-15, 15),\n",
    "            shear={\"x\": (-10, 10), \"y\": (-5, 5)},\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmap_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_aug = []\n",
    "segmaps_aug = []\n",
    "for i in range(len(segmap_list)):\n",
    "    image = img_list[i]\n",
    "    segmap = segmap_list[i]\n",
    "    images_aug_i, segmaps_aug_i = seq(image=image, segmentation_maps=segmap)\n",
    "    images_aug.append(images_aug_i)\n",
    "    segmaps_aug.append(segmaps_aug_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmaps_aug[0].get_arr().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "plt.imshow(images_aug[idx][:, :, 0])\n",
    "plt.imshow(segmaps_aug[idx].get_arr()[:, :, 0], alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_arr = seq(images=reshaped_arr)\n",
    "aug_arr = np.array(aug_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_arr[3, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_arr[3, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo = tr.kymo_handle()\n",
    "kymo.import_wrap(aug_arr[:, :, :, 0])\n",
    "kymo = kymo.return_unwrap()\n",
    "plt.imshow(kymo[:, :600])\n",
    "plt.show()\n",
    "plt.imshow(aug_arr[3, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_augmentation:\n",
    "    def __init__(self, mode=\"a\", p_flip=0.5, max_rot=10, min_padding=20):\n",
    "        if mode not in [\"a\", \"m\", \"w\"]:\n",
    "            raise ValueError(\"Not a valid augmentation mode\")\n",
    "        self.mode = mode\n",
    "        self.p_flip = p_flip\n",
    "        self.max_rot = max_rot\n",
    "        self.min_padding = min_padding\n",
    "\n",
    "    def random_crop(self, img_arr, seg_arr):\n",
    "        false_arr = np.zeros(img_arr.shape[2:4], dtype=bool)\n",
    "        random_crop_len_y = np.random.uniform(\n",
    "            low=0.3, high=1.0, size=(1, img_arr.shape[0])\n",
    "        )\n",
    "        random_crop_len_x = np.random.uniform(\n",
    "            low=0.5, high=1.0, size=(1, img_arr.shape[0])\n",
    "        )\n",
    "\n",
    "        random_crop_len = np.concatenate([random_crop_len_y, random_crop_len_x], axis=0)\n",
    "\n",
    "        random_crop_remainder = 1.0 - random_crop_len\n",
    "        random_crop_start = (\n",
    "            np.random.uniform(low=0.0, high=1.0, size=(2, img_arr.shape[0]))\n",
    "        ) * random_crop_remainder\n",
    "        low_crop = np.floor(\n",
    "            random_crop_start * np.array(img_arr.shape[2:4])[:, np.newaxis]\n",
    "        ).astype(\"int32\")\n",
    "        high_crop = np.floor(\n",
    "            low_crop + (random_crop_len * np.array(img_arr.shape[2:4])[:, np.newaxis])\n",
    "        ).astype(\"int32\")\n",
    "\n",
    "        out_arr = []\n",
    "        out_seg_arr = []\n",
    "        center = (img_arr.shape[2] // 2, img_arr.shape[3] // 2)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            mask = copy.copy(false_arr)\n",
    "            working_arr = copy.copy(img_arr[t, 0, :, :])\n",
    "            working_seg_arr = copy.copy(seg_arr[t, 0, :, :])\n",
    "\n",
    "            dim_0_range = high_crop[0, t] - low_crop[0, t]\n",
    "            dim_1_range = high_crop[1, t] - low_crop[1, t]\n",
    "            top_left = (center[0] - dim_0_range // 2, center[1] - dim_1_range // 2)\n",
    "\n",
    "            dim_0_maxscale = img_arr.shape[2] / dim_0_range\n",
    "            dim_1_maxscale = img_arr.shape[3] / dim_1_range\n",
    "\n",
    "            dim_0_scale = np.clip(\n",
    "                np.random.normal(loc=1.0, scale=0.1), 0.8, dim_0_maxscale\n",
    "            )\n",
    "            dim_1_scale = np.clip(\n",
    "                np.random.normal(loc=1.0, scale=0.1), 0.8, dim_1_maxscale\n",
    "            )\n",
    "\n",
    "            #             dim_0_scale = 1.\n",
    "            #             dim_1_scale = 1.\n",
    "\n",
    "            rescaled_img = sk.transform.rescale(\n",
    "                working_arr[\n",
    "                    low_crop[0, t] : high_crop[0, t], low_crop[1, t] : high_crop[1, t]\n",
    "                ],\n",
    "                (dim_0_scale, dim_1_scale),\n",
    "                preserve_range=True,\n",
    "            ).astype(int)\n",
    "            rescaled_seg = (\n",
    "                sk.transform.rescale(\n",
    "                    working_seg_arr[\n",
    "                        low_crop[0, t] : high_crop[0, t],\n",
    "                        low_crop[1, t] : high_crop[1, t],\n",
    "                    ]\n",
    "                    == 1,\n",
    "                    (dim_0_scale, dim_1_scale),\n",
    "                )\n",
    "                > 0.5\n",
    "            ).astype(\"int8\")\n",
    "\n",
    "            if self.mode is \"m\":\n",
    "                rescaled_border = (\n",
    "                    sk.transform.rescale(\n",
    "                        working_seg_arr[\n",
    "                            low_crop[0, t] : high_crop[0, t],\n",
    "                            low_crop[1, t] : high_crop[1, t],\n",
    "                        ]\n",
    "                        == 2,\n",
    "                        (dim_0_scale, dim_1_scale),\n",
    "                    )\n",
    "                    > 0.5\n",
    "                )\n",
    "                rescaled_seg[rescaled_border] = 2\n",
    "\n",
    "            top_left = (\n",
    "                center[0] - rescaled_img.shape[0] // 2,\n",
    "                center[1] - rescaled_img.shape[1] // 2,\n",
    "            )\n",
    "            working_arr[\n",
    "                top_left[0] : top_left[0] + rescaled_img.shape[0],\n",
    "                top_left[1] : top_left[1] + rescaled_img.shape[1],\n",
    "            ] = rescaled_img\n",
    "            working_seg_arr[\n",
    "                top_left[0] : top_left[0] + rescaled_img.shape[0],\n",
    "                top_left[1] : top_left[1] + rescaled_img.shape[1],\n",
    "            ] = rescaled_seg\n",
    "\n",
    "            mask[\n",
    "                top_left[0] : top_left[0] + rescaled_img.shape[0],\n",
    "                top_left[1] : top_left[1] + rescaled_img.shape[1],\n",
    "            ] = True\n",
    "            working_arr[~mask] = 0\n",
    "            working_seg_arr[~mask] = False\n",
    "\n",
    "            out_arr.append(working_arr)\n",
    "            out_seg_arr.append(working_seg_arr)\n",
    "        out_arr = np.expand_dims(np.array(out_arr), 1)\n",
    "        out_seg_arr = np.expand_dims(np.array(out_seg_arr), 1)\n",
    "        return out_arr, out_seg_arr\n",
    "\n",
    "    def random_x_flip(self, img_arr, seg_arr, p=0.5):\n",
    "        choices = np.random.choice(\n",
    "            np.array([True, False]), size=img_arr.shape[0], p=np.array([p, 1.0 - p])\n",
    "        )\n",
    "        out_img_arr = copy.copy(img_arr)\n",
    "        out_seg_arr = copy.copy(seg_arr)\n",
    "        out_img_arr[choices, 0, :, :] = np.flip(img_arr[choices, 0, :, :], axis=1)\n",
    "        out_seg_arr[choices, 0, :, :] = np.flip(seg_arr[choices, 0, :, :], axis=1)\n",
    "        return out_img_arr, out_seg_arr\n",
    "\n",
    "    def random_y_flip(self, img_arr, seg_arr, p=0.5):\n",
    "        choices = np.random.choice(\n",
    "            np.array([True, False]), size=img_arr.shape[0], p=np.array([p, 1.0 - p])\n",
    "        )\n",
    "        out_img_arr = copy.copy(img_arr)\n",
    "        out_seg_arr = copy.copy(seg_arr)\n",
    "        out_img_arr[choices, 0, :, :] = np.flip(img_arr[choices, 0, :, :], axis=2)\n",
    "        out_seg_arr[choices, 0, :, :] = np.flip(seg_arr[choices, 0, :, :], axis=2)\n",
    "        return out_img_arr, out_seg_arr\n",
    "\n",
    "    def change_brightness(self, img_arr, num_control_points=3):\n",
    "        out_img_arr = copy.copy(img_arr)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            control_points = (\n",
    "                np.add.accumulate(np.ones(num_control_points + 2)) - 1.0\n",
    "            ) / (num_control_points + 1)\n",
    "            control_point_locations = (control_points * 65535).astype(int)\n",
    "            orig_locations = copy.copy(control_point_locations)\n",
    "            random_points = np.random.uniform(\n",
    "                low=0, high=65535, size=num_control_points\n",
    "            ).astype(int)\n",
    "            sorted_points = np.sort(random_points)\n",
    "            control_point_locations[1:-1] = sorted_points\n",
    "            mapping = interpolate.PchipInterpolator(\n",
    "                orig_locations, control_point_locations\n",
    "            )\n",
    "            out_img_arr[t, 0, :, :] = mapping(img_arr[t, 0, :, :])\n",
    "        return out_img_arr\n",
    "\n",
    "    def add_padding(self, img_arr, seg_arr, max_rot=20, min_padding=20):\n",
    "        hyp_length = np.ceil(\n",
    "            (img_arr.shape[2] ** 2 + img_arr.shape[3] ** 2) ** (1 / 2)\n",
    "        ).astype(int)\n",
    "        max_rads = ((90 - max_rot) / 360) * (2 * np.pi)\n",
    "        min_rads = (90 / 360) * (2 * np.pi)\n",
    "        max_y = np.maximum(\n",
    "            np.ceil(hyp_length * np.sin(max_rads)),\n",
    "            np.ceil(hyp_length * np.sin(min_rads)),\n",
    "        ).astype(int)\n",
    "        max_x = np.maximum(\n",
    "            np.ceil(hyp_length * np.cos(max_rads)),\n",
    "            np.ceil(hyp_length * np.cos(min_rads)),\n",
    "        ).astype(int)\n",
    "        delta_y = max_y - img_arr.shape[2]\n",
    "        delta_x = max_x - img_arr.shape[3]\n",
    "        if delta_x % 2 == 1:\n",
    "            delta_x += 1\n",
    "        if delta_y % 2 == 1:\n",
    "            delta_y += 1\n",
    "        delta_y = np.maximum(delta_y, 2 * min_padding)\n",
    "        delta_x = np.maximum(delta_x, 2 * min_padding)\n",
    "        padded_img_arr = np.pad(\n",
    "            img_arr,\n",
    "            (\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (delta_y // 2, delta_y // 2),\n",
    "                (delta_x // 2, delta_x // 2),\n",
    "            ),\n",
    "            \"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        padded_seg_arr = np.pad(\n",
    "            seg_arr,\n",
    "            (\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (delta_y // 2, delta_y // 2),\n",
    "                (delta_x // 2, delta_x // 2),\n",
    "            ),\n",
    "            \"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        return padded_img_arr, padded_seg_arr\n",
    "\n",
    "    def translate(self, pad_img_arr, pad_seg_arr, img_arr, seg_arr):\n",
    "        trans_img_arr = copy.copy(pad_img_arr)\n",
    "        trans_seg_arr = copy.copy(pad_seg_arr)\n",
    "        delta_y = pad_img_arr.shape[2] - img_arr.shape[2]\n",
    "        delta_x = pad_img_arr.shape[3] - img_arr.shape[3]\n",
    "        for t in range(pad_img_arr.shape[0]):\n",
    "            trans_y = np.random.randint(-(delta_y // 2), high=delta_y // 2)\n",
    "            trans_x = np.random.randint(-(delta_x // 2), high=delta_x // 2)\n",
    "            trans_img_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ] = 0\n",
    "            trans_seg_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ] = 0\n",
    "            trans_img_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 + trans_y : delta_y // 2 + img_arr.shape[2] + trans_y,\n",
    "                delta_x // 2 + trans_x : delta_x // 2 + img_arr.shape[3] + trans_x,\n",
    "            ] = pad_img_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ]\n",
    "            trans_seg_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 + trans_y : delta_y // 2 + img_arr.shape[2] + trans_y,\n",
    "                delta_x // 2 + trans_x : delta_x // 2 + img_arr.shape[3] + trans_x,\n",
    "            ] = pad_seg_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ]\n",
    "        return trans_img_arr, trans_seg_arr\n",
    "\n",
    "    def rotate(self, img_arr, seg_arr, max_rot=20):\n",
    "        rot_img_arr = copy.copy(img_arr)\n",
    "        rot_seg_arr = copy.copy(seg_arr)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            r = np.random.uniform(low=-max_rot, high=max_rot)\n",
    "            rot_img_arr[t, 0, :, :] = sk.transform.rotate(\n",
    "                img_arr[t, 0, :, :], r, preserve_range=True\n",
    "            ).astype(\"int32\")\n",
    "            rot_seg = (sk.transform.rotate(seg_arr[t, 0, :, :] == 1, r) > 0.5).astype(\n",
    "                \"int8\"\n",
    "            )\n",
    "            if self.mode is \"m\":\n",
    "                rot_border = sk.transform.rotate(seg_arr[t, 0, :, :] == 2, r) > 0.5\n",
    "                rot_seg[rot_border] = 2\n",
    "            rot_seg_arr[t, 0, :, :] = rot_seg\n",
    "        return rot_img_arr, rot_seg_arr\n",
    "\n",
    "    def deform_img_arr(self, img_arr, seg_arr):\n",
    "        def_img_arr = copy.copy(img_arr)\n",
    "        def_seg_arr = copy.copy(seg_arr)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            y_steps = np.linspace(0.0, 4.0, num=img_arr.shape[2])\n",
    "            x_steps = np.linspace(0.0, 4.0, num=img_arr.shape[3])\n",
    "            grid = np.random.normal(scale=1.0, size=(2, 4, 4))\n",
    "            dx = RectBivariateSpline(np.arange(4), np.arange(4), grid[0]).ev(\n",
    "                y_steps[:, np.newaxis], x_steps[np.newaxis, :]\n",
    "            )\n",
    "            dy = RectBivariateSpline(np.arange(4), np.arange(4), grid[1]).ev(\n",
    "                y_steps[:, np.newaxis], x_steps[np.newaxis, :]\n",
    "            )\n",
    "            y, x = np.meshgrid(\n",
    "                np.arange(img_arr.shape[2]), np.arange(img_arr.shape[3]), indexing=\"ij\"\n",
    "            )\n",
    "            indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n",
    "            elastic_img = map_coordinates(\n",
    "                img_arr[t, 0, :, :], indices, order=1\n",
    "            ).reshape(img_arr.shape[2:4])\n",
    "\n",
    "            def_img_arr[t, 0, :, :] = elastic_img\n",
    "\n",
    "            elastic_cell = map_coordinates(\n",
    "                seg_arr[t, 0, :, :] == 1, indices, order=1\n",
    "            ).reshape(seg_arr.shape[2:4])\n",
    "            elastic_cell = sk.morphology.binary_closing(elastic_cell)\n",
    "            def_seg_arr[t, 0, elastic_cell] = 1\n",
    "            if self.mode is \"m\":\n",
    "                elastic_border = map_coordinates(\n",
    "                    seg_arr[t, 0, :, :] == 2, indices, order=1\n",
    "                ).reshape(seg_arr.shape[2:4])\n",
    "                def_seg_arr[t, 0, elastic_border] = 2\n",
    "        return def_img_arr, def_seg_arr\n",
    "\n",
    "    def add_borders(self, seg_arr):\n",
    "        output_arr = copy.copy(seg_arr)\n",
    "        for k in range(seg_arr.shape[0]):\n",
    "            working_mask = seg_arr[k, 0] == 1\n",
    "            expanded = sk.morphology.binary_dilation(working_mask)\n",
    "            border = expanded ^ working_mask\n",
    "            output_arr[k, 0, border] = 2\n",
    "        return output_arr\n",
    "\n",
    "    def repair_borders(self, seg_arr):\n",
    "        output_arr = copy.copy(seg_arr)\n",
    "        output_arr[output_arr == 2] = 0\n",
    "        output_arr = self.add_borders(output_arr)\n",
    "        return output_arr\n",
    "\n",
    "    def get_augmented_data(self, img_arr, seg_arr):\n",
    "        img_arr, seg_arr = (img_arr.astype(\"int32\"), seg_arr.astype(\"int8\"))\n",
    "        if self.mode is \"m\":\n",
    "            seg_arr = self.add_borders(seg_arr)\n",
    "        img_arr, seg_arr = self.random_crop(img_arr, seg_arr)\n",
    "        img_arr, seg_arr = self.random_x_flip(img_arr, seg_arr, p=self.p_flip)\n",
    "        img_arr, seg_arr = self.random_y_flip(img_arr, seg_arr, p=self.p_flip)\n",
    "        img_arr = self.change_brightness(img_arr)\n",
    "        pad_img_arr, pad_seg_arr = self.add_padding(\n",
    "            img_arr, seg_arr, max_rot=self.max_rot + 5\n",
    "        )\n",
    "        img_arr, seg_arr = self.translate(pad_img_arr, pad_seg_arr, img_arr, seg_arr)\n",
    "        del pad_img_arr\n",
    "        del pad_seg_arr\n",
    "        img_arr, seg_arr = self.rotate(img_arr, seg_arr, max_rot=self.max_rot)\n",
    "        img_arr, seg_arr = self.deform_img_arr(img_arr, seg_arr)\n",
    "        seg_arr = self.repair_borders(seg_arr)\n",
    "        #         img_arr,seg_arr = (img_arr.astype(\"int32\"),seg_arr.astype(\"int8\"))\n",
    "        return img_arr, seg_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_augmentation(mode=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 22\n",
    "t = 10\n",
    "\n",
    "with h5py.File(\n",
    "    \"/n/scratch2/de64/2019-05-31_validation_data/kymograph/kymograph_0.hdf5\", \"r\"\n",
    ") as infile:\n",
    "    img = infile[\"Phase\"][k, t]\n",
    "\n",
    "with h5py.File(\n",
    "    \"/n/scratch2/de64/2019-05-31_validation_data/fluorsegmentation/segmentation_0.hdf5\",\n",
    "    \"r\",\n",
    ") as infile:\n",
    "    seg = infile[\"data\"][k, t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1, 4, 1)\n",
    "ax.imshow(img)\n",
    "ax = plt.subplot(1, 4, 2)\n",
    "ax.imshow(seg)\n",
    "ax = plt.subplot(1, 4, 3)\n",
    "img_arr, seg_arr = test.get_augmented_data(\n",
    "    img[np.newaxis, np.newaxis], seg[np.newaxis, np.newaxis]\n",
    ")\n",
    "ax.imshow(img_arr[0, 0])\n",
    "ax = plt.subplot(1, 4, 4)\n",
    "ax.imshow(seg_arr[0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightmap_generator:\n",
    "    def __init__(self, mode, w0=0.0, wm_sigma=0.0):\n",
    "        if mode not in [\"a\", \"m\", \"w\"]:\n",
    "            raise ValueError(\"Not a valid augmentation mode\")\n",
    "        self.mode = mode\n",
    "        self.w0 = w0\n",
    "        self.wm_sigma = wm_sigma\n",
    "\n",
    "    def make_one_class_weightmap(self, single_mask):\n",
    "        ttl_count = single_mask.size\n",
    "\n",
    "        backround_mask = single_mask == 0\n",
    "        cell_mask = single_mask == 1\n",
    "\n",
    "        background_count = np.sum(backround_mask)\n",
    "        cell_count = np.sum(cell_mask)\n",
    "\n",
    "        class_weight = np.array(\n",
    "            [ttl_count / (background_count + 1), ttl_count / (cell_count + 1)]\n",
    "        )\n",
    "        class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "        weight_map = np.zeros(single_mask.shape, dtype=float)\n",
    "        weight_map[backround_mask] = class_weight[0]\n",
    "        weight_map[cell_mask] = class_weight[1]\n",
    "\n",
    "        return weight_map\n",
    "\n",
    "    def make_two_class_weightmap(self, single_mask):\n",
    "        ttl_count = single_mask.size\n",
    "\n",
    "        backround_mask = single_mask == 0\n",
    "        cell_mask = single_mask == 1\n",
    "        border_mask = single_mask == 2\n",
    "\n",
    "        background_count = np.sum(backround_mask)\n",
    "        cell_count = np.sum(cell_mask)\n",
    "        border_count = np.sum(border_mask)\n",
    "\n",
    "        class_weight = np.array(\n",
    "            [\n",
    "                ttl_count / (background_count + 1),\n",
    "                ttl_count / (cell_count + 1),\n",
    "                ttl_count / (border_count + 1),\n",
    "            ]\n",
    "        )\n",
    "        class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "        weight_map = np.zeros(single_mask.shape, dtype=float)\n",
    "        weight_map[backround_mask] = class_weight[0]\n",
    "        weight_map[cell_mask] = class_weight[1]\n",
    "        weight_map[border_mask] = class_weight[2]\n",
    "\n",
    "        return weight_map\n",
    "\n",
    "    def make_unet_weight_map(self, single_mask):\n",
    "        binary_mask = single_mask == 1\n",
    "\n",
    "        ttl_count = binary_mask.size\n",
    "        cell_count = np.sum(binary_mask == 1)\n",
    "        background_count = ttl_count - cell_count\n",
    "        class_weight = np.array(\n",
    "            [ttl_count / (background_count + 1), ttl_count / (cell_count + 1)]\n",
    "        )\n",
    "        class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "        labeled = sk.measure.label(binary_mask)\n",
    "        labels = np.unique(labeled)[1:]\n",
    "\n",
    "        dist_maps = []\n",
    "        borders = []\n",
    "\n",
    "        num_labels = len(labels)\n",
    "\n",
    "        if num_labels == 0:\n",
    "            weight_map = np.ones(binary_mask.shape) * class_weight[0]\n",
    "        elif num_labels == 1:\n",
    "            cell = labeled == 1\n",
    "            #             dilated = sk.morphology.binary_dilation(cell)\n",
    "            eroded = sk.morphology.binary_dilation(cell)\n",
    "            border = eroded ^ cell\n",
    "            weight_map = np.ones(binary_mask.shape) * class_weight[0]\n",
    "            weight_map[binary_mask] += class_weight[1]\n",
    "        #             weight[border] = 0.\n",
    "        else:\n",
    "            for i in labels:\n",
    "                cell = labeled == i\n",
    "                #                 dilated = sk.morphology.binary_dilation(cell)\n",
    "                eroded = sk.morphology.binary_dilation(cell)\n",
    "                border = eroded ^ cell\n",
    "                borders.append(border)\n",
    "                dist_map = scipy.ndimage.morphology.distance_transform_edt(~border)\n",
    "                dist_maps.append(dist_map)\n",
    "            dist_maps = np.array(dist_maps)\n",
    "            borders = np.array(borders)\n",
    "            borders = np.max(borders, axis=0)\n",
    "            dist_maps = np.sort(dist_maps, axis=0)\n",
    "            weight_map = self.w0 * np.exp(\n",
    "                -((dist_maps[0] + dist_maps[1]) ** 2) / (2 * (self.wm_sigma**2))\n",
    "            )\n",
    "            weight_map[binary_mask] += class_weight[1]\n",
    "            weight_map[~binary_mask] += class_weight[0]\n",
    "        #             weight[borders] = 0.\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_Training_DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nndatapath=\"\",\n",
    "        experimentname=\"\",\n",
    "        output_names=[\"train\", \"test\", \"val\"],\n",
    "        output_modes=[\"a\", \"m\"],\n",
    "        num_epochs=10,\n",
    "        input_paths=[],\n",
    "    ):\n",
    "        self.nndatapath = nndatapath\n",
    "        self.experimentname = experimentname\n",
    "        self.output_names = output_names\n",
    "        self.output_modes = output_modes\n",
    "        self.num_epochs = num_epochs\n",
    "        self.input_paths = input_paths\n",
    "\n",
    "        self.metapath = self.nndatapath + \"/metadata.hdf5\"\n",
    "\n",
    "    def get_metadata(self, headpath):\n",
    "        meta_handle = pandas_hdf5_handler(headpath + \"/metadata.hdf5\")\n",
    "        global_handle = meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        kymo_handle = meta_handle.read_df(\"kymograph\", read_metadata=True)\n",
    "        fovdf = kymo_handle.reset_index(inplace=False)\n",
    "        fovdf = fovdf.set_index(\n",
    "            [\"fov\", \"row\", \"trench\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        fovdf = fovdf.sort_index()\n",
    "\n",
    "        channel_list = global_handle.metadata[\"channels\"]\n",
    "        fov_list = kymo_handle[\"fov\"].unique().tolist()\n",
    "        t_len = len(kymo_handle.index.get_level_values(\"timepoints\").unique())\n",
    "        ttl_trenches = len(fovdf[\"trenchid\"].unique())\n",
    "        trench_dict = {\n",
    "            fov: len(fovdf.loc[fov][\"trenchid\"].unique()) for fov in fov_list\n",
    "        }\n",
    "        shape_y = kymo_handle.metadata[\"kymograph_params\"][\"ttl_len_y\"]\n",
    "        shape_x = kymo_handle.metadata[\"kymograph_params\"][\"trench_width_x\"]\n",
    "        kymograph_img_shape = tuple((shape_y, shape_x))\n",
    "        return (\n",
    "            channel_list,\n",
    "            fov_list,\n",
    "            t_len,\n",
    "            trench_dict,\n",
    "            ttl_trenches,\n",
    "            kymograph_img_shape,\n",
    "        )\n",
    "\n",
    "    def inter_get_selection(self):\n",
    "        output_tabs = []\n",
    "        for i in range(len(self.output_names)):\n",
    "            dset_tabs = []\n",
    "            for j in range(len(self.input_paths)):\n",
    "                (\n",
    "                    channel_list,\n",
    "                    fov_list,\n",
    "                    t_len,\n",
    "                    trench_dict,\n",
    "                    ttl_trenches,\n",
    "                    kymograph_img_shape,\n",
    "                ) = self.get_metadata(self.input_paths[j])\n",
    "\n",
    "                feature_dropdown = ipyw.Dropdown(\n",
    "                    options=channel_list,\n",
    "                    value=channel_list[0],\n",
    "                    description=\"Feature Channel:\",\n",
    "                    disabled=False,\n",
    "                )\n",
    "                max_samples = ipyw.IntText(\n",
    "                    value=0, description=\"Maximum Samples per Dataset:\", disabled=False\n",
    "                )\n",
    "                t_range = ipyw.IntRangeSlider(\n",
    "                    value=[0, t_len - 1],\n",
    "                    description=\"Timepoint Range:\",\n",
    "                    min=0,\n",
    "                    max=t_len - 1,\n",
    "                    step=1,\n",
    "                    disabled=False,\n",
    "                    continuous_update=False,\n",
    "                )\n",
    "\n",
    "                working_tab = ipyw.VBox(\n",
    "                    children=[feature_dropdown, max_samples, t_range]\n",
    "                )\n",
    "                dset_tabs.append(working_tab)\n",
    "\n",
    "            dset_ipy_tabs = ipyw.Tab(children=dset_tabs)\n",
    "            for j in range(len(self.input_paths)):\n",
    "                dset_ipy_tabs.set_title(j, self.input_paths[j].split(\"/\")[-1])\n",
    "            output_tabs.append(dset_ipy_tabs)\n",
    "        output_ipy_tabs = ipyw.Tab(children=output_tabs)\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            output_ipy_tabs.set_title(i, output_name)\n",
    "        self.tab = output_ipy_tabs\n",
    "\n",
    "        return self.tab\n",
    "\n",
    "    def get_import_params(self):\n",
    "        self.import_param_dict = {}\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            self.import_param_dict[output_name] = {}\n",
    "            for j, input_path in enumerate(self.input_paths):\n",
    "                working_vbox = self.tab.children[i].children[j]\n",
    "                self.import_param_dict[output_name][input_path] = {\n",
    "                    child.description: child.value for child in working_vbox.children\n",
    "                }\n",
    "\n",
    "        print(\"======== Import Params ========\")\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            print(str(output_name))\n",
    "            for j, input_path in enumerate(self.input_paths):\n",
    "                (\n",
    "                    channel_list,\n",
    "                    fov_list,\n",
    "                    t_len,\n",
    "                    trench_dict,\n",
    "                    ttl_trenches,\n",
    "                    kymograph_img_shape,\n",
    "                ) = self.get_metadata(input_path)\n",
    "                ttl_possible_samples = t_len * ttl_trenches\n",
    "                param_dict = self.import_param_dict[output_name][input_path]\n",
    "                requested_samples = param_dict[\"Maximum Samples per Dataset:\"]\n",
    "                if requested_samples > 0:\n",
    "                    print(str(input_path))\n",
    "                    for key, val in param_dict.items():\n",
    "                        print(key + \" \" + str(val))\n",
    "                    print(\n",
    "                        \"Requested Samples / Total Samples: \"\n",
    "                        + str(requested_samples)\n",
    "                        + \"/\"\n",
    "                        + str(ttl_possible_samples)\n",
    "                    )\n",
    "\n",
    "        del self.tab\n",
    "\n",
    "    def export_chunk(self, output_name, init_idx, chunk_size, chunk_idx):\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        output_df = output_meta_handle.read_df(output_name)\n",
    "        working_df = output_df[init_idx : init_idx + chunk_size]\n",
    "        nndatapath = (\n",
    "            self.nndatapath + \"/\" + output_name + \"_\" + str(chunk_idx) + \".hdf5\"\n",
    "        )\n",
    "\n",
    "        dset_paths = working_df.index.get_level_values(0).unique().tolist()\n",
    "        for dset_path in dset_paths:\n",
    "            dset_path_key = dset_path.split(\"/\")[-1]\n",
    "            dset_df = working_df.loc[dset_path]\n",
    "            param_dict = self.import_param_dict[output_name][dset_path]\n",
    "            feature_channel = param_dict[\"Feature Channel:\"]\n",
    "\n",
    "            img_arr_list = []\n",
    "            seg_arr_list = []\n",
    "\n",
    "            file_indices = dset_df.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "            for file_idx in file_indices:\n",
    "                file_df = dset_df.loc[file_idx]\n",
    "\n",
    "                img_path = dset_path + \"/kymograph/kymograph_\" + str(file_idx) + \".hdf5\"\n",
    "                seg_path = (\n",
    "                    dset_path\n",
    "                    + \"/fluorsegmentation/segmentation_\"\n",
    "                    + str(file_idx)\n",
    "                    + \".hdf5\"\n",
    "                )\n",
    "\n",
    "                with h5py.File(img_path, \"r\") as imgfile:\n",
    "                    working_arr = imgfile[feature_channel][:]\n",
    "\n",
    "                for trench_idx, row in file_df.iterrows():\n",
    "                    img_arr = working_arr[trench_idx, row[\"timepoints\"]][\n",
    "                        np.newaxis, np.newaxis, :, :\n",
    "                    ]  # 1,1,y,x img\n",
    "                    img_arr = img_arr.astype(\"int32\")\n",
    "                    img_arr_list.append(img_arr)\n",
    "\n",
    "                with h5py.File(seg_path, \"r\") as segfile:\n",
    "                    working_arr = segfile[\"data\"][:]\n",
    "\n",
    "                for trench_idx, row in file_df.iterrows():\n",
    "                    seg_arr = working_arr[trench_idx, row[\"timepoints\"]][\n",
    "                        np.newaxis, np.newaxis, :, :\n",
    "                    ]\n",
    "                    seg_arr = seg_arr.astype(\"int8\")\n",
    "                    seg_arr_list.append(seg_arr)\n",
    "\n",
    "            output_img_arr = np.concatenate(img_arr_list, axis=0)\n",
    "            output_seg_arr = np.concatenate(seg_arr_list, axis=0)\n",
    "            chunk_shape = (1, 1, output_img_arr.shape[2], output_img_arr.shape[3])\n",
    "\n",
    "            with h5py.File(nndatapath, \"w\") as outfile:\n",
    "                for output_mode in self.output_modes:\n",
    "                    augmenter = data_augmentation(mode=output_mode)\n",
    "                    for epoch in range(self.num_epochs):\n",
    "                        img_arr, seg_arr = augmenter.get_augmented_data(\n",
    "                            output_img_arr, output_seg_arr\n",
    "                        )\n",
    "                        img_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/img\",\n",
    "                            data=img_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int32\",\n",
    "                        )\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/seg\",\n",
    "                            data=seg_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "        return init_idx\n",
    "\n",
    "    #         if augment:\n",
    "    #             img_arr,seg_arr = self.data_augmentation.get_augmented_data(img_arr,seg_arr)\n",
    "\n",
    "    #         chunk_shape = (1,1,img_arr.shape[2],img_arr.shape[3])\n",
    "\n",
    "    #         with h5py.File(nndatapath,\"w\") as outfile:\n",
    "    #             img_handle = outfile.create_dataset(\"img\",data=img_arr,chunks=chunk_shape,dtype='int32')\n",
    "    #             seg_handle = outfile.create_dataset(\"seg\",data=seg_arr,chunks=chunk_shape,dtype='int8')\n",
    "\n",
    "    #         for item in weight_grid_list:\n",
    "    #             w0,wm_sigma = item\n",
    "    #             weightmap_gen = weightmap_generator(self.nndatapath,w0,wm_sigma)\n",
    "    #             weightmap_arr = weightmap_gen.make_weightmaps(seg_arr)\n",
    "    #             with h5py.File(nndatapath,\"a\") as outfile:\n",
    "    #                 weightmap_handle = outfile.create_dataset(\"weight_\" + str(item),data=weightmap_arr,chunks=chunk_shape,dtype='int32')\n",
    "\n",
    "    #         return file_idx\n",
    "\n",
    "    def gather_chunks(self, output_name, init_idx_list, chunk_idx_list, chunk_size):\n",
    "\n",
    "        #                       outputdf,output_metadata,selectionname,file_idx_list,weight_grid_list):\n",
    "        nnoutputpath = self.nndatapath + \"/\" + output_name + \".hdf5\"\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        output_df = output_meta_handle.read_df(output_name)\n",
    "\n",
    "        dset_paths = output_df.index.get_level_values(0).unique().tolist()\n",
    "        for dset_path in dset_paths:\n",
    "            dset_path_key = dset_path.split(\"/\")[-1]\n",
    "            dset_df = output_df.loc[dset_path]\n",
    "\n",
    "            tempdatapath = self.nndatapath + \"/\" + output_name + \"_0.hdf5\"\n",
    "            with h5py.File(tempdatapath, \"r\") as infile:\n",
    "                img_shape = infile[\n",
    "                    dset_path_key + \"/\" + self.output_modes[0] + \"/epoch_0/img\"\n",
    "                ].shape\n",
    "\n",
    "            output_shape = (len(dset_df.index), 1, img_shape[2], img_shape[3])\n",
    "            chunk_shape = (1, 1, img_shape[2], img_shape[3])\n",
    "\n",
    "            with h5py.File(nnoutputpath, \"w\") as outfile:\n",
    "                for output_mode in self.output_modes:\n",
    "                    for epoch in range(self.num_epochs):\n",
    "                        img_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/img\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int32\",\n",
    "                        )\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/seg\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "        #             for item in weight_grid_list:\n",
    "        #                 weightmap_handle = outfile.create_dataset(\"weight_\" + str(item),output_shape,chunks=chunk_shape,dtype='int32')\n",
    "        current_dset_path = \"\"\n",
    "        for i, init_idx in enumerate(init_idx_list):\n",
    "            chunk_idx = chunk_idx_list[i]\n",
    "            nndatapath = (\n",
    "                self.nndatapath + \"/\" + output_name + \"_\" + str(chunk_idx) + \".hdf5\"\n",
    "            )\n",
    "            working_df = output_df[init_idx : init_idx + chunk_size]\n",
    "            dset_paths = working_df.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "            with h5py.File(nndatapath, \"r\") as infile:\n",
    "\n",
    "                for dset_path in dset_paths:\n",
    "                    if dset_path != current_dset_path:\n",
    "                        current_idx = 0\n",
    "                        current_dset_path = dset_path\n",
    "\n",
    "                    dset_path_key = dset_path.split(\"/\")[-1]\n",
    "                    dset_df = output_df.loc[dset_path]\n",
    "\n",
    "                    with h5py.File(nnoutputpath, \"a\") as outfile:\n",
    "\n",
    "                        for output_mode in self.output_modes:\n",
    "                            for epoch in range(self.num_epochs):\n",
    "                                img_arr = infile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/img\"\n",
    "                                ][\n",
    "                                    :\n",
    "                                ]  # k,1,y,x\n",
    "                                seg_arr = infile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/seg\"\n",
    "                                ][\n",
    "                                    :\n",
    "                                ]  # k,1,y,x\n",
    "                                num_indices = img_arr.shape[0]\n",
    "\n",
    "                                outfile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/img\"\n",
    "                                ][current_idx : current_idx + num_indices] = img_arr\n",
    "                                outfile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/seg\"\n",
    "                                ][current_idx : current_idx + num_indices] = seg_arr\n",
    "                    current_idx += num_indices\n",
    "\n",
    "            os.remove(nndatapath)\n",
    "\n",
    "    #                 img_arr = infile[\"img\"][:]\n",
    "    #                 seg_arr = infile[\"seg\"][:]\n",
    "    #                 weight_arr_list = []\n",
    "    #                 for item in weight_grid_list:\n",
    "    #                     weight_arr_list.append(infile[\"weight_\" + str(item)][:])\n",
    "    #             num_indices = img_arr.shape[0]\n",
    "    #             with h5py.File(nnoutputpath,\"a\") as outfile:\n",
    "    #                 outfile[\"img\"][current_idx:current_idx+num_indices] = img_arr\n",
    "    #                 outfile[\"seg\"][current_idx:current_idx+num_indices] = seg_arr\n",
    "    #                 for i,item in enumerate(weight_grid_list):\n",
    "    #                     outfile[\"weight_\" + str(item)][current_idx:current_idx+num_indices] = weight_arr_list[i]\n",
    "    #             current_idx += num_indices\n",
    "    #             os.remove(nndatapath)\n",
    "\n",
    "    #     def export_data(self,dask_controller):\n",
    "    def export_data(self, dask_controller, chunk_size=250):\n",
    "\n",
    "        dask_controller.futures = {}\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        all_output_dfs = {}\n",
    "\n",
    "        for output_name, _ in self.import_param_dict.items():\n",
    "            output_df = []\n",
    "\n",
    "            for input_path, param_dict in self.import_param_dict[output_name].items():\n",
    "                input_meta_handle = pandas_hdf5_handler(input_path + \"/metadata.hdf5\")\n",
    "\n",
    "                num_samples = param_dict[\"Maximum Samples per Dataset:\"]\n",
    "                feature_channel = param_dict[\"Feature Channel:\"]\n",
    "                t_range = param_dict[\"Timepoint Range:\"]\n",
    "\n",
    "                kymodf = input_meta_handle.read_df(\"kymograph\", read_metadata=True)\n",
    "                kymodf[\"filepath\"] = input_path\n",
    "                trenchdf = kymodf.reset_index(inplace=False)\n",
    "                trenchdf = trenchdf.set_index(\n",
    "                    [\"filepath\", \"trenchid\", \"timepoints\"],\n",
    "                    drop=True,\n",
    "                    append=False,\n",
    "                    inplace=False,\n",
    "                )\n",
    "                trenchdf = trenchdf.sort_index()\n",
    "                trenchdf = trenchdf.loc[\n",
    "                    pd.IndexSlice[:, :, t_range[0] : t_range[1] + 1], :\n",
    "                ]\n",
    "\n",
    "                trenchdf_subset = trenchdf.sample(n=num_samples)\n",
    "                filedf_subset = trenchdf_subset.reset_index(inplace=False)\n",
    "                filedf_subset = filedf_subset.set_index(\n",
    "                    [\"filepath\", \"File Index\", \"File Trench Index\"],\n",
    "                    drop=True,\n",
    "                    append=False,\n",
    "                    inplace=False,\n",
    "                )\n",
    "                filedf_subset = filedf_subset.sort_index()\n",
    "                output_df.append(filedf_subset)\n",
    "            output_df = pd.concat(output_df)\n",
    "            output_meta_handle.write_df(output_name, output_df)\n",
    "            all_output_dfs[output_name] = output_df\n",
    "\n",
    "        for output_name in all_output_dfs.keys():\n",
    "\n",
    "            output_df = all_output_dfs[output_name]\n",
    "\n",
    "            ## split into equal computation chunks here\n",
    "\n",
    "            chunk_idx_list = []\n",
    "            for chunk_idx, init_idx in enumerate(range(0, len(output_df), chunk_size)):\n",
    "                future = dask_controller.daskclient.submit(\n",
    "                    self.export_chunk,\n",
    "                    output_name,\n",
    "                    init_idx,\n",
    "                    chunk_size,\n",
    "                    chunk_idx,\n",
    "                    retries=1,\n",
    "                )\n",
    "                dask_controller.futures[\"Chunk Number: \" + str(chunk_idx)] = future\n",
    "                chunk_idx_list.append(chunk_idx)\n",
    "\n",
    "            init_idx_list = dask_controller.daskclient.gather(\n",
    "                [\n",
    "                    dask_controller.futures[\"Chunk Number: \" + str(chunk_idx)]\n",
    "                    for chunk_idx in chunk_idx_list\n",
    "                ]\n",
    "            )\n",
    "            self.gather_chunks(output_name, init_idx_list, chunk_idx_list, chunk_size)\n",
    "\n",
    "\n",
    "#         outputdf = filedf.reset_index(inplace=False)\n",
    "#         outputdf = outputdf.set_index([\"trenchid\",\"timepoints\"], drop=True, append=False, inplace=False)\n",
    "#         outputdf = outputdf.sort_index()\n",
    "\n",
    "#         del outputdf[\"File Index\"]\n",
    "#         del outputdf[\"File Trench Index\"]\n",
    "\n",
    "#         selection_keys = [\"channel\", \"fov_list\", \"t_subsample_step\", \"t_range\", \"max_trenches\", \"ttl_imgs\", \"kymograph_img_shape\"]\n",
    "#         selection = {selection_keys[i]:item for i,item in enumerate(selection)}\n",
    "#         selection[\"experiment_name\"],selection[\"data_name\"] = (self.experimentname, dataname)\n",
    "#         selection[\"W0 List\"], selection[\"Wm Sigma List\"] = (self.grid_dict['W0 (Border Region Weight):'],self.grid_dict['Wm Sigma (Border Region Spread):'])\n",
    "\n",
    "#         output_metadata = {\"nndataset\" : selection}\n",
    "\n",
    "#         segparampath = datapath + \"/fluorescent_segmentation.par\"\n",
    "#         with open(segparampath, 'rb') as infile:\n",
    "#             seg_param_dict = pkl.load(infile)\n",
    "\n",
    "#         output_metadata[\"segmentation\"] = seg_param_dict\n",
    "\n",
    "#         input_meta_handle = pandas_hdf5_handler(datapath + \"/metadata.hdf5\")\n",
    "#         for item in [\"global\",\"kymograph\"]:\n",
    "#             indf = input_meta_handle.read_df(item,read_metadata=True)\n",
    "#             output_metadata[item] = indf.metadata\n",
    "\n",
    "#         output_meta_handle.write_df(selectionname,outputdf,metadata=output_metadata)\n",
    "\n",
    "#         file_idx_list = dask_controller.daskclient.gather([dask_controller.futures[\"File Number: \" + str(file_idx)] for file_idx in filelist])\n",
    "#         self.gather_chunks(outputdf,output_metadata,selectionname,file_idx_list,weight_grid_list)\n",
    "\n",
    "\n",
    "#     def display_grid(self):\n",
    "#         tab_dict = {'W0 (Border Region Weight):':[1., 3., 5., 10.],'Wm Sigma (Border Region Spread):':[1., 2., 3., 4., 5.]}\n",
    "#         children = [ipyw.SelectMultiple(options=val,value=(val[1],),description=key,disabled=False) for key,val in tab_dict.items()]\n",
    "#         self.tab = ipyw.Tab()\n",
    "#         self.tab.children = children\n",
    "#         for i,key in enumerate(tab_dict.keys()):\n",
    "#             self.tab.set_title(i, key[:-1])\n",
    "#         return self.tab\n",
    "\n",
    "#     def get_grid_params(self):\n",
    "#         if hasattr(self,'tab'):\n",
    "#             self.grid_dict = {child.description:child.value for child in self.tab.children}\n",
    "#             delattr(self, 'tab')\n",
    "#         elif hasattr(self,'grid_dict'):\n",
    "#             pass\n",
    "#         else:\n",
    "#             raise \"No selection defined.\"\n",
    "#         print(\"======== Grid Params ========\")\n",
    "#         for key,val in self.grid_dict.items():\n",
    "#             print(key + \" \" + str(val))\n",
    "\n",
    "#     def export_all_data(self,n_workers=20,memory='4GB'):\n",
    "#         writedir(self.nndatapath,overwrite=True)\n",
    "\n",
    "#         grid_keys = self.grid_dict.keys()\n",
    "#         grid_combinations = list(itertools.product(*list(self.grid_dict.values())))\n",
    "\n",
    "#         self.data_augmentation = data_augmentation()\n",
    "\n",
    "#         dask_cont = dask_controller(walltime='01:00:00',local=False,n_workers=n_workers,memory=memory)\n",
    "#         dask_cont.startdask()\n",
    "# #         dask_cont.daskcluster.start_workers()\n",
    "#         dask_cont.displaydashboard()\n",
    "\n",
    "#         try:\n",
    "#             for selectionname in [\"train\",\"test\",\"val\"]:\n",
    "#                 if selectionname == \"train\":\n",
    "#                     self.export_data(selectionname,dask_cont,grid_combinations,augment=True)\n",
    "#        dataloader         else:\n",
    "#                     self.export_data(selectionname,dask_cont,grid_combinations,augment=False)\n",
    "#             dask_cont.shutdown()\n",
    "#         except:\n",
    "#             dask_cont.shutdown()\n",
    "#             raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = UNet_Training_DataLoader(\n",
    "    nndatapath=\"/n/scratch2/de64/nntest7\",\n",
    "    experimentname=\"First NN\",\n",
    "    input_paths=[\"/n/scratch2/de64/2019-05-31_validation_data\"],\n",
    "    output_modes=[\"a\", \"m\"],\n",
    "    num_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inter_get_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.get_import_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.export_data(dask_controller, chunk_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/n/scratch2/de64/nntest7/train.hdf5\", \"r\") as infile:\n",
    "    data = infile[\"2019-05-31_validation_data/m/epoch_0/img\"][:1000]\n",
    "    segdata = infile[\"2019-05-31_validation_data/m/epoch_0/seg\"][:1000]\n",
    "    data1 = infile[\"2019-05-31_validation_data/m/epoch_1/img\"][:1000]\n",
    "    segdata1 = infile[\"2019-05-31_validation_data/m/epoch_1/seg\"][:1000]\n",
    "#     img_arr = infile[\"2019-05-31_validation_data/img\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "idx = 110\n",
    "plt.imshow(data[idx, 0])\n",
    "plt.show()\n",
    "plt.imshow(segdata[idx, 0])\n",
    "plt.show()\n",
    "plt.imshow(data1[idx, 0])\n",
    "plt.show()\n",
    "plt.imshow(segdata1[idx, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.import_param_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
