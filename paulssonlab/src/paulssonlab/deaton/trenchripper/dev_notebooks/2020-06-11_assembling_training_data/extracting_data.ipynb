{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "import copy\n",
    "import os\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "\n",
    "import h5py\n",
    "import h5py_cache\n",
    "import ipywidgets as ipyw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nd2reader import ND2Reader\n",
    "from parse import compile\n",
    "from tifffile import imread, imsave\n",
    "\n",
    "from .utils import pandas_hdf5_handler, writedir\n",
    "\n",
    "\n",
    "class hdf5_fov_extractor:\n",
    "    def __init__(self,nd2filename,headpath,tpts_per_file=100,ignore_fovmetadata=False,nd2reader_override={}): #note this chunk size has a large role in downstream steps...make sure is less than 1 MB\n",
    "        self.nd2filename = nd2filename\n",
    "        self.headpath = headpath\n",
    "        self.metapath = self.headpath + \"/metadata.hdf5\"\n",
    "        self.hdf5path = self.headpath + \"/hdf5\"\n",
    "        self.tpts_per_file = tpts_per_file\n",
    "        self.ignore_fovmetadata = ignore_fovmetadata\n",
    "        self.nd2reader_override = nd2reader_override\n",
    "\n",
    "        self.organism = ''\n",
    "        self.microscope = ''\n",
    "        self.notes = ''\n",
    "\n",
    "    def writemetadata(self,t_range=None,fov_list=None):\n",
    "        ndmeta_handle = nd_metadata_handler(self.nd2filename,ignore_fovmetadata=self.ignore_fovmetadata,nd2reader_override=self.nd2reader_override)\n",
    "        if self.ignore_fovmetadata:\n",
    "            exp_metadata = ndmeta_handle.get_metadata()\n",
    "        else:\n",
    "            exp_metadata,fov_metadata = ndmeta_handle.get_metadata()\n",
    "\n",
    "        if t_range is not None:\n",
    "            exp_metadata[\"frames\"] = exp_metadata[\"frames\"][t_range[0]:t_range[1]+1]\n",
    "            exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "            fov_metadata = fov_metadata.loc[pd.IndexSlice[:,slice(t_range[0],t_range[1])],:]  #4 -> 70\n",
    "\n",
    "        if fov_list is not None:\n",
    "            fov_metadata = fov_metadata.loc[list(fov_list)]\n",
    "            exp_metadata[\"fields_of_view\"] = list(fov_list)\n",
    "\n",
    "        self.chunk_shape = (1,exp_metadata[\"height\"],exp_metadata[\"width\"])\n",
    "        chunk_bytes = (2*np.multiply.accumulate(np.array(self.chunk_shape))[-1])\n",
    "        self.chunk_cache_mem_size = 2*chunk_bytes\n",
    "        exp_metadata[\"chunk_shape\"],exp_metadata[\"chunk_cache_mem_size\"] = (self.chunk_shape,self.chunk_cache_mem_size)\n",
    "        exp_metadata[\"Organism\"],exp_metadata[\"Microscope\"],exp_metadata[\"Notes\"] = (self.organism,self.microscope,self.notes)\n",
    "        self.meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "\n",
    "        if self.ignore_fovmetadata:\n",
    "            assignment_metadata = self.assignidx(exp_metadata,metadf=None)\n",
    "            assignment_metadata.astype({\"File Index\":int,\"Image Index\":int})\n",
    "        else:\n",
    "            assignment_metadata = self.assignidx(exp_metadata,metadf=fov_metadata)\n",
    "            assignment_metadata.astype({\"t\":float,\"x\": float,\"y\":float,\"z\":float,\"File Index\":int,\"Image Index\":int})\n",
    "\n",
    "        self.meta_handle.write_df(\"global\",assignment_metadata,metadata=exp_metadata)\n",
    "\n",
    "    def assignidx(self,expmeta,metadf=None):\n",
    "\n",
    "        if metadf is None:\n",
    "            numfovs = len(expmeta[\"fields_of_view\"])\n",
    "            timepoints_per_fov = len(expmeta[\"frames\"])\n",
    "\n",
    "        else:\n",
    "            numfovs = len(metadf.index.get_level_values(0).unique().tolist())\n",
    "            timepoints_per_fov = len(metadf.index.get_level_values(1).unique().tolist())\n",
    "\n",
    "        files_per_fov = (timepoints_per_fov//self.tpts_per_file) + 1\n",
    "        remainder = timepoints_per_fov%self.tpts_per_file\n",
    "        ttlfiles = numfovs*files_per_fov\n",
    "        fov_file_idx = np.repeat(list(range(files_per_fov)), self.tpts_per_file)[:-(self.tpts_per_file-remainder)]\n",
    "        file_idx = np.concatenate([fov_file_idx+(fov_idx*files_per_fov) for fov_idx in range(numfovs)])\n",
    "        fov_img_idx = np.repeat(np.array(list(range(self.tpts_per_file)))[np.newaxis,:],files_per_fov,axis=0)\n",
    "        fov_img_idx = fov_img_idx.flatten()[:-(self.tpts_per_file-remainder)]\n",
    "        img_idx = np.concatenate([fov_img_idx for fov_idx in range(numfovs)])\n",
    "\n",
    "        if metadf is None:\n",
    "            fov_idx = np.repeat(list(range(numfovs)), timepoints_per_fov)\n",
    "            timepoint_idx = np.repeat(np.array(list(range(timepoints_per_fov)))[np.newaxis,:],numfovs,axis=0).flatten()\n",
    "\n",
    "            data = {\"fov\" : fov_idx,\"timepoints\" : timepoint_idx,\"File Index\" : file_idx, \"Image Index\" : img_idx}\n",
    "            outdf = pd.DataFrame(data)\n",
    "            outdf = outdf.set_index([\"fov\",\"timepoints\"], drop=True, append=False, inplace=False)\n",
    "\n",
    "        else:\n",
    "            outdf = copy.deepcopy(metadf)\n",
    "            outdf[\"File Index\"] = file_idx\n",
    "            outdf[\"Image Index\"] = img_idx\n",
    "        return outdf\n",
    "\n",
    "    def read_metadata(self):\n",
    "        writedir(self.hdf5path,overwrite=True)\n",
    "        self.writemetadata()\n",
    "        metadf = self.meta_handle.read_df(\"global\",read_metadata=True)\n",
    "        self.metadata = metadf.metadata\n",
    "        metadf = metadf.reset_index(inplace=False)\n",
    "        metadf = metadf.set_index([\"File Index\",\"Image Index\"], drop=True, append=False, inplace=False)\n",
    "        self.metadf = metadf.sort_index()\n",
    "\n",
    "    def set_params(self,fov_list,t_range,organism,microscope,notes):\n",
    "        self.fov_list = fov_list\n",
    "        self.t_range = t_range\n",
    "        self.organism = organism\n",
    "        self.microscope = microscope\n",
    "        self.notes = notes\n",
    "\n",
    "    def inter_set_params(self):\n",
    "        self.read_metadata()\n",
    "        t0,tf = (self.metadata['frames'][0],self.metadata['frames'][-1])\n",
    "        available_fov_list = self.metadf[\"fov\"].unique().tolist()\n",
    "        selection = ipyw.interactive(self.set_params, {\"manual\":True}, fov_list=ipyw.SelectMultiple(options=available_fov_list),\\\n",
    "                t_range=ipyw.IntRangeSlider(value=[t0, tf],\\\n",
    "                min=t0,max=tf,step=1,description='Time Range:',disabled=False), organism=ipyw.Textarea(value='',\\\n",
    "                placeholder='Organism imaged in this experiment.',description='Organism:',disabled=False),\\\n",
    "                microscope=ipyw.Textarea(value='',placeholder='Microscope used in this experiment.',\\\n",
    "                description='Microscope:',disabled=False),notes=ipyw.Textarea(value='',\\\n",
    "                placeholder='General experiment notes.',description='Notes:',disabled=False),)\n",
    "        display(selection)\n",
    "\n",
    "    def extract(self,dask_controller,retries=1):\n",
    "        dask_controller.futures = {}\n",
    "\n",
    "        self.writemetadata(t_range=self.t_range,fov_list=self.fov_list)\n",
    "        metadf = self.meta_handle.read_df(\"global\",read_metadata=True)\n",
    "        self.metadata = metadf.metadata\n",
    "        metadf = metadf.reset_index(inplace=False)\n",
    "        metadf = metadf.set_index([\"File Index\",\"Image Index\"], drop=True, append=False, inplace=False)\n",
    "        self.metadf = metadf.sort_index()\n",
    "\n",
    "        def writehdf5(fovnum,num_entries,timepoint_list,file_idx,num_fovs):\n",
    "            with ND2Reader(self.nd2filename) as nd2file:\n",
    "                for key,item in self.nd2reader_override.items():\n",
    "                    nd2file.metadata[key] = item\n",
    "                y_dim = self.metadata['height']\n",
    "                x_dim = self.metadata['width']\n",
    "                with h5py_cache.File(self.hdf5path + \"/hdf5_\" + str(file_idx) + \".hdf5\",\"w\",chunk_cache_mem_size=self.chunk_cache_mem_size) as h5pyfile:\n",
    "                    for i,channel in enumerate(self.metadata[\"channels\"]):\n",
    "                        hdf5_dataset = h5pyfile.create_dataset(str(channel),\\\n",
    "                        (num_entries,y_dim,x_dim), chunks=self.chunk_shape, dtype='uint16')\n",
    "                        for j in range(len(timepoint_list)):\n",
    "                            frame = timepoint_list[j]\n",
    "                            nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
    "                            hdf5_dataset[j,:,:] = nd2_image\n",
    "            return \"Done.\"\n",
    "\n",
    "        file_list = self.metadf.index.get_level_values(\"File Index\").unique().values\n",
    "        num_jobs = len(file_list)\n",
    "        random_priorities = np.random.uniform(size=(num_jobs,))\n",
    "\n",
    "        for k,file_idx in enumerate(file_list):\n",
    "            priority = random_priorities[k]\n",
    "            filedf = self.metadf.loc[file_idx]\n",
    "\n",
    "            fovnum = filedf[0:1][\"fov\"].values[0]\n",
    "            num_entries = len(filedf.index.get_level_values(\"Image Index\").values)\n",
    "            timepoint_list = filedf[\"timepoints\"].tolist()\n",
    "\n",
    "            future = dask_controller.daskclient.submit(writehdf5,fovnum,num_entries,timepoint_list,file_idx,self.metadata[\"num_fovs\"],retries=retries,priority=priority)\n",
    "            dask_controller.futures[\"extract file: \" + str(file_idx)] = future\n",
    "\n",
    "        extracted_futures = [dask_controller.futures[\"extract file: \" + str(file_idx)] for file_idx in file_list]\n",
    "        pause_for_extract = dask_controller.daskclient.gather(extracted_futures,errors='skip')\n",
    "\n",
    "        futures_name_list = [\"extract file: \" + str(file_idx) for file_idx in file_list]\n",
    "        failed_files = [futures_name_list[k] for k,item in enumerate(extracted_futures) if item.status is not \"finished\"]\n",
    "        failed_file_idx = [int(item.split(\":\")[1]) for item in failed_files]\n",
    "        outdf = self.meta_handle.read_df(\"global\",read_metadata=False)\n",
    "\n",
    "        tempmeta = outdf.reset_index(inplace=False)\n",
    "        tempmeta = tempmeta.set_index([\"File Index\",\"Image Index\"], drop=True, append=False, inplace=False)\n",
    "        failed_fovs = tempmeta.loc[failed_file_idx][\"fov\"].unique().tolist()\n",
    "\n",
    "        outdf  = outdf.drop(failed_fovs)\n",
    "\n",
    "        if self.t_range is not None:\n",
    "            outdf = outdf.reset_index(inplace=False)\n",
    "            outdf[\"timepoints\"] = outdf[\"timepoints\"] - self.t_range[0]\n",
    "            outdf = outdf.set_index([\"fov\",\"timepoints\"], drop=True, append=False, inplace=False)\n",
    "\n",
    "        self.meta_handle.write_df(\"global\",outdf,metadata=self.metadata)\n",
    "\n",
    "class nd_metadata_handler:\n",
    "    def __init__(self,nd2filename,ignore_fovmetadata=False,nd2reader_override={}):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.ignore_fovmetadata = ignore_fovmetadata\n",
    "        self.nd2reader_override = nd2reader_override\n",
    "\n",
    "    def decode_unidict(self,unidict):\n",
    "        outdict = {}\n",
    "        for key, val in unidict.items():\n",
    "            if type(key) == bytes:\n",
    "                key = key.decode('utf8')\n",
    "            if type(val) == bytes:\n",
    "                val = val.decode('utf8')\n",
    "            outdict[key] = val\n",
    "        return outdict\n",
    "\n",
    "    def read_specsettings(self,SpecSettings):\n",
    "        spec_list = SpecSettings.decode('utf-8').split('\\r\\n')[1:]\n",
    "        spec_list = [item for item in spec_list if \":\" in item]\n",
    "        spec_dict = {item.split(\": \")[0].replace(\" \", \"_\"):item.split(\": \")[1].replace(\" \", \"_\") for item in spec_list}\n",
    "        return spec_dict\n",
    "\n",
    "    def get_imaging_settings(self,nd2file):\n",
    "        raw_metadata = nd2file.parser._raw_metadata\n",
    "        imaging_settings = {}\n",
    "        for key,meta in raw_metadata.image_metadata_sequence[b'SLxPictureMetadata'][b'sPicturePlanes'][b'sSampleSetting'].items():\n",
    "            camera_settings = meta[b'pCameraSetting']\n",
    "            camera_name = camera_settings[b'CameraUserName'].decode('utf-8')\n",
    "            channel_name = camera_settings[b'Metadata'][b'Channels'][b'Channel_0'][b'Name'].decode('utf-8')\n",
    "            obj_settings = self.decode_unidict(meta[b'pObjectiveSetting'])\n",
    "            spec_settings = self.read_specsettings(meta[b'sSpecSettings'])\n",
    "            imaging_settings[channel_name] = {'camera_name':camera_name,'obj_settings':obj_settings,**spec_settings}\n",
    "        return imaging_settings\n",
    "\n",
    "    def make_fov_df(self,nd2file, exp_metadata): #only records values for single timepoints, does not seperate between channels....\n",
    "        img_metadata = nd2file.parser._raw_metadata\n",
    "        num_fovs = exp_metadata['num_fovs']\n",
    "        num_frames = exp_metadata['num_frames']\n",
    "        num_images_expected = num_fovs*num_frames\n",
    "\n",
    "        if img_metadata.x_data is not None:\n",
    "            x = np.reshape(img_metadata.x_data,(-1,num_fovs)).T\n",
    "            y = np.reshape(img_metadata.y_data,(-1,num_fovs)).T\n",
    "            z = np.reshape(img_metadata.z_data,(-1,num_fovs)).T\n",
    "        else:\n",
    "            positions = img_metadata.image_metadata[b'SLxExperiment'][b'ppNextLevelEx'][b''][b'uLoopPars'][b'Points'][b'']\n",
    "            x = []\n",
    "            y = []\n",
    "            z = []\n",
    "            for position in positions:\n",
    "                x.append([position[b'dPosX']]*num_frames)\n",
    "                y.append([position[b'dPosY']]*num_frames)\n",
    "                z.append([position[b'dPosZ']]*num_frames)\n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            z = np.array(z)\n",
    "\n",
    "\n",
    "        time_points = x.shape[1]\n",
    "        acq_times = np.reshape(np.array(list(img_metadata.acquisition_times)[:num_images_expected]),(-1,num_fovs)).T\n",
    "        pos_label = np.repeat(np.expand_dims(np.add.accumulate(np.ones(num_fovs,dtype=int))-1,1),time_points,1) ##???\n",
    "        time_point_labels = np.repeat(np.expand_dims(np.add.accumulate(np.ones(time_points,dtype=int))-1,1),num_fovs,1).T\n",
    "\n",
    "        output = pd.DataFrame({'fov':pos_label.flatten(),'timepoints':time_point_labels.flatten(),'t':acq_times.flatten(),'x':x.flatten(),'y':y.flatten(),'z':z.flatten()})\n",
    "        output = output.astype({'fov': int, 'timepoints':int, 't': float, 'x': float,'y': float,'z': float})\n",
    "\n",
    "        output = output[~((output['x'] == 0.)&(output['y'] == 0.)&(output['z'] == 0.))].reset_index(drop=True) ##bootstrapped to fix issue when only some FOVs are selected (return if it causes problems in the future)\n",
    "        output = output.set_index([\"fov\",\"timepoints\"], drop=True, append=False, inplace=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metadata(self):\n",
    "        # Manual numbers are for broken .nd2 files (from when Elements crashes)\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        for key,item in self.nd2reader_override.items():\n",
    "            nd2file.metadata[key] = item\n",
    "        exp_metadata = copy.copy(nd2file.metadata)\n",
    "        wanted_keys = ['height', 'width', 'date', 'fields_of_view', 'frames', 'z_levels', 'z_coordinates', 'total_images_per_channel', 'channels', 'pixel_microns', 'num_frames', 'experiment']\n",
    "        exp_metadata = dict([(k, exp_metadata[k]) for k in wanted_keys if k in exp_metadata])\n",
    "        exp_metadata[\"num_fovs\"] = len(exp_metadata['fields_of_view'])\n",
    "        exp_metadata[\"settings\"] = self.get_imaging_settings(nd2file)\n",
    "        if not self.ignore_fovmetadata:\n",
    "            fov_metadata = self.make_fov_df(nd2file, exp_metadata)\n",
    "            nd2file.close()\n",
    "            return exp_metadata,fov_metadata\n",
    "        else:\n",
    "            nd2file.close()\n",
    "            return exp_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdf5_fov_extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        headpath,\n",
    "        tiffpath,\n",
    "        format_string=\"t{time:d}xy{frame:d}c{channel:d}\",\n",
    "        tpts_per_file=100,\n",
    "    ):\n",
    "        \"\"\"Utility to convert individual tiff files to hdf5 archives.\n",
    "\n",
    "        Attributes:\n",
    "            headpath (str): base directory for data analysis\n",
    "            tiffpath (str): directory where tiff files are located\n",
    "            metapath (str): metadata path\n",
    "            hdf5path (str): where to store hdf5 data\n",
    "            tpts_per_file (int): number of timepoints to put in each hdf5 file\n",
    "            format_string (str): format of filenames from which to extract metadata (using parse library)\n",
    "        \"\"\"\n",
    "        self.tiffpath = tiffpath\n",
    "        self.headpath = headpath\n",
    "        self.metapath = self.headpath + \"/metadata.hdf5\"\n",
    "        self.hdf5path = self.headpath + \"/hdf5\"\n",
    "        self.tpts_per_file = tpts_per_file\n",
    "        self.format_string = format_string\n",
    "\n",
    "        self.organism = \"\"\n",
    "        self.microscope = \"\"\n",
    "        self.notes = \"\"\n",
    "\n",
    "    def writemetadata(self, t_range=None, fov_list=None):\n",
    "        ndmeta_handle = nd_metadata_handler(\n",
    "            self.nd2filename,\n",
    "            ignore_fovmetadata=self.ignore_fovmetadata,\n",
    "            nd2reader_override=self.nd2reader_override,\n",
    "        )\n",
    "        if self.ignore_fovmetadata:\n",
    "            exp_metadata = ndmeta_handle.get_metadata()\n",
    "        else:\n",
    "            exp_metadata, fov_metadata = ndmeta_handle.get_metadata()\n",
    "\n",
    "        if t_range is not None:\n",
    "            exp_metadata[\"frames\"] = exp_metadata[\"frames\"][t_range[0] : t_range[1] + 1]\n",
    "            exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "            fov_metadata = fov_metadata.loc[\n",
    "                pd.IndexSlice[:, slice(t_range[0], t_range[1])], :\n",
    "            ]  # 4 -> 70\n",
    "\n",
    "        if fov_list is not None:\n",
    "            fov_metadata = fov_metadata.loc[list(fov_list)]\n",
    "            exp_metadata[\"fields_of_view\"] = list(fov_list)\n",
    "\n",
    "        self.chunk_shape = (1, exp_metadata[\"height\"], exp_metadata[\"width\"])\n",
    "        chunk_bytes = 2 * np.multiply.accumulate(np.array(self.chunk_shape))[-1]\n",
    "        self.chunk_cache_mem_size = 2 * chunk_bytes\n",
    "        exp_metadata[\"chunk_shape\"], exp_metadata[\"chunk_cache_mem_size\"] = (\n",
    "            self.chunk_shape,\n",
    "            self.chunk_cache_mem_size,\n",
    "        )\n",
    "        exp_metadata[\"Organism\"], exp_metadata[\"Microscope\"], exp_metadata[\"Notes\"] = (\n",
    "            self.organism,\n",
    "            self.microscope,\n",
    "            self.notes,\n",
    "        )\n",
    "        self.meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "\n",
    "        if self.ignore_fovmetadata:\n",
    "            assignment_metadata = self.assignidx(exp_metadata, metadf=None)\n",
    "            assignment_metadata.astype({\"File Index\": int, \"Image Index\": int})\n",
    "        else:\n",
    "            assignment_metadata = self.assignidx(exp_metadata, metadf=fov_metadata)\n",
    "            assignment_metadata.astype(\n",
    "                {\n",
    "                    \"t\": float,\n",
    "                    \"x\": float,\n",
    "                    \"y\": float,\n",
    "                    \"z\": float,\n",
    "                    \"File Index\": int,\n",
    "                    \"Image Index\": int,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        self.meta_handle.write_df(\"global\", assignment_metadata, metadata=exp_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nd_metadata_handler:\n",
    "    def __init__(self, nd2filename, ignore_fovmetadata=False, nd2reader_override={}):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.ignore_fovmetadata = ignore_fovmetadata\n",
    "        self.nd2reader_override = nd2reader_override\n",
    "\n",
    "    def decode_unidict(self, unidict):\n",
    "        outdict = {}\n",
    "        for key, val in unidict.items():\n",
    "            if type(key) == bytes:\n",
    "                key = key.decode(\"utf8\")\n",
    "            if type(val) == bytes:\n",
    "                val = val.decode(\"utf8\")\n",
    "            outdict[key] = val\n",
    "        return outdict\n",
    "\n",
    "    def read_specsettings(self, SpecSettings):\n",
    "        spec_list = SpecSettings.decode(\"utf-8\").split(\"\\r\\n\")[1:]\n",
    "        spec_list = [item for item in spec_list if \":\" in item]\n",
    "        spec_dict = {\n",
    "            item.split(\": \")[0].replace(\" \", \"_\"): item.split(\": \")[1].replace(\" \", \"_\")\n",
    "            for item in spec_list\n",
    "        }\n",
    "        return spec_dict\n",
    "\n",
    "    def get_imaging_settings(self, nd2file):\n",
    "        raw_metadata = nd2file.parser._raw_metadata\n",
    "        imaging_settings = {}\n",
    "        for key, meta in raw_metadata.image_metadata_sequence[b\"SLxPictureMetadata\"][\n",
    "            b\"sPicturePlanes\"\n",
    "        ][b\"sSampleSetting\"].items():\n",
    "            camera_settings = meta[b\"pCameraSetting\"]\n",
    "            camera_name = camera_settings[b\"CameraUserName\"].decode(\"utf-8\")\n",
    "            channel_name = camera_settings[b\"Metadata\"][b\"Channels\"][b\"Channel_0\"][\n",
    "                b\"Name\"\n",
    "            ].decode(\"utf-8\")\n",
    "            obj_settings = self.decode_unidict(meta[b\"pObjectiveSetting\"])\n",
    "            spec_settings = self.read_specsettings(meta[b\"sSpecSettings\"])\n",
    "            imaging_settings[channel_name] = {\n",
    "                \"camera_name\": camera_name,\n",
    "                \"obj_settings\": obj_settings,\n",
    "                **spec_settings,\n",
    "            }\n",
    "        return imaging_settings\n",
    "\n",
    "    def make_fov_df(\n",
    "        self, nd2file, exp_metadata\n",
    "    ):  # only records values for single timepoints, does not seperate between channels....\n",
    "        img_metadata = nd2file.parser._raw_metadata\n",
    "        num_fovs = exp_metadata[\"num_fovs\"]\n",
    "        num_frames = exp_metadata[\"num_frames\"]\n",
    "        num_images_expected = num_fovs * num_frames\n",
    "\n",
    "        if img_metadata.x_data is not None:\n",
    "            x = np.reshape(img_metadata.x_data, (-1, num_fovs)).T\n",
    "            y = np.reshape(img_metadata.y_data, (-1, num_fovs)).T\n",
    "            z = np.reshape(img_metadata.z_data, (-1, num_fovs)).T\n",
    "        else:\n",
    "            positions = img_metadata.image_metadata[b\"SLxExperiment\"][b\"ppNextLevelEx\"][\n",
    "                b\"\"\n",
    "            ][b\"uLoopPars\"][b\"Points\"][b\"\"]\n",
    "            x = []\n",
    "            y = []\n",
    "            z = []\n",
    "            for position in positions:\n",
    "                x.append([position[b\"dPosX\"]] * num_frames)\n",
    "                y.append([position[b\"dPosY\"]] * num_frames)\n",
    "                z.append([position[b\"dPosZ\"]] * num_frames)\n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            z = np.array(z)\n",
    "\n",
    "        time_points = x.shape[1]\n",
    "        acq_times = np.reshape(\n",
    "            np.array(list(img_metadata.acquisition_times)[:num_images_expected]),\n",
    "            (-1, num_fovs),\n",
    "        ).T\n",
    "        pos_label = np.repeat(\n",
    "            np.expand_dims(np.add.accumulate(np.ones(num_fovs, dtype=int)) - 1, 1),\n",
    "            time_points,\n",
    "            1,\n",
    "        )  ##???\n",
    "        time_point_labels = np.repeat(\n",
    "            np.expand_dims(np.add.accumulate(np.ones(time_points, dtype=int)) - 1, 1),\n",
    "            num_fovs,\n",
    "            1,\n",
    "        ).T\n",
    "\n",
    "        output = pd.DataFrame(\n",
    "            {\n",
    "                \"fov\": pos_label.flatten(),\n",
    "                \"timepoints\": time_point_labels.flatten(),\n",
    "                \"t\": acq_times.flatten(),\n",
    "                \"x\": x.flatten(),\n",
    "                \"y\": y.flatten(),\n",
    "                \"z\": z.flatten(),\n",
    "            }\n",
    "        )\n",
    "        output = output.astype(\n",
    "            {\n",
    "                \"fov\": int,\n",
    "                \"timepoints\": int,\n",
    "                \"t\": float,\n",
    "                \"x\": float,\n",
    "                \"y\": float,\n",
    "                \"z\": float,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        output = output[\n",
    "            ~((output[\"x\"] == 0.0) & (output[\"y\"] == 0.0) & (output[\"z\"] == 0.0))\n",
    "        ].reset_index(\n",
    "            drop=True\n",
    "        )  ##bootstrapped to fix issue when only some FOVs are selected (return if it causes problems in the future)\n",
    "        output = output.set_index(\n",
    "            [\"fov\", \"timepoints\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metadata(self):\n",
    "        # Manual numbers are for broken .nd2 files (from when Elements crashes)\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        for key, item in self.nd2reader_override.items():\n",
    "            nd2file.metadata[key] = item\n",
    "        exp_metadata = copy.copy(nd2file.metadata)\n",
    "        wanted_keys = [\n",
    "            \"height\",\n",
    "            \"width\",\n",
    "            \"date\",\n",
    "            \"fields_of_view\",\n",
    "            \"frames\",\n",
    "            \"z_levels\",\n",
    "            \"z_coordinates\",\n",
    "            \"total_images_per_channel\",\n",
    "            \"channels\",\n",
    "            \"pixel_microns\",\n",
    "            \"num_frames\",\n",
    "            \"experiment\",\n",
    "        ]\n",
    "        exp_metadata = dict(\n",
    "            [(k, exp_metadata[k]) for k in wanted_keys if k in exp_metadata]\n",
    "        )\n",
    "        exp_metadata[\"num_fovs\"] = len(exp_metadata[\"fields_of_view\"])\n",
    "        exp_metadata[\"settings\"] = self.get_imaging_settings(nd2file)\n",
    "        if not self.ignore_fovmetadata:\n",
    "            fov_metadata = self.make_fov_df(nd2file, exp_metadata)\n",
    "            nd2file.close()\n",
    "            return exp_metadata, fov_metadata\n",
    "        else:\n",
    "            nd2file.close()\n",
    "            return exp_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2reader\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_tags(filepath):\n",
    "    with tifffile.TiffFile(filepath) as tiff:\n",
    "        tiff_tags = {}\n",
    "        for tag in tiff.pages[0].tags.values():\n",
    "            name, value = tag.name, tag.value\n",
    "            tiff_tags[name] = value\n",
    "    return tiff_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "!ls /n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_tags = get_tiff_tags(\n",
    "    \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/t000001xy01c1.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "tiff_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Mapping, Sequence, defaultdict\n",
    "\n",
    "import xmltodict\n",
    "\n",
    "\n",
    "def parse_nikon_tiff_metadata(tags):\n",
    "    metadata = {}\n",
    "    for tag, data in tags.items():\n",
    "        if data == b\"\\x00\\x00\\x00\\x00\":\n",
    "            # metadata[tag] = data\n",
    "            continue\n",
    "        elif tag == \"270\":\n",
    "            if data:\n",
    "                try:\n",
    "                    parsed_data = xmltodict.parse(data)\n",
    "                except xml.parsers.expat.ExpatError:\n",
    "                    parsed_data = data\n",
    "            else:\n",
    "                parsed_data = \"\"\n",
    "            metadata[\"image_description\"] = parsed_data\n",
    "        elif tag == \"65330\":\n",
    "            # SEEMS TO STORE (with null bytes): CameraTemp1, Camera_ExposureTime1, PFS_OFFSET, PFS_STATUS\n",
    "            label = _nikon_tiff_label(b\"SLxImageTextInfo\")\n",
    "            idx = data.index(label) - 2\n",
    "            md = nd2reader.common.read_metadata(data[idx:], 1)\n",
    "            metadata[\"image_text_info\"] = _stringify_dict_keys(md)\n",
    "        elif tag == \"65331\":\n",
    "            label = _nikon_tiff_label(b\"SLxPictureMetadata\")\n",
    "            idx = data.index(label) - 2\n",
    "            md = nd2reader.common.read_metadata(data[idx:], 1)\n",
    "            metadata[\"image_metadata_sequence\"] = _stringify_dict_keys(md)\n",
    "        elif tag == \"65332\":\n",
    "            # SEEMS TO STORE (UTF-16 encoded): AppInfo_V1_0, CustomDataV2_0, GrabberCameraSettingsV1_0, LUTDataV1_0\n",
    "            label = _nikon_tiff_label(b\"AppInfo_V1_0\")\n",
    "            idx = (\n",
    "                data.index(label) + len(label) + 9\n",
    "            )  # TODO: no idea what these bytes are\n",
    "            # print(tag,label,idx,data[idx:idx+100])\n",
    "            # from IPython import embed;embed()\n",
    "            md = xmltodict.parse(data[idx:])\n",
    "            metadata[\"app_info\"] = md\n",
    "        elif tag == \"65333\":\n",
    "            # SEEMS TO STORE (with null bytes): hhh\n",
    "            label = _nikon_tiff_label(b\"MetadataTiffV1_0\")\n",
    "            idx = (\n",
    "                data.index(label) + len(label) + 17\n",
    "            )  # TODO: total hack, no idea why 17\n",
    "            md = xmltodict.parse(data[idx:])\n",
    "            metadata[\"metadata_tiff\"] = md\n",
    "        else:\n",
    "            metadata[tag] = data\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _nikon_tiff_label(label):\n",
    "    if type(label) != bytes:\n",
    "        label = label.encode(\"utf-8\")\n",
    "    return b\"\\x00\".join([label[i : i + 1] for i in range(len(label))])\n",
    "\n",
    "\n",
    "def _stringify_dict_keys(d):\n",
    "    return recursive_map(\n",
    "        lambda s: s.decode(\"utf-8\"), d, shortcircuit=bytes, ignore=True, keys=True\n",
    "    )\n",
    "\n",
    "\n",
    "# FROM: https://stackoverflow.com/questions/42095393/python-map-a-function-over-recursive-iterables/42095505\n",
    "# TODO: document!!!\n",
    "def recursive_map(\n",
    "    func,\n",
    "    data,\n",
    "    shortcircuit=(),\n",
    "    ignore=(),\n",
    "    keys=False,\n",
    "    max_level=None,\n",
    "    predicate=None,\n",
    "    key_predicate=None,\n",
    "):\n",
    "    if max_level is not None and max_level is not False:\n",
    "        if max_level == 0:\n",
    "            return func(data)\n",
    "        max_level -= 1\n",
    "    apply = lambda x: recursive_map(\n",
    "        func,\n",
    "        x,\n",
    "        shortcircuit=shortcircuit,\n",
    "        ignore=ignore,\n",
    "        keys=keys,\n",
    "        max_level=max_level,\n",
    "        predicate=predicate,\n",
    "        key_predicate=key_predicate,\n",
    "    )\n",
    "    if isinstance(data, shortcircuit):\n",
    "        return func(data)\n",
    "    # to avoid an infinite recursion error\n",
    "    # we need to hardcode that strs are ignored, because str[0] is a str, and hence a Sequence\n",
    "    elif isinstance(data, str) or ignore is not True and isinstance(data, ignore):\n",
    "        return data\n",
    "    elif isinstance(data, Mapping):\n",
    "        if keys:\n",
    "            values = {apply(k): apply(v) for k, v in data.items()}\n",
    "        else:\n",
    "            values = {k: apply(v) for k, v in data.items()}\n",
    "        if key_predicate is not None:\n",
    "            values = {k: v for k, v in values.items() if key_predicate(k)}\n",
    "        if predicate is not None:\n",
    "            values = {k: v for k, v in values.items() if predicate(v)}\n",
    "        return type(data)(values)\n",
    "    elif isinstance(data, Sequence):\n",
    "        values = [apply(v) for v in data]\n",
    "        if predicate is not None:\n",
    "            values = [v for v in values if predicate(v)]\n",
    "        return type(data)(values)\n",
    "    elif ignore is True or True in ignore:\n",
    "        return data\n",
    "    else:\n",
    "        return func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = parse_nikon_tiff_metadata(tif_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"metadata_tiff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_str = \"t000001xy02c1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = compile(\"t{time:d}xy{frame:d}c{channel:d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = parser.search(ex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict([(key, [value]) for key, value in match.named.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make/overwrite directory\n",
    "writedir(self.hdf5path, overwrite=True)\n",
    "# Create parser\n",
    "parser = compile(self.format_string)\n",
    "# Search specified directory recursively for tiff files\n",
    "tiff_files = []\n",
    "for root, _, files in os.walk(self.tiffpath):\n",
    "    tiff_files.extend(\n",
    "        [os.path.join(root, f) for f in files if \".tif\" in os.path.splitext(f)[1]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_metadata = {}\n",
    "exp_metadata = {}\n",
    "assignment_metadata = {}\n",
    "\n",
    "first_successful_file = True\n",
    "for f in tiff_files:\n",
    "    match = parser.search(f)\n",
    "    # ignore any files that don't match the regex\n",
    "    if match is not None:\n",
    "        if first_successful_file:\n",
    "            # Build metadata\n",
    "            first_img = imread(f)\n",
    "            # get dimensions by loading file\n",
    "            exp_metadata[\"height\"] = first_img.shape[0]\n",
    "            exp_metadata[\"width\"] = first_img.shape[1]\n",
    "            exp_metadata[\"Organism\"] = self.organism\n",
    "            exp_metadata[\"Microscope\"] = self.microscope\n",
    "            exp_metadata[\"Notes\"] = self.notes\n",
    "\n",
    "            self.chunk_shape = (1, exp_metadata[\"height\"], exp_metadata[\"width\"])\n",
    "            chunk_bytes = 2 * np.multiply.accumulate(np.array(self.chunk_shape))[-1]\n",
    "            self.chunk_cache_mem_size = 2 * chunk_bytes\n",
    "            exp_metadata[\"chunk_shape\"], exp_metadata[\"chunk_cache_mem_size\"] = (\n",
    "                self.chunk_shape,\n",
    "                self.chunk_cache_mem_size,\n",
    "            )\n",
    "            # get metadata from the file name\n",
    "            fov_metadata = dict([(key, [value]) for key, value in match.named.items()])\n",
    "            fov_metadata[\"Image Path\"] = [f]\n",
    "            first_successful_file = False\n",
    "        else:\n",
    "            # Add to dictionary\n",
    "            fov_frame_dict = match.named\n",
    "            for key, value in fov_frame_dict.items():\n",
    "                fov_metadata[key].append(value)\n",
    "            fov_metadata[\"Image Path\"].append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsestr = \"t{timepoint:d}xy{fov:d}c{channel:d}.tif\"\n",
    "tiffpath = \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_tags(filepath):\n",
    "    with tifffile.TiffFile(filepath) as tiff:\n",
    "        tiff_tags = {}\n",
    "        for tag in tiff.pages[0].tags.values():\n",
    "            name, value = tag.name, tag.value\n",
    "            tiff_tags[name] = value\n",
    "    return tiff_tags\n",
    "\n",
    "\n",
    "def get_metadata(\n",
    "    tiffpath,\n",
    "    channels,\n",
    "    parsestr=\"t{timepoint:d}xy{fov:d}c{channel:d}.tif\",\n",
    "    zero_base_keys=[\"timepoint\", \"fov\", \"channel\"],\n",
    "):\n",
    "    parser = compile(parsestr)\n",
    "    parse_keys = [\n",
    "        item.split(\"}\")[0].split(\":\")[0] for item in parsestr.split(\"{\")[1:]\n",
    "    ] + [\"image_paths\"]\n",
    "\n",
    "    exp_metadata = {}\n",
    "    fov_metadata = {key: [] for key in parse_keys}\n",
    "\n",
    "    tiff_files = []\n",
    "    for root, _, files in os.walk(tiffpath):\n",
    "        tiff_files.extend(\n",
    "            [os.path.join(root, f) for f in files if \".tif\" in os.path.splitext(f)[1]]\n",
    "        )\n",
    "\n",
    "    tags = get_tiff_tags(tiff_files[0])\n",
    "    exp_metadata[\"height\"] = tags[\"ImageLength\"]\n",
    "    exp_metadata[\"width\"] = tags[\"ImageWidth\"]\n",
    "    exp_metadata[\"pixel_microns\"] = tags[\"65326\"]\n",
    "\n",
    "    for f in tiff_files:\n",
    "        match = parser.search(f)\n",
    "        # ignore any files that don't match the regex\n",
    "        if match is not None:\n",
    "            # Add to dictionary\n",
    "            fov_frame_dict = match.named\n",
    "            for key, value in fov_frame_dict.items():\n",
    "                fov_metadata[key].append(value)\n",
    "            fov_metadata[\"image_paths\"].append(f)\n",
    "\n",
    "    for zero_base_key in zero_base_keys:\n",
    "        if 0 not in fov_metadata[zero_base_key]:\n",
    "            fov_metadata[zero_base_key] = [\n",
    "                item - 1 for item in fov_metadata[zero_base_key]\n",
    "            ]\n",
    "\n",
    "    exp_metadata[\"num_fovs\"] = len(set(fov_metadata[\"fov\"]))\n",
    "    exp_metadata[\"frames\"] = list(set(fov_metadata[\"timepoint\"]))\n",
    "    exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "\n",
    "    fov_metadata = pd.DataFrame(fov_metadata)\n",
    "    fov_metadata[\"channel\"] = fov_metadata[\"channel\"].apply(lambda x: channels[x])\n",
    "    fov_metadata = fov_metadata.set_index([\"fov\", \"timepoint\"]).sort_index()\n",
    "\n",
    "    return exp_metadata, fov_metadata\n",
    "\n",
    "\n",
    "#     nd2file = ND2Reader(self.nd2filename)\n",
    "#     for key,item in self.nd2reader_override.items():\n",
    "#         nd2file.metadata[key] = item\n",
    "#     exp_metadata = copy.copy(nd2file.metadata)\n",
    "#     wanted_keys = ['height', 'width', 'date', 'fields_of_view', 'frames', 'z_levels', 'z_coordinates', 'total_images_per_channel', 'channels', 'pixel_microns', 'num_frames', 'experiment']\n",
    "#     exp_metadata = dict([(k, exp_metadata[k]) for k in wanted_keys if k in exp_metadata])\n",
    "#     exp_metadata[\"num_fovs\"] = len(exp_metadata['fields_of_view'])\n",
    "#     exp_metadata[\"settings\"] = self.get_imaging_settings(nd2file)\n",
    "#     if not self.ignore_fovmetadata:\n",
    "#         fov_metadata = self.make_fov_df(nd2file, exp_metadata)\n",
    "#         nd2file.close()\n",
    "#         return exp_metadata,fov_metadata\n",
    "#     else:\n",
    "#         nd2file.close()\n",
    "#         return exp_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = (0, 5)\n",
    "fov_list = None\n",
    "\n",
    "exp_metadata, fov_metadata = get_metadata(tiffpath, [\"channel1\", \"channel2\"])\n",
    "\n",
    "if t_range is not None:\n",
    "    exp_metadata[\"frames\"] = exp_metadata[\"frames\"][t_range[0] : t_range[1] + 1]\n",
    "    exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "    fov_metadata = fov_metadata.loc[\n",
    "        pd.IndexSlice[:, slice(t_range[0], t_range[1])], :\n",
    "    ]  # 4 -> 70\n",
    "\n",
    "if fov_list is not None:\n",
    "    fov_metadata = fov_metadata.loc[list(fov_list)]\n",
    "    exp_metadata[\"fields_of_view\"] = list(fov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ree.apply(lambda x: channels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "assignidx(exp_metadata, fov_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpts_per_file = 25\n",
    "\n",
    "numfovs = len(fov_metadata.index.get_level_values(\"fov\").unique().tolist())\n",
    "timepoints_per_fov = len(\n",
    "    fov_metadata.index.get_level_values(\"timepoint\").unique().tolist()\n",
    ")\n",
    "files_per_fov = (timepoints_per_fov // tpts_per_file) + 1\n",
    "remainder = timepoints_per_fov % tpts_per_file\n",
    "ttlfiles = numfovs * files_per_fov\n",
    "fov_file_idx = np.repeat(list(range(files_per_fov)), tpts_per_file)[\n",
    "    : -(tpts_per_file - remainder)\n",
    "]\n",
    "file_idx = np.concatenate(\n",
    "    [fov_file_idx + (fov_idx * files_per_fov) for fov_idx in range(numfovs)]\n",
    ")\n",
    "fov_img_idx = np.repeat(\n",
    "    np.array(list(range(tpts_per_file)))[np.newaxis, :], files_per_fov, axis=0\n",
    ")\n",
    "fov_img_idx = fov_img_idx.flatten()[: -(tpts_per_file - remainder)]\n",
    "img_idx = np.concatenate([fov_img_idx for fov_idx in range(numfovs)])\n",
    "fov_idx = np.repeat(list(range(numfovs)), timepoints_per_fov)\n",
    "timepoint_idx = np.repeat(\n",
    "    np.array(list(range(timepoints_per_fov)))[np.newaxis, :], numfovs, axis=0\n",
    ").flatten()\n",
    "\n",
    "data = {\n",
    "    \"fov\": fov_idx,\n",
    "    \"timepoints\": timepoint_idx,\n",
    "    \"File Index\": file_idx,\n",
    "    \"Image Index\": img_idx,\n",
    "}\n",
    "outdf = pd.DataFrame(data)\n",
    "outdf = outdf.set_index([\"fov\", \"timepoints\"], drop=True, append=False, inplace=False)\n",
    "\n",
    "# outdf[\"image_paths\"] = fov_metadata[\"image_paths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_metadata.loc[outdf[0:1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = (1, 5)\n",
    "fov_list = None\n",
    "\n",
    "exp_metadata, fov_metadata = get_metadata(tiffpath, [\"channel1\", \"channel2\"])\n",
    "\n",
    "if t_range is not None:\n",
    "    exp_metadata[\"frames\"] = exp_metadata[\"frames\"][t_range[0] : t_range[1] + 1]\n",
    "    exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "    fov_metadata = fov_metadata.loc[\n",
    "        pd.IndexSlice[:, slice(t_range[0], t_range[1])], :\n",
    "    ]  # 4 -> 70\n",
    "\n",
    "if fov_list is not None:\n",
    "    fov_metadata = fov_metadata.loc[list(fov_list)]\n",
    "    exp_metadata[\"fields_of_view\"] = list(fov_list)\n",
    "\n",
    "# self.chunk_shape = (1,exp_metadata[\"height\"],exp_metadata[\"width\"])\n",
    "# chunk_bytes = (2*np.multiply.accumulate(np.array(self.chunk_shape))[-1])\n",
    "# self.chunk_cache_mem_size = 2*chunk_bytes\n",
    "# exp_metadata[\"chunk_shape\"],exp_metadata[\"chunk_cache_mem_size\"] = (self.chunk_shape,self.chunk_cache_mem_size)\n",
    "# exp_metadata[\"Organism\"],exp_metadata[\"Microscope\"],exp_metadata[\"Notes\"] = (self.organism,self.microscope,self.notes)\n",
    "# self.meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "assignment_metadata = self.assignidx(exp_metadata, metadf=None)\n",
    "\n",
    "# self.meta_handle.write_df(\"global\",assignment_metadata,metadata=exp_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_tags[\"65331\"].decode(\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsestr.split(\"{\")\n",
    "parse_keys = [item.split(\"}\")[0].split(\":\")[0] for item in parsestr.split(\"{\")[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsestr[parsestr.find(\"{\") + 1 : parsestr.find(\"}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_metadata[:2].index.get_level_values(\"fov\").unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(fov_metadata[:2][\"channel\"], fov_metadata[:2][\"image_paths\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tifffile.imread(\n",
    "    \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/t000001xy01c1.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_tags(filepath):\n",
    "    with tifffile.TiffFile(filepath) as tiff:\n",
    "        tiff_tags = {}\n",
    "        for tag in tiff.pages[0].tags.values():\n",
    "            name, value = tag.name, tag.value\n",
    "            tiff_tags[name] = value\n",
    "    return tiff_tags\n",
    "\n",
    "\n",
    "class tiff_extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tiffpath,\n",
    "        headpath,\n",
    "        channels,\n",
    "        tpts_per_file=100,\n",
    "        parsestr=\"t{timepoints:d}xy{fov:d}c{channel:d}.tif\",\n",
    "        zero_base_keys=[\"timepoints\", \"fov\", \"channel\"],\n",
    "    ):  # note this chunk size has a large role in downstream steps...make sure is less than 1 MB\n",
    "        \"\"\"Utility to convert individual tiff files to hdf5 archives.\n",
    "\n",
    "        Attributes:\n",
    "            headpath (str): base directory for data analysis\n",
    "            tiffpath (str): directory where tiff files are located\n",
    "            metapath (str): metadata path\n",
    "            hdf5path (str): where to store hdf5 data\n",
    "            tpts_per_file (int): number of timepoints to put in each hdf5 file\n",
    "            parsestr (str): format of filenames from which to extract metadata (using parse library)\n",
    "        \"\"\"\n",
    "        self.tiffpath = tiffpath\n",
    "        self.headpath = headpath\n",
    "        self.channels = channels\n",
    "        self.metapath = self.headpath + \"/metadata.hdf5\"\n",
    "        self.hdf5path = self.headpath + \"/hdf5\"\n",
    "        self.tpts_per_file = tpts_per_file\n",
    "        self.parsestr = parsestr\n",
    "        self.zero_base_keys = zero_base_keys\n",
    "\n",
    "        self.organism = \"\"\n",
    "        self.microscope = \"\"\n",
    "        self.notes = \"\"\n",
    "\n",
    "    def get_metadata(\n",
    "        self,\n",
    "        tiffpath,\n",
    "        channels,\n",
    "        parsestr=\"t{timepoints:d}xy{fov:d}c{channel:d}.tif\",\n",
    "        zero_base_keys=[\"timepoints\", \"fov\", \"channel\"],\n",
    "    ):\n",
    "        parser = compile(parsestr)\n",
    "        parse_keys = [\n",
    "            item.split(\"}\")[0].split(\":\")[0] for item in parsestr.split(\"{\")[1:]\n",
    "        ] + [\"image_paths\"]\n",
    "\n",
    "        exp_metadata = {}\n",
    "        fov_metadata = {key: [] for key in parse_keys}\n",
    "\n",
    "        tiff_files = []\n",
    "        for root, _, files in os.walk(tiffpath):\n",
    "            tiff_files.extend(\n",
    "                [\n",
    "                    os.path.join(root, f)\n",
    "                    for f in files\n",
    "                    if \".tif\" in os.path.splitext(f)[1]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        tags = get_tiff_tags(tiff_files[0])\n",
    "        exp_metadata[\"height\"] = tags[\"ImageLength\"]\n",
    "        exp_metadata[\"width\"] = tags[\"ImageWidth\"]\n",
    "        exp_metadata[\"pixel_microns\"] = tags[\"65326\"]\n",
    "        exp_metadata[\"channels\"] = channels\n",
    "\n",
    "        for f in tiff_files:\n",
    "            match = parser.search(f)\n",
    "            # ignore any files that don't match the regex\n",
    "            if match is not None:\n",
    "                # Add to dictionary\n",
    "                fov_frame_dict = match.named\n",
    "                for key, value in fov_frame_dict.items():\n",
    "                    fov_metadata[key].append(value)\n",
    "                fov_metadata[\"image_paths\"].append(f)\n",
    "\n",
    "        for zero_base_key in zero_base_keys:\n",
    "            if 0 not in fov_metadata[zero_base_key]:\n",
    "                fov_metadata[zero_base_key] = [\n",
    "                    item - 1 for item in fov_metadata[zero_base_key]\n",
    "                ]\n",
    "\n",
    "        exp_metadata[\"num_fovs\"] = len(set(fov_metadata[\"fov\"]))\n",
    "        exp_metadata[\"frames\"] = list(set(fov_metadata[\"timepoints\"]))\n",
    "        exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "\n",
    "        fov_metadata = pd.DataFrame(fov_metadata)\n",
    "        fov_metadata[\"channel\"] = fov_metadata[\"channel\"].apply(lambda x: channels[x])\n",
    "        fov_metadata = fov_metadata.set_index([\"fov\", \"timepoints\"]).sort_index()\n",
    "\n",
    "        output_fov_metadata = []\n",
    "        step = len(channels)\n",
    "        for i in range(0, len(fov_metadata), step):\n",
    "            rows = fov_metadata[i : i + step]\n",
    "            channel_path_entry = dict(zip(rows[\"channel\"], rows[\"image_paths\"]))\n",
    "            fov_entry = rows.index.get_level_values(\"fov\").unique()[0]\n",
    "            timepoint_entry = rows.index.get_level_values(\"timepoints\").unique()[0]\n",
    "            fov_metadata_entry = {\n",
    "                \"fov\": fov_entry,\n",
    "                \"timepoints\": timepoint_entry,\n",
    "                \"channel_paths\": channel_path_entry,\n",
    "            }\n",
    "            output_fov_metadata.append(fov_metadata_entry)\n",
    "        fov_metadata = pd.DataFrame(output_fov_metadata).set_index(\n",
    "            [\"fov\", \"timepoints\"]\n",
    "        )\n",
    "\n",
    "        return exp_metadata, fov_metadata\n",
    "\n",
    "    def assignidx(self, fov_metadata):\n",
    "        numfovs = len(fov_metadata.index.get_level_values(\"fov\").unique().tolist())\n",
    "        timepoints_per_fov = len(\n",
    "            fov_metadata.index.get_level_values(\"timepoints\").unique().tolist()\n",
    "        )\n",
    "\n",
    "        files_per_fov = (timepoints_per_fov // self.tpts_per_file) + 1\n",
    "        remainder = timepoints_per_fov % self.tpts_per_file\n",
    "        ttlfiles = numfovs * files_per_fov\n",
    "        fov_file_idx = np.repeat(list(range(files_per_fov)), self.tpts_per_file)[\n",
    "            : -(self.tpts_per_file - remainder)\n",
    "        ]\n",
    "        file_idx = np.concatenate(\n",
    "            [fov_file_idx + (fov_idx * files_per_fov) for fov_idx in range(numfovs)]\n",
    "        )\n",
    "        fov_img_idx = np.repeat(\n",
    "            np.array(list(range(self.tpts_per_file)))[np.newaxis, :],\n",
    "            files_per_fov,\n",
    "            axis=0,\n",
    "        )\n",
    "        fov_img_idx = fov_img_idx.flatten()[: -(self.tpts_per_file - remainder)]\n",
    "        img_idx = np.concatenate([fov_img_idx for fov_idx in range(numfovs)])\n",
    "\n",
    "        fov_idx = np.repeat(list(range(numfovs)), timepoints_per_fov)\n",
    "        timepoint_idx = np.repeat(\n",
    "            np.array(list(range(timepoints_per_fov)))[np.newaxis, :], numfovs, axis=0\n",
    "        ).flatten()\n",
    "\n",
    "        outdf = copy.deepcopy(fov_metadata)\n",
    "        outdf[\"File Index\"] = file_idx\n",
    "        outdf[\"Image Index\"] = img_idx\n",
    "        return outdf\n",
    "\n",
    "    def writemetadata(self, t_range=None, fov_list=None):\n",
    "        exp_metadata, fov_metadata = self.get_metadata(\n",
    "            self.tiffpath,\n",
    "            self.channels,\n",
    "            parsestr=self.parsestr,\n",
    "            zero_base_keys=self.zero_base_keys,\n",
    "        )\n",
    "\n",
    "        if t_range is not None:\n",
    "            exp_metadata[\"frames\"] = exp_metadata[\"frames\"][t_range[0] : t_range[1] + 1]\n",
    "            exp_metadata[\"num_frames\"] = len(exp_metadata[\"frames\"])\n",
    "            fov_metadata = fov_metadata.loc[\n",
    "                pd.IndexSlice[:, slice(t_range[0], t_range[1])], :\n",
    "            ]  # 4 -> 70\n",
    "\n",
    "        if fov_list is not None:\n",
    "            fov_metadata = fov_metadata.loc[list(fov_list)]\n",
    "            exp_metadata[\"fields_of_view\"] = list(fov_list)\n",
    "\n",
    "        self.chunk_shape = (1, exp_metadata[\"height\"], exp_metadata[\"width\"])\n",
    "        chunk_bytes = 2 * np.multiply.accumulate(np.array(self.chunk_shape))[-1]\n",
    "        self.chunk_cache_mem_size = 2 * chunk_bytes\n",
    "        exp_metadata[\"chunk_shape\"], exp_metadata[\"chunk_cache_mem_size\"] = (\n",
    "            self.chunk_shape,\n",
    "            self.chunk_cache_mem_size,\n",
    "        )\n",
    "        exp_metadata[\"Organism\"], exp_metadata[\"Microscope\"], exp_metadata[\"Notes\"] = (\n",
    "            self.organism,\n",
    "            self.microscope,\n",
    "            self.notes,\n",
    "        )\n",
    "        self.meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "\n",
    "        assignment_metadata = self.assignidx(fov_metadata)\n",
    "        assignment_metadata.astype({\"File Index\": int, \"Image Index\": int})\n",
    "\n",
    "        self.meta_handle.write_df(\"global\", assignment_metadata, metadata=exp_metadata)\n",
    "\n",
    "    def read_metadata(self):\n",
    "        writedir(self.hdf5path, overwrite=True)\n",
    "        self.writemetadata()\n",
    "        metadf = self.meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        self.metadata = metadf.metadata\n",
    "        metadf = metadf.reset_index(inplace=False)\n",
    "        metadf = metadf.set_index(\n",
    "            [\"File Index\", \"Image Index\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        self.metadf = metadf.sort_index()\n",
    "\n",
    "    def set_params(self, fov_list, t_range, organism, microscope, notes):\n",
    "        self.fov_list = fov_list\n",
    "        self.t_range = t_range\n",
    "        self.organism = organism\n",
    "        self.microscope = microscope\n",
    "        self.notes = notes\n",
    "\n",
    "    def inter_set_params(self):\n",
    "        self.read_metadata()\n",
    "        t0, tf = (self.metadata[\"frames\"][0], self.metadata[\"frames\"][-1])\n",
    "        available_fov_list = self.metadf[\"fov\"].unique().tolist()\n",
    "        selection = ipyw.interactive(\n",
    "            self.set_params,\n",
    "            {\"manual\": True},\n",
    "            fov_list=ipyw.SelectMultiple(options=available_fov_list),\n",
    "            t_range=ipyw.IntRangeSlider(\n",
    "                value=[t0, tf],\n",
    "                min=t0,\n",
    "                max=tf,\n",
    "                step=1,\n",
    "                description=\"Time Range:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "            organism=ipyw.Textarea(\n",
    "                value=\"\",\n",
    "                placeholder=\"Organism imaged in this experiment.\",\n",
    "                description=\"Organism:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "            microscope=ipyw.Textarea(\n",
    "                value=\"\",\n",
    "                placeholder=\"Microscope used in this experiment.\",\n",
    "                description=\"Microscope:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "            notes=ipyw.Textarea(\n",
    "                value=\"\",\n",
    "                placeholder=\"General experiment notes.\",\n",
    "                description=\"Notes:\",\n",
    "                disabled=False,\n",
    "            ),\n",
    "        )\n",
    "        display(selection)\n",
    "\n",
    "    def extract(self, dask_controller, retries=1):\n",
    "        dask_controller.futures = {}\n",
    "\n",
    "        self.writemetadata(t_range=self.t_range, fov_list=self.fov_list)\n",
    "        metadf = self.meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        self.metadata = metadf.metadata\n",
    "        metadf = metadf.reset_index(inplace=False)\n",
    "        metadf = metadf.set_index(\n",
    "            [\"File Index\", \"Image Index\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        self.metadf = metadf.sort_index()\n",
    "\n",
    "        def writehdf5(fovnum, num_entries, timepoint_list, file_idx):\n",
    "            y_dim = self.metadata[\"height\"]\n",
    "            x_dim = self.metadata[\"width\"]\n",
    "            filedf = self.metadf.loc[file_idx].reset_index(inplace=False)\n",
    "            filedf = filedf.set_index(\n",
    "                [\"timepoints\"], drop=True, append=False, inplace=False\n",
    "            )\n",
    "            filedf = filedf.sort_index()\n",
    "\n",
    "            with h5py_cache.File(\n",
    "                self.hdf5path + \"/hdf5_\" + str(file_idx) + \".hdf5\",\n",
    "                \"w\",\n",
    "                chunk_cache_mem_size=self.chunk_cache_mem_size,\n",
    "            ) as h5pyfile:\n",
    "                for i, channel in enumerate(self.metadata[\"channels\"]):\n",
    "                    hdf5_dataset = h5pyfile.create_dataset(\n",
    "                        str(channel),\n",
    "                        (num_entries, y_dim, x_dim),\n",
    "                        chunks=self.chunk_shape,\n",
    "                        dtype=\"uint16\",\n",
    "                    )\n",
    "                    for j in range(len(timepoint_list)):\n",
    "                        frame = timepoint_list[j]\n",
    "                        entry = filedf.loc[frame][\"channel_paths\"]\n",
    "                        file_path = entry[channel]\n",
    "                        img = tifffile.imread(file_path)\n",
    "                        hdf5_dataset[j, :, :] = img\n",
    "            return \"Done.\"\n",
    "\n",
    "        file_list = self.metadf.index.get_level_values(\"File Index\").unique().values\n",
    "        num_jobs = len(file_list)\n",
    "        random_priorities = np.random.uniform(size=(num_jobs,))\n",
    "\n",
    "        for k, file_idx in enumerate(file_list):\n",
    "            priority = random_priorities[k]\n",
    "            filedf = self.metadf.loc[file_idx]\n",
    "\n",
    "            fovnum = filedf[0:1][\"fov\"].values[0]\n",
    "            num_entries = len(filedf.index.get_level_values(\"Image Index\").values)\n",
    "            timepoint_list = filedf[\"timepoints\"].tolist()\n",
    "\n",
    "            future = dask_controller.daskclient.submit(\n",
    "                writehdf5,\n",
    "                fovnum,\n",
    "                num_entries,\n",
    "                timepoint_list,\n",
    "                file_idx,\n",
    "                retries=retries,\n",
    "                priority=priority,\n",
    "            )\n",
    "            dask_controller.futures[\"extract file: \" + str(file_idx)] = future\n",
    "\n",
    "        extracted_futures = [\n",
    "            dask_controller.futures[\"extract file: \" + str(file_idx)]\n",
    "            for file_idx in file_list\n",
    "        ]\n",
    "        pause_for_extract = dask_controller.daskclient.gather(\n",
    "            extracted_futures, errors=\"skip\"\n",
    "        )\n",
    "\n",
    "        futures_name_list = [\"extract file: \" + str(file_idx) for file_idx in file_list]\n",
    "        failed_files = [\n",
    "            futures_name_list[k]\n",
    "            for k, item in enumerate(extracted_futures)\n",
    "   
         if item.status is not \"finished\"\n",
    "        ]\n",
    "        failed_file_idx = [int(item.split(\":\")[1]) for item in failed_files]\n",
    "        outdf = self.meta_handle.read_df(\"global\", read_metadata=False)\n",
    "\n",
    "        tempmeta = outdf.reset_index(inplace=False)\n",
    "        tempmeta = tempmeta.set_index(\n",
    "            [\"File Index\", \"Image Index\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        failed_fovs = tempmeta.loc[failed_file_idx][\"fov\"].unique().tolist()\n",
    "\n",
    "        outdf = outdf.drop(failed_fovs)\n",
    "\n",
    "        if self.t_range is not None:\n",
    "            outdf = outdf.reset_index(inplace=False)\n",
    "            outdf[\"timepoints\"] = outdf[\"timepoints\"] - self.t_range[0]\n",
    "            outdf = outdf.set_index(\n",
    "                [\"fov\", \"timepoints\"], drop=True, append=False, inplace=False\n",
    "            )\n",
    "\n",
    "        self.meta_handle.write_df(\"global\", outdf, metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = tiff_extractor(\n",
    "    \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation\",\n",
    "    \"/n/scratch3/users/d/de64/test\",\n",
    "    [\"channel1\", \"channel2\"],\n",
    "    tpts_per_file=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_metadata, fov_metadata = extract.get_metadata(extract.tiffpath, extract.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_metadata.loc[0, 0][\"channel_paths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.metadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "\n",
    "import h5py\n",
    "import h5py_cache\n",
    "import ipywidgets as ipyw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nd2reader import ND2Reader\n",
    "from parse import compile\n",
    "from tifffile import imread, imsave\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "from paulssonlab.deaton.trenchripper.trenchripper.utils import (\n",
    "    pandas_hdf5_handler,\n",
    "    writedir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/trenchripper\"\n",
    "tiffpath = \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = tiff_extractor(tiffpath, headpath, [\"channel1\", \"channel2\"], tpts_per_file=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.inter_set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.view_image_interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
