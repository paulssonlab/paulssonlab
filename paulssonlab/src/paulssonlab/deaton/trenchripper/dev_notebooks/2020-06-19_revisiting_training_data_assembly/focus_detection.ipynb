{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/190922_20x_phase_gfp_segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(headpath + \"/kymograph/kymograph_0.hdf5\", \"r\") as infile:\n",
    "    data = infile[\"GFP\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "import skimage as sk\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 40, 2):\n",
    "    with h5py.File(\n",
    "        headpath + \"/kymograph/kymograph_\" + str(i) + \".hdf5\", \"r\"\n",
    "    ) as infile:\n",
    "        data = infile[\"GFP\"][0, 0]\n",
    "    plt.imshow(data / np.prod(data.shape))\n",
    "    plt.show()\n",
    "\n",
    "    H = data.shape[0]\n",
    "    W = data.shape[1]\n",
    "    ten = np.sum(sk.filters.sobel_h(data) ** 2 + sk.filters.sobel_v(data) ** 2)\n",
    "    print(ten)\n",
    "\n",
    "    AC = np.sum(data[:, :-1] * data[:, 1:]) - np.sum(data[:, :-5] * data[:, 5:])\n",
    "    print(AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focus_score(img_arr):\n",
    "    # computes focus score from single image\n",
    "    img_min = np.min(img_arr)\n",
    "    img_max = np.max(img_arr)\n",
    "    I = (img_arr - img_min) / (img_max - img_min)\n",
    "\n",
    "    Sx = sk.filters.sobel_h(I)\n",
    "    Sy = sk.filters.sobel_v(I)\n",
    "    Ften = np.sum(Sx**2 + Sy**2)\n",
    "    return Ften"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_list = []\n",
    "img_list = []\n",
    "for i in range(0, 200):\n",
    "    with h5py.File(\n",
    "        headpath + \"/kymograph/kymograph_\" + str(i) + \".hdf5\", \"r\"\n",
    "    ) as infile:\n",
    "        for j in range(5):\n",
    "            data = infile[\"GFP\"][j, 0]\n",
    "\n",
    "            if np.sum(data) / np.prod(data.shape) > 2000:\n",
    "                ten = get_focus_score(data)\n",
    "\n",
    "                #                 ten = np.sum(sk.filters.sobel_h(data)**2 + sk.filters.sobel_v(data)**2)\n",
    "\n",
    "                #     AC = np.sum(data[:,:-1]*data[:,1:]) - np.sum(data[:,:-2]*data[:,2:])\n",
    "                F_list.append(ten)\n",
    "                img_list.append(data)\n",
    "\n",
    "F_arr = np.array(F_list)\n",
    "img_arr = np.array(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F_list, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_F = F_arr > 80.0\n",
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(img_arr[high_F][:])\n",
    "plt.imshow(handle.return_unwrap()[:, :1000])\n",
    "plt.show()\n",
    "\n",
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(img_arr[~high_F][:])\n",
    "plt.imshow(handle.return_unwrap()[:, :1500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_F = AC_arr > 1.3\n",
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(img_arr[high_F])\n",
    "plt.imshow(handle.return_unwrap()[:, :500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(handle.return_unwrap()[:, :500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(handle.return_unwrap()[:, :500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Implementing in Kymograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002/190917_20x_phase_gfp_segmentation002/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=10,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=25, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "now want histogram and examples\n",
    "\n",
    "\n",
    "show histogram -> apply filter -> plot random sample of both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from ipywidgets import (\n",
    "    Dropdown,\n",
    "    FloatRangeSlider,\n",
    "    FloatSlider,\n",
    "    IntRangeSlider,\n",
    "    IntSlider,\n",
    "    IntText,\n",
    "    Select,\n",
    "    SelectMultiple,\n",
    "    fixed,\n",
    "    interact,\n",
    "    interact_manual,\n",
    "    interactive,\n",
    ")\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class focus_filter:\n",
    "    def __init__(self, headpath):\n",
    "        self.headpath = headpath\n",
    "        self.kymographpath = headpath + \"/kymograph\"\n",
    "        self.df = dd.read_parquet(self.kymographpath + \"/metadata\")\n",
    "\n",
    "        self.final_params = {}\n",
    "\n",
    "    def choose_filter_channel(self, channel):\n",
    "        self.final_params[\"Filter Channel\"] = channel\n",
    "        self.channel = channel\n",
    "\n",
    "    def choose_filter_channel_inter(self):\n",
    "        channel_options = [\n",
    "            column[:-12]\n",
    "            for column in self.df.columns.tolist()\n",
    "            if column[-11:] == \"Focus Score\"\n",
    "        ]\n",
    "\n",
    "        choose_channel = interactive(\n",
    "            self.choose_filter_channel,\n",
    "            {\"manual\": True},\n",
    "            channel=Dropdown(options=channel_options, value=channel_options[0]),\n",
    "        )\n",
    "        display(choose_channel)\n",
    "\n",
    "    def subsample_df(self, df, n_samples):\n",
    "        ttl_rows = len(df)\n",
    "        n_samples = min(n_samples, ttl_rows)\n",
    "        frac = min((n_samples / ttl_rows) * 1.1, 1.0)\n",
    "        subsampled_df = df.sample(frac=frac, replace=False).compute()[:n_samples]\n",
    "        return subsampled_df\n",
    "\n",
    "    def plot_histograms(self, n_samples=10000):\n",
    "        subsampled_df = self.subsample_df(self.df, n_samples)\n",
    "        focus_vals = subsampled_df[self.channel + \" Focus Score\"]\n",
    "        self.focus_max = np.max(focus_vals)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.hist(focus_vals, bins=50)\n",
    "        ax.set_title(\"Focus Score Distribution\", fontsize=20)\n",
    "        ax.set_xlabel(\"Focus Score\", fontsize=15)\n",
    "        fig.set_size_inches(9, 6)\n",
    "        fig.show()\n",
    "\n",
    "        intensity_vals = subsampled_df[self.channel + \" Mean Intensity\"]\n",
    "        self.intensity_max = np.max(intensity_vals)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.hist(intensity_vals, bins=50)\n",
    "        ax.set_title(\"Mean Intensity Distribution\", fontsize=20)\n",
    "        ax.set_xlabel(\"Mean Intensity\", fontsize=15)\n",
    "        fig.set_size_inches(9, 6)\n",
    "        fig.show()\n",
    "\n",
    "    def plot_trench_sample(self, df, cmap=\"Greys_r\", title=\"\"):\n",
    "        array_list = []\n",
    "        for index, row in df.iterrows():\n",
    "            file_idx = row[\"File Index\"]\n",
    "            row_idx = str(row[\"row\"])\n",
    "            trench_idx = row[\"trench\"]\n",
    "            timepoint_idx = row[\"timepoints\"]\n",
    "\n",
    "            with h5py.File(\n",
    "                self.kymographpath + \"/kymograph_processed_\" + str(file_idx) + \".hdf5\",\n",
    "                \"r\",\n",
    "            ) as hdf5_handle:\n",
    "                array = hdf5_handle[row_idx + \"/\" + self.channel][\n",
    "                    trench_idx, timepoint_idx\n",
    "                ]\n",
    "            array_list.append(array)\n",
    "        output_array = np.concatenate(np.expand_dims(array_list, axis=0), axis=0)\n",
    "        kymo = tr.kymo_handle()\n",
    "        kymo.import_wrap(output_array)\n",
    "        kymo = kymo.return_unwrap()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.set_title(title, fontsize=20)\n",
    "        ax.imshow(kymo, cmap=cmap)\n",
    "        fig.set_size_inches(18, 12)\n",
    "        fig.show()\n",
    "\n",
    "    def plot_focus_threshold(\n",
    "        self, focus_thr=60, intensity_thr=0, perc_above_thr=1.0, n_images=50\n",
    "    ):\n",
    "        self.final_params[\"Focus Threshold\"] = focus_thr\n",
    "        self.final_params[\"Intensity Threshold\"] = intensity_thr\n",
    "        self.final_params[\"Percent Of Kymograph\"] = perc_above_thr\n",
    "\n",
    "        thr_bool = (self.df[self.channel + \" Focus Score\"] > focus_thr) & (\n",
    "            self.df[self.channel + \" Mean Intensity\"] > intensity_thr\n",
    "        )\n",
    "\n",
    "        above_thr_df = self.df[thr_bool]\n",
    "        below_thr_df = self.df[~thr_bool]\n",
    "\n",
    "        above_thr_df = self.subsample_df(above_thr_df, n_images).sort_index()\n",
    "        below_thr_df = self.subsample_df(below_thr_df, n_images).sort_index()\n",
    "\n",
    "        self.plot_trench_sample(above_thr_df, title=\"Above Threshold\")\n",
    "        self.plot_trench_sample(below_thr_df, title=\"Below Threshold\")\n",
    "\n",
    "    def plot_focus_threshold_inter(self):\n",
    "        focus_threshold = interactive(\n",
    "            self.plot_focus_threshold,\n",
    "            {\"manual\": True},\n",
    "            focus_thr=IntSlider(value=0, min=0, max=self.focus_max, step=1),\n",
    "            intensity_thr=IntSlider(value=0, min=0, max=self.intensity_max, step=1),\n",
    "            perc_above_thr=FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05),\n",
    "            n_images=IntText(value=50, description=\"Number of images:\", disabled=False),\n",
    "        )\n",
    "\n",
    "        display(focus_threshold)\n",
    "\n",
    "    def write_param_file(self):\n",
    "        with open(self.headpath + \"/focus_filter.par\", \"wb\") as outfile:\n",
    "            pickle.dump(self.final_params, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = focus_filter(kymoclust.headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(kymoclust.kymographpath + \"/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "[column[:-12] for column in df.columns.tolist() if column[-11:] == \"Focus Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_vals = subsample_df(df[\"GFP Focus Score\"], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(focus_vals, bins=50)\n",
    "ax.set_title(\"Focus Score Distribution\", fontsize=20)\n",
    "ax.set_xlabel(\"Focus Score\", fontsize=15)\n",
    "fig.set_size_inches(9, 6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_focus_threshold(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_focus_threshold(df,focus_thr=60,n_images=100,channel=\"GFP\")\n",
    "    thr_bool = df[channel + \" Focus Score\"]>focus_thr\n",
    "    above_thr_df = df[thr_bool]\n",
    "    below_thr_df = df[~thr_bool]\n",
    "\n",
    "    above_thr_df = subsample_df(above_thr_df,n_images).sort_index()\n",
    "    below_thr_df = subsample_df(below_thr_df,n_images).sort_index()\n",
    "\n",
    "    plot_trench_sample(above_thr_df,\"GFP\")\n",
    "    plot_trench_sample(below_thr_df,\"GFP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_bool = df[\"GFP Focus Score\"] > focus_thr\n",
    "above_thr_df = df[thr_bool]\n",
    "below_thr_df = df[~thr_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "above_thr_df\n",
    "\n",
    "focus_vals = (\n",
    "    df[\"GFP Focus Score\"].sample(frac=frac, replace=False).compute()[:n_samples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.fig_size = (fig_size_y, fig_size_x)\n",
    "self.img_per_row = img_per_row\n",
    "\n",
    "rand_trench_arr = np.random.choice(self.trenchid_arr, size=(n_trenches,), replace=False)\n",
    "self.selecteddf = self.kymodf.loc[\n",
    "    list(zip(rand_trench_arr, np.zeros(len(rand_trench_arr)).astype(int)))\n",
    "]\n",
    "selectedlist = list(\n",
    "    zip(\n",
    "        self.selecteddf[\"File Index\"].tolist(),\n",
    "        self.selecteddf[\"File Trench Index\"].tolist(),\n",
    "    )\n",
    ")\n",
    "\n",
    "array_list = []\n",
    "for item in selectedlist:\n",
    "    with h5py.File(\n",
    "        self.kymographpath + \"/kymograph_\" + str(item[0]) + \".hdf5\", \"r\"\n",
    "    ) as hdf5_handle:\n",
    "        if t_range[1] == None:\n",
    "            array = hdf5_handle[self.seg_channel][\n",
    "                item[1], t_range[0] :: t_subsample_step\n",
    "            ]\n",
    "        else:\n",
    "            array = hdf5_handle[self.seg_channel][\n",
    "                item[1], t_range[0] : t_range[1] + 1 : t_subsample_step\n",
    "            ]\n",
    "    array_list.append(array)\n",
    "output_array = np.concatenate(np.expand_dims(array_list, axis=0), axis=0)\n",
    "self.t_tot = output_array.shape[1]\n",
    "self.plot_kymographs(output_array)\n",
    "self.output_array = output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(kymoclust.kymographpath + \"/metadata/\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"moo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 4\n",
    "with h5py.File(\n",
    "    \"/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002/190917_20x_phase_gfp_segmentation002/kymograph/kymograph_\"\n",
    "    + str(file_idx)\n",
    "    + \".hdf5\",\n",
    "    \"r\",\n",
    ") as infile:\n",
    "    data = infile[\"GFP\"][:]\n",
    "    data = data.reshape(-1, data.shape[2], data.shape[3])\n",
    "    handle = tr.kymo_handle()\n",
    "    handle.import_wrap(data)\n",
    "plt.imshow(handle.return_unwrap()[:, 0:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"GFP Focus Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.import_wrap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(handle.return_unwrap())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(kymoclust.kymographpath + \"/metadata\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(df[\"GFP Focus Score\"].tolist(), bins=100, range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"GFP Focus Score\"] > 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trench_group = df.groupby([\"fov\", \"row\", \"trench\"])\n",
    "\n",
    "max_discont = first_gen_df.groupby(\"trenchid\").apply(compute_del_area)\n",
    "max_discont_filter = max_discont < 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(kymoclust.kymographpath + \"/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgroup = df.groupby(\"trenchid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_list_to_column(df, list_to_add, column_name):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=False)\n",
    "    idx = df[\"index\"].compute()\n",
    "\n",
    "    list_to_add = pd.DataFrame(list_to_add)\n",
    "    list_to_add[\"index\"] = idx\n",
    "    df = df.join(list_to_add.set_index(\"index\"), how=\"left\", on=\"index\")\n",
    "\n",
    "    df = df.drop([\"index\"], axis=1)\n",
    "\n",
    "    df.columns = df.columns.tolist()[:-1] + [column_name]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_focus(channel, df, focus_threshold=0.1, perc_above=0.5):\n",
    "    num_above = np.round(len(df[\"timepoints\"].unique()) * perc_above).astype(int)\n",
    "\n",
    "    trench_group = df.groupby(\"trenchid\")\n",
    "    focus_filter = trench_group.apply(\n",
    "        lambda x: np.sum(x[channel + \" Focus Score\"] > focus_threshold) > num_above\n",
    "    ).compute()\n",
    "    focus_filter = pd.DataFrame({\"focus filter\": focus_filter})\n",
    "    out_df = df.join(focus_filter, on=\"trenchid\")\n",
    "    out_df = out_df[out_df[\"focus filter\"]]\n",
    "    out_df = out_df.drop(labels=\"focus filter\", axis=1)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def reindex_trenches(df):\n",
    "    num_timepoints = len(df[\"timepoints\"].unique())\n",
    "    new_trenches = df.groupby([\"fov\", \"row\"]).apply(\n",
    "        lambda x: np.repeat(\n",
    "            list(range(0, len(x[\"trench\"].unique()))), repeats=num_timepoints\n",
    "        )\n",
    "    )\n",
    "    new_trenches = [element for list_ in new_trenches for element in list_]\n",
    "    df = df.drop([\"trenchid\", \"trench\"], axis=1)\n",
    "\n",
    "    df = add_list_to_column(df, new_trenches, \"trench\")\n",
    "    cols = df.columns.tolist()\n",
    "    reordered_columns = cols[:2] + cols[-1:] + cols[2:-1]\n",
    "    df = df[reordered_columns]\n",
    "\n",
    "    fov_idx = (\n",
    "        df.apply(\n",
    "            lambda x: int(\n",
    "                f'{x[\"fov\"]:04}{x[\"row\"]:04}{x[\"trench\"]:04}{x[\"timepoints\"]:04}'\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        .compute()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    df = add_list_to_column(df, fov_idx, \"FOV Parquet Index\")\n",
    "    df = df.set_index(\"FOV Parquet Index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = filter_focus(\"GFP\", df, focus_threshold=1.0, perc_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = reindex_trenches(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = kymoclust.add_trenchids(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(kymoclust.kymographpath + \"/metadata\").persist()\n",
    "df = filter_focus(\"GFP\", df, focus_threshold=1.0, perc_above=0.5)\n",
    "\n",
    "trenchid_list = df[\"trenchid\"].unique().compute().tolist()\n",
    "file_list = df[\"File Index\"].unique().compute().tolist()\n",
    "outputdf = df.drop(columns=[\"File Index\", \"Image Index\"]).persist()\n",
    "trenchiddf = df.set_index(\"trenchid\").persist()\n",
    "\n",
    "num_tpts = len(trenchiddf[\"timepoints\"].unique().compute().tolist())\n",
    "chunk_size = kymoclust.trenches_per_file * num_tpts\n",
    "if len(trenchid_list) % kymoclust.trenches_per_file == 0:\n",
    "    num_files = len(trenchid_list) // kymoclust.trenches_per_file\n",
    "else:\n",
    "    num_files = (len(trenchid_list) // kymoclust.trenches_per_file) + 1\n",
    "\n",
    "\n",
    "file_indices = np.repeat(np.array(range(num_files)), chunk_size)[: len(outputdf)]\n",
    "file_trenchid = np.repeat(np.array(range(kymoclust.trenches_per_file)), num_tpts)\n",
    "file_trenchid = np.repeat(file_trenchid[:, np.newaxis], num_files, axis=1).T.flatten()[\n",
    "    : len(outputdf)\n",
    "]\n",
    "file_indices = pd.DataFrame(file_indices)\n",
    "file_trenchid = pd.DataFrame(file_trenchid)\n",
    "file_indices.index = outputdf.index\n",
    "file_trenchid.index = outputdf.index\n",
    "\n",
    "outputdf = add_list_to_column(outputdf, file_indices[0].tolist(), \"File Index\")\n",
    "outputdf = add_list_to_column(outputdf, file_trenchid[0].tolist(), \"File Trench Index\")\n",
    "\n",
    "parq_file_idx = outputdf.apply(\n",
    "    lambda x: int(\n",
    "        f'{int(x[\"File Index\"]):04}{int(x[\"File Trench Index\"]):04}{int(x[\"timepoints\"]):04}'\n",
    "    ),\n",
    "    axis=1,\n",
    "    meta=int,\n",
    ")\n",
    "outputdf[\"File Parquet Index\"] = parq_file_idx\n",
    "outputdf = outputdf.astype(\n",
    "    {\"File Index\": int, \"File Trench Index\": int, \"File Parquet Index\": int}\n",
    ")\n",
    "\n",
    "\n",
    "outputdf = reindex_trenches(outputdf)\n",
    "outputdf = kymoclust.add_trenchids(outputdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, num_files):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "output_file_path = kymoclust.kymographpath + \"/kymograph_\" + str(k) + \".hdf5\"\n",
    "with h5py.File(output_file_path, \"w\") as outfile:\n",
    "    for channel in kymoclust.all_channels:\n",
    "        trenchids = trenchid_list[\n",
    "            k * kymoclust.trenches_per_file : (k + 1) * kymoclust.trenches_per_file\n",
    "        ]\n",
    "        working_trenchdf = trenchiddf.loc[trenchids].compute()\n",
    "        fov_list = working_trenchdf[\"fov\"].unique().tolist()\n",
    "        trench_arr_fovs = []\n",
    "        for fov in fov_list:\n",
    "            working_fovdf = working_trenchdf[working_trenchdf[\"fov\"] == fov]\n",
    "            file_list = working_fovdf[\"File Index\"].unique().tolist()\n",
    "\n",
    "            trench_arr_files = []\n",
    "            for file_idx in file_list:\n",
    "                proc_file_path = (\n",
    "                    kymoclust.kymographpath\n",
    "                    + \"/kymograph_processed_\"\n",
    "                    + str(file_idx)\n",
    "                    + \".hdf5\"\n",
    "                )\n",
    "                with h5py.File(proc_file_path, \"r\") as infile:\n",
    "                    working_filedf = working_fovdf[\n",
    "                        working_fovdf[\"File Index\"] == file_idx\n",
    "                    ]\n",
    "                    row_list = working_filedf[\"row\"].unique().tolist()\n",
    "\n",
    "                    trench_arr_rows = []\n",
    "                    for row in row_list:\n",
    "                        working_rowdf = working_filedf[working_filedf[\"row\"] == row]\n",
    "                        #                                 trenches = working_rowdf[\"trench\"].unique().tolist()\n",
    "                        #                                 first_trench_idx,last_trench_idx = (trenches[0],trenches[-1])\n",
    "                        #                                 kymo_arr = infile[str(row) + \"/\" + channel][first_trench_idx:(last_trench_idx+1)]\n",
    "                        trenches = working_rowdf[\"trench\"].unique().tolist()\n",
    "                        print(trenches)\n",
    "                        kymo_arr = infile[str(row) + \"/\" + channel][trenches]\n",
    "                        trench_arr_rows.append(kymo_arr)\n",
    "                trench_arr_rows = np.concatenate(\n",
    "                    trench_arr_rows, axis=0\n",
    "                )  # k x t x y x x\n",
    "                trench_arr_files.append(trench_arr_rows)\n",
    "            trench_arr_files = np.concatenate(trench_arr_files, axis=1)  # k x t x y x x\n",
    "            trench_arr_fovs.append(trench_arr_files)\n",
    "        trench_arr_fovs = np.concatenate(trench_arr_fovs, axis=0)  # k x t x y x x\n",
    "        hdf5_dataset = outfile.create_dataset(\n",
    "            str(channel), data=trench_arr_fovs, dtype=\"uint16\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trench_arr_fovs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(self, dask_controller):\n",
    "    dask_controller.futures = {}\n",
    "\n",
    "    df = dd.read_parquet(self.kymographpath + \"/metadata\").persist()\n",
    "    #         df = self.add_trenchids(df).persist() #NEW\n",
    "\n",
    "    trenchid_list = df[\"trenchid\"].unique().compute().tolist()\n",
    "    file_list = df[\"File Index\"].unique().compute().tolist()\n",
    "    outputdf = df.drop(columns=[\"File Index\", \"Image Index\"]).persist()\n",
    "    trenchiddf = df.set_index(\"trenchid\").persist()\n",
    "\n",
    "    #         with open(self.kymographpath + \"/metadata.pkl\", 'rb') as handle:\n",
    "    #             metadata = pickle.load(handle)\n",
    "\n",
    "    num_tpts = len(trenchiddf[\"timepoints\"].unique().compute().tolist())\n",
    "    chunk_size = self.trenches_per_file * num_tpts\n",
    "    if len(trenchid_list) % self.trenches_per_file == 0:\n",
    "        num_files = len(trenchid_list) // self.trenches_per_file\n",
    "    else:\n",
    "        num_files = (len(trenchid_list) // self.trenches_per_file) + 1\n",
    "\n",
    "    file_indices = np.repeat(np.array(range(num_files)), chunk_size)[: len(outputdf)]\n",
    "    file_trenchid = np.repeat(np.array(range(self.trenches_per_file)), num_tpts)\n",
    "    file_trenchid = np.repeat(\n",
    "        file_trenchid[:, np.newaxis], num_files, axis=1\n",
    "    ).T.flatten()[: len(outputdf)]\n",
    "    file_indices = pd.DataFrame(file_indices)\n",
    "    file_trenchid = pd.DataFrame(file_trenchid)\n",
    "    file_indices.index = outputdf.index\n",
    "    file_trenchid.index = outputdf.index\n",
    "\n",
    "    outputdf[\"File Index\"] = file_indices[0]\n",
    "    outputdf[\"File Trench Index\"] = file_trenchid[0]\n",
    "    parq_file_idx = outputdf.apply(\n",
    "        lambda x: int(\n",
    "            f'{int(x[\"File Index\"]):04}{int(x[\"File Trench Index\"]):04}{int(x[\"timepoints\"]):04}'\n",
    "        ),\n",
    "        axis=1,\n",
    "        meta=int,\n",
    "    )\n",
    "    outputdf[\"File Parquet Index\"] = parq_file_idx\n",
    "    outputdf = outputdf.astype(\n",
    "        {\"File Index\": int, \"File Trench Index\": int, \"File Parquet Index\": int}\n",
    "    )\n",
    "\n",
    "    random_priorities = np.random.uniform(size=(num_files,))\n",
    "    for k in range(0, num_files):\n",
    "        priority = random_priorities[k]\n",
    "        future = dask_controller.daskclient.submit(\n",
    "            self.reorg_kymograph,\n",
    "            k,\n",
    "            df,\n",
    "            trenchid_list,\n",
    "            trenchiddf,\n",
    "            retries=1,\n",
    "            priority=priority,\n",
    "        )\n",
    "        dask_controller.futures[\"Kymograph Reorganized: \" + str(k)] = future\n",
    "\n",
    "    reorg_futures = [\n",
    "        dask_controller.futures[\"Kymograph Reorganized: \" + str(k)]\n",
    "        for k in range(num_files)\n",
    "    ]\n",
    "    future = dask_controller.daskclient.submit(\n",
    "        self.cleanup_kymographs, reorg_futures, file_list, retries=1, priority=priority\n",
    "    )\n",
    "    dask_controller.futures[\"Kymographs Cleaned Up\"] = future\n",
    "    dask_controller.daskclient.gather([future])\n",
    "\n",
    "    dd.to_parquet(\n",
    "        outputdf,\n",
    "        self.kymographpath + \"/metadata\",\n",
    "        engine=\"fastparquet\",\n",
    "        compression=\"gzip\",\n",
    "        write_metadata_file=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "proc_file_path = \"/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002/190917_20x_phase_gfp_segmentation002//kymograph/kymograph_processed_1.hdf5\"\n",
    "with h5py.File(proc_file_path, \"r\") as infile:\n",
    "    kymo_arr = infile[str(0) + \"/\" + \"GFP\"][[0, 4, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = kymoclust.add_trenchids(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_trenches(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=False)\n",
    "    idx = df[\"index\"].compute()\n",
    "\n",
    "    num_timepoints = len(df[\"timepoints\"].unique())\n",
    "    new_trenches = df.groupby([\"fov\", \"row\"]).apply(\n",
    "        lambda x: np.repeat(\n",
    "            list(range(0, len(x[\"trench\"].unique()))), repeats=num_timepoints\n",
    "        )\n",
    "    )\n",
    "    new_trenches = [element for list_ in new_trenches for element in list_]\n",
    "    new_trenches = pd.DataFrame(new_trenches)\n",
    "    new_trenches[\"index\"] = idx\n",
    "    df = df.join(new_trenches.set_index(\"index\"), how=\"left\", on=\"index\")\n",
    "    df = df.drop([\"index\", \"trenchid\", \"trench\"], axis=1)\n",
    "    df.columns = df.columns.tolist()[:-1] + [\"trench\"]\n",
    "    reordered_columns = cols[:2] + cols[-1:] + cols[3:-1]\n",
    "    df = df[reordered_columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "\n",
    "\n",
    "def get_focus_score(img_arr):\n",
    "    # computes focus score from single image\n",
    "\n",
    "    Sx = sk.filters.sobel_h(img_arr)\n",
    "    Sy = sk.filters.sobel_v(img_arr)\n",
    "    Ften = np.sum(Sx**2 + Sy**2)\n",
    "    return Ften"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_rowdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "First, make focus measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focus_scores(file_idx):\n",
    "    df = dd.read_parquet(kymoclust.kymographpath + \"/metadata/\")\n",
    "\n",
    "    working_rowdfs = []\n",
    "\n",
    "    proc_file_path = (\n",
    "        kymoclust.kymographpath + \"/kymograph_processed_\" + str(file_idx) + \".hdf5\"\n",
    "    )\n",
    "    with h5py.File(proc_file_path, \"r\") as infile:\n",
    "        working_filedf = df[df[\"File Index\"] == file_idx]\n",
    "        row_list = working_filedf[\"row\"].unique().compute().tolist()\n",
    "        for row in row_list:\n",
    "            working_rowdf = working_filedf[working_filedf[\"row\"] == row].compute()\n",
    "            kymo_arr = infile[str(row) + \"/\" + channel][:]\n",
    "            original_shape = kymo_arr.shape\n",
    "            kymo_arr = kymo_arr.reshape(-1, original_shape[2], original_shape[3])\n",
    "            focus_scores = [\n",
    "                get_focus_score(kymo_arr[i]) for i in range(kymo_arr.shape[0])\n",
    "            ]\n",
    "            working_rowdf[\"Focus Score\"] = focus_scores\n",
    "            working_rowdfs.append(working_rowdf)\n",
    "\n",
    "    out_df = pd.concat(working_rowdfs)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from time import sleep\n",
    "\n",
    "import h5py\n",
    "from dask import delayed\n",
    "from distributed.client import futures_of\n",
    "\n",
    "\n",
    "def get_focus_scores(self, file_idx):\n",
    "    df = dd.read_parquet(self.kymographpath + \"/metadata\")\n",
    "\n",
    "    working_rowdfs = []\n",
    "\n",
    "    proc_file_path = (\n",
    "        self.kymographpath + \"/kymograph_processed_\" + str(file_idx) + \".hdf5\"\n",
    "    )\n",
    "    with h5py.File(proc_file_path, \"r\") as infile:\n",
    "        working_filedf = df[df[\"File Index\"] == file_idx]\n",
    "        row_list = working_filedf[\"row\"].unique().compute().tolist()\n",
    "        for row in row_list:\n",
    "            working_rowdf = working_filedf[working_filedf[\"row\"] == row].compute()\n",
    "            kymo_arr = infile[str(row) + \"/\" + channel][:]\n",
    "            original_shape = kymo_arr.shape\n",
    "            kymo_arr = kymo_arr.reshape(-1, original_shape[2], original_shape[3])\n",
    "            focus_scores = [\n",
    "                get_focus_score(kymo_arr[i]) for i in range(kymo_arr.shape[0])\n",
    "            ]\n",
    "            working_rowdf[\"Focus Score\"] = focus_scores\n",
    "            working_rowdfs.append(working_rowdf)\n",
    "\n",
    "    out_df = pd.concat(working_rowdfs)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_all_focus_scores(self, channel):\n",
    "    df = dd.read_parquet(self.kymographpath + \"/metadata\")\n",
    "\n",
    "    file_list = df[\"File Index\"].unique().compute().tolist()\n",
    "\n",
    "    delayed_list = []\n",
    "\n",
    "    for file_idx in file_list:\n",
    "        df_delayed = delayed(get_focus_scores)(file_idx)\n",
    "        delayed_list.append(df_delayed.persist())\n",
    "\n",
    "    ## filtering out non-failed dataframes ##\n",
    "    all_delayed_futures = []\n",
    "    for item in delayed_list:\n",
    "        all_delayed_futures += futures_of(item)\n",
    "    while any(future.status == \"pending\" for future in all_delayed_futures):\n",
    "        sleep(0.1)\n",
    "\n",
    "    good_delayed = []\n",
    "    for item in delayed_list:\n",
    "        if all([future.status == \"finished\" for future in futures_of(item)]):\n",
    "            good_delayed.append(item)\n",
    "\n",
    "    ## compiling output dataframe ##\n",
    "    df_out = dd.from_delayed(good_delayed).persist()\n",
    "    df_out = df_out.repartition(partition_size=\"25MB\").persist()\n",
    "    tr.writedir(self.kymographpath + \"/metadata\", overwrite=True)\n",
    "    dd.to_parquet(\n",
    "        df_out,\n",
    "        self.kymographpath + \"/metadata/\",\n",
    "        engine=\"fastparquet\",\n",
    "        compression=\"gzip\",\n",
    "        write_metadata_file=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_focus_scores(\"GFP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(kymoclust.kymographpath + \"/metadata\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "            df_delayed = delayed(self.save_coords)(file_idx)\n",
    "            self.delayed_list.append(df_delayed.persist())\n",
    "\n",
    "        ## filtering out non-failed dataframes ##\n",
    "        all_delayed_futures = []\n",
    "        for item in self.delayed_list:\n",
    "            all_delayed_futures+=futures_of(item)\n",
    "        while any(future.status == 'pending' for future in all_delayed_futures):\n",
    "            sleep(0.1)\n",
    "\n",
    "        good_delayed = []\n",
    "        for item in self.delayed_list:\n",
    "            if all([future.status == 'finished' for future in futures_of(item)]):\n",
    "                good_delayed.append(item)\n",
    "\n",
    "        ## compiling output dataframe ##\n",
    "        df_out = dd.from_delayed(good_delayed).persist()\n",
    "        df_out = df_out.repartition(partition_size=\"25MB\").persist()\n",
    "        dd.to_parquet(df_out, self.kymographpath + \"/metadata/\",engine='fastparquet',compression='gzip',write_metadata_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.apply_over_axes(get_focus_score, kymo_arr, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[]\n",
    "\n",
    "trenchids = trenchid_list[k*self.trenches_per_file:(k+1)*self.trenches_per_file]\n",
    "working_trenchdf = trenchiddf.loc[trenchids].compute()\n",
    "fov_list = working_trenchdf[\"fov\"].unique().tolist()\n",
    "trench_arr_fovs = []\n",
    "for fov in fov_list:\n",
    "    working_fovdf = working_trenchdf[working_trenchdf[\"fov\"]==fov]\n",
    "    file_list = working_fovdf[\"File Index\"].unique().tolist()\n",
    "\n",
    "    trench_arr_files = []\n",
    "    for file_idx in file_list:\n",
    "        proc_file_path = self.kymographpath+\"/kymograph_processed_\"+str(file_idx)+\".hdf5\"\n",
    "        with h5py.File(proc_file_path,\"r\") as infile:\n",
    "            working_filedf = working_fovdf[working_fovdf[\"File Index\"]==file_idx]\n",
    "            row_list = working_filedf[\"row\"].unique().tolist()\n",
    "\n",
    "            trench_arr_rows = []\n",
    "            for row in row_list:\n",
    "                working_rowdf = working_filedf[working_filedf[\"row\"]==row]\n",
    "                trenches = working_rowdf[\"trench\"].unique().tolist()\n",
    "                first_trench_idx,last_trench_idx = (trenches[0],trenches[-1])\n",
    "                kymo_arr = infile[str(row) + \"/\" + channel][first_trench_idx:(last_trench_idx+1)]\n",
    "                trench_arr_rows.append(kymo_arr)\n",
    "        trench_arr_rows = np.concatenate(trench_arr_rows,axis=0) # k x t x y x x\n",
    "        trench_arr_files.append(trench_arr_rows)\n",
    "    trench_arr_files = np.concatenate(trench_arr_files,axis=1) # k x t x y x x\n",
    "    trench_arr_fovs.append(trench_arr_files)\n",
    "trench_arr_fovs = np.concatenate(trench_arr_fovs,axis=0) # k x t x y x x\n",
    "hdf5_dataset = outfile.create_dataset(str(channel), data=trench_arr_fovs, dtype=\"uint16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
