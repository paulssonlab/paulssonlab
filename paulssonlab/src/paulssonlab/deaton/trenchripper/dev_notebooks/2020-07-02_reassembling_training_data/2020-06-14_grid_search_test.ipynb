{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import ipywidgets as ipyw\n",
    "import pandas as pd\n",
    "import qgrid\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "from paulssonlab.deaton.trenchripper.trenchripper.utils import pandas_hdf5_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "from scipy.ndimage import convolve1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flows(labeled, eps=0.00001):\n",
    "    rps = sk.measure.regionprops(labeled)\n",
    "    centers = np.array([np.round(rp.centroid).astype(\"uint16\") for rp in rps])\n",
    "    y_lens = np.array([rp.bbox[2] - rp.bbox[0] for rp in rps])\n",
    "    x_lens = np.array([rp.bbox[3] - rp.bbox[1] for rp in rps])\n",
    "    N_arr = 2 * (y_lens + x_lens)\n",
    "    kernel = np.ones(3, float) / 3.0\n",
    "\n",
    "    x_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "    y_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "\n",
    "    for cell_idx in range(1, len(rps) + 1):\n",
    "        cell_mask = labeled == cell_idx\n",
    "        cell_center = centers[cell_idx - 1]\n",
    "        diffusion_arr = np.zeros(cell_mask.shape, dtype=np.float32)\n",
    "        for i in range(N_arr[cell_idx - 1]):\n",
    "            diffusion_arr[cell_center] += 1.0\n",
    "            diffusion_arr = convolve1d(\n",
    "                convolve1d(diffusion_arr, kernel, axis=0), kernel, axis=1\n",
    "            )\n",
    "            diffusion_arr[~cell_mask] = 0.0\n",
    "\n",
    "        y_grad, x_grad = np.gradient(diffusion_arr)\n",
    "\n",
    "        norm = np.sqrt(y_grad**2 + x_grad**2)\n",
    "\n",
    "        y_grad, x_grad = (y_grad / (norm + eps)), (x_grad / (norm + eps))\n",
    "        y_grad[~cell_mask] = 0.0\n",
    "        x_grad[~cell_mask] = 0.0\n",
    "\n",
    "        y_grad_arr += y_grad\n",
    "        x_grad_arr += x_grad\n",
    "\n",
    "    return y_grad_arr, x_grad_arr\n",
    "\n",
    "\n",
    "def get_two_class(labeled):\n",
    "    segmentation = tr.get_segmentation(\n",
    "        labeled, mode_list=[\"background\", \"mask\", \"border\"]\n",
    "    )\n",
    "    weightmap = tr.get_standard_weightmap(segmentation)\n",
    "    if np.any(np.isnan(segmentation)) or np.any(np.isnan(weightmap)):\n",
    "        print(\"two_class\")\n",
    "        segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "        weightmap = np.ones(segmentation.shape, dtype=np.float32)\n",
    "    return segmentation, weightmap\n",
    "\n",
    "\n",
    "def get_one_class(labeled, W0=5.0, Wsigma=2.0):\n",
    "    segmentation = tr.get_segmentation(\n",
    "        labeled, mode_list=[\"background\", \"masknoborder\"]\n",
    "    ).astype(bool)\n",
    "    weightmap = tr.get_unet_weightmap(labeled, W0=W0, Wsigma=Wsigma)\n",
    "    if np.any(np.isnan(segmentation)) or np.any(np.isnan(weightmap)):\n",
    "        print(\"one_class\")\n",
    "        segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "        weightmap = np.ones(segmentation.shape, dtype=np.float32)\n",
    "    return segmentation, weightmap\n",
    "\n",
    "\n",
    "def get_cellpose(labeled):\n",
    "    segmentation = tr.get_segmentation(\n",
    "        labeled, mode_list=[\"background\", \"mask\"]\n",
    "    ).astype(bool)\n",
    "    y_grad_arr, x_grad_arr = get_flows(labeled)\n",
    "    if (\n",
    "        np.any(np.isnan(segmentation))\n",
    "        or np.any(np.isnan(y_grad_arr))\n",
    "        or np.any(np.isnan(x_grad_arr))\n",
    "    ):\n",
    "        print(\"cellpose\")\n",
    "        segmentation = np.zeros(labeled.shape, dtype=\"uint8\")\n",
    "        x_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "        y_grad_arr = np.zeros(labeled.shape, dtype=np.float32)\n",
    "    return segmentation, y_grad_arr, x_grad_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_opts = [\n",
    "    \"/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002/190917_20x_phase_gfp_segmentation002\",\n",
    "    \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/190922_20x_phase_gfp_segmentation\",\n",
    "    \"/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation/190925_20x_phase_yfp_segmentation\",\n",
    "    \"/n/scratch3/users/d/de64/ezrdm training sb7/ezrdm training sb7\",\n",
    "    \"/n/scratch3/users/d/de64/mbm training sb7/mbm training sb7\",\n",
    "    \"/n/scratch3/users/d/de64/Sb7_L35/Sb7_L35\",\n",
    "    \"/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9/MM_DVCvecto_TOP_1_9\",\n",
    "    \"/n/scratch3/users/d/de64/Vibrio_2_1_TOP/Vibrio_2_1_TOP\",\n",
    "    \"/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms/Vibrio_A_B_VZRDM--04--RUN_80ms\",\n",
    "    \"/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X/RpoSOutliers_WT_hipQ_100X\",\n",
    "    \"/n/scratch3/users/d/de64/Main_Experiment/Main_Experiment\",\n",
    "    \"/n/scratch3/users/d/de64/bde17_gotime/bde17_gotime\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_idx = 2\n",
    "\n",
    "for j in range(3):\n",
    "    img_idx = j\n",
    "\n",
    "    with h5py.File(\n",
    "        path_opts[path_idx]\n",
    "        + \"/fluorsegmentation/segmentation_\"\n",
    "        + str(img_idx)\n",
    "        + \".hdf5\",\n",
    "        \"r\",\n",
    "    ) as segfile:\n",
    "        seg_arr = segfile[\"data\"][:, :3]\n",
    "        seg_arr = seg_arr.reshape(-1, seg_arr.shape[2], seg_arr.shape[3])\n",
    "    for i in range(seg_arr.shape[0]):\n",
    "        segmentation, weightmap = get_two_class(seg_arr[i])\n",
    "        segmentation, weightmap = get_one_class(seg_arr[i], W0=5.0, Wsigma=2.0)\n",
    "#         segmentation,y_grad_arr,x_grad_arr = get_cellpose(seg_arr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(seg_arr.shape[0]):\n",
    "    segmentation, weightmap = get_one_class(seg_arr[i], W0=5.0, Wsigma=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/190922_20x_phase_gfp_segmentation/fluorsegmentation/segmentation_1.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "['/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002',\n",
    " '/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/ezrdm\\\\ training\\\\ sb7',\n",
    " '/n/scratch3/users/d/de64/mbm\\\\ training\\\\ sb7',\n",
    " '/n/scratch3/users/d/de64/Sb7_L35',\n",
    " '/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9',\n",
    " '/n/scratch3/users/d/de64/Vibrio_2_1_TOP',\n",
    " '/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms',\n",
    " '/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X',\n",
    " '/n/scratch3/users/d/de64/Main_Experiment',\n",
    " '/n/scratch3/users/d/de64/bde17_gotime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = tr.UNet_Training_DataLoader(nndatapath=\"/n/scratch3/users/d/de64/2020-06-14_NN\",experimentname=\"2020-06-14 Neural Net\",\\\n",
    "#                            input_paths=[\"/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002/190917_20x_phase_gfp_segmentation002\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation/190922_20x_phase_gfp_segmentation\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation/190925_20x_phase_yfp_segmentation\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/ezrdm training sb7/ezrdm training sb7\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/mbm training sb7/mbm training sb7\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/Sb7_L35/Sb7_L35\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9/MM_DVCvecto_TOP_1_9\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/Vibrio_2_1_TOP/Vibrio_2_1_TOP\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms/Vibrio_A_B_VZRDM--04--RUN_80ms\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X/RpoSOutliers_WT_hipQ_100X\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/Main_Experiment/Main_Experiment\",\\\n",
    "#                                         \"/n/scratch3/users/d/de64/bde17_gotime/bde17_gotime\"],\\\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = tr.UNet_Training_DataLoader(\n",
    "    nndatapath=\"/n/scratch3/users/d/de64/2020-07-05_NN\",\n",
    "    experimentname=\"2020-07-05 Neural Net\",\n",
    "    input_paths=[\n",
    "        \"/n/scratch3/users/d/de64/ezrdm_training_sb7/ezrdm_training_sb7\",\n",
    "        \"/n/scratch3/users/d/de64/mbm_training_sb7/mbm_training_sb7\",\n",
    "        \"/n/scratch3/users/d/de64/Sb7_L35/Sb7_L35\",\n",
    "        \"/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9/MM_DVCvecto_TOP_1_9\",\n",
    "        \"/n/scratch3/users/d/de64/Vibrio_2_1_TOP/Vibrio_2_1_TOP\",\n",
    "        \"/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms/Vibrio_A_B_VZRDM--04--RUN_80ms\",\n",
    "        \"/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X/RpoSOutliers_WT_hipQ_100X\",\n",
    "        \"/n/scratch3/users/d/de64/Main_Experiment/Main_Experiment\",\n",
    "        \"/n/scratch3/users/d/de64/bde17_gotime/bde17_gotime\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.inter_get_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "E. coli Ti6\n",
    "2) /n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/!!Jacob Quinn Shenker/190917/190917_20x_phase_gfp_segmentation002      11GB (seg: ok)\n",
    "3) /n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/Noah/190922/20x_segmentation_data/190922_20x_phase_gfp_segmentation    358GB (seg: ok)\n",
    "4) /n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/Noah/190922/20x_segmentation_data/190925_20x_phase_yfp_segmentation    ?? (seg: ok)\n",
    "5) /n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/Carlos/Data_Ti6/SB7_trainingdata_Unet/ezrdm\\ training\\ sb7.nd2         85GB (seg: ok)\n",
    "6) /n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/Carlos/Data_Ti6/SB7_trainingdata_Unet/mbm\\ training\\ sb7.nd2           85GB (seg: ok)\n",
    "\n",
    "E. coli Ti5\n",
    "7) /n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/Carlos/Data_Ti5/before bl2+/SB7_trainingdata_NN_MM/Sb7_L35.nd2         66GB (seg: good)\n",
    "\n",
    "E. coli, Ti3\n",
    "11) /n/files/SysBio/PAULSSON\\ LAB/SILVIA/Ti3Data/2020_01_29/RpoSOutliers_WT_hipQ_100X.nd2                                  476GB (seg: good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.get_import_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "daskcont = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    cores=2,\n",
    "    n_workers=20,\n",
    "    memory=\"8GB\",\n",
    "    working_directory=\"/n/scratch3/users/d/de64/2020-06-14_NN\" + \"/dask\",\n",
    ")\n",
    "daskcont.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "daskcont.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.export_data(daskcont, chunk_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "daskcont.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = tr.GridSearch(\"/n/scratch3/users/d/de64/2020-07-05_NN/\", numepochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.display_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_grid_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.run_grid_search(mem=\"16G\", hours=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue --user=de64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "nntrainer = tr.unet.UNet_Trainer(\n",
    "    \"/n/scratch3/users/d/de64/2020-07-05_NN/\",\n",
    "    0,\n",
    "    \"class\",\n",
    "    gpuon=True,\n",
    "    numepochs=100,\n",
    "    batch_size=10,\n",
    "    layers=2,\n",
    "    hidden_size=32,\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0001,\n",
    "    dropout=0.0,\n",
    "    W0=5.0,\n",
    "    Wsigma=2.0,\n",
    "    warm_epochs=10,\n",
    "    cool_epochs=50,\n",
    ")\n",
    "nntrainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "nntrainer = tr.unet.UNet_Trainer(\n",
    "    \"/n/scratch3/users/d/de64/2020-06-14_NN//\",\n",
    "    0,\n",
    "    \"class\",\n",
    "    gpuon=False,\n",
    "    numepochs=100,\n",
    "    batch_size=25,\n",
    "    layers=4,\n",
    "    hidden_size=64,\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0001,\n",
    "    dropout=0.0,\n",
    "    W0=5.0,\n",
    "    Wsigma=2.0,\n",
    "    warm_epochs=10,\n",
    "    cool_epochs=50,\n",
    ")\n",
    "\n",
    "train_data = tr.SegmentationDataset(\n",
    "    nntrainer.nndatapath + \"train.hdf5\",\n",
    "    mode=nntrainer.mode,\n",
    "    W0=nntrainer.W0,\n",
    "    Wsigma=nntrainer.Wsigma,\n",
    ")\n",
    "train_iter = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=nntrainer.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=tr.numpy_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_iter:\n",
    "    hand = tr.kymo_handle()\n",
    "    hand.import_wrap(item[\"img\"][:, 0])\n",
    "    img = hand.return_unwrap()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose and fix loss function errors\n",
    "# test all three models for 1 epoch\n",
    "# run 3 layer, 64 hidden state NNs with minimal augmentation (overfit)\n",
    "# recompute examples (accuracy seems low...)\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "trainer = tr.UNet_Trainer(\n",
    "    \"/n/scratch3/users/d/de64/2020-06-14_NN/\",\n",
    "    100,\n",
    "    \"class\",\n",
    "    numepochs=100,\n",
    "    batch_size=100,\n",
    "    gpuon=True,\n",
    "    lr=0.05,\n",
    "    cool_epochs=30,\n",
    "    layers=2,\n",
    "    hidden_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f trainer.perepoch trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.chunk_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.chunk_dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tr.SegmentationDataset(\n",
    "    trainer.nndatapath + \"train.hdf5\",\n",
    "    mode=trainer.mode,\n",
    "    W0=trainer.W0,\n",
    "    Wsigma=trainer.Wsigma,\n",
    ")\n",
    "train_iter = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=trainer.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=tr.numpy_collate,\n",
    ")\n",
    "for i, item in enumerate(train_iter):\n",
    "    if i == 245:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr, seg_arr, y_grad_arr, x_grad_arr = (\n",
    "    item[\"img\"],\n",
    "    item[\"seg\"],\n",
    "    item[\"y_grad\"],\n",
    "    item[\"x_grad\"],\n",
    ")\n",
    "y_grad_arr, x_grad_arr, seg_arr = y_grad_arr[:, 0], x_grad_arr[:, 0], seg_arr[:, 0]\n",
    "x = torch.Tensor(img_arr.astype(float))\n",
    "y = np.stack([y_grad_arr, x_grad_arr, seg_arr], axis=1)\n",
    "y = torch.Tensor(y)\n",
    "if trainer.gpuon:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "\n",
    "fx = trainer.model.forward(x)\n",
    "mask_pred = F.sigmoid(fx[:, 2])\n",
    "\n",
    "mse = F.mse_loss(fx[:, :2], y[:, :2], reduction=\"none\")  ## N,* to N,*\n",
    "cross_entropy = F.binary_cross_entropy(mask_pred, y[:, 2], reduction=\"none\")\n",
    "\n",
    "loss = cross_entropy + 5.0 * mse[:, 0] + 5.0 * mse[:, 1]\n",
    "# loss = trainer.cellpose_train(x,y)\n",
    "# loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(loss.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([value[0] for value in test_data.dset_shapes.values()]) // 1001) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SegmentationDataset(\n",
    "    trainer.nndatapath + \"train.hdf5\",\n",
    "    mode=trainer.mode,\n",
    "    W0=trainer.W0,\n",
    "    Wsigma=trainer.Wsigma,\n",
    ")\n",
    "test_data = SegmentationDataset(\n",
    "    trainer.nndatapath + \"test.hdf5\",\n",
    "    mode=trainer.mode,\n",
    "    W0=trainer.W0,\n",
    "    Wsigma=trainer.Wsigma,\n",
    ")\n",
    "val_data = SegmentationDataset(\n",
    "    trainer.nndatapath + \"val.hdf5\",\n",
    "    mode=trainer.mode,\n",
    "    W0=trainer.W0,\n",
    "    Wsigma=trainer.Wsigma,\n",
    ")\n",
    "\n",
    "train_data_size = train_data.size\n",
    "test_data_size = test_data.size\n",
    "val_data_size = val_data.size\n",
    "\n",
    "for e in range(0, trainer.numepochs):\n",
    "    train_iter = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=trainer.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=tr.numpy_collate,\n",
    "    )\n",
    "    test_iter = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=trainer.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=tr.numpy_collate,\n",
    "    )\n",
    "    val_iter = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=trainer.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=tr.numpy_collate,\n",
    "    )\n",
    "    for i, b in enumerate(train_iter):\n",
    "        img_arr, seg_arr, weight_arr = (b[\"img\"], b[\"seg\"], b[\"weight\"])\n",
    "    for i, b in enumerate(test_iter):\n",
    "        img_arr, seg_arr, weight_arr = (b[\"img\"], b[\"seg\"], b[\"weight\"])\n",
    "    for i, b in enumerate(val_iter):\n",
    "        img_arr, seg_arr, weight_arr = (b[\"img\"], b[\"seg\"], b[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.chunk_dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.dset_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.chunk_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingVisualizer:\n",
    "    def __init__(self, trainpath, modeldbpath):\n",
    "        self.trainpath = trainpath\n",
    "        self.modelpath = trainpath + \"/models\"\n",
    "        self.modeldfpath = trainpath + \"/model_metadata.hdf5\"\n",
    "        self.modeldbpath = modeldbpath\n",
    "        self.paramdbpath = modeldbpath + \"/Parameters\"\n",
    "        self.update_dfs()\n",
    "        if os.path.exists(self.modeldfpath):\n",
    "            self.models_widget = qgrid.show_grid(self.model_df.sort_index())\n",
    "\n",
    "    def update_dfs(self):\n",
    "        df_idx_list = []\n",
    "        for path in os.listdir(self.modelpath):\n",
    "            if \"training_metadata\" in path:\n",
    "                df_idx = int(path.split(\"_\")[-1][:-5])\n",
    "                df_idx_list.append(df_idx)\n",
    "        df_list = []\n",
    "        for df_idx in df_idx_list:\n",
    "            dfpath = self.modelpath + \"/training_metadata_\" + str(df_idx) + \".hdf5\"\n",
    "            df_handle = pandas_hdf5_handler(dfpath)\n",
    "            df = df_handle.read_df(\"data\")\n",
    "            df_list.append(copy.deepcopy(df))\n",
    "            del df\n",
    "        self.train_df = pd.concat(df_list)\n",
    "        if os.path.exists(self.modeldfpath):\n",
    "            modeldfhandle = pandas_hdf5_handler(self.modeldfpath)\n",
    "            self.model_df = modeldfhandle.read_df(\"data\").sort_index()\n",
    "\n",
    "    def select_df_columns(self, selected_columns):\n",
    "        df = copy.deepcopy(self.model_df)\n",
    "        for column in df.columns.tolist():\n",
    "            if column not in selected_columns:\n",
    "                df = df.drop(column, 1)\n",
    "        self.model_widget = qgrid.show_grid(df)\n",
    "\n",
    "    def inter_df_columns(self):\n",
    "        column_list = self.model_df.columns.tolist()\n",
    "        inter = ipyw.interactive(\n",
    "            self.select_df_columns,\n",
    "            {\"manual\": True},\n",
    "            selected_columns=ipyw.SelectMultiple(\n",
    "                options=column_list, description=\"Columns to Display:\", disabled=False\n",
    "            ),\n",
    "        )\n",
    "        display(inter)\n",
    "\n",
    "    def handle_filter_changed(self, event, widget):\n",
    "        df = widget.get_changed_df().sort_index()\n",
    "\n",
    "        all_model_indices = (\n",
    "            self.train_df.index.get_level_values(\"Model #\").unique().tolist()\n",
    "        )\n",
    "        current_model_indices = df.index.get_level_values(\"Model #\").unique().tolist()\n",
    "\n",
    "        all_epochs = []\n",
    "        all_loss = []\n",
    "        for model_idx in all_model_indices:\n",
    "            if model_idx in current_model_indices:\n",
    "                filter_df = df.loc[model_idx]\n",
    "                epochs, loss = (\n",
    "                    filter_df.index.get_level_values(\"Epoch\").tolist(),\n",
    "                    filter_df[self.losskey].tolist(),\n",
    "                )\n",
    "                all_epochs += epochs\n",
    "                all_loss += loss\n",
    "                self.line_dict[model_idx].set_data(epochs, loss)\n",
    "                self.line_dict[model_idx].set_label(str(model_idx))\n",
    "            else:\n",
    "                epochs_empty, loss_empty = ([], [])\n",
    "                self.line_dict[model_idx].set_data(epochs_empty, loss_empty)\n",
    "                self.line_dict[model_idx].set_label(\"_nolegend_\")\n",
    "\n",
    "        self.ax.set_xlim(min(all_epochs), max(all_epochs) + 1)\n",
    "        self.ax.set_ylim(0, max(all_loss) * 1.1)\n",
    "        self.ax.legend()\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def inter_plot_loss(self, losskey):\n",
    "        self.losskey = losskey\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.grid_widget = qgrid.show_grid(self.train_df.sort_index())\n",
    "        current_df = self.grid_widget.get_changed_df()\n",
    "\n",
    "        self.line_dict = {}\n",
    "        for model_idx in current_df.index.get_level_values(\"Model #\").unique().tolist():\n",
    "            filter_df = current_df.loc[model_idx]\n",
    "            epochs, loss = (\n",
    "                filter_df.index.get_level_values(\"Epoch\").tolist(),\n",
    "                filter_df[losskey].tolist(),\n",
    "            )\n",
    "            (line,) = self.ax.plot(epochs, loss, label=str(model_idx))\n",
    "            self.line_dict[model_idx] = line\n",
    "\n",
    "        self.ax.set_xlabel(\"Epoch\")\n",
    "        self.ax.set_ylabel(losskey)\n",
    "        self.ax.legend()\n",
    "\n",
    "    def export_models(self):\n",
    "        writedir(self.modeldbpath, overwrite=False)\n",
    "        writedir(self.modeldbpath + \"/Parameters\", overwrite=False)\n",
    "        modeldbhandle = pandas_hdf5_handler(self.modeldbpath + \"/Models.hdf5\")\n",
    "        if \"Models.hdf5\" in os.listdir(self.modeldbpath):\n",
    "            old_df = modeldbhandle.read_df(\"data\")\n",
    "            current_df = self.models_widget.get_changed_df()\n",
    "            current_df = pd.concat([old_df, current_df])\n",
    "        else:\n",
    "            current_df = self.models_widget.get_changed_df()\n",
    "        modeldbhandle.write_df(\"data\", current_df)\n",
    "\n",
    "        indices = current_df.index.tolist()\n",
    "        exp_names = [str(item[0]) for item in indices]\n",
    "        model_numbers = [str(item[1]) for item in indices]\n",
    "        dates = [item.replace(\" \", \"_\") for item in current_df[\"Date/Time\"].tolist()]\n",
    "\n",
    "        for i in range(len(model_numbers)):\n",
    "            exp_name, model_number, date = (exp_names[i], model_numbers[i], dates[i])\n",
    "            shutil.copyfile(\n",
    "                self.modelpath + \"/\" + str(model_number) + \".pt\",\n",
    "                self.paramdbpath\n",
    "                + \"/\"\n",
    "                + exp_name\n",
    "                + \"_\"\n",
    "                + model_number\n",
    "                + \"_\"\n",
    "                + date\n",
    "                + \".pt\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = TrainingVisualizer(\n",
    "    \"/n/scratch3/users/d/de64/2020-07-05_NN\", \"/n/scratch3/users/d/de64/nndb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "vis.inter_plot_loss(\"Val Loss\")\n",
    "vis.grid_widget.on(\"filter_changed\", vis.handle_filter_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.grid_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.inter_df_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.model_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(vis.model_df[\"Val F1 Cell Scores\"][1], bins=50)\n",
    "plt.xlabel(\"F-Score\")\n",
    "plt.ylabel(\"Occurances\")\n",
    "plt.xticks(np.arange(0, 1.01, step=0.5))\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = tr.TrainingVisualizer(\"/n/scratch2/de64/nntest7\", \"/n/scratch2/de64/nndb\")\n",
    "\n",
    "# :\n",
    "#     def __init__(self,trainpath,modeldbpath):\n",
    "#         self.trainpath = trainpath\n",
    "#         self.modelpath = trainpath + \"/models\"\n",
    "#         self.modeldfpath = trainpath + \"/model_metadata.hdf5\"\n",
    "#         self.modeldbpath = modeldbpath\n",
    "#         self.paramdbpath = modeldbpath+\"/Parameters\"\n",
    "#         self.update_dfs()\n",
    "#         if os.path.exists(self.modeldfpath):\n",
    "#             self.models_widget = qgrid.show_grid(self.model_df.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import ipywidgets as ipyw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qgrid\n",
    "import skimage as sk\n",
    "import skimage.morphology\n",
    "import sklearn as skl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.heatmaps import HeatmapsOnImage\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import convolve1d\n",
    "from torch._six import container_abcs, int_classes, string_classes\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tr.UNet_Trainer(\"/n/scratch2/de64/nntest7\", 100, \"class\", lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "    test.model.parameters(),\n",
    "    lr=test.lr,\n",
    "    momentum=test.momentum,\n",
    "    weight_decay=test.weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_epochs = 10\n",
    "cool_epochs = 100\n",
    "warm_lambda = 1.0 / warm_epochs\n",
    "cool_lambda = 1.0 / cool_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealfn(epoch):\n",
    "    numepochs = 500\n",
    "    warm_epochs = 10\n",
    "    cool_epochs = 100\n",
    "    warm_lambda = 1.0 / warm_epochs\n",
    "    cool_lambda = 1.0 / cool_epochs\n",
    "\n",
    "    if epoch < warm_epochs:\n",
    "        return warm_lambda * epoch\n",
    "    elif epoch > (numepochs - cool_epochs):\n",
    "        return max(0.0, cool_lambda * (numepochs - epoch))\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(test.optimizer, lr_lambda=annealfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "annealfn(460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.LambdaLR(test.optimizer, lr_lambda=annealfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    test.scheduler.step()\n",
    "    print(test.scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(0, self.numepochs):\n",
    "    train_iter = DataLoader(\n",
    "        train_data, batch_size=self.batch_size, shuffle=False, collate_fn=numpy_collate\n",
    "    )\n",
    "    test_iter = DataLoader(\n",
    "        test_data, batch_size=self.batch_size, shuffle=False, collate_fn=numpy_collate\n",
    "    )\n",
    "    val_iter = DataLoader(\n",
    "        val_data, batch_size=self.batch_size, shuffle=False, collate_fn=numpy_collate\n",
    "    )\n",
    "    df_out = self.perepoch(\n",
    "        e,\n",
    "        train_iter,\n",
    "        test_iter,\n",
    "        val_iter,\n",
    "        train_data_size,\n",
    "        test_data_size,\n",
    "        val_data_size,\n",
    "    )\n",
    "\n",
    "    self.write_metadata(\n",
    "        self.nndatapath\n",
    "        + \"/models/training_metadata_\"\n",
    "        + str(self.model_number)\n",
    "        + \".hdf5\",\n",
    "        \"w\",\n",
    "        df_out,\n",
    "    )\n",
    "end = time.time()\n",
    "time_elapsed = (end - start) / 60.0\n",
    "torch.save(\n",
    "    self.model.state_dict(),\n",
    "    self.nndatapath + \"/models/\" + str(self.model_number) + \".pt\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    if self.mode == \"class\" or self.mode == \"multiclass\":\n",
    "        val_f = self.get_class_fscore(val_iter)\n",
    "        test_f = self.get_class_fscore(test_iter)\n",
    "    elif self.mode == \"cellpose\":\n",
    "        val_f = self.get_cellpose_fscore(val_iter)\n",
    "        test_f = self.get_cellpose_fscore(test_iter)\n",
    "except:\n",
    "    print(\"Failed to compute F-scores\")\n",
    "    val_f = [np.NaN]\n",
    "    test_f = [np.NaN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.UNet_Trainer(\n",
    "    \"/n/scratch3/users/d/de64/2020-06-14_NN\",\n",
    "    100,\n",
    "    \"multiclass\",\n",
    "    numepochs=1,\n",
    "    batch_size=50,\n",
    "    layers=3,\n",
    "    hidden_size=32,\n",
    "    lr=0.2,\n",
    "    gpuon=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "train.model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/n/scratch3/users/d/de64/2020-06-14_NN/models/4.pt\", map_location=device\n",
    "    )\n",
    ")\n",
    "train.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tr.SegmentationDataset(\n",
    "    train.nndatapath + \"/test.hdf5\", mode=train.mode, W0=train.W0, Wsigma=train.Wsigma\n",
    ")\n",
    "test_iter = DataLoader(\n",
    "    test_data, batch_size=train.batch_size, shuffle=False, collate_fn=tr.numpy_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pred(pred, thr, border_buffer=2):\n",
    "    output = []\n",
    "    for i in range(pred.shape[0]):\n",
    "        out_pred = pred[i, 1] > thr\n",
    "        out_pred = sk.segmentation.clear_border(out_pred, buffer_size=border_buffer)\n",
    "        output.append(out_pred)\n",
    "    output = np.array(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test_iter:\n",
    "    img = item[\"img\"]\n",
    "    x = torch.Tensor(img.astype(float))\n",
    "    pred = train.model.forward(x).data.numpy()\n",
    "    proc_pred = process_pred(pred, 0.5, border_buffer=0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(img[:, 0])\n",
    "imgs = handle.return_unwrap()\n",
    "plt.imshow(imgs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(pred[:, 1])\n",
    "preds = handle.return_unwrap()\n",
    "plt.imshow(preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(pred[:, 2])\n",
    "preds = handle.return_unwrap()\n",
    "plt.imshow(preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap((pred[:, 2] < 0.3) * (pred[:, 1] > 0.5))\n",
    "preds = handle.return_unwrap()\n",
    "plt.imshow(preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tr.kymo_handle()\n",
    "handle.import_wrap(proc_pred)\n",
    "proc_preds = handle.return_unwrap()\n",
    "plt.imshow(proc_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_imgs = (imgs - np.min(imgs)) / (np.max(imgs) - np.min(imgs))\n",
    "scaled_preds = (preds - np.min(preds)) / (np.max(preds) - np.min(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(scaled_imgs)\n",
    "plt.show()\n",
    "plt.imshow(scaled_preds > 0.7)\n",
    "plt.show()\n",
    "plt.imshow(sk.segmentation.clear_border(scaled_preds > 0.7, buffer_size=2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f = train.get_cellpose_fscore(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(test_f, range=(0, 1), bins=50)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
