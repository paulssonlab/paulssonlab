{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# TrenchRipper Master Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook contains the entire `TrenchRipper` pipline, divided into simple steps. This pipline is ideal for Mother <br>Machine image data where cells possess fluorescent segmentation markers. Segmentation on phase or brightfield data <br>is being developed, but is still an experimental feature.\n",
    "\n",
    "The steps in this pipeline are as follows:\n",
    "1. Extracting your Mother Machine data (.nd2) into hdf5 format\n",
    "2. Identifying and cropping individual trenches into kymographs\n",
    "3. Segmenting cells with a fluorescent marker\n",
    "4. Determining lineages and object properties\n",
    "\n",
    "In each step, the user will dynamically specify parameters using a series of interactive diagnostics on their dataset. <br>Following this, a parameter file will be written to disk and then used to deploy a parallel computation on the <br>dataset, either locally or on a SLURM cluster.\n",
    "\n",
    "\n",
    "This is intended as an end-to-end solution to analyzing Mother Machine data. As such, **it is not trivial to plug data <br>directly into intermediate steps**, as it will lack the correct formatting and associated metadata. A notable <br>exception to this is using another program to segment data. The library references binary segmentation masks using <br>only metadata derived from their associated kymographs. As such, it is possible to generate segmentations on these <br>kymographs elsewhere and place them into the segmentation data path to have `TrenchRipper` act on those <br>segmentations instead. More on this in the segmentation section..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Run this section to import all relavent packages and libraries used in this notebook. You must run this everytime you open a new python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Part 1: GFP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/gfp/\"\n",
    "# hdf5inputpath = \"/home/de64/scratch/de64/sync_folder/2021-01-28_lDE14/run/\"\n",
    "nd2file = \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/GFP.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=40,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_extractor = tr.marlin_extractor(hdf5inputpath, headpath, metaparsestr='metadata_{timepoint:d}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.ndextract.hdf5_fov_extractor(\n",
    "    nd2file,\n",
    "    headpath,\n",
    "    tpts_per_file=50,\n",
    "    ignore_fovmetadata=False,\n",
    "    nd2reader_override={\"z_levels\": [], \"z_coordinates\": []},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_extractor = tr.ndextract.tiff_extractor(\n",
    "#     tiffpath,\n",
    "#     headpath,\n",
    "#     [\"Phase\",\"YFP\"],tpts_per_file=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "[ '/n/scratch3/users/d/de64/190917_20x_phase_gfp_segmentation002',\n",
    " '/n/scratch3/users/d/de64/190922_20x_phase_gfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/190925_20x_phase_yfp_segmentation',\n",
    " '/n/scratch3/users/d/de64/ezrdm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/mbm_training_sb7',\n",
    " '/n/scratch3/users/d/de64/Sb7_L35',\n",
    " '/n/scratch3/users/d/de64/MM_DVCvecto_TOP_1_9',\n",
    " '/n/scratch3/users/d/de64/Vibrio_2_1_TOP',\n",
    " '/n/scratch3/users/d/de64/Vibrio_A_B_VZRDM--04--RUN_80ms',\n",
    " '/n/scratch3/users/d/de64/RpoSOutliers_WT_hipQ_100X',\n",
    " '/n/scratch3/users/d/de64/Main_Experiment',\n",
    " '/n/scratch3/users/d/de64/bde17_gotime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath, persist_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=2000, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms(intensity_range=(0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## Fluorescence Segmentation\n",
    "\n",
    "Now that you have copped your data into kymographs, we will now perform segmentation/cell detection <br>\n",
    "on your kymographs. Currently, this pipeline only supports segmentation of fluorescence images; however, <br>\n",
    "segmentation of transmitted light imaging techniques is in development.\n",
    "\n",
    "The output of this step will be a set of `segmentation_[File #].hdf5` files stored in `headpath/fluorsegmentation`.<br>\n",
    "The image data stored in these files takes the exact same form as the kymograph data, `(K,T,Y,X)` arrays <br>\n",
    "where K is the trench index, T is time, and Y,X are the crop dimensions. These arrays are accessible using <br>\n",
    "keys of the form `\"[Trench Row Number]\"`.\n",
    "\n",
    "Since no metadata is generated by this step, it is possible to use another segmentation algorithm on the kymograph <br>\n",
    "data. The output of segmentation must be split into `segmentation_[File #].hdf5` files, where `[File #]` agrees with the<br>\n",
    "corresponding `kymograph_[File #].hdf5` file. Additionally, the `(K,T,Y,X)` arrays must be of the same shape as the <br>\n",
    "kymograph arrays and accessible at the corresponding `\"[Trench Row Number]\"` key. These files must be placed into <br>\n",
    "their own folder at `headpath/foldername`. This folder may then be used in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "##### Initialize the interactive segmentation class\n",
    "\n",
    "As a first step, initialize the `tr.fluo_segmentation_interactive` class that will be handling all steps of generating a segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation = tr.fluo_segmentation_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### Choose channel to segment on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.choose_seg_channel_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Import data\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**fov_idx (int)** :\n",
    "\n",
    "**n_trenches (int)** :\n",
    "\n",
    "**t_range (tuple)** :\n",
    "\n",
    "**t_subsample_step (int)** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.import_array_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_processed_inter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "#### Determine Cell Mask Envelope\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**cell_mask_method (str)** : Thresholding method, can be a local or global Otsu threshold.\n",
    "\n",
    "**cell_otsu_scaling (float)** : Scaling factor applied to determined threshold.\n",
    "\n",
    "**local_otsu_r (int)** : Radius of thresholding kernel used in the local otsu thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_cell_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_eig_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_dist_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.plot_marker_mask_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Generate Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"01:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = tr.segment.fluo_segmentation_cluster(headpath, paramfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment.dask_segment(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Stop Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## Region Properties (No Lineage)\n",
    "\n",
    "Note this does not require a dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = tr.regionprops_extractor(\n",
    "    headpath,\n",
    "    \"fluorsegmentation\",\n",
    "    intensity_channel_list=[\"RFP-Penta\", \"GFP-Penta\"],\n",
    "    include_background=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.export_all_data(n_workers=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "At this point you may want to use your output. The output of this step is a set of `.hdf5` files stored in <br>`headpath/kymograph`. The image data stored in these files takes the form of `(K,T,Y,X)` arrays <br>where K is the trench index, T is time, and Y,X are the crop dimensions.\n",
    "\n",
    "These arrays are accessible using keys of the form `\"[Trench Row Number]/[Image Channel]\"`. <br>For example, looking up phase channel data of trenches in the topmost row of an image will require <br>the key `\"0/Phase\"` The metadata associated with these files is a large pandas dataframe relating <br>crops to original FOVs, accessible using the \"kymograph\" key on `headpath/metadata.hdf5`\n",
    "\n",
    "To assist in accessing this file, you may use the `trenchripper.pandas_hdf5_handler` object to <br>interface with this file as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "# Part 2: Barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be <br>processing. This line should be run everytime you open a new python kernel.\n",
    "\n",
    "The format should be: `headpath = \"/path/to/folder\"` and `nd2file = \"/path/to/file.nd2\"`\n",
    "\n",
    "For example:\n",
    "```\n",
    "headpath = \"/n/scratch2/de64/2019-05-31_validation_data\"\n",
    "nd2file = \"/n/scratch2/de64/2019-05-31_validation_data/Main_Experiment.nd2\"\n",
    "```\n",
    "\n",
    "Ideally, these files should be placed in a storage location with relatively fast I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/barcodes/\"\n",
    "hdf5inputpath = \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/run/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Extract to hdf5 files\n",
    "\n",
    "In this section, we will be extracting our image data. Currently this notebook only supports `.nd2` format; however <br>there are `.tiff` extractors in the TrenchRipper source files that are being added to `Master.ipynb` soon.\n",
    "\n",
    "In the abstract, this step will take a single `.nd2` file and split it into a set of `.hdf5` files stored in <br>`headpath/hdf5`. Splitting the file up in this way will facilitate quick procesing in later steps. Each field of <br>view will be split into one or more `.hdf5` files, depending on the number of images per file requested (more on <br>this later). \n",
    "\n",
    "To keep track of which output files correspond to which FOVs, as well as to keep track of experiment metadata, the <br>extractor also outputs a `metadata.hdf5` file in the `headpath` folder. The data from this step is accessible in <br>that `metadata.hdf5` file under the `global` key. If you would like to look at this metadata, you may use the <br>`tr.utils.pandas_hdf5_handler` to read from this file. Later steps will add additional metadata under different <br>keys into the `metadata.hdf5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "#### Start Dask Workers\n",
    "\n",
    "First, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=40,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "##### Perform Extraction\n",
    "\n",
    "Now that we have our cluster scheduler spun up, it is time to convert files. This will be handled by the <br>`hdf5_extractor` object. This extractor will pull up each FOV and split it such that each derived `.hdf5` file <br>contains, at maximum, N timepoints of that FOV per file. The image data stored in these files takes the <br>form of `(N,Y,X)` arrays that are accessible using the desired channel name as a key. \n",
    "\n",
    "The arguments for this extractor are:\n",
    "\n",
    " - **nd2file** : The filepath to the `.nd2` file you intend to extract.\n",
    " \n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **tpts_per_file** : The maximum number of timepoints stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **ignore_fovmetadata** : Used when `.nd2` data is corrupted and does not possess records for stage positions or <br>timepoints. Only set `False` if the extractor throws errors on metadata handling.\n",
    "\n",
    " - **nd2reader_override** : Overrides values in metadata recovered using the `nd2reader`. Currently set to <br>`{\"z_levels\":[],\"z_coordinates\":[]}` by default to correct a known issue where z coordinates are mistakenly <br>interpreted as a z stack. See the [nd2reader](https://rbnvrw.github.io/nd2reader/) documentation for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.marlin_extractor(\n",
    "    hdf5inputpath, headpath, metaparsestr=\"metadata_{timepoint:d}.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "##### Extraction Parameters\n",
    "\n",
    "Here, you may set the time interval you want to extract. Useful for cropping data to the period exhibiting the dynamics of interest.\n",
    "\n",
    "Optionally take notes to add to the `metadata.hdf5` file. Notes may also be taken directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.inter_set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "##### Begin Extraction \n",
    "\n",
    "Running the following line will start the extraction process. This may be monitored by examining the `Dask Dashboard` <br> under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "This step may take a long time, though it is possible to speed it up using additional workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once extraction is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Kymographs\n",
    "\n",
    "Now that you have extracted your data into a series of `.hdf5` files, we will now perform identification and cropping <br>of the individual trenches/growth channels present in the images. This algorithm assumes that your growth trenches <br>are vertically aligned and that they alternate in their orientation from top to bottom. See the example image for the <br>correct geometry:\n",
    "\n",
    "![example_image](./resources/example_image.jpg)\n",
    "\n",
    "The output of this step will be a set of `.hdf5` files stored in `headpath/kymograph`. The image data stored in these <br>files takes the form of `(K,T,Y,X)` arrays where K is the trench index, T is time, and Y,X are the crop dimensions. <br>These arrays are accessible using keys of the form `\"[Image Channel]\"`. For example, looking up phase channel <br>data of trenches in the topmost row of an image will require the key `\"Phase\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "### Test Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "##### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be help us choose the <br>parameters we will use to generate kymographs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph = tr.kymograph_interactive(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tr.hdf5_viewer(headpath, persist_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "##### Examine Images\n",
    "\n",
    "Here you can manually inspect images before beginning parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.view(width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "You will now want to select a few test FOVs to try out parameters on, the channel you want to detect trenches on, and <br>the time interval on which you will perform your processing.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    "- **seg_channel (string)** : The channel name that you would like to segment on.\n",
    "\n",
    "- **invert (list)** : Whether or not you want to invert the image before detecting trenches. By default, it is assumed that <br>the trenches have a high pixel intensity relative to the background. This should be the case for Phase Contrast and <br>Fluorescence Imageing, but may not be the case for Brightfield Imaging, in which case you will want to invert the image.\n",
    "\n",
    "- **fov_list (list)** : List of integers corresponding to the FOVs that you wish to make test kymographs of.\n",
    "\n",
    "- **t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in <br>between 5 and 10 timepoints for quick processing.\n",
    "\n",
    "Hit the \"Run Interact\" button to lock in your parameters. The button will become transparent briefly and become solid again <br>when processing is complete. After that has occured, move on to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.import_hdf5_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Normalize the signal by linearly scaling 0. and 1. to the minimum and maximum, respectively\n",
    "4. Use a set threshold to determine the trench row poisitons\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **y_percentile_threshold (float)** : Threshold to use in step 4.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_precentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "##### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Use the remaining rows to compute the drift in y in each image.\n",
    "4. Apply the drift to the initally detected rows to get rows in all timepoints.\n",
    "5. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from <br>the feeding channel).\n",
    "\n",
    "Step 5 performs a simple algorithm to determine the orientation of each trench:\n",
    "\n",
    "```\n",
    "row_orientations = [] # A list of row orientations, starting from the topmost row\n",
    "if the number of detected rows == 'Number of Rows': \n",
    "    row_orientations.append('Orientation')\n",
    "elif the number of detected rows < 'Number of Rows':\n",
    "    row_orientations.append('Orientation when < expected rows')\n",
    "for row in rows:\n",
    "    if row_orientations[-1] == downward:\n",
    "        row_orientations.append(upward)\n",
    "    elif row_orientations[-1] == upward:\n",
    "        row_orientations.append(downward)\n",
    "```\n",
    "\n",
    "Additionally, if the device tranches face a single direction, alternation of row orientation may be turned off by setting the<br> `Alternate Orientation?` argument to False. The `Use Median Drift?` argument, when set to True, will use the<br> median drift in y across all FOVs for drift correction, instead of doing drift correction independently for all FOVs. <br>This can be useful if there are a large fraction of FOVs which are failing drift correction. Note that `Use Median Drift?` <br>sets this behavior for both y and x drift correction.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **y_min_edge_dist (int)** : Minimum row length necessary for detection (filters out small detected objects).\n",
    "\n",
    " - **padding_y (int)** : Padding to add to the end of trench row when cropping in the y-dimension.\n",
    "\n",
    " - **trench_len_y (int)** : Length from the end of each trench row to the feeding channel side of the crop.\n",
    "\n",
    " - **Number of Rows (int)** : The number of rows to expect in your image. For instance, two in the example image.\n",
    " \n",
    " - **Alternate Orientation? (bool)** : Whether or not to alternate the orientation of consecutive rows.\n",
    "\n",
    " - **Orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench <br>opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    " - **Orientation when < expected rows(int)** : The orientation of the top-most row when the number of detected rows is less than <br>expected. Useful if your trenches drift out of your image in some FOVs.\n",
    " \n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    " - **images_per_row(int)** : How many images to output per row for this widget.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_y_crop_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "##### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smoothing this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use an [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "After this, x-dimension drift correction of our detected midpoints will be performed as follows:\n",
    "\n",
    "6. Begin at t=1\n",
    "7. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,<br>\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "8. Compute the translation of each midpoint at time.\n",
    "9. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **t (int)** : Timepoint to examine the percentiles and threshold in.\n",
    "\n",
    " - **x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    " - **background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    " - **smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    " - **otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold <br>value for each fov will be displayed as a red line. In addition, it will display the detected midpoints for each of your timepoints. <br>If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_x_percentiles_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "##### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them <br>\n",
    "to produce an output kymograph. **Note that the current implementation does not allow trench crops to overlap**. If your<br>\n",
    "trench crops do overlap, the error will not be caught here, but will cause issues later in the pipeline. As such, try <br>\n",
    "to crop your trenches as closely as possible. This issue will be fixed in a later update.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    " - **trench_present_thr (float)** : Trenches that appear in less than this percent of FOVs will be eliminated from the dataset.<br>\n",
    "If not removed, missing positions will be inferred from the image drift.\n",
    "\n",
    " - **Use Median Drift? (bool)** : Whether to use the median detected drift across all FOVs, instead of the drift detected in each FOV individually.\n",
    "\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov and will also produce midpoint plots <br>showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.preview_kymographs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "##### Export and save hyperparameters\n",
    "\n",
    "Run the following line to register and display the parameters you have selected for kymograph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "If you are satisfied with the above parameters, run the following line to write these parameters to disk at `headpath/kymograph.par`<br>\n",
    "This file will be used to perform kymograph creation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markd
own",
   "id": "136",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "After running the above line, you will have a running Dask client. Run the line below and click the link to supervise <br>the computation being administered by the scheduler. \n",
    "\n",
    "Don't be alarmed if the screen starts mostly blank, it may take time for your workers to spin up. If you get a 404 <br>error on a cluster, it is likely that your ports are not being forwarded properly. If this occurs, please register <br>the issue on github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "##### Perform Kymograph Cropping\n",
    "\n",
    "Now that we have our cluster scheduler spun up, we will extract kymographs using the parameters stored in `headpath/kymograph.par`. <br>\n",
    "This will be handled by the `kymograph_cluster` object. This will detect trenches in all of the files present in `headpath/hdf5` that <br>\n",
    "you created in the first step. It will then crop these trenches and place the crops in a series of `.hdf5` files in `headpath/kymograph`. <br>\n",
    "These files will store image data in the form of `(K,T,Y,X)` arrays where K is the trench index, T is time and Y,X are the image dimensions <br>\n",
    "of the crop.\n",
    "\n",
    "The arguments for this step are:\n",
    "\n",
    " - **headpath** : The folder in which processing is occuring. Should be the same for each step in the pipeline.\n",
    "\n",
    " - **trenches_per_file** : The maximum number of trenches stored in each output `.hdf5` file. Typical values are between 25 <br>and 100.\n",
    "\n",
    " - **paramfile** : Set to true if you want to use parameters from `headpath/kymograph.par` Otherwise, you will have to specify <br>\n",
    " parameters as direct arguments to `kymograph_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(\n",
    "    headpath=headpath, trenches_per_file=2000, paramfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "##### Begin Kymograph Cropping \n",
    "\n",
    "Running the following line will start the cropping process. This may be monitored by examining the `Dask Dashboard` <br>\n",
    "under the link displayed earlier. Once the computation is complete, move to the next line.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = tr.focus_filter(headpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.choose_filter_channel_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_histograms(intensity_range=(0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.plot_focus_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "##### Post-process Images\n",
    "\n",
    "After the above step, kymographs will have been created for each `.hdf5` input file. They will now need to be reorganized <br>\n",
    "into a new set of files such that each file has, at most, `trenches_per_file` trenches in each file.\n",
    "\n",
    "**Do not move on until all tasks are displayed as 'in memory' in Dask.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "##### Check kymograph statistics\n",
    "\n",
    "Run the next line to display some statistics from kymograph creation. The outputs are:\n",
    "\n",
    " - **fovs processed** : The number of FOVs successfully processed out of the total number of FOVs\n",
    " - **rows processed** : The number of rows of trenches processed out of the total number of rows\n",
    " - **trenches processed** : The number of trenches successfully processed\n",
    " - **row/fov** : The average number of rows successfully processed per FOV\n",
    " - **trenches/fov** : The average number of trenches successfully processed per FOV\n",
    " - **failed fovs** : A list of failed FOVs. Spot check these FOVs in the viewer to determine potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "##### Shutdown Dask\n",
    "\n",
    "Once cropping is complete, it is likely that you will want to shutdown your `dask_controller` if you are on a <br>\n",
    "cluster. This is because the specifications of the current `dask_controller` will not be optimal for later steps. <br>\n",
    "To do this, run the following line and wait for it to complete. If it hangs, interrupt your kernel and re-run it. <br>\n",
    "If this also fails to shutdown your workers, you will have to manually shut them down using `scancel` in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "## FISH Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "##### Start Dask Workers\n",
    "\n",
    "Again, we start a `dask_controller` instance which will handle all of our parallel processing. The default parameters <br>here work well on O2 for kymograph creation. The critical arguments here are:\n",
    "\n",
    "**walltime** : For a cluster, the length of time you will request each node for.\n",
    "\n",
    "**local** : `True` if you want to perform computation locally. `False` if you want to perform it on a SLURM cluster.\n",
    "\n",
    "**n_workers** : Number of nodes to request if on the cluster, or number of processes if computing locally.\n",
    "\n",
    "**memory** : For a cluster, the amount of memory you will request each node for.\n",
    "\n",
    "**working_directory** : For a cluster, the directory in which data will be spilled to disk. Usually set as a folder in <br>the `headpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"02:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    memory=\"4GB\",\n",
    "    working_directory=headpath + \"/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "#### Get Barcode Signal (Percentile Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tr.get_all_image_measurements(\n",
    "    headpath,\n",
    "    headpath + \"/percentiles\",\n",
    "    [\"RFP\", \"Cy5\", \"Cy7\"],\n",
    "    \"98th Percentile\",\n",
    "    np.percentile,\n",
    "    98,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "#### Determine Barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test = tr.fish_analysis(\n",
    "    headpath,\n",
    "    \"./lDE15_final_df.tsv\",\n",
    "    \"./lDE15_final_df.json\",\n",
    "    hamming_thr=1,\n",
    "    channel_names=[\"RFP 98th Percentile\", \"Cy5 98th Percentile\", \"Cy7 98th Percentile\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.plot_signal_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.get_bit_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.bit_threshold_list = [\n",
    "    800,\n",
    "    800,\n",
    "    1000,\n",
    "    1200,\n",
    "    1200,\n",
    "    1200,\n",
    "    1500,\n",
    "    1500,\n",
    "    1200,\n",
    "    1500,\n",
    "    2500,\n",
    "    4000,\n",
    "    4000,\n",
    "    3000,\n",
    "    3500,\n",
    "    3000,\n",
    "    3500,\n",
    "    3500,\n",
    "    2500,\n",
    "    5000,\n",
    "    400,\n",
    "    400,\n",
    "    400,\n",
    "    300,\n",
    "    500,\n",
    "    400,\n",
    "    400,\n",
    "    500,\n",
    "    400,\n",
    "    400,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.plot_bit_threshold_inter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test.output_barcode_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/barcodes/metadata.hdf5\"\n",
    ")\n",
    "barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "#### Compute Bit-wise Error (if singleton errors allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_barcodes = np.array(\n",
    "    barcode_df[\"barcode\"].apply(lambda x: list(barcode_to_FISH(x))).tolist()\n",
    ").astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_barcodes = np.array(\n",
    "    [list(item) for item in barcode_df[\"Barcode\"].tolist()]\n",
    ").astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_barcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_barcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_error = (\n",
    "    np.sum(np.logical_xor(true_barcodes, read_barcodes), axis=0)\n",
    "    / true_barcodes.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(range(30), bit_error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178",
   "metadata": {},
   "source": [
    "#### Compute Call Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_true = np.sum([item == True for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_false = np.sum([item == False for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_none = np.sum([item == \"Unknown\" for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_called = ttl_true + ttl_false\n",
    "ttl_trenches = barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0 - percent_called_w_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184",
   "metadata": {},
   "source": [
    "#### Estimating Error from Recall Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 29\n",
    "p_err_out = 1.0 - percent_called_w_cells\n",
    "H_vals, H_counts = np.unique(\n",
    "    barcode_df[\"Closest Hamming Distance\"].values, return_counts=True\n",
    ")\n",
    "p_Hdist = H_counts / np.sum(H_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = [-p_err_out]\n",
    "for idx, j in enumerate(H_vals):\n",
    "    bin_coeff = sp.special.binom(N, j)\n",
    "    coeff = bin_coeff - p_Hdist[idx]\n",
    "    coeffs.append(coeff)\n",
    "coeffs = np.array(coeffs)[::-1]\n",
    "roots = np.roots(coeffs)\n",
    "epsilon = np.real(roots[~np.iscomplex(roots)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_err_in = 0.0\n",
    "for idx, j in enumerate(H_vals):\n",
    "    p_err_in += p_Hdist[idx] * (epsilon**j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epsilon Estimate: \" + str(epsilon))\n",
    "print(\"P(error in) Estimate: \" + str(p_err_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "Guessing epislon based on single bit error rate (using soft matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "eps_range = np.linspace(0.0, 0.2)\n",
    "for epsilon in eps_range:\n",
    "    #     epsilon = 0.05\n",
    "    p_err_in = 0.0\n",
    "    for idx, j in enumerate(H_vals):\n",
    "        p_err_in += p_Hdist[idx] * (epsilon**j)\n",
    "    output.append(p_err_in * 100)\n",
    "\n",
    "\n",
    "plt.plot(eps_range, output)\n",
    "plt.xlabel(\"Epsilon\", fontsize=20)\n",
    "plt.xlabel(\"Error Rate\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# print(\"Epsilon Estimate: \" + str(epsilon))\n",
    "# print(\"P(error in) Estimate: \" + str(p_err_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191",
   "metadata": {},
   "source": [
    "#### Import GFP Regionprops Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/gfp/analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "#### Get trenchwise GFP signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "mchy_df = analysis_df[analysis_df[\"Intensity Channel\"] == \"RFP-Penta\"]\n",
    "mchy_groupby = mchy_df.groupby([\"trenchid\", \"timepoints\"])\n",
    "\n",
    "gfp_df = analysis_df[analysis_df[\"Intensity Channel\"] == \"GFP-Penta\"]\n",
    "gfp_groupby = gfp_df.groupby([\"trenchid\", \"timepoints\"])\n",
    "\n",
    "gfp_intensity_wo_bkd = (\n",
    "    gfp_groupby.apply(\n",
    "        lambda x: (\n",
    "            x[\"mean_intensity\"] - x[x[\"Objectid\"] == 0][\"mean_intensity\"].iloc[0]\n",
    "        ).to_dict(),\n",
    "        meta=(\"mean_intensity\", float),\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .compute()\n",
    "    .to_list()\n",
    ")\n",
    "gfp_intensity_wo_bkd = {k: v for d in gfp_intensity_wo_bkd for k, v in d.items()}\n",
    "gfp_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    gfp_intensity_wo_bkd, orient=\"index\", columns=[\"mean_intensity_wo_bkd\"]\n",
    ")\n",
    "gfp_df = gfp_df.join(gfp_intensity_wo_bkd).persist()\n",
    "del gfp_intensity_wo_bkd\n",
    "\n",
    "mchy_intensity_wo_bkd = (\n",
    "    mchy_groupby.apply(\n",
    "        lambda x: (\n",
    "            x[\"mean_intensity\"] - x[x[\"Objectid\"] == 0][\"mean_intensity\"].iloc[0]\n",
    "        ).to_dict(),\n",
    "        meta=(\"mean_intensity\", float),\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .compute()\n",
    "    .to_list()\n",
    ")\n",
    "mchy_intensity_wo_bkd = {k: v for d in mchy_intensity_wo_bkd for k, v in d.items()}\n",
    "mchy_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    mchy_intensity_wo_bkd, orient=\"index\", columns=[\"mean_intensity_wo_bkd\"]\n",
    ")\n",
    "mchy_df = mchy_df.join(mchy_intensity_wo_bkd).persist()\n",
    "del mchy_intensity_wo_bkd\n",
    "\n",
    "gfp_df_nobkd = gfp_df[gfp_df[\"Objectid\"] != 0]\n",
    "gfp_df_nobkd[\"Object Parquet Index\"] = gfp_df_nobkd.apply(\n",
    "    lambda x: int(\n",
    "        f\"{x['File Index']:04}{x['File Trench Index']:04}{x['timepoints']:04}{x['Objectid']:02}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "gfp_df_nobkd = gfp_df_nobkd.set_index(\"Object Parquet Index\")\n",
    "\n",
    "mchy_df_nobkd = mchy_df[mchy_df[\"Objectid\"] != 0]\n",
    "mchy_df_nobkd[\"Object Parquet Index\"] = mchy_df_nobkd.apply(\n",
    "    lambda x: int(\n",
    "        f\"{x['File Index']:04}{x['File Trench Index']:04}{x['timepoints']:04}{x['Objectid']:02}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "mchy_df_nobkd = mchy_df_nobkd.set_index(\"Object Parquet Index\")\n",
    "\n",
    "ratio_series = (\n",
    "    gfp_df_nobkd[\"mean_intensity_wo_bkd\"] / mchy_df_nobkd[\"mean_intensity_wo_bkd\"]\n",
    ")\n",
    "gfp_df_nobkd[\"gfp/mchy Ratio\"] = ratio_series\n",
    "\n",
    "trenchid_groupby = gfp_df_nobkd.groupby(\"trenchid\")\n",
    "median_ratio = trenchid_groupby[\"gfp/mchy Ratio\"].apply(np.median).compute()\n",
    "median_ratio = median_ratio.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    median_ratio,\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    label=\"Measured Dark GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.savefig(\"./2021-03-10_lDE14_figure_2.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "#### Apply GFP Signal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2.0\n",
    "\n",
    "dark_gfp = median_ratio < threshold\n",
    "perc_gfp = 1.0 - (np.sum(dark_gfp) / len(median_ratio))\n",
    "print(perc_gfp)\n",
    "\n",
    "plt.hist(\n",
    "    median_ratio[median_ratio < threshold],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    label=\"Measured Dark GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    median_ratio[median_ratio > threshold],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    label=\"Measured GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.savefig(\"./2021-03-10_lDE14_figure_2.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/gfp/kymograph/metadata\"\n",
    ")\n",
    "barcode_kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/barcodes/kymograph/metadata\"\n",
    ")\n",
    "\n",
    "max_gfp_tpt = gfp_kymo_df.loc[:1000][\"timepoints\"].max().compute()\n",
    "min_barcode_tpt = barcode_kymo_df.loc[:1000][\"timepoints\"].min().compute()\n",
    "\n",
    "last_gfp_tpt_df = gfp_kymo_df[gfp_kymo_df[\"timepoints\"] == max_gfp_tpt].compute()\n",
    "first_barcode_tpt_df = barcode_kymo_df[\n",
    "    barcode_kymo_df[\"timepoints\"] == min_barcode_tpt\n",
    "].compute()\n",
    "\n",
    "trenchid_map = tr.get_trenchid_map(first_barcode_tpt_df, last_gfp_tpt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "#### Get GFP Call Error and Recovery Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_df[\"Measured Dark GFP\"] = barcode_df.apply(\n",
    "    tr.map_Series, axis=1, args=(dark_gfp, trenchid_map)\n",
    ")\n",
    "barcode_df[\"Measured GFP Ratio\"] = barcode_df.apply(\n",
    "    tr.map_Series, axis=1, args=(median_ratio, trenchid_map)\n",
    ")\n",
    "called_df = barcode_df[barcode_df[\"Measured Dark GFP\"] != \"Unknown\"]\n",
    "ttl_correct = np.sum(called_df[\"dark_gfp\"] == called_df[\"Measured Dark GFP\"])\n",
    "ttl_called = len(called_df)\n",
    "recovery_rate = len(called_df) / len(dark_gfp)\n",
    "n_barcodes = called_df[\"barcodeid\"].nunique()\n",
    "n_trenches = called_df[\"trenchid\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error Rate:\" + str(1.0 - ttl_correct / ttl_called))\n",
    "print(\"Recovery Rate:\" + str(recovery_rate))\n",
    "print(\"Unique Barcodes:\" + str(n_barcodes))\n",
    "print(\"Total Trenches:\" + str(n_trenches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "called_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.hist(\n",
    "    called_df[called_df[\"dark_gfp\"] == True][\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.hist(\n",
    "    called_df[called_df[\"dark_gfp\"] == False][\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.xlabel(\"Lineage GFP/mCherry Intensity Ratio\", fontsize=20)\n",
    "plt.ylabel(\"Lineages\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"./2021-03-29_lDE15_figure_1.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {},
   "source": [
    "#### Get Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_mat(df):\n",
    "    TP = np.sum((df[\"dark_gfp\"] == False) & (df[\"Measured Dark GFP\"] == False))\n",
    "    TN = np.sum((df[\"dark_gfp\"] == True) & (df[\"Measured Dark GFP\"] == True))\n",
    "    FP = np.sum((df[\"dark_gfp\"] == False) & (df[\"Measured Dark GFP\"] == True))\n",
    "    FN = np.sum((df[\"dark_gfp\"] == True) & (df[\"Measured Dark GFP\"] == False))\n",
    "\n",
    "    error = (FP + FN) / (TP + TN + FP + FN)\n",
    "    FP_error = FP / (TP + TN + FP + FN)\n",
    "    FN_error = FN / (TP + TN + FP + FN)\n",
    "\n",
    "    return error, FP_error, FN_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "error, FP_error, FN_error = get_confusion_mat(called_df)\n",
    "print(\"Error: \" + str(error))\n",
    "print(\"FP error: \" + str(FP_error))\n",
    "print(\"FN error: \" + str(FN_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(hamming_n_barcodes) / hamming_n_barcodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set()\n",
    "\n",
    "hamming_filters = list(range(1, 5))\n",
    "hamming_n_barcodes = []\n",
    "hamming_errors = []\n",
    "for i in hamming_filters:\n",
    "    filtered_df = called_df[called_df[\"Closest Hamming Distance\"] >= i]\n",
    "    n_barcode = len(filtered_df)\n",
    "    error, FP_error, FN_error = get_confusion_mat(filtered_df)\n",
    "    error = np.round(100 * error, decimals=2)\n",
    "    hamming_errors.append(error)\n",
    "    hamming_n_barcodes.append(n_barcode)\n",
    "\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "# sns.lineplot(hamming_filters,hamming_errors,linewidth=4,marker='o',markersize=15)\n",
    "# plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "# plt.xlabel(\"Minimum Hamming Distance\",fontsize=20)\n",
    "# plt.ylabel(\"Error Rate (%)\",fontsize=20)\n",
    "# plt.xticks([1,2,3,4],fontsize=20)\n",
    "# plt.yticks(fontsize=20)\n",
    "# # plt.savefig(\"./2021-03-29_lDE15_figure_2.png\",dpi=300,bbox_inches=\"tight\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.xlabel(\"Minimum Hamming Distance\",fontsize=20)\n",
    "# plt.ylabel(\"Error Rate\",fontsize=20)\n",
    "# plt.xticks(fontsize=20)\n",
    "# plt.yticks(fontsize=20)\n",
    "# plt.legend(fontsize=20)\n",
    "# plt.savefig(\"./2021-03-29_lDE15_figure_1.png\",dpi=300,bbox_inches=\"tight\")\n",
    "# plt.show()\n",
    "\n",
    "perc_lib = 100 * (np.array(hamming_n_barcodes) / hamming_n_barcodes[0])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.lineplot(hamming_filters, perc_lib, linewidth=4, marker=\"o\", markersize=15)\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "plt.ylim(0, 110)\n",
    "plt.xlabel(\"Minimum Hamming Distance\", fontsize=20)\n",
    "plt.ylabel(\"Percent of Library\", fontsize=20)\n",
    "plt.xticks([1, 2, 3, 4], fontsize=20)\n",
    "plt.yticks(\n",
    "    [0, 20, 40, 60, 80, 100],\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.savefig(\"./2021-03-29_lDE15_figure_2b.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_subsample(df, value, subsample_n, bootstrap_n=500):\n",
    "    bootstrap_errors = []\n",
    "    for i in range(bootstrap_n):\n",
    "        sub_df = df.sample(n=subsample_n)\n",
    "        error, FP_error, FN_error = get_confusion_mat(sub_df)\n",
    "        error = np.round(100 * error, decimals=2)\n",
    "        bootstrap_errors.append(error)\n",
    "    percentile = sp.stats.percentileofscore(bootstrap_errors, value)\n",
    "\n",
    "    return percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_subsample(\n",
    "    called_df, hamming_errors[-1], hamming_n_barcodes[-1], bootstrap_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gmm_params(values):\n",
    "    gmm = skl.mixture.GaussianMixture(n_components=2, n_init=10)\n",
    "    gmm.fit(values.reshape(-1, 1))\n",
    "    #     probs = gmm.predict_proba(values.reshape(-1,1))\n",
    "    return gmm.means_[:, 0], ((gmm.covariances_) ** (1 / 2))[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "n_std = np.linspace(0, 2, 20)\n",
    "n_barcodes = []\n",
    "errors = []\n",
    "FP_errors = []\n",
    "FN_errors = []\n",
    "\n",
    "means, stds = get_gmm_params(called_df[\"Measured GFP Ratio\"].values)\n",
    "\n",
    "if means[0] > means[1]:\n",
    "    means = means[::-1]\n",
    "    stds = stds[::-1]\n",
    "\n",
    "for i in n_std:\n",
    "    upper_bound = means + stds * i\n",
    "    lower_bound = means - stds * i\n",
    "\n",
    "    #     valid_dark = (called_df_barcodes[\"Measured Median GFP\"] < upper_bound[0]) &\\\n",
    "    #     (called_df_barcodes[\"Measured Median GFP\"] > lower_bound[0])\n",
    "    #     valid_gfp = (called_df_barcodes[\"Measured Median GFP\"] < upper_bound[1]) &\\\n",
    "    #     (called_df_barcodes[\"Measured Median GFP\"] > lower_bound[1])\n",
    "    #     valid = valid_dark|valid_gfp\n",
    "    valid_dark = called_df[\"Measured GFP Ratio\"] < upper_bound[0]\n",
    "    valid_gfp = called_df[\"Measured GFP Ratio\"] > lower_bound[1]\n",
    "    valid = valid_dark | valid_gfp\n",
    "\n",
    "    filtered_df = called_df[valid]\n",
    "    n_barcode = len(filtered_df)\n",
    "    error, FP_error, FN_error = get_confusion_mat(filtered_df)\n",
    "    error = np.round(100 * error, decimals=2)\n",
    "    FP_error = np.round(100 * FP_error, decimals=2)\n",
    "    FN_error = np.round(100 * FN_error, decimals=2)\n",
    "    errors.append(error)\n",
    "    FP_errors.append(FP_error)\n",
    "    FN_errors.append(FN_error)\n",
    "    n_barcodes.append(n_barcode)\n",
    "\n",
    "sns.lineplot(n_std, errors, linewidth=5, label=\"Error\")\n",
    "sns.lineplot(n_std, FP_errors, linewidth=5, label=\"FP Error\")\n",
    "sns.lineplot(n_std, FN_errors, linewidth=5, label=\"FN Error\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(n_std, n_barcodes, linewidth=5)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215",
   "metadata": {},
   "source": [
    "#### Sources of error\n",
    "\n",
    "There are around twice the number of false negatives (predicted to be a Dark GFP, but measured as bright) as there are false positives (predicted to be GFP, but measured as dark).\n",
    "\n",
    "Some theories for these error classes:\n",
    "\n",
    "False Positives:\n",
    "    \n",
    "    - Mutations in the promoter (should be constant within barcodes)\n",
    "    \n",
    "    - Strain variation (should be lower when averaging among strains)\n",
    "    \n",
    "    - Misread of barcodes\n",
    "    \n",
    "False Negatives:\n",
    "    \n",
    "    - Bleed from adjacent cells (should be corrected by averging among strains)\n",
    "    \n",
    "    - Multiple strains per trench (?)\n",
    "    \n",
    "    - Misread of barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216",
   "metadata": {},
   "source": [
    "#### Median GFP Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_gfp_df = called_df.groupby(\"Barcode\").apply(\n",
    "    lambda x: x[\"Measured GFP Ratio\"].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    median_gfp_df[median_gfp_df < threshold],\n",
    "    range=(0, 20),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    label=\"Measured Dark GFP\",\n",
    ")\n",
    "plt.hist(\n",
    "    median_gfp_df[median_gfp_df > threshold],\n",
    "    range=(0, 20),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    label=\"Measured GFP\",\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.savefig(\"./2020-10-10_lDE11_figure_2.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "called_df_barcodes = called_df.set_index([\"Barcode\"]).sort_index()\n",
    "called_df_barcodes[\"Measured Median GFP\"] = median_gfp_df\n",
    "called_df_barcodes.reset_index(drop=False)\n",
    "called_df_barcodes = called_df_barcodes.groupby(\"Barcode\").apply(lambda x: x.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {},
   "outputs": [],
   "source": [
    "called_df_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_correct = np.sum(\n",
    "    called_df_barcodes[\"dark_gfp\"] == (called_df_barcodes[\"Measured Median GFP\"] < 2.0)\n",
    ")\n",
    "ttl_called = len(called_df_barcodes)\n",
    "print(\"Percent Correct:\" + str(ttl_correct / ttl_called))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
