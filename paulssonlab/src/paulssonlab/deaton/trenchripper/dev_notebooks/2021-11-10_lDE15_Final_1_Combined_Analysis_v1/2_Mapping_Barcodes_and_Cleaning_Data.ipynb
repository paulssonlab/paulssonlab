{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Barcodes and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/barcodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=50,\n",
    "    death_timeout=5.0,\n",
    "    memory=\"16GB\",\n",
    "    working_directory=\"/home/de64/scratch/de64/temp/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/barcodes/metadata.hdf5\"\n",
    ")\n",
    "pandas_barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)\n",
    "barcode_df = dd.from_pandas(pandas_barcode_df, npartitions=500, sort=True)\n",
    "barcode_df = barcode_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_called = len(barcode_df.index)\n",
    "ttl_trenches = pandas_barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_cells = pandas_barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_cells = ttl_called / ttl_trenches_w_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttl_called)\n",
    "print(ttl_trenches)\n",
    "print(ttl_trenches_w_cells)\n",
    "print(percent_called)\n",
    "print(percent_called_w_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/GFP/analysis\"\n",
    ")\n",
    "last_trenchid = int(analysis_df.tail(1)[\"trenchid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "\n",
    "\n",
    "def hrm_find_mode(series, max_iter=1000, min_binsize=50):\n",
    "    working_series = series\n",
    "    for i in range(max_iter):\n",
    "        range_max, range_min = np.max(working_series), np.min(working_series)\n",
    "        midpoint = (range_max + range_min) / 2\n",
    "        above_middle = working_series[working_series > midpoint]\n",
    "        below_middle = working_series[working_series <= midpoint]\n",
    "\n",
    "        count_above = len(above_middle)\n",
    "        count_below = len(below_middle)\n",
    "\n",
    "        if count_above > count_below:\n",
    "            working_series = above_middle\n",
    "        else:\n",
    "            working_series = below_middle\n",
    "\n",
    "        if i > 0:\n",
    "            if (len(working_series) < min_binsize) or (last_midpoint == midpoint):\n",
    "                return np.mean(working_series)\n",
    "\n",
    "        last_midpoint = midpoint\n",
    "\n",
    "\n",
    "def bootstrap_hrm(series, n_bootstraps=100, max_n_per_bootstrap=100):\n",
    "    modes = []\n",
    "\n",
    "    series_len = len(series)\n",
    "\n",
    "    n_per_bootstrap = min(series_len, max_n_per_bootstrap)\n",
    "\n",
    "    for n in range(n_bootstraps):\n",
    "        modes.append(hrm_find_mode(series.sample(n=n_per_bootstrap)))\n",
    "    return np.mean(modes)\n",
    "\n",
    "\n",
    "def get_GFPpos_modes(\n",
    "    GFP_series, series_groupby, frac=0.01, n_bootstraps=100, max_n_per_bootstrap=100\n",
    "):\n",
    "    gfp_vals = GFP_series.sample(frac=frac).compute()\n",
    "    tri_thr = sk.filters.threshold_triangle(gfp_vals)\n",
    "    mode_series = (\n",
    "        series_groupby.apply(\n",
    "            lambda x: bootstrap_hrm(\n",
    "                x[x > tri_thr],\n",
    "                n_bootstraps=n_bootstraps,\n",
    "                max_n_per_bootstrap=max_n_per_bootstrap,\n",
    "            )\n",
    "        )\n",
    "        .compute()\n",
    "        .sort_index()\n",
    "    )\n",
    "    return mode_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables over FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df_nobkd = analysis_df[analysis_df[\"Objectid\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_rescale = [\"RFP-Penta mean_intensity\", \"GFP-Penta mean_intensity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\"Median mCherry Intensity\", \"Median GFPmut2 Intensity\"]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale):\n",
    "    fov_series_groupby = analysis_df_nobkd.groupby(\"fov\")[label]\n",
    "    if label == \"RFP-Penta mean_intensity\":\n",
    "        fov_mode_series = (\n",
    "            fov_series_groupby.apply(\n",
    "                lambda x: bootstrap_hrm(x, max_n_per_bootstrap=100)\n",
    "            )\n",
    "            .compute()\n",
    "            .sort_index()\n",
    "        )\n",
    "    elif label == \"GFP-Penta mean_intensity\":\n",
    "        fov_mode_series = get_GFPpos_modes(\n",
    "            analysis_df[\"GFP-Penta mean_intensity\"],\n",
    "            fov_series_groupby,\n",
    "            max_n_per_bootstrap=100,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Weird Label\")\n",
    "\n",
    "    fov_correction_series = fov_mode_series / np.max(fov_mode_series)\n",
    "    fov_correction_dict = fov_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(fov_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"FOV #\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    label_scaling = analysis_df[\"fov\"].apply(lambda x: fov_correction_dict[x]).persist()\n",
    "    analysis_df[label + \": FOV Corrected\"] = (\n",
    "        analysis_df[label] / label_scaling\n",
    "    ).persist()\n",
    "plt.savefig(\"FOV_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df_nobkd = analysis_df[analysis_df[\"Objectid\"] != 0]\n",
    "values_to_rescale_step_2 = [value + \": FOV Corrected\" for value in values_to_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add real time later when fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "values_names = [\"Median mCherry Intensity\", \"Median GFPmut2 Intensity\"]\n",
    "\n",
    "for i, label in enumerate(values_to_rescale_step_2):\n",
    "    time_series_groupby = analysis_df_nobkd.groupby(\"timepoints\")[label]\n",
    "    if label == \"RFP-Penta mean_intensity: FOV Corrected\":\n",
    "        time_mode_series = (\n",
    "            time_series_groupby.apply(\n",
    "                lambda x: bootstrap_hrm(x, max_n_per_bootstrap=100)\n",
    "            )\n",
    "            .compute()\n",
    "            .sort_index()\n",
    "        )\n",
    "    elif label == \"GFP-Penta mean_intensity: FOV Corrected\":\n",
    "        time_mode_series = get_GFPpos_modes(\n",
    "            analysis_df[\"GFP-Penta mean_intensity\"],\n",
    "            time_series_groupby,\n",
    "            max_n_per_bootstrap=100,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Weird Label\")\n",
    "\n",
    "    time_correction_series = time_mode_series / np.max(time_mode_series)\n",
    "    time_correction_dict = time_correction_series.to_dict()\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(time_correction_series)\n",
    "    plt.title(values_names[i], fontsize=22)\n",
    "    plt.xlabel(\"Timepoint (3 min steps)\", fontsize=18)\n",
    "    plt.ylabel(\"Scaling\", fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0.0, 1.2)\n",
    "    label_scaling = analysis_df[\"timepoints\"].apply(lambda x: time_correction_dict[x])\n",
    "    analysis_df[label + \": Time Corrected\"] = (\n",
    "        analysis_df[label] / label_scaling\n",
    "    ).persist()\n",
    "plt.savefig(\"Time_correction.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwrite Variables with Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in values_to_rescale:\n",
    "    analysis_df[label] = analysis_df[label + \": FOV Corrected: Time Corrected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis_df[\n",
    "    [\n",
    "        \"File Index\",\n",
    "        \"File Trench Index\",\n",
    "        \"timepoints\",\n",
    "        \"Objectid\",\n",
    "        \"centroid_y\",\n",
    "        \"centroid_x\",\n",
    "        \"area\",\n",
    "        \"fov\",\n",
    "        \"row\",\n",
    "        \"trench\",\n",
    "        \"time (s)\",\n",
    "        \"lane orientation\",\n",
    "        \"y (local)\",\n",
    "        \"x (local)\",\n",
    "        \"y (global)\",\n",
    "        \"x (global)\",\n",
    "        \"trenchid\",\n",
    "        \"Trenchid Timepoint Index\",\n",
    "    ]\n",
    "    + values_to_rescale\n",
    "].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFP Quantification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_background_subtract(series, intensity_key):\n",
    "    intensity_vals = series[intensity_key]\n",
    "    bkd_val = series[series[\"Objectid\"] == 0][intensity_key].iloc[0]\n",
    "    bkd_sub_intensity = intensity_vals - bkd_val\n",
    "    bkd_sub_intensity = bkd_sub_intensity.to_dict()\n",
    "    return bkd_sub_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis_df_trenchtimepoint_sorted = (\n",
    "    analysis_df.reset_index(drop=False)\n",
    "    .set_index(\"Trenchid Timepoint Index\", sorted=False)\n",
    "    .persist()\n",
    ")\n",
    "analysis_df_trenchtimepoint_groupby = analysis_df_trenchtimepoint_sorted.groupby(\n",
    "    \"Trenchid Timepoint Index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gfp_intensity_wo_bkd = (\n",
    "    analysis_df_trenchtimepoint_groupby.apply(\n",
    "        lambda x: local_background_subtract(x, \"GFP-Penta mean_intensity\"),\n",
    "        meta=(\"GFP-Penta mean_intensity\", float),\n",
    "    )\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    "    .to_list()\n",
    ")\n",
    "gfp_intensity_wo_bkd = {k: v for d in gfp_intensity_wo_bkd for k, v in d.items()}\n",
    "gfp_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    gfp_intensity_wo_bkd, orient=\"index\", columns=[\"GFP-Penta mean_intensity_wo_bkd\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mchy_intensity_wo_bkd = (\n",
    "    analysis_df_trenchtimepoint_groupby.apply(\n",
    "        lambda x: local_background_subtract(x, \"RFP-Penta mean_intensity\"),\n",
    "        meta=(\"RFP-Penta mean_intensity\", float),\n",
    "    )\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    "    .to_list()\n",
    ")\n",
    "mchy_intensity_wo_bkd = {k: v for d in mchy_intensity_wo_bkd for k, v in d.items()}\n",
    "mchy_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    mchy_intensity_wo_bkd, orient=\"index\", columns=[\"RFP-Penta mean_intensity_wo_bkd\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis_df.join([mchy_intensity_wo_bkd, gfp_intensity_wo_bkd]).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df_nobkd = analysis_df[analysis_df[\"Objectid\"] != 0]\n",
    "analysis_df_nobkd[\"Object Parquet Index\"] = analysis_df_nobkd.apply(\n",
    "    lambda x: int(\n",
    "        f\"{x['File Index']:04n}{x['File Trench Index']:04n}{x['timepoints']:04n}{x['Objectid']:02n}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "analysis_df_nobkd = analysis_df_nobkd.set_index(\"Object Parquet Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_series = (\n",
    "    analysis_df_nobkd[\"GFP-Penta mean_intensity_wo_bkd\"]\n",
    "    / analysis_df_nobkd[\"RFP-Penta mean_intensity_wo_bkd\"]\n",
    ")\n",
    "analysis_df_nobkd[\"gfp/mchy Ratio\"] = ratio_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchid_groupby = analysis_df_nobkd.groupby(\"trenchid\")\n",
    "median_ratio = trenchid_groupby[\"gfp/mchy Ratio\"].apply(np.median).compute()\n",
    "median_ratio = median_ratio.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "dark_gfp = median_ratio < threshold\n",
    "perc_gfp = 1.0 - (np.sum(dark_gfp) / len(median_ratio))\n",
    "print(perc_gfp)\n",
    "\n",
    "plt.hist(\n",
    "    median_ratio[median_ratio < threshold],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    label=\"Measured Dark GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    median_ratio[median_ratio > threshold],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    label=\"Measured GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=26)\n",
    "plt.xticks(fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.legend(fontsize=26)\n",
    "# plt.savefig(\"./GFP_Threshold_Distribution_1.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write this...\n",
    "# reference_df = filter_df(lineage_df,[\"`Trench Score` < -75\"],client=dask_controller,repartition=False).persist()\n",
    "# query_df = filter_df(lineage_df,[\"`Mother CellID` != -1\",\"`Daughter CellID 1` != -1\",\"`Daughter CellID 2` != -1\",\\\n",
    "#                                               \"`Sister CellID` != -1\",\"`Trench Score` < -75\"],client=dask_controller,repartition=False).persist()\n",
    "# init_cells = get_growth_and_division_stats(query_df,reference_df)\n",
    "\n",
    "# del reference_df\n",
    "# del query_df\n",
    "# del lineage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Call Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/GFP/analysis\"\n",
    ")\n",
    "last_trenchid = int(analysis_df.tail(1)[\"trenchid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_true = np.sum([item == True for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_false = np.sum([item == False for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_none = np.sum([item == \"Unknown\" for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_called = ttl_true + ttl_false\n",
    "ttl_trenches = barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_signal = barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_signal = ttl_called / ttl_trenches_w_signal\n",
    "\n",
    "percent_called_w_gfp_call = ttl_called / last_trenchid\n",
    "percent_signal_w_gfp_call = ttl_trenches_w_signal / last_trenchid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_gfp_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_signal_w_gfp_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating Error from Recall Rate\n",
    "\n",
    "math for this in scanned doc; using empirical hamming distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 29\n",
    "p_err_out = 1.0 - percent_called_w_signal\n",
    "H_vals, H_counts = np.unique(\n",
    "    barcode_df[\"Closest Hamming Distance\"].values, return_counts=True\n",
    ")\n",
    "p_Hdist = H_counts / np.sum(H_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = [-p_err_out]\n",
    "for idx, j in enumerate(H_vals):\n",
    "    bin_coeff = sp.special.binom(N, j)\n",
    "    coeff = bin_coeff - p_Hdist[idx]\n",
    "    coeffs.append(coeff)\n",
    "coeffs = np.array(coeffs)[::-1]\n",
    "roots = np.roots(coeffs)\n",
    "epsilon = np.real(roots[~np.iscomplex(roots)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_err_in = 0.0\n",
    "for idx, j in enumerate(H_vals):\n",
    "    p_err_in += p_Hdist[idx] * (epsilon**j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epsilon Estimate: \" + str(epsilon))\n",
    "print(\"P(error in) Estimate: \" + str(p_err_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guessing epislon based on single bit error rate (using soft matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "eps_range = np.linspace(0.0, 0.2)\n",
    "for epsilon in eps_range:\n",
    "    #     epsilon = 0.05\n",
    "    p_err_in = 0.0\n",
    "    for idx, j in enumerate(H_vals):\n",
    "        p_err_in += p_Hdist[idx] * (epsilon**j)\n",
    "    output.append(p_err_in * 100)\n",
    "\n",
    "\n",
    "plt.plot(eps_range, output)\n",
    "plt.xlabel(\"Epsilon\", fontsize=20)\n",
    "plt.ylabel(\"Error Rate (%)\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get trenchwise GFP signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mchy_df = analysis_df[analysis_df[\"Intensity Channel\"] == \"RFP-Penta\"]\n",
    "mchy_groupby = mchy_df.groupby([\"trenchid\", \"timepoints\"])\n",
    "\n",
    "gfp_df = analysis_df[analysis_df[\"Intensity Channel\"] == \"GFP-Penta\"]\n",
    "gfp_groupby = gfp_df.groupby([\"trenchid\", \"timepoints\"])\n",
    "\n",
    "gfp_intensity_wo_bkd = (\n",
    "    gfp_groupby.apply(\n",
    "        lambda x: (\n",
    "            x[\"mean_intensity\"] - x[x[\"Objectid\"] == 0][\"mean_intensity\"].iloc[0]\n",
    "        ).to_dict(),\n",
    "        meta=(\"mean_intensity\", float),\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .compute()\n",
    "    .to_list()\n",
    ")\n",
    "gfp_intensity_wo_bkd = {k: v for d in gfp_intensity_wo_bkd for k, v in d.items()}\n",
    "gfp_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    gfp_intensity_wo_bkd, orient=\"index\", columns=[\"mean_intensity_wo_bkd\"]\n",
    ")\n",
    "gfp_df = gfp_df.join(gfp_intensity_wo_bkd).persist()\n",
    "del gfp_intensity_wo_bkd\n",
    "\n",
    "mchy_intensity_wo_bkd = (\n",
    "    mchy_groupby.apply(\n",
    "        lambda x: (\n",
    "            x[\"mean_intensity\"] - x[x[\"Objectid\"] == 0][\"mean_intensity\"].iloc[0]\n",
    "        ).to_dict(),\n",
    "        meta=(\"mean_intensity\", float),\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .compute()\n",
    "    .to_list()\n",
    ")\n",
    "mchy_intensity_wo_bkd = {k: v for d in mchy_intensity_wo_bkd for k, v in d.items()}\n",
    "mchy_intensity_wo_bkd = pd.DataFrame.from_dict(\n",
    "    mchy_intensity_wo_bkd, orient=\"index\", columns=[\"mean_intensity_wo_bkd\"]\n",
    ")\n",
    "mchy_df = mchy_df.join(mchy_intensity_wo_bkd).persist()\n",
    "del mchy_intensity_wo_bkd\n",
    "\n",
    "gfp_df_nobkd = gfp_df[gfp_df[\"Objectid\"] != 0]\n",
    "gfp_df_nobkd[\"Object Parquet Index\"] = gfp_df_nobkd.apply(\n",
    "    lambda x: int(\n",
    "        f\"{x['File Index']:04}{x['File Trench Index']:04}{x['timepoints']:04}{x['Objectid']:02}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "gfp_df_nobkd = gfp_df_nobkd.set_index(\"Object Parquet Index\")\n",
    "\n",
    "mchy_df_nobkd = mchy_df[mchy_df[\"Objectid\"] != 0]\n",
    "mchy_df_nobkd[\"Object Parquet Index\"] = mchy_df_nobkd.apply(\n",
    "    lambda x: int(\n",
    "        f\"{x['File Index']:04}{x['File Trench Index']:04}{x['timepoints']:04}{x['Objectid']:02}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "mchy_df_nobkd = mchy_df_nobkd.set_index(\"Object Parquet Index\")\n",
    "\n",
    "ratio_series = (\n",
    "    gfp_df_nobkd[\"mean_intensity_wo_bkd\"] / mchy_df_nobkd[\"mean_intensity_wo_bkd\"]\n",
    ")\n",
    "gfp_df_nobkd[\"gfp/mchy Ratio\"] = ratio_series\n",
    "\n",
    "trenchid_groupby = gfp_df_nobkd.groupby(\"trenchid\")\n",
    "median_ratio = trenchid_groupby[\"gfp/mchy Ratio\"].apply(np.median).compute()\n",
    "median_ratio = median_ratio.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    median_ratio,\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    label=\"Measured Dark GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=26)\n",
    "plt.xticks(fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.legend(fontsize=26)\n",
    "# plt.savefig(\"./GFP_Intensity_Ratio_Dist.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply GFP Signal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "dark_gfp = median_ratio < threshold\n",
    "perc_gfp = 1.0 - (np.sum(dark_gfp) / len(median_ratio))\n",
    "print(perc_gfp)\n",
    "\n",
    "plt.hist(\n",
    "    median_ratio[median_ratio < threshold],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    label=\"Measured Dark GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    median_ratio[median_ratio > threshold],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    label=\"Measured GFP\",\n",
    "    density=True,\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=26)\n",
    "plt.xticks(fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.legend(fontsize=26)\n",
    "# plt.savefig(\"./GFP_Threshold_Distribution_1.png\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Trench Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/GFP/kymograph/metadata\"\n",
    ")\n",
    "barcode_kymo_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-10-21_lDE15_Final_1/barcodes/kymograph/metadata\"\n",
    ")\n",
    "\n",
    "max_gfp_tpt = gfp_kymo_df.loc[:1000][\"timepoints\"].max().compute()\n",
    "min_barcode_tpt = barcode_kymo_df.loc[:1000][\"timepoints\"].min().compute()\n",
    "\n",
    "last_gfp_tpt_df = gfp_kymo_df[gfp_kymo_df[\"timepoints\"] == max_gfp_tpt].compute()\n",
    "first_barcode_tpt_df = barcode_kymo_df[\n",
    "    barcode_kymo_df[\"timepoints\"] == min_barcode_tpt\n",
    "].compute()\n",
    "\n",
    "trenchid_map = tr.get_trenchid_map(first_barcode_tpt_df, last_gfp_tpt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get GFP Call Error and Recovery Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_df[\"Measured Dark GFP\"] = barcode_df.apply(\n",
    "    tr.map_Series, axis=1, args=(dark_gfp, trenchid_map)\n",
    ")\n",
    "barcode_df[\"Measured GFP Ratio\"] = barcode_df.apply(\n",
    "    tr.map_Series, axis=1, args=(median_ratio, trenchid_map)\n",
    ")\n",
    "called_df = barcode_df[barcode_df[\"Measured Dark GFP\"] != \"Unknown\"]\n",
    "ttl_correct = np.sum(called_df[\"dark_gfp\"] == called_df[\"Measured Dark GFP\"])\n",
    "ttl_called = len(called_df)\n",
    "recovery_rate = len(called_df) / len(dark_gfp)\n",
    "n_barcodes = called_df[\"barcodeid\"].nunique()\n",
    "n_trenches = called_df[\"trenchid\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error Rate:\" + str(1.0 - ttl_correct / ttl_called))\n",
    "print(\"Recovery Rate:\" + str(recovery_rate))\n",
    "print(\"Unique Barcodes:\" + str(n_barcodes))\n",
    "print(\"Total Trenches:\" + str(n_trenches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_kymo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_kymo_idx = gfp_kymo_df[\"trenchid\"].unique().compute().tolist()\n",
    "valid_barcode_df = barcode_df[\n",
    "    barcode_df[\"trenchid\"].isin(trenchid_map.keys())\n",
    "].compute()\n",
    "barcode_df_mapped_trenchids = valid_barcode_df[\"trenchid\"].apply(\n",
    "    lambda x: trenchid_map[x]\n",
    ")\n",
    "valid_init_df_indices = barcode_df_mapped_trenchids.isin(gfp_kymo_idx)\n",
    "barcode_df_mapped_trenchids = barcode_df_mapped_trenchids[valid_init_df_indices]\n",
    "final_valid_barcode_df_indices = barcode_df_mapped_trenchids.index.to_list()\n",
    "called_df = barcode_df.loc[final_valid_barcode_df_indices]\n",
    "called_df[\"phenotype trenchid\"] = barcode_df_mapped_trenchids\n",
    "called_df = (\n",
    "    called_df.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", drop=True, sorted=False)\n",
    "    .compute()\n",
    ")\n",
    "# called_df = called_df.repartition(npartitions=1).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cells = init_cells.rename(columns={\"trenchid\": \"phenotype trenchid\"})\n",
    "init_cells = (\n",
    "    init_cells.reset_index()\n",
    "    .set_index(\"phenotype trenchid\", drop=True, sorted=False)\n",
    "    .compute()\n",
    ")\n",
    "init_cells = init_cells.merge(called_df, how=\"inner\", left_index=True, right_index=True)\n",
    "init_cells = init_cells.drop([\"Barcode Signal\"], axis=1)\n",
    "init_cells = init_cells.reset_index().set_index(\"Global CellID\")\n",
    "init_cells = init_cells.sort_index()\n",
    "final_output_df = dd.from_pandas(init_cells, npartitions=200).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complete_barcode_df = barcode_df[barcode_df[\"trenchid\"].isin(trenchid_map.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_barcode_df[\"kymograph\"] = complete_barcode_df.apply(\n",
    "    lambda x: trenchid_map[x[\"trenchid\"]], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.hist(\n",
    "    called_df[called_df[\"dark_gfp\"] == True][\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.hist(\n",
    "    called_df[called_df[\"dark_gfp\"] == False][\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.xlabel(\"Lineage GFP/mCherry Intensity Ratio\", fontsize=26)\n",
    "plt.ylabel(\"Lineages\", fontsize=26)\n",
    "plt.xticks(fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.savefig(\"./GFP_Threshold_Distribution_2.svg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_mat(df):\n",
    "    TP = np.sum((df[\"dark_gfp\"] == False) & (df[\"Measured Dark GFP\"] == False))\n",
    "    TN = np.sum((df[\"dark_gfp\"] == True) & (df[\"Measured Dark GFP\"] == True))\n",
    "    FP = np.sum((df[\"dark_gfp\"] == False) & (df[\"Measured Dark GFP\"] == True))\n",
    "    FN = np.sum((df[\"dark_gfp\"] == True) & (df[\"Measured Dark GFP\"] == False))\n",
    "\n",
    "    error = (FP + FN) / (TP + TN + FP + FN)\n",
    "    FP_error = FP / (TP + TN + FP + FN)\n",
    "    FN_error = FN / (TP + TN + FP + FN)\n",
    "\n",
    "    return error, FP_error, FN_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error, FP_error, FN_error = get_confusion_mat(called_df)\n",
    "print(\"Error: \" + str(error))\n",
    "print(\"FP error: \" + str(FP_error))\n",
    "print(\"FN error: \" + str(FN_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_filters = list(range(1, 5))\n",
    "hamming_n_barcodes = []\n",
    "hamming_errors = []\n",
    "for i in hamming_filters:\n",
    "    filtered_df = called_df[called_df[\"Closest Hamming Distance\"] >= i]\n",
    "    n_barcode = len(filtered_df)\n",
    "    error, FP_error, FN_error = get_confusion_mat(filtered_df)\n",
    "    error = np.round(100 * error, decimals=2)\n",
    "    hamming_errors.append(error)\n",
    "    hamming_n_barcodes.append(n_barcode)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.lineplot(hamming_filters, hamming_errors, linewidth=4, marker=\"o\", markersize=15)\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "plt.xlabel(\"Minimum Hamming Distance\", fontsize=26)\n",
    "plt.ylabel(\"Error Rate (%)\", fontsize=26)\n",
    "plt.xticks([1, 2, 3, 4], fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.savefig(\"./Hamming_Dist_vs_Error.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "sns.lineplot(hamming_filters, hamming_errors, linewidth=4, marker=\"o\", markersize=15)\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "plt.xlabel(\"Minimum Hamming Distance\", fontsize=26)\n",
    "plt.ylabel(\"Error Rate (%)\", fontsize=26)\n",
    "plt.xticks([1, 2, 3, 4], fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.ylim(-5.0, 100.0)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(\"./Hamming_Dist_vs_Error_big.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_lib = 100 * (np.array(hamming_n_barcodes) / hamming_n_barcodes[0])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.lineplot(hamming_filters, perc_lib, linewidth=4, marker=\"o\", markersize=15)\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "plt.ylim(0, 110)\n",
    "plt.xlabel(\"Minimum Hamming Distance\", fontsize=26)\n",
    "plt.ylabel(\"Percent of Library\", fontsize=26)\n",
    "plt.xticks([1, 2, 3, 4], fontsize=26)\n",
    "plt.yticks(\n",
    "    [0, 20, 40, 60, 80, 100],\n",
    "    fontsize=26,\n",
    ")\n",
    "plt.savefig(\"./Hamming_Dist_vs_Lib_Size.svg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying The GFP Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "called_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_threshold = 1.0\n",
    "called_negative_df = called_df[called_df[\"Measured GFP Ratio\"] < negative_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TN = np.sum(\n",
    "    (called_negative_df[\"dark_gfp\"] == True)\n",
    "    & (called_negative_df[\"Measured Dark GFP\"] == True)\n",
    ")\n",
    "FP = np.sum(\n",
    "    (called_negative_df[\"dark_gfp\"] == False)\n",
    "    & (called_negative_df[\"Measured Dark GFP\"] == True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TN / (TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_mat(df):\n",
    "    TP = np.sum((df[\"dark_gfp\"] == False) & (df[\"Measured Dark GFP\"] == False))\n",
    "    TN = np.sum((df[\"dark_gfp\"] == True) & (df[\"Measured Dark GFP\"] == True))\n",
    "    FP = np.sum((df[\"dark_gfp\"] == False) & (df[\"Measured Dark GFP\"] == True))\n",
    "    FN = np.sum((df[\"dark_gfp\"] == True) & (df[\"Measured Dark GFP\"] == False))\n",
    "\n",
    "    error = (FP + FN) / (TP + TN + FP + FN)\n",
    "    FP_error = FP / (TP + TN + FP + FN)\n",
    "    FN_error = FN / (TP + TN + FP + FN)\n",
    "\n",
    "    return error, FP_error, FN_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gmm_params(values):\n",
    "    gmm = skl.mixture.GaussianMixture(n_components=2, n_init=10)\n",
    "    gmm.fit(values.reshape(-1, 1))\n",
    "    #     probs = gmm.predict_proba(values.reshape(-1,1))\n",
    "    return gmm.means_[:, 0], ((gmm.covariances_) ** (1 / 2))[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_std = 0.5\n",
    "\n",
    "means, stds = get_gmm_params(called_df[\"Measured GFP Ratio\"].values)\n",
    "if means[0] > means[1]:\n",
    "    means = means[::-1]\n",
    "    stds = stds[::-1]\n",
    "\n",
    "upper_bound = means + test_std\n",
    "lower_bound = means - test_std\n",
    "\n",
    "valid_dark = called_df[\"Measured GFP Ratio\"] < upper_bound[0]\n",
    "valid_gfp = called_df[\"Measured GFP Ratio\"] > lower_bound[1]\n",
    "valid = valid_dark | valid_gfp\n",
    "\n",
    "filtered_df = called_df[valid]\n",
    "filtered_df_complement = called_df[~valid]\n",
    "\n",
    "plt.title(\"0.5 Standard Deviations\", fontsize=20)\n",
    "plt.hist(\n",
    "    filtered_df_complement[\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.hist(\n",
    "    filtered_df[\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"red\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_std = 1.0\n",
    "\n",
    "means, stds = get_gmm_params(called_df[\"Measured GFP Ratio\"].values)\n",
    "if means[0] > means[1]:\n",
    "    means = means[::-1]\n",
    "    stds = stds[::-1]\n",
    "\n",
    "upper_bound = means + test_std\n",
    "lower_bound = means - test_std\n",
    "\n",
    "valid_dark = called_df[\"Measured GFP Ratio\"] < upper_bound[0]\n",
    "valid_gfp = called_df[\"Measured GFP Ratio\"] > lower_bound[1]\n",
    "valid = valid_dark | valid_gfp\n",
    "\n",
    "filtered_df = called_df[valid]\n",
    "filtered_df_complement = called_df[~valid]\n",
    "\n",
    "plt.title(\"1.0 Standard Deviations\", fontsize=20)\n",
    "plt.hist(\n",
    "    filtered_df_complement[\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.hist(\n",
    "    filtered_df[\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"red\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_std = 1.5\n",
    "\n",
    "means, stds = get_gmm_params(called_df[\"Measured GFP Ratio\"].values)\n",
    "if means[0] > means[1]:\n",
    "    means = means[::-1]\n",
    "    stds = stds[::-1]\n",
    "\n",
    "upper_bound = means + test_std\n",
    "lower_bound = means - test_std\n",
    "\n",
    "valid_dark = called_df[\"Measured GFP Ratio\"] < upper_bound[0]\n",
    "valid_gfp = called_df[\"Measured GFP Ratio\"] > lower_bound[1]\n",
    "valid = valid_dark | valid_gfp\n",
    "\n",
    "filtered_df = called_df[valid]\n",
    "filtered_df_complement = called_df[~valid]\n",
    "\n",
    "plt.title(\"1.5 Standard Deviations\", fontsize=20)\n",
    "plt.hist(\n",
    "    filtered_df_complement[\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.hist(\n",
    "    filtered_df[\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"red\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "n_std = np.linspace(0, 2, 20)\n",
    "n_barcodes = []\n",
    "errors = []\n",
    "FP_errors = []\n",
    "FN_errors = []\n",
    "\n",
    "means, stds = get_gmm_params(called_df[\"Measured GFP Ratio\"].values)\n",
    "\n",
    "if means[0] > means[1]:\n",
    "    means = means[::-1]\n",
    "    stds = stds[::-1]\n",
    "\n",
    "for i in n_std:\n",
    "    upper_bound = means + stds * i\n",
    "    lower_bound = means - stds * i\n",
    "\n",
    "    #     valid_dark = (called_df_barcodes[\"Measured Median GFP\"] < upper_bound[0]) &\\\n",
    "    #     (called_df_barcodes[\"Measured Median GFP\"] > lower_bound[0])\n",
    "    #     valid_gfp = (called_df_barcodes[\"Measured Median GFP\"] < upper_bound[1]) &\\\n",
    "    #     (called_df_barcodes[\"Measured Median GFP\"] > lower_bound[1])\n",
    "    #     valid = valid_dark|valid_gfp\n",
    "    valid_dark = called_df[\"Measured GFP Ratio\"] < upper_bound[0]\n",
    "    valid_gfp = called_df[\"Measured GFP Ratio\"] > lower_bound[1]\n",
    "    valid = valid_dark | valid_gfp\n",
    "\n",
    "    filtered_df = called_df[valid]\n",
    "    n_barcode = len(filtered_df)\n",
    "    error, FP_error, FN_error = get_confusion_mat(filtered_df)\n",
    "    error = np.round(100 * error, decimals=2)\n",
    "    FP_error = np.round(100 * FP_error, decimals=2)\n",
    "    FN_error = np.round(100 * FN_error, decimals=2)\n",
    "    errors.append(error)\n",
    "    FP_errors.append(FP_error)\n",
    "    FN_errors.append(FN_error)\n",
    "    n_barcodes.append(n_barcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(n_std, errors, linewidth=5, label=\"Error\")\n",
    "sns.lineplot(n_std, FP_errors, linewidth=5, label=\"FP Error\")\n",
    "sns.lineplot(n_std, FN_errors, linewidth=5, label=\"FN Error\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel(\"N $\\sigma$s Around Peak\", fontsize=20)\n",
    "plt.ylabel(\"Error Rate (%)\", fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"./GFP_Error_vs_Sigma.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.lineplot(n_std, n_barcodes, linewidth=5)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.xlabel(\"N $\\sigma$s Around Peak\", fontsize=20)\n",
    "plt.ylabel(\"Library Size Past Filter\", fontsize=20)\n",
    "plt.savefig(\"./Library_Size_vs_Sigma.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources of error\n",
    "\n",
    "There are around twice the number of false negatives (predicted to be a Dark GFP, but measured as bright) as there are false positives (predicted to be GFP, but measured as dark).\n",
    "\n",
    "Some theories for these error classes:\n",
    "\n",
    "False Positives:\n",
    "    \n",
    "    - Mutations in the promoter (should be constant within barcodes)\n",
    "    \n",
    "    - Strain variation (should be lower when averaging among strains)\n",
    "    \n",
    "    - Misread of barcodes\n",
    "    \n",
    "False Negatives:\n",
    "    \n",
    "    - Bleed from adjacent cells (should be corrected by averging among strains)\n",
    "    \n",
    "    - Multiple strains per trench (?)\n",
    "    \n",
    "    - Misread of barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median GFP Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_gfp_df = called_df.groupby(\"Barcode\").apply(\n",
    "    lambda x: x[\"Measured GFP Ratio\"].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    median_gfp_df[median_gfp_df < threshold],\n",
    "    range=(0, 20),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    label=\"Measured Dark GFP\",\n",
    ")\n",
    "plt.hist(\n",
    "    median_gfp_df[median_gfp_df > threshold],\n",
    "    range=(0, 20),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    label=\"Measured GFP\",\n",
    ")\n",
    "plt.xlabel(\"Mean Intensity Ratio\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"./Pooled_correction.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "called_df_barcodes = called_df.set_index([\"Barcode\"]).sort_index()\n",
    "called_df_barcodes[\"Measured Median GFP\"] = median_gfp_df\n",
    "called_df_barcodes.reset_index(drop=False)\n",
    "called_df_barcodes = called_df_barcodes.groupby(\"Barcode\").apply(lambda x: x.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_correct = np.sum(\n",
    "    called_df_barcodes[\"dark_gfp\"]\n",
    "    == (called_df_barcodes[\"Measured Median GFP\"] < threshold)\n",
    ")\n",
    "ttl_called = len(called_df_barcodes)\n",
    "print(\"Percent Correct:\" + str(ttl_correct / ttl_called))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error With One Mismatch (Hamming Distance Up to 1, Eliminate Bad Bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Barcode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_handle = tr.pandas_hdf5_handler(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/barcodes/barcode_df_hamming_1.hdf5\"\n",
    ")\n",
    "barcode_df = meta_handle.read_df(\"barcodes\", read_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Call Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = dd.read_parquet(\n",
    "    \"/home/de64/scratch/de64/sync_folder/2021-03-07_lDE15/gfp/analysis\"\n",
    ")\n",
    "last_trenchid = int(analysis_df.tail(1)[\"trenchid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_true = np.sum([item == True for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_false = np.sum([item == False for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_none = np.sum([item == \"Unknown\" for item in barcode_df[\"dark_gfp\"].tolist()])\n",
    "ttl_called = ttl_true + ttl_false\n",
    "ttl_trenches = barcode_df.metadata[\"Total Trenches\"]\n",
    "ttl_trenches_w_signal = barcode_df.metadata[\"Total Trenches With Cells\"]\n",
    "percent_called = ttl_called / ttl_trenches\n",
    "percent_called_w_signal = ttl_called / ttl_trenches_w_signal\n",
    "\n",
    "percent_called_w_gfp_call = ttl_called / last_trenchid\n",
    "percent_signal_w_gfp_call = ttl_trenches_w_signal / last_trenchid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_called_w_gfp_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_signal_w_gfp_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get GFP Call Error and Recovery Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_df[\"Measured Dark GFP\"] = barcode_df.apply(\n",
    "    tr.map_Series, axis=1, args=(dark_gfp, trenchid_map)\n",
    ")\n",
    "barcode_df[\"Measured GFP Ratio\"] = barcode_df.apply(\n",
    "    tr.map_Series, axis=1, args=(median_ratio, trenchid_map)\n",
    ")\n",
    "called_df = barcode_df[barcode_df[\"Measured Dark GFP\"] != \"Unknown\"]\n",
    "ttl_correct = np.sum(called_df[\"dark_gfp\"] == called_df[\"Measured Dark GFP\"])\n",
    "ttl_called = len(called_df)\n",
    "recovery_rate = len(called_df) / len(dark_gfp)\n",
    "n_barcodes = called_df[\"barcodeid\"].nunique()\n",
    "n_trenches = called_df[\"trenchid\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error Rate:\" + str(1.0 - ttl_correct / ttl_called))\n",
    "print(\"Recovery Rate:\" + str(recovery_rate))\n",
    "print(\"Unique Barcodes:\" + str(n_barcodes))\n",
    "print(\"Total Trenches:\" + str(n_trenches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.hist(\n",
    "    called_df[called_df[\"dark_gfp\"] == True][\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"grey\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted Dark GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.hist(\n",
    "    called_df[called_df[\"dark_gfp\"] == False][\"Measured GFP Ratio\"],\n",
    "    range=(0, 10),\n",
    "    bins=50,\n",
    "    color=\"green\",\n",
    "    alpha=0.7,\n",
    "    label=\"Predicted GFP\",\n",
    "    density=False,\n",
    ")\n",
    "plt.xlabel(\"Lineage GFP/mCherry Intensity Ratio\", fontsize=20)\n",
    "plt.ylabel(\"Lineages\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\n",
    "    \"./GFP_Threshold_Distribution_2_hamming_1.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
