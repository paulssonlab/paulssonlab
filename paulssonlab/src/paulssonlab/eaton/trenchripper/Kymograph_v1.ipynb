{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelizing the kymograph code (with dask)\n",
    "\n",
    "Xref: Journal/10_15_18/generate_kymograph.ipynb\n",
    "\n",
    "\n",
    "Let's copy everything useful from that notebook...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import scipy.signal\n",
    "import shutil\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import time\n",
    "import os\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I understand what is going on generally, here I start rewriting Suyang's kymograph code, outlined briefly here:\n",
    "\n",
    "0. Drift correct (to be added later)\n",
    "1. Overlay the first 50 frames\n",
    "2. Take 85th percentile intensity value from each row (or take mean)\n",
    "3. Find the \"top\" (i.e. trench end) peak for trenches.\n",
    "4. Use in combination with known length (free parameter) to crop out top and bottom\n",
    "5. Repeat (2) along columns. Find midpoints of trenches.\n",
    "6). Crop out fixed length around midpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEED TO HAVE A PARAMETER TUNING INTERFACE, take a look at charles' code?\n",
    "\n",
    "\n",
    "class kymograph:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_path,\n",
    "        output_path,\n",
    "        temp_path,\n",
    "        all_channels,\n",
    "        trench_len_y,\n",
    "        padding_y,\n",
    "        trench_width_x,\n",
    "        top_and_bottom=True,\n",
    "        t_chunk=25,\n",
    "        y_percentile=85,\n",
    "        y_time_percentile=85,\n",
    "        smoothing_kernel_y=(9, 3),\n",
    "        y_threshold_bins=100,\n",
    "        y_threshold_distance_bin_ratio=0.08,\n",
    "        y_threshold_backup_percentile=70,\n",
    "        x_percentile=85,\n",
    "        background_kernel_x=301,\n",
    "        smoothing_kernel_x=9,\n",
    "        x_threshold_bins=20,\n",
    "        x_threshold_distance_bin_ratio=0.1,\n",
    "        x_threshold_backup_percentile=60,\n",
    "    ):\n",
    "        \"\"\"The kymograph class is used to generate and visualize kymographs. The central function of this\n",
    "        class is the method 'generate_kymograph', which takes an hdf5 file of images from a single fov and\n",
    "        outputs an hdf5 file containing kymographs from all detected trenches.\n",
    "\n",
    "        Args:\n",
    "            input_path (string): Path to the input hdf5 file.\n",
    "            output_path (string): Path to write the output file to. Must be within *existing* directories.\n",
    "            all_channels (list): list of strings corresponding to the different image channels\n",
    "            available in the input hdf5 file, with the channel used for segmenting trenches in\n",
    "            the first position. NOTE: these names must match those of the input hdf5 file datasets.\n",
    "            temp_path (string): Path to a temporary folder to be used during computation,\n",
    "            will be created if it does not exist and overwrites any files that start in this\n",
    "            directory.\n",
    "            trench_len_y (int): Length from the end of the tenches to be used when cropping in the\n",
    "            y-dimension.\n",
    "            padding_y (int): Padding to be used when cropping in the y-dimension.\n",
    "            trench_width_x (int): Width to be used when cropping in the x-dimension.\n",
    "            top_and_bottom (bool): Whether or not to look for top and bottom trenches, or only top.\n",
    "            NOTE: will change this soon to be more flexible.\n",
    "            t_chunk (int): Time chunk to load into memory during computation. Inceasing this number\n",
    "            will lead to faster processing, but will also increase memory load.\n",
    "\n",
    "            All entries below should not be tuned except in special cases...\n",
    "\n",
    "            y_percentile (int): Used for reducing signal in xyt to only the yt dimension when cropping\n",
    "            in the y-dimension.\n",
    "            y_time_percentile (int): Used for reducing signal in yt dimension to only the y dimension\n",
    "            when cropping in the y-dimension.\n",
    "            smoothing_kernel_y (tuple): Two-entry tuple specifying a kernel size for smoothing out yt\n",
    "            signal when cropping in the y-dimension.\n",
    "            y_threshold_bins (int): Number of bins to be used when determining a threshold for trench\n",
    "            detection in the y-dimension.\n",
    "            y_threshold_distance_bin_ratio (float): Tunes the ratio between peak-to-peak distance and\n",
    "            the resolution of the y thresholding histogram.\n",
    "            y_threshold_backup_percentile (int): Backup percentile cutoff to be used when histogram-based\n",
    "            thresholding in y fails.\n",
    "\n",
    "            x_percentile (int): Used for reducing signal in xyt to only the xt dimension when cropping\n",
    "            in the x-dimension.\n",
    "            background_kernel_x (int): Kernel used for performing background subtraction on xt signal when\n",
    "            cropping in the x-dimension.\n",
    "            smoothing_kernel_x (int): Kernel used for performing smoothing on xt signal when cropping\n",
    "            in the x-dimension.\n",
    "            x_threshold_bins (int): Number of bins to be used when determining a threshold for trench\n",
    "            detection in the x-dimension.\n",
    "            x_threshold_distance_bin_ratio (float): Tunes the ratio between peak-to-peak distance and\n",
    "            the resolution of the x thresholding histogram.\n",
    "            x_threshold_backup_percentile (int): Backup percentile cutoff to be used when histogram-based\n",
    "            thresholding in x fails.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.hdf5file = h5py.File(input_path, \"r\")\n",
    "        self.temp_path = temp_path\n",
    "        self.writedir(self.temp_path, overwrite=True)\n",
    "        self.output_path = output_path\n",
    "        self.all_channels = all_channels\n",
    "        self.seg_channel = self.all_channels[0]\n",
    "        self.y_dim, self.x_dim, self.t_dim = self.hdf5file[self.seg_channel].shape\n",
    "        if top_and_bottom:\n",
    "            self.top_bottom_list = [\"top\", \"bottom\"]\n",
    "        else:\n",
    "            self.top_bottom_list = [\"top\"]\n",
    "        self.t_chunk = t_chunk\n",
    "\n",
    "        #### important paramaters to set\n",
    "        self.trench_len_y = trench_len_y\n",
    "        self.padding_y = padding_y\n",
    "        self.ttl_len_y = trench_len_y + padding_y\n",
    "        self.trench_width_x = trench_width_x\n",
    "        self.top_and_bottom = top_and_bottom\n",
    "\n",
    "        #### params for y\n",
    "        ## parameter for reducing signal to one dim\n",
    "        self.y_percentile = y_percentile\n",
    "        ## parameters for threshold finding\n",
    "        self.y_time_percentile = y_time_percentile\n",
    "        self.smoothing_kernel_y = smoothing_kernel_y\n",
    "        self.y_threshold_bins = y_threshold_bins\n",
    "        self.y_threshold_distance_bin_ratio = y_threshold_distance_bin_ratio\n",
    "        self.y_threshold_backup_percentile = y_threshold_backup_percentile\n",
    "\n",
    "        #### params for x\n",
    "        ## parameter for reducing signal to one dim\n",
    "        self.x_percentile = x_percentile\n",
    "        ## parameters for midpoint finding\n",
    "        self.background_kernel_x = background_kernel_x\n",
    "        self.smoothing_kernel_x = smoothing_kernel_x\n",
    "        ## parameters for threshold finding\n",
    "        self.x_threshold_bins = x_threshold_bins\n",
    "        self.x_threshold_distance_bin_ratio = x_threshold_distance_bin_ratio\n",
    "        self.x_threshold_backup_percentile = x_threshold_backup_percentile\n",
    "\n",
    "    def writedir(self, directory, overwrite=False):\n",
    "        \"\"\"Creates an empty directory at the specified location. If a directory is\n",
    "        already at this location, it will be overwritten if 'overwrite' is true,\n",
    "        otherwise it will be left alone.\n",
    "\n",
    "        Args:\n",
    "            directory (str): Path to directory to be overwritten/created.\n",
    "            overwrite (bool, optional): Whether to overwrite a directory that\n",
    "            already exists in this location.\n",
    "        \"\"\"\n",
    "        print(directory)\n",
    "        if overwrite:\n",
    "            shutil.rmtree(directory)\n",
    "            os.makedirs(directory)\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "    def removefile(self, filepath):\n",
    "        \"\"\"Removes a file at the specified path, if it exists.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to file for deletion.\n",
    "        \"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "    def find_background_threshold(\n",
    "        self, statistic, bins, distance_bin_ratio, backup_percentile\n",
    "    ):\n",
    "        \"\"\"Determines an appropriate background threshold by finding a value that seperates the\n",
    "        two principle populations of values in the signal. It does this by making a histogram\n",
    "        of the signal, finding the two principle peaks, and taking the least populated value between\n",
    "        them as the threshold point. If this fails, a simple percentile is used as a backup\n",
    "        threshold.\n",
    "\n",
    "        Args:\n",
    "            statistic (array): One dimensional input signal.\n",
    "            bins (int): number of bins to be used in the histogram.\n",
    "            distance_bin_ratio (float): Tunes the ratio between peak-to-peak distance and\n",
    "            the resolution of the histogram.\n",
    "            backup_percentile (int): Backup percentile to be used as the threshold value if\n",
    "            \"smart\" thresholding fails.\n",
    "\n",
    "        Returns:\n",
    "            float: Threshold value to be used to detect trenches.\n",
    "        \"\"\"\n",
    "        histogram_freqs, histogram_values = np.histogram(statistic, bins=bins)\n",
    "        normalized_distance = int(bins * distance_bin_ratio)\n",
    "        normalized_height = int(len(statistic) / bins)\n",
    "        peaks, _ = scipy.signal.find_peaks(\n",
    "            histogram_freqs, distance=normalized_distance, height=normalized_height\n",
    "        )\n",
    "        if len(peaks) > 0:\n",
    "            background_threshold = histogram_values[\n",
    "                np.argmin(histogram_freqs[: peaks[-1]])\n",
    "            ]\n",
    "        else:\n",
    "            background_threshold = np.percentile(statistic, backup_percentile, axis=0)\n",
    "        return background_threshold\n",
    "\n",
    "    def median_filter_1d(self, statistic, kernel):\n",
    "        \"\"\"One-dimensional median filter, with average smoothing at the signal edges.\n",
    "\n",
    "        Args:\n",
    "            statistic (array): One dimensional input signal.\n",
    "            kernel (int): Kernel size to be used in the median filter.\n",
    "\n",
    "        Returns:\n",
    "            array: Median-filtered signal.\n",
    "        \"\"\"\n",
    "        kernel_pad = kernel // 2 + 1\n",
    "        med_filter = scipy.signal.medfilt(statistic, kernel_size=kernel)\n",
    "        start_edge = np.mean(med_filter[kernel_pad:kernel])\n",
    "        end_edge = np.mean(med_filter[-kernel:-kernel_pad])\n",
    "        med_filter[:kernel_pad] = start_edge\n",
    "        med_filter[-kernel_pad:] = end_edge\n",
    "        return med_filter\n",
    "\n",
    "    def median_filter_2d(self, array_list):\n",
    "        \"\"\"Two-dimensional median filter, with average smoothing at the signal edges in\n",
    "        the first dimension.\n",
    "\n",
    "        Args:\n",
    "            array_list (list): List containing a single array of yt signal to be smoothed.\n",
    "\n",
    "        Returns:\n",
    "            array: Median-filtered yt signal.\n",
    "        \"\"\"\n",
    "        kernel = np.array(self.smoothing_kernel_y)\n",
    "        kernel_pad = kernel // 2 + 1\n",
    "        med_filter = scipy.signal.medfilt(array_list[0], kernel_size=kernel)\n",
    "        start_edge = np.mean(med_filter[kernel_pad[0] : kernel[0]])\n",
    "        end_edge = np.mean(med_filter[-kernel[0] : -kernel_pad[0]])\n",
    "        med_filter[: kernel_pad[0]] = start_edge\n",
    "        med_filter[-kernel_pad[0] :] = end_edge\n",
    "        return med_filter\n",
    "\n",
    "    def reassign_idx(self, array, values, indices, axis):\n",
    "        \"\"\"Performs in-line value reassignment on numpy arrays, normally handled\n",
    "        with the array[:,indices] = values syntax, with the ability to supply\n",
    "        the axis as an argument.\n",
    "\n",
    "        Args:\n",
    "            array (array): Input array to have values reassigned.\n",
    "            values (array): New value positions in array.\n",
    "            indices (array): Positions in the input array to be reassigned.\n",
    "            axis (int): Axis along which to reassign values.\n",
    "        \"\"\"\n",
    "        str_constructor = \"\".join((len(array.shape) * [\":,\"]))[:-1]\n",
    "        str_constructor = \"[\" + str_constructor + \"]\"\n",
    "        str_constructor = (\n",
    "            \"array\"\n",
    "            + str_constructor[: axis * 2 + 1]\n",
    "            + \"indices\"\n",
    "            + str_constructor[axis * 2 + 2 :]\n",
    "            + \" = values\"\n",
    "        )\n",
    "        exec(str_constructor)\n",
    "\n",
    "    def write_hdf5(self, file_name, array, ti, t_len, t_dim_out, dataset_name):\n",
    "        \"\"\"Writes an array to a particular dataset in an hdf5 file. Positions\n",
    "        in time are left variable to enable chunking the dataset in time.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): Name of the hdf5 file, assumed to be in the temp folder\n",
    "            initialized by this class.\n",
    "            array (array): Array to be written.\n",
    "            ti (int): Initial time position to write array values to.\n",
    "            t_len (int): Total size of the target time dimension.\n",
    "            t_dim_out (int): Axis of the target time dimension.\n",
    "            dataset_name (str): The name of the hdf5 dataset to write to.\n",
    "        \"\"\"\n",
    "        with h5py.File(self.temp_path + file_name + \".hdf5\", \"a\") as h5pyfile:\n",
    "            indices = list(range(ti, min(ti + self.t_chunk, t_len)))\n",
    "            self.reassign_idx(h5pyfile[dataset_name], array, indices, t_dim_out)\n",
    "\n",
    "    def delete_hdf5(self, file_handle):\n",
    "        \"\"\"Deletes an hdf5 file, given its file handle, and closes the handle\n",
    "        itself.\n",
    "\n",
    "        Args:\n",
    "            file_handle (hdf5file): Hdf5 file handle.\n",
    "        \"\"\"\n",
    "        filepath = file_handle.filename\n",
    "        file_handle.close()\n",
    "        self.removefile(filepath)\n",
    "\n",
    "    def init_hdf5(self, file_name, dataset_name, array, t_len, t_dim_out):\n",
    "        \"\"\"Initializes an empty hdf5 file and dataset to write to, given an array\n",
    "        with the target shape in all axes but the time axis. The time axis\n",
    "        is then specified by t_len.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): Name of the hdf5 file, assumed to be in the temp folder\n",
    "            initialized by this class.\n",
    "            dataset_name (str): The name of the hdf5 dataset to initialize.\n",
    "            array (array): Array which is of the same size as the dataset,\n",
    "            except in the time dimension.\n",
    "            t_len (int): Total size of the dataset time dimension.\n",
    "            t_dim_out (int): Axis of the dataset time dimension.\n",
    "        \"\"\"\n",
    "        out_shape = list(array.shape)\n",
    "        out_shape[t_dim_out] = t_len\n",
    "        out_shape = tuple(out_shape)\n",
    "        with h5py.File(self.temp_path + file_name + \".hdf5\", \"a\") as h5pyfile:\n",
    "            hdf5_dataset = h5pyfile.create_dataset(\n",
    "                dataset_name, out_shape, dtype=\"uint16\"\n",
    "            )\n",
    "\n",
    "    def chunk_t(\n",
    "        self,\n",
    "        hdf5_array_list,\n",
    "        t_dim_in_list,\n",
    "        t_dim_out,\n",
    "        function,\n",
    "        file_name,\n",
    "        dataset_name,\n",
    "        *args,\n",
    "    ):  ##combine this with chunk_t_2d into a general function\n",
    "        \"\"\"Applies a given function to any number of input hdf5 arrays, chunking this processing in the\n",
    "        time dimension, and outputs another hdf5 file.\n",
    "\n",
    "        Args:\n",
    "            hdf5_array_list (list): List of input arrays to be operated on by the function.\n",
    "            t_dim_in_list (list): List of ints that specify the time axis of each input array.\n",
    "            t_dim_out (int): Specifies the time axis of the output array.\n",
    "            function (function): Function to apply to the input arrays (must be ready to take a list of\n",
    "            arrays as input.\n",
    "            file_name (str): Name of the output hdf5 file, assumed to be in the temp folder\n",
    "            initialized by this class.\n",
    "            dataset_name (str): The name of the hdf5 dataset to write to.\n",
    "            *args: Extra arguments to be passed to the function, that do not need to be chunked in time.\n",
    "\n",
    "        Returns:\n",
    "            hdf5file: Hdf5 file handle corresponding to the output array.\n",
    "        \"\"\"\n",
    "        t_len = hdf5_array_list[0].shape[t_dim_in_list[0]]\n",
    "        for ti in range(0, t_len, self.t_chunk):\n",
    "            indices = list(range(ti, min(ti + self.t_chunk, t_len)))\n",
    "            chunk_list = [\n",
    "                np.take(hdf5_array_list[i], indices, axis=t_dim_in_list[i])\n",
    "                for i in range(len(hdf5_array_list))\n",
    "            ]\n",
    "            f_chunk = function(chunk_list, *args)\n",
    "            if ti == 0:\n",
    "                self.init_hdf5(file_name, dataset_name, f_chunk, t_len, t_dim_out)\n",
    "            self.write_hdf5(file_name, f_chunk, ti, t_len, t_dim_out, dataset_name)\n",
    "        out_hdf5_handle = h5py.File(self.temp_path + file_name + \".hdf5\", \"r\")\n",
    "        return out_hdf5_handle\n",
    "\n",
    "    def import_hdf5(self, t_dim_in, t_dim_out, file_name, dataset_name):\n",
    "        \"\"\"Stripped down version of 'self.chunk_t' that performs initial import of the hdf5 file to be\n",
    "        processed. Simply converts the input hdf5 file's \"channel\" datasets into the first dimension\n",
    "        of the array, ordered as specified by 'self.all_channels'\n",
    "\n",
    "        Args:\n",
    "            t_dim_in (int): Specifies the time axis of the input array.\n",
    "            t_dim_out (int): Specifies the time axis of the output array.\n",
    "            file_name (str): Name of the output hdf5 file, assumed to be in the temp folder\n",
    "            initialized by this class.\n",
    "            dataset_name (str): The name of the hdf5 dataset to write to.\n",
    "\n",
    "        Returns:\n",
    "            hdf5file: Hdf5 file handle corresponding to the output array.\n",
    "        \"\"\"\n",
    "        t_len = self.hdf5file[self.seg_channel].shape[t_dim_in]\n",
    "        for ti in range(0, t_len, self.t_chunk):\n",
    "            indices = list(range(ti, min(ti + self.t_chunk, t_len)))\n",
    "            chunk_array = np.array(\n",
    "                [\n",
    "                    np.take(self.hdf5file[channel], indices, axis=t_dim_in)\n",
    "                    for channel in self.all_channels\n",
    "                ]\n",
    "            )\n",
    "            if ti == 0:\n",
    "                self.init_hdf5(file_name, dataset_name, chunk_array, t_len, t_dim_out)\n",
    "            self.write_hdf5(file_name, chunk_array, ti, t_len, t_dim_out, dataset_name)\n",
    "        out_hdf5_handle = h5py.File(self.temp_path + file_name + \".hdf5\", \"r\")\n",
    "        return out_hdf5_handle\n",
    "\n",
    "    def get_y_percentile(self, array_list):\n",
    "        \"\"\"Converts an input xyt array to a yt array using a percentile cutoff.\n",
    "\n",
    "        Args:\n",
    "            array_list (list): Singleton list containing input array.\n",
    "\n",
    "        Returns:\n",
    "            array: Output yt array.\n",
    "        \"\"\"\n",
    "        return np.percentile(array_list[0], self.y_percentile, axis=1)\n",
    "\n",
    "    def crop_y(self, array_list):\n",
    "        \"\"\"Performs cropping of the images in the y-dimension (may be possible to shorten/split this?)\n",
    "\n",
    "        Args:\n",
    "            array_list (list): List of arrays containing [smoothed yt signal, input image hdf5 array]\n",
    "\n",
    "        Returns:\n",
    "            array: Cropped output array.\n",
    "        \"\"\"\n",
    "        y_percentiles_smoothed = array_list[0]\n",
    "        img_arr = array_list[1]\n",
    "        y_time_percentile = np.percentile(\n",
    "            y_percentiles_smoothed, self.y_time_percentile, axis=1\n",
    "        )\n",
    "        background_threshold = self.find_background_threshold(\n",
    "            y_time_percentile,\n",
    "            self.y_threshold_bins,\n",
    "            self.y_threshold_distance_bin_ratio,\n",
    "            self.y_threshold_backup_percentile,\n",
    "        )\n",
    "        trench_mask_y = y_percentiles_smoothed > background_threshold\n",
    "\n",
    "        time_list = []\n",
    "        for t in range(trench_mask_y.shape[1]):\n",
    "            trench_edge_mask = trench_mask_y[1:, t] != trench_mask_y[:-1, t]\n",
    "            trench_edges_y = np.where(trench_edge_mask)[0]\n",
    "\n",
    "            if self.top_and_bottom:\n",
    "                top_trench_edge_y = trench_edges_y[0]\n",
    "                top_upper = max(top_trench_edge_y - self.padding_y, 0)\n",
    "                top_lower = min(\n",
    "                    top_trench_edge_y + self.trench_len_y, trench_mask_y.shape[0]\n",
    "                )\n",
    "\n",
    "                bottom_trench_edge = trench_edges_y[-1]\n",
    "                bottom_upper = max(bottom_trench_edge - self.trench_len_y, 0)\n",
    "                bottom_lower = min(\n",
    "                    bottom_trench_edge + self.padding_y, trench_mask_y.shape[0]\n",
    "                )\n",
    "\n",
    "                channel_list = []\n",
    "                for i, channel in enumerate(self.all_channels):\n",
    "                    output_array = img_arr[i, :, :, t]\n",
    "                    top_bottom_list = [\n",
    "                        output_array[top_upper:top_lower],\n",
    "                        output_array[bottom_upper:bottom_lower],\n",
    "                    ]\n",
    "                    channel_list.append(top_bottom_list)\n",
    "                time_list.append(channel_list)\n",
    "            else:\n",
    "                top_trench_edge_y = trench_edges_y[0]\n",
    "                top_upper = max(top_trench_edge_y - self.padding_y, 0)\n",
    "                top_lower = min(\n",
    "                    top_trench_edge_y + self.trench_len_y, trench_mask_y.shape[0]\n",
    "                )\n",
    "\n",
    "                channel_list = []\n",
    "                for channel in self.all_channels:\n",
    "                    output_array = img_arr[i, :, :, t]\n",
    "                    top_bottom_list = [output_array[top_upper:top_lower]]\n",
    "                    channel_list.append(top_bottom_list)\n",
    "                time_list.append(channel_list)\n",
    "\n",
    "        cropped_in_y = np.array(time_list)\n",
    "        cropped_in_y = np.moveaxis(cropped_in_y, (0, 1, 2, 3, 4), (4, 1, 0, 2, 3))\n",
    "        return cropped_in_y\n",
    "\n",
    "    def crop_trenches_in_y(self, imported_hdf5_handle):\n",
    "        \"\"\"Master function for cropping the input hdf5 file in the y-dimension.\n",
    "\n",
    "        Args:\n",
    "            imported_hdf5_handle (hdf5file): File handle for the imported hdf5 file.\n",
    "\n",
    "        Returns:\n",
    "            hdf5file: File handle for the output y-cropped hdf5 file.\n",
    "        \"\"\"\n",
    "        y_percentiles_handle = self.chunk_t(\n",
    "            [imported_hdf5_handle[\"data\"][0]],\n",
    "            [2],\n",
    "            1,\n",
    "            self.get_y_percentile,\n",
    "            \"y_percentile\",\n",
    "            \"data\",\n",
    "        )\n",
    "        y_percentiles_smoothed_handle = self.chunk_t(\n",
    "            [y_percentiles_handle[\"data\"]],\n",
    "            [1],\n",
    "            1,\n",
    "            self.median_filter_2d,\n",
    "            \"y_percentile_smoothed\",\n",
    "            \"data\",\n",
    "        )\n",
    "        self.delete_hdf5(y_percentiles_handle)\n",
    "        all_cropped_in_y_handle = self.chunk_t(\n",
    "            [y_percentiles_smoothed_handle[\"data\"], imported_hdf5_handle[\"data\"]],\n",
    "            [1, 3],\n",
    "            4,\n",
    "            self.crop_y,\n",
    "            \"cropped_in_y\",\n",
    "            \"data\",\n",
    "        )\n",
    "        self.delete_hdf5(y_percentiles_smoothed_handle)\n",
    "        if (\n",
    "            len(all_cropped_in_y_handle[\"data\"].shape) != 5\n",
    "        ):  # this is a stopgap for more complex problems\n",
    "            return None\n",
    "        return all_cropped_in_y_handle\n",
    "\n",
    "    def get_midpoints_from_mask(self, mask):\n",
    "        \"\"\"Using a boolean x mask, computes the positions of trench midpoints.\n",
    "\n",
    "        Args:\n",
    "            mask (array): x boolean array, specifying where trenches are present.\n",
    "\n",
    "        Returns:\n",
    "            array: array of trench midpoint x positions.\n",
    "        \"\"\"\n",
    "        transitions = mask[:-1].astype(int) - mask[1:].astype(int)\n",
    "\n",
    "        trans_up = np.where((transitions == -1))[0]\n",
    "        trans_dn = np.where((transitions == 1))[0]\n",
    "\n",
    "        if len(np.where(trans_dn > trans_up[0])[0]) > 0:\n",
    "            first_dn = np.where(trans_dn > trans_up[0])[0][0]\n",
    "            trans_dn = trans_dn[first_dn:]\n",
    "            trans_up = trans_up[: len(trans_dn)]\n",
    "            midpoints = (trans_dn + trans_up) // 2\n",
    "        else:\n",
    "            midpoints = []\n",
    "        return midpoints\n",
    "\n",
    "    def get_midpoints(self, x_percentile):\n",
    "        \"\"\"Given an array of signal in x, determines the position of trench midpoints.\n",
    "\n",
    "        Args:\n",
    "            x_percentile (array): array of trench intensities in x.\n",
    "\n",
    "        Returns:\n",
    "            array: array of trench midpoint x positions.\n",
    "        \"\"\"\n",
    "        background_filtered = x_percentile - self.median_filter_1d(\n",
    "            x_percentile, self.background_kernel_x\n",
    "        )\n",
    "        smooth_filtered = self.median_filter_1d(\n",
    "            background_filtered, self.smoothing_kernel_x\n",
    "        )\n",
    "        background_threshold = self.find_background_threshold(\n",
    "            smooth_filtered,\n",
    "            self.x_threshold_bins,\n",
    "            self.x_threshold_distance_bin_ratio,\n",
    "            self.x_threshold_backup_percentile,\n",
    "        )\n",
    "        x_mask = smooth_filtered > background_threshold\n",
    "        midpoints = self.get_midpoints_from_mask(x_mask)\n",
    "        return midpoints\n",
    "\n",
    "    def get_all_midpoints(self, all_cropped_in_y_handle, top_bottom, channel):\n",
    "        \"\"\"Given a file handle for images cropped in y, finds all midpoints in time for a given\n",
    "        trench row (top/bottom) and channel.\n",
    "\n",
    "        Args:\n",
    "            all_cropped_in_y_handle (hdf5file): File handle for the output y-cropped hdf5 file.\n",
    "            top_bottom (int): Int specifying whether we are looking at the top (0) or bottom (1) trench.\n",
    "            channel (int): Int specifying which channel we are getting midpoints from (order specified by\n",
    "            self.all_channels).\n",
    "\n",
    "        Returns:\n",
    "            array: array of midpoints of shape (t_dim,x_dim)\n",
    "        \"\"\"\n",
    "        cropped_in_y = all_cropped_in_y_handle[\"data\"]\n",
    "        all_midpoints = []\n",
    "        x_percentile = np.percentile(\n",
    "            cropped_in_y[top_bottom, channel, :, :, 0], self.x_percentile, axis=0\n",
    "        )\n",
    "        midpoints = self.get_midpoints(x_percentile)\n",
    "        if len(midpoints) == 0:\n",
    "            return None\n",
    "        all_midpoints.append(midpoints)\n",
    "        for t in range(1, self.t_dim):\n",
    "            x_percentile = np.percentile(\n",
    "                cropped_in_y[top_bottom, channel, :, :, t], self.x_percentile, axis=0\n",
    "            )\n",
    "            midpoints = self.get_midpoints(x_percentile)\n",
    "            if len(midpoints) / (len(all_midpoints[-1]) + 1) < 0.5:\n",
    "                all_midpoints.append(all_midpoints[-1])\n",
    "            else:\n",
    "                all_midpoints.append(midpoints)\n",
    "        all_midpoints = np.array(all_midpoints)\n",
    "        return all_midpoints\n",
    "\n",
    "    def get_x_drift(self, all_midpoints):\n",
    "        \"\"\"Given an t by x array of midpoints, computed the average drift in x for every timepoint.\n",
    "\n",
    "        Args:\n",
    "            all_midpoints (array): array of midpoints of shape (t_dim,x_dim).\n",
    "\n",
    "        Returns:\n",
    "            array: t length array of x drift values over time.\n",
    "        \"\"\"\n",
    "        x_drift = []\n",
    "        for t in range(len(all_midpoints) - 1):\n",
    "            diff_mat = np.subtract.outer(all_midpoints[t + 1], all_midpoints[t])\n",
    "            min_dist_idx = np.argmin(abs(diff_mat), axis=0)\n",
    "            min_dists = diff_mat[min_dist_idx]\n",
    "            median_translation = int(np.median(min_dists))\n",
    "            x_drift.append(median_translation)\n",
    "        net_x_drift = np.append(np.array([0]), np.add.accumulate(x_drift))\n",
    "        return net_x_drift\n",
    "\n",
    "    def init_counting_arr(self):\n",
    "        \"\"\"Initializes a counting array of shape (x_dim,t_dim) which counts from 0 to\n",
    "        x_dim on axis 0 for all positions in axis 1.\n",
    "\n",
    "        Returns:\n",
    "            array: Counting array to be used for masking out trenches in x.\n",
    "        \"\"\"\n",
    "        ones_arr = np.ones(self.x_dim)\n",
    "        counting_arr = np.add.accumulate(np.ones(self.x_dim)).astype(int) - 1\n",
    "        counting_arr_repeated = np.repeat(\n",
    "            counting_arr[:, np.newaxis], self.t_chunk, axis=1\n",
    "        )\n",
    "        return counting_arr_repeated\n",
    "\n",
    "    def get_k_mask(self, in_bounds_list, counting_arr, k):\n",
    "        \"\"\"Generates a boolean trench mask of shape (x_dim,t_dim) for a given trench k, using\n",
    "        the trench boundary values in in_bounds_list.\n",
    "\n",
    "        Args:\n",
    "            in_bounds_list (list): Singleton list containing the in_bounds array, which is a shape (2,t_dim,k_dim)\n",
    "            array specifying the start and end bounds in x of a given trench k over time.\n",
    "            counting_arr (array): Counting array to be used for masking out trenches in x.\n",
    "            k (int): Int specifying the trench to generate a mask for.\n",
    "\n",
    "        Returns:\n",
    "            array: Boolean trench mask of shape (x_dim,t_dim) for a given trench k.\n",
    "        \"\"\"\n",
    "        in_bounds = in_bounds_list[0]\n",
    "        working_t_dim = in_bounds.shape[1]\n",
    "        cropped_counting_arr = counting_arr[:, :working_t_dim]\n",
    "        k_mask = np.logical_and(\n",
    "            cropped_counting_arr > in_bounds[0, :, k],\n",
    "            cropped_counting_arr < in_bounds[1, :, k],\n",
    "        ).T\n",
    "        return k_mask\n",
    "\n",
    "    def apply_kymo_mask(self, array_list, top_bottom, channel):\n",
    "        \"\"\"Given a y-cropped image and a boolean trench mask of shape (x_dim,t_dim), masks that image in\n",
    "        xt to generate an output kymograph of shape (y_dim,x_dim,t_dim).\n",
    "\n",
    "        Args:\n",
    "            array_list (list): List containing two arrays: 1) An hdf5 array of a y-cropped image\n",
    "            2) A boolean trench mask of shape (x_dim,t_dim) for a given trench k.\n",
    "            top_bottom (int): Int specifying whether we are looking at the top (0) or bottom (1) trench.\n",
    "            channel (int): Int specifying which channel we are getting midpoints from (order specified by\n",
    "            self.all_channels).\n",
    "\n",
    "        Returns:\n",
    "            array: Kymograph array of shape (y_dim,x_dim,t_dim).\n",
    "        \"\"\"\n",
    "        img_arr, mask_arr = tuple(array_list)\n",
    "        working_img_arr = img_arr[top_bottom, channel]\n",
    "        reshaped_arr = np.swapaxes(working_img_arr, 1, 2)\n",
    "        masked_arr = reshaped_arr[:, mask_arr.astype(bool)]\n",
    "        reshaped_masked_arr = masked_arr.reshape(\n",
    "            reshaped_arr.shape[0], reshaped_arr.shape[1], -1\n",
    "        )\n",
    "        swapped_masked_arr = np.swapaxes(reshaped_masked_arr, 1, 2)\n",
    "        return swapped_masked_arr\n",
    "\n",
    "    def crop_in_x(\n",
    "        self, all_cropped_in_y_handle, top_bottom, top_bottom_i, all_midpoints, x_drift\n",
    "    ):\n",
    "        \"\"\"Given a y-cropped hdf5 file, detected midpoints and the estimated drift in x, generates\n",
    "        complete kymograph files for all trenches in the fov in every channel listed in 'self.all_channels'.\n",
    "        A single hdf5 file is written for the fov containing datasets of the format top_bottom/channel/trench_k.\n",
    "\n",
    "        Args:\n",
    "            all_cropped_in_y_handle (hdf5file): File handle for a y-cropped hdf5 file.\n",
    "            top_bottom (str): str specifying whether we are working with the top or bottom trench.\n",
    "            top_bottom_i (int): Int specifying whether we are looking at the top (0) or bottom (1) trench.\n",
    "            all_midpoints (array): array of midpoints of shape (t_dim,x_dim)\n",
    "            x_drift (array): t length array of x drift values over time.\n",
    "        \"\"\"\n",
    "        corrected_midpoints = x_drift[:, np.newaxis] + all_midpoints[0][np.newaxis, :]\n",
    "        midpoints_up, midpoints_dn = (\n",
    "            corrected_midpoints - self.trench_width_x // 2,\n",
    "            corrected_midpoints + self.trench_width_x // 2 + 1,\n",
    "        )\n",
    "        valid_mask = np.all(midpoints_up >= 0, axis=0) * np.all(\n",
    "            midpoints_dn <= self.x_dim, axis=0\n",
    "        )\n",
    "        in_bounds = np.array([midpoints_up[:, valid_mask], midpoints_dn[:, valid_mask]])\n",
    "        counting_arr = self.init_counting_arr()\n",
    "        k_mask_file_name = \"k_mask_\" + str(top_bottom)\n",
    "        for k in range(in_bounds.shape[2]):\n",
    "            k_mask_handle = self.chunk_t(\n",
    "                [in_bounds],\n",
    "                [1],\n",
    "                0,\n",
    "                self.get_k_mask,\n",
    "                k_mask_file_name,\n",
    "                str(k),\n",
    "                counting_arr,\n",
    "                k,\n",
    "            )\n",
    "            k_mask_handle.close()\n",
    "\n",
    "        k_mask_handle = h5py.File(self.temp_path + k_mask_file_name + \".hdf5\", \"r\")\n",
    "\n",
    "        for j, channel in enumerate(self.all_channels):\n",
    "            kymograph_handles = []\n",
    "            for k in range(in_bounds.shape[2]):\n",
    "                dataset_name = str(top_bottom) + \"/\" + str(channel) + \"/\" + str(k)\n",
    "                kymograph_handle = self.chunk_t(\n",
    "                    [all_cropped_in_y_handle[\"data\"], k_mask_handle[str(k)]],\n",
    "                    [4, 0],\n",
    "                    2,\n",
    "                    self.apply_kymo_mask,\n",
    "                    \"output\",\n",
    "                    dataset_name,\n",
    "                    top_bottom_i,\n",
    "                    j,\n",
    "                )\n",
    "                kymograph_handle.close()\n",
    "\n",
    "    def generate_kymograph(self):\n",
    "        \"\"\"Master function for generating kymographs for the target fov. A single hdf5 file is written for the fov\n",
    "        containing datasets of the format top_bottom/channel/trench_k. If nothing is written, this function\n",
    "        returns None.\n",
    "\n",
    "        \"\"\"\n",
    "        imported_hdf5_handle = self.import_hdf5(2, 3, \"imported_hdf5\", \"data\")\n",
    "        all_cropped_in_y_handle = self.crop_trenches_in_y(imported_hdf5_handle)\n",
    "        if all_cropped_in_y_handle is None:\n",
    "            return None\n",
    "\n",
    "        for i, top_bottom in enumerate(self.top_bottom_list):\n",
    "            all_midpoints = self.get_all_midpoints(all_cropped_in_y_handle, i, 0)\n",
    "            if all_midpoints is None:\n",
    "                continue\n",
    "            else:\n",
    "                x_drift = self.get_x_drift(all_midpoints)\n",
    "                self.crop_in_x(\n",
    "                    all_cropped_in_y_handle, top_bottom, i, all_midpoints, x_drift\n",
    "                )\n",
    "        output_file_path = self.temp_path + \"output.hdf5\"\n",
    "        os.rename(output_file_path, self.output_path)\n",
    "\n",
    "    def plot_kymograph(self, kymograph):\n",
    "        \"\"\"Helper function for plotting kymographs. Takes a kymograph array of shape (y_dim,x_dim,t_dim).\n",
    "\n",
    "        Args:\n",
    "            kymograph (array): kymograph array of shape (y_dim,x_dim,t_dim).\n",
    "        \"\"\"\n",
    "        list_in_t = [kymograph[:, :, t] for t in range(kymograph.shape[2])]\n",
    "        plt.imshow(np.concatenate(list_in_t, axis=1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/tiff_extraction/test_out_4/fov_70.hdf5\"\n",
    "temp_path = \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/tempfiles/\"\n",
    "output_path = (\n",
    "    \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/test_output.hdf5\"\n",
    ")\n",
    "kymo = kymograph(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    temp_path,\n",
    "    [\"channel_BF\", \"channel_RFP\"],\n",
    "    270,\n",
    "    20,\n",
    "    30,\n",
    "    t_chunk=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymographs = kymo.generate_kymograph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File(\n",
    "    \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/output/fov_1.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo.plot_kymograph(data[\"top/channel_RFP/20\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dask_controller:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_workers=6,\n",
    "        local=True,\n",
    "        queue=\"short\",\n",
    "        walltime=\"01:30:00\",\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        memory=\"6GB\",\n",
    "    ):\n",
    "        self.local = local\n",
    "        self.n_workers = n_workers\n",
    "        self.walltime = walltime\n",
    "        self.queue = queue\n",
    "        self.processes = processes\n",
    "        self.memory = memory\n",
    "        self.cores = cores\n",
    "\n",
    "    def writedir(self, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    def startdask(self):\n",
    "        if self.local:\n",
    "            self.daskclient = Client()\n",
    "            self.daskclient.cluster.scale(self.n_workers)\n",
    "        else:\n",
    "            # note the specifed walltime, don't use too much or too little, 01:30:00 is a good baseline,\n",
    "            # you just need enough time to finish 'gathering' to props_all before the jobs die\n",
    "            # you can always spin up more jobs later\n",
    "            # you will launch many jobs, so you don't need multiple processes, a lot of ram or multiple threads\n",
    "            self.daskcluster = SLURMCluster(\n",
    "                queue=self.queue,\n",
    "                walltime=self.walltime,\n",
    "                processes=self.processes,\n",
    "                memory=self.memory,\n",
    "                cores=self.cores,\n",
    "            )\n",
    "            self.workers = self.daskcluster.start_workers(self.n_workers)\n",
    "            self.daskclient = Client(self.daskcluster)\n",
    "\n",
    "    def printprogress(self):\n",
    "        complete = len([item for item in self.futures if item.status == \"finished\"])\n",
    "        print(str(complete) + \"/\" + str(len(self.futures)))\n",
    "\n",
    "    def writekymo(self, numfovs, all_channels, input_path, output_path, temp_path):\n",
    "        self.writedir(output_path)\n",
    "        fovs = list(range(numfovs))\n",
    "\n",
    "        def genkymo(\n",
    "            fovnum,\n",
    "            all_channels=all_channels,\n",
    "            input_path=input_path,\n",
    "            output_path=output_path,\n",
    "            temp_path=temp_path,\n",
    "        ):\n",
    "            full_input_path = input_path + \"/fov_\" + str(fovnum) + \".hdf5\"\n",
    "            full_output_path = output_path + \"/fov_\" + str(fovnum) + \".hdf5\"\n",
    "            full_temp_path = temp_path + \"/fov_\" + str(fovnum) + \"/\"\n",
    "            writedir(full_temp_path)\n",
    "            kymo = kymograph(\n",
    "                full_input_path,\n",
    "                full_output_path,\n",
    "                full_temp_path,\n",
    "                all_channels,\n",
    "                270,\n",
    "                20,\n",
    "                30,\n",
    "                t_chunk=25,\n",
    "            )\n",
    "            kymo.generate_kymograph()\n",
    "\n",
    "        self.futures = self.daskclient.map(genkymo, fovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = '/n/groups/paulsson/Daniel/Image_analysis_pipeline/tiff_extraction/test_out_4/fov_70.hdf5'\n",
    "# temp_path = '/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/tempfiles/'\n",
    "# output_path = '/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/test_output.hdf5'\n",
    "# kymo = kymograph(input_path,output_path,temp_path,[\"channel_BF\",\"channel_RFP\"],270,20,30,t_chunk=25)\n",
    "\n",
    "\n",
    "def writedir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "all_channels = [\"channel_BF\", \"channel_RFP\"]\n",
    "input_path = (\n",
    "    \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/tiff_extraction/test_out_4\"\n",
    ")\n",
    "output_path = \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/output\"\n",
    "temp_path = \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/tempfiles\"\n",
    "\n",
    "writedir(output_path)\n",
    "\n",
    "\n",
    "def genkymo(\n",
    "    fovnum,\n",
    "    all_channels=all_channels,\n",
    "    input_path=input_path,\n",
    "    output_path=output_path,\n",
    "    temp_path=temp_path,\n",
    "):\n",
    "    full_input_path = input_path + \"/fov_\" + str(fovnum) + \".hdf5\"\n",
    "    full_output_path = output_path + \"/fov_\" + str(fovnum) + \".hdf5\"\n",
    "    full_temp_path = temp_path + \"/fov_\" + str(fovnum) + \"/\"\n",
    "    writedir(full_temp_path)\n",
    "    kymo = kymograph(\n",
    "        full_input_path,\n",
    "        full_output_path,\n",
    "        full_temp_path,\n",
    "        all_channels,\n",
    "        270,\n",
    "        20,\n",
    "        30,\n",
    "        t_chunk=25,\n",
    "    )\n",
    "    kymo.generate_kymograph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief test and memory usage benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit genkymo(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genkymo(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory reqs are bad...wont scale well with t..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_controller = dask_controller(\n",
    "    walltime=\"00:30:00\", local=False, n_workers=30, memory=\"2GB\"\n",
    ")\n",
    "kymo_controller.startdask()\n",
    "kymo_controller.daskcluster.start_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = [\"channel_BF\", \"channel_RFP\"]\n",
    "input_path = (\n",
    "    \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/tiff_extraction/test_out_4\"\n",
    ")\n",
    "output_path = \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/output\"\n",
    "temp_path = \"/n/groups/paulsson/Daniel/Image_analysis_pipeline/kymographs/tempfiles\"\n",
    "kymo_controller.writekymo(\n",
    "    80, [\"channel_BF\", \"channel_RFP\"], input_path, output_path, temp_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_controller.printprogress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = kymo_controller.daskclient.gather(\n",
    "    writer1.futures\n",
    ")  # this will hang until all futures are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_controller.daskcluster.stop_workers(writer1.workers)  # this is still not working\n",
    "kymo_controller.daskcluster.stop_all_jobs()  # this seems to work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
