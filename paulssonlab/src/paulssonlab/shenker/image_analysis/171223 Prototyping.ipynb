{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "import datashader as ds\n",
    "from holoviews.operation.datashader import (\n",
    "    aggregate,\n",
    "    datashade,\n",
    "    dynspread,\n",
    "    shade,\n",
    "    regrid,\n",
    ")\n",
    "from holoviews.operation import decimate\n",
    "from holoviews.streams import Stream, param\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hex2color\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import nd2reader\n",
    "from numcodecs import Blosc, Delta\n",
    "import zarr\n",
    "import skimage\n",
    "import skimage.morphology\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# from sklearn import metrics\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import peakutils\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "from holoborodko_diff import holo_diff\n",
    "import functools\n",
    "from functools import partial\n",
    "from itertools import zip_longest\n",
    "from collections import Counter, defaultdict\n",
    "from bokeh.models import WheelZoomTool\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "\n",
    "# from bokeh.layouts import row\n",
    "# from bokeh.plotting import figure\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "hv.notebook_extension(\"bokeh\")\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_z = zarr.open_array(\"/home/jqs1/scratch/fidelity/test/171018.zarr\", mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = nd2reader.ND2Reader('/home/jqs1/scratch/fidelity/171018/20171018_TrxnError_ID.nd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_colors = {\n",
    "    \"BF\": \"#ffffff\",\n",
    "    \"MCHERRY\": \"#e22400\",\n",
    "    \"GFP\": \"#76ba40\",\n",
    "    \"CY5\": \"#e292fe\",\n",
    "    \"BFP\": \"#3a87fd\",\n",
    "    \"YFP\": \"#f5eb00\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=250\n",
    "\n",
    "channels = frames_z.attrs[\"metadata\"][\"channels\"]\n",
    "n_channels = len(channels)\n",
    "colors = [hex2color(channel_colors[channel]) for channel in channels]\n",
    "num_timepoints = len(frames_z.attrs[\"metadata\"][\"frames\"])\n",
    "num_fovs = len(frames_z.attrs[\"metadata\"][\"fields_of_view\"])\n",
    "\n",
    "channel_boxes = []\n",
    "channel_widgets = []\n",
    "for channel in channels:\n",
    "    solo_button = widgets.Button(description=\"S\", layout=widgets.Layout(width=\"10%\"))\n",
    "    enabled_button = widgets.ToggleButton(description=channel, value=True)\n",
    "    solo_button._button_to_enable = enabled_button\n",
    "    color_picker = widgets.ColorPicker(concise=True, value=channel_colors[channel])\n",
    "    channel_box = widgets.HBox([solo_button, enabled_button, color_picker])\n",
    "    channel_widgets.append([solo_button, enabled_button, color_picker, channel_box])\n",
    "solo_buttons, enabled_buttons, color_pickers, channel_boxes = zip(*channel_widgets)\n",
    "channels_box = widgets.VBox(channel_boxes)\n",
    "t_slider = widgets.IntSlider(\n",
    "    label=\"t\", min=0, max=num_timepoints, step=1, value=0, continuous_update=False\n",
    ")\n",
    "v_slider = widgets.IntSlider(\n",
    "    min=0, max=num_fovs, step=1, value=0, continuous_update=False\n",
    ")\n",
    "slider_box = widgets.VBox([v_slider, t_slider])\n",
    "control_box = widgets.HBox([channels_box, slider_box])\n",
    "output = widgets.Output()\n",
    "main_box = widgets.VBox([control_box, output])\n",
    "display(main_box)\n",
    "\n",
    "max_val = 2**14\n",
    "\n",
    "Frame = Stream.define(\"Frame\", t=0, v=0)\n",
    "frame = Frame()\n",
    "DisplaySettings = Stream.define(\n",
    "    \"DisplaySettings\", channel_enabled=np.array([True] * n_channels)\n",
    ")\n",
    "display_settings = DisplaySettings()\n",
    "\n",
    "\n",
    "def composite_image(t, v, channel_enabled):\n",
    "    # def composite_image(t, v):\n",
    "    # channel_enabled = [True] * n_channels\n",
    "    # channel_imgs = [frames.get_frame_2D(c=i, t=t, v=v) for i in range(n_channels)]\n",
    "    channel_imgs = [frames_z[v, c, t, :, :] for c in range(n_channels)]\n",
    "    scaled_imgs = [\n",
    "        channel_imgs[i][:, :, np.newaxis] / np.percentile(channel_imgs[i], 99.9)\n",
    "        for i in range(n_channels)\n",
    "    ]\n",
    "    for scaled_img in scaled_imgs:\n",
    "        np.clip(scaled_img, 0, 1, scaled_img)  # clip in place\n",
    "    colored_imgs = [scaled_imgs[i] * np.array(colors[i]) for i in range(n_channels)]\n",
    "    imgs_to_combine = [colored_imgs[i] for i in range(n_channels) if channel_enabled[i]]\n",
    "    if not len(imgs_to_combine):\n",
    "        imgs_to_combine = [np.ones(colored_imgs[0].shape)]  # white placeholder\n",
    "    img = imgs_to_combine[0]\n",
    "    for img2 in imgs_to_combine[1:]:\n",
    "        img = 1 - (1 - img) * (1 - img2)\n",
    "    return hv.RGB(img, bounds=(-1, -1, 1, 1))  # .opts(plot={'size': 250}, tools=[''])\n",
    "\n",
    "\n",
    "t_slider.observe(lambda change: frame.event(t=change[\"new\"]), names=\"value\")\n",
    "v_slider.observe(lambda change: frame.event(v=change[\"new\"]), names=\"value\")\n",
    "\n",
    "\n",
    "def update_enabled_channels(change):\n",
    "    channel_enabled = np.array([button.value for button in enabled_buttons])\n",
    "    display_settings.event(channel_enabled=channel_enabled)\n",
    "\n",
    "\n",
    "def update_solo(solo_button):\n",
    "    if (\n",
    "        solo_button._button_to_enable.value\n",
    "        and sum([b.value for b in enabled_buttons]) == 1\n",
    "    ):\n",
    "        for enabled_button in enabled_buttons:\n",
    "            enabled_button.value = True\n",
    "    else:\n",
    "        for enabled_button in enabled_buttons:\n",
    "            enabled_button.value = enabled_button == solo_button._button_to_enable\n",
    "    # update_enabled_channels(None)\n",
    "\n",
    "\n",
    "for solo_button in solo_buttons:\n",
    "    solo_button.on_click(update_solo)\n",
    "\n",
    "for enabled_button in enabled_buttons:\n",
    "    enabled_button.observe(update_enabled_channels, names=\"value\")\n",
    "# for color_picker in color_pickers:\n",
    "#    color_picker.observe(update_image, names='value')\n",
    "\n",
    "# hv.DynamicMap(composite_image, kdims=['t', 'v', 'channel_enabled']).select(t=0,v=0,channel_enabled=np.array([True,False,False,False,False]))\n",
    "image_viewer = hv.DynamicMap(composite_image, streams=[frame, display_settings])\n",
    "regrid(image_viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trench detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize_cluster_labels(X, fit):\n",
    "    mean = defaultdict(lambda: 0)\n",
    "    count = defaultdict(lambda: 0)\n",
    "    for i in range(len(fit.labels_)):\n",
    "        mean[fit.labels_[i]] += X[i]\n",
    "        count[fit.labels_[i]] += 1\n",
    "    for k, v in mean.items():\n",
    "        mean[k] = v / count[k]\n",
    "    label_mapping = dict(zip(mean.keys(), np.lexsort(list(mean.values()))))\n",
    "    for i, old_label in enumerate(fit.labels_):\n",
    "        fit.labels_[i] = label_mapping[old_label]\n",
    "\n",
    "\n",
    "def cluster_binary_image(bin_img):\n",
    "    X = np.array(np.where(bin_img)).T\n",
    "    X2 = StandardScaler().fit_transform(X.astype(np.float32))\n",
    "    fit = sklearn.cluster.MiniBatchKMeans(\n",
    "        init=\"k-means++\", n_clusters=2, n_init=10, max_no_improvement=10, verbose=0\n",
    "    )\n",
    "    fit.fit(X2)\n",
    "    _standardize_cluster_labels(X, fit)\n",
    "    return X, fit\n",
    "\n",
    "\n",
    "def label_binary_image(bin_img):\n",
    "    X, fit = cluster_binary_image(bin_img)\n",
    "    label_img = np.zeros_like(bin_img, dtype=np.int8)  # TODO: fixing dtype\n",
    "    for i in range(len(fit.labels_)):\n",
    "        label_img[X[i, 0], X[i, 1]] = fit.labels_[i] + 1\n",
    "    return label_img\n",
    "\n",
    "\n",
    "def drop_rare_labels(labels):\n",
    "    counter = Counter(labels)\n",
    "    total = sum(counter)\n",
    "    good_labels = []\n",
    "    for label, count in counter.iteritems():\n",
    "        print(count / total)\n",
    "        if count / total > 0.01:\n",
    "            good_labels.append(label)\n",
    "    return good_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_limits(img):\n",
    "    x_min = y_min = 0\n",
    "    x_max, y_max = img.shape\n",
    "    # TODO: what convention should we use, should max be inclusive??\n",
    "    # x_max = img.shape[0] - 1\n",
    "    # y_max = img.shape[1] - 1\n",
    "    x_lim = (x_min, x_max)\n",
    "    y_lim = (y_min, y_max)\n",
    "    return x_lim, y_lim\n",
    "\n",
    "\n",
    "def detect_rotation(bin_img):\n",
    "    h, theta, d = skimage.transform.hough_line(bin_img)\n",
    "    abs_diff_h = np.diff(h.astype(np.int32), axis=1).var(axis=0)\n",
    "    theta_idx = abs_diff_h.argmax()\n",
    "    angle1 = theta[theta_idx]\n",
    "    h2, theta2, d2 = skimage.transform.hough_line(\n",
    "        bin_img, theta=np.linspace(0.9 * angle1, 1.1 * angle1, 200)\n",
    "    )\n",
    "    abs_diff_h2 = np.diff(h2.astype(np.int32), axis=1).var(axis=0)\n",
    "    theta_idx2 = abs_diff_h2.argmax()\n",
    "    angle2 = theta2[theta_idx2]\n",
    "    d_profile = h2[:, theta_idx2].astype(np.int32)\n",
    "    freqs = np.abs(np.fft.fft(d_profile))\n",
    "    peak_idxs = peakutils.indexes(d_profile, thres=0.4, min_dist=5)\n",
    "    peaks = d2[peak_idxs]\n",
    "    spacing = scipy.stats.mode(np.diff(peaks)).mode[0]\n",
    "    return np.pi / 2 - angle2, peaks\n",
    "\n",
    "\n",
    "def get_rough_spacing(dists):\n",
    "    spacing = scipy.stats.mode(np.diff(dists).astype(int)).mode[0]\n",
    "    return spacing\n",
    "\n",
    "\n",
    "def point_linspace(anchor0, anchor1, num_points):\n",
    "    for s in np.linspace(0, 1, num_points)[1:-1]:\n",
    "        anchor = (1 - s) * anchor0 + s * anchor1\n",
    "        yield anchor\n",
    "\n",
    "\n",
    "def coords_along(x0, x1):\n",
    "    length = int(np.sqrt(np.sum((x1 - x0) ** 2)))\n",
    "    xs = np.linspace(x0[0], x1[0], length).astype(np.int_)[1:-1]\n",
    "    ys = np.linspace(x0[1], x1[1], length).astype(np.int_)[1:-1]\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def edge_point(x0, theta, x_lim, y_lim):\n",
    "    x_min, x_max = x_lim\n",
    "    y_min, y_max = y_lim\n",
    "    theta = theta % (2 * np.pi)\n",
    "    if 0 <= theta < np.pi / 2:\n",
    "        corner_x, corner_y = x_min, y_max\n",
    "    elif np.pi / 2 <= theta < np.pi:\n",
    "        corner_x, corner_y = x_max, y_max\n",
    "    elif np.pi <= theta < 3 / 2 * np.pi:\n",
    "        corner_x, corner_y = x_max, y_min\n",
    "    elif 3 / 2 * np.pi <= theta <= 2 * np.pi:\n",
    "        corner_x, corner_y = x_min, y_min\n",
    "    angle_to_corner = np.arctan2(corner_y - x0[1], x0[0] - corner_x) % (2 * np.pi)\n",
    "    if (\n",
    "        (theta >= angle_to_corner and 0 <= theta < np.pi / 2)\n",
    "        or (theta < angle_to_corner and np.pi / 2 <= theta < np.pi)\n",
    "        or (theta >= angle_to_corner and np.pi <= theta < 3 / 2 * np.pi)\n",
    "        or (theta < angle_to_corner and 3 / 2 * np.pi <= theta < 2 * np.pi)\n",
    "    ):\n",
    "        # top/bottom\n",
    "        x1 = np.array([x0[0] - (corner_y - x0[1]) / np.tan(theta), corner_y])\n",
    "    else:\n",
    "        # left/right\n",
    "        x1 = np.array([corner_x, x0[1] - (corner_x - x0[0]) * np.tan(theta)])\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_array(\n",
    "    anchors, theta, x_lim, y_lim, start=None, stop=None, bidirectional=False\n",
    "):\n",
    "    if bidirectional:\n",
    "        line_array1 = line_array(\n",
    "            anchors, theta, x_lim, y_lim, start=start, stop=stop, bidirectional=False\n",
    "        )\n",
    "        line_array2 = line_array(\n",
    "            anchors,\n",
    "            theta + np.pi,\n",
    "            x_lim,\n",
    "            y_lim,\n",
    "            start=start,\n",
    "            stop=stop,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        for (x0, x1), (y0, y1) in zip(line_array1, line_array2):\n",
    "            yield x0, x1, y1\n",
    "        return\n",
    "    if start is None:\n",
    "        start = 0\n",
    "    if stop is None:\n",
    "        stop = 0\n",
    "    if not stop >= start >= 0:\n",
    "        raise ValueError(\"need stop >= start >= 0\")\n",
    "    theta = theta % (2 * np.pi)\n",
    "    for anchor in anchors:\n",
    "        x0 = anchor\n",
    "        x1 = edge_point(x0, theta, x_lim, y_lim)\n",
    "        max_length = np.sqrt(((x1 - x0) ** 2).sum())\n",
    "        y0, y1 = x0, x1\n",
    "        if start:\n",
    "            y0 = min(start / max_length, 1) * (x1 - x0) + x0\n",
    "        if stop:\n",
    "            y1 = min(stop / max_length, 1) * (x1 - x0) + x0\n",
    "        if not np.array_equal(y0, y1):\n",
    "            yield y0, y1\n",
    "\n",
    "\n",
    "def get_anchors(theta, x_lim, y_lim):\n",
    "    x_min = np.array([x_lim[0], y_lim[0]])\n",
    "    x_max = np.array([x_lim[1], y_lim[1]])\n",
    "    x0 = x_min + (x_max - x_min) / 2\n",
    "    anchor0 = edge_point(x0, theta, x_lim, y_lim)\n",
    "    anchor1 = edge_point(x0, theta + np.pi, x_lim, y_lim)\n",
    "    return anchor0, anchor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_trench_region(bin_img, theta):\n",
    "    x_lim, y_lim = get_img_limits(bin_img)\n",
    "    anchor0, anchor1 = get_anchors(theta, x_lim, y_lim)\n",
    "    cross_sections = []\n",
    "    anchors = list(point_linspace(anchor0, anchor1, 40))[3:-3]  # TODO: parameterize\n",
    "    lines = list(\n",
    "        line_array(anchors, np.pi / 2 + theta, x_lim, y_lim, bidirectional=True)\n",
    "    )\n",
    "    for x0, x1, x2 in lines:\n",
    "        xs, ys = coords_along(x1, x2)\n",
    "        cross_sections.append(bin_img[ys, xs])\n",
    "    cross_section_vars = np.array([cs.var() for cs in cross_sections])\n",
    "    idx = cross_section_vars.argmax()\n",
    "    return anchors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://stackoverflow.com/questions/23815327/numpy-one-liner-for-combining-unequal-length-np-array-to-a-matrixor-2d-array\n",
    "def stack_jagged(arys, fill=0):\n",
    "    return np.array(list(zip_longest(*arys, fillvalue=fill))).T\n",
    "\n",
    "\n",
    "def detect_periodic_peaks(signal):\n",
    "    idxs = peakutils.indexes(signal, thres=0.2, min_dist=5)\n",
    "    xs = peakutils.interpolate(np.arange(len(signal)), signal, ind=idxs)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot()\n",
    "    plt.plot(np.arange(len(signal)), signal)\n",
    "    plt.scatter(xs, signal[xs.astype(np.int_)], c=\"r\")\n",
    "    dxs = np.diff(xs)\n",
    "    period_min = np.percentile(dxs, 10)\n",
    "    period_max = dxs.max()\n",
    "    num_periods = 100\n",
    "    periods = np.linspace(period_min, period_max, num_periods)\n",
    "    # std = ((dxs + periods[:,np.newaxis]/2) % periods[:,np.newaxis]).std(axis=1)\n",
    "    std = scipy.stats.iqr(((xs) % periods[:, np.newaxis]), axis=1) / periods\n",
    "    period_idx = std.argmin()\n",
    "    period = periods[period_idx]\n",
    "    plt.figure()\n",
    "    plt.plot(periods, std)\n",
    "    plt.scatter([period], [std[period_idx]], c=\"r\")\n",
    "    periods2 = np.linspace(period * 0.98, period * 1.02, num_periods)\n",
    "    std2 = scipy.stats.iqr(((xs) % periods2[:, np.newaxis]), axis=1) / periods2\n",
    "    period_idx2 = std2.argmin()\n",
    "    period2 = periods2[period_idx2]\n",
    "    plt.figure()\n",
    "    plt.plot(periods2, std2)\n",
    "    plt.scatter([period2], [std2[period_idx2]], c=\"r\")\n",
    "    offsets = np.linspace(0, period2, num_periods)\n",
    "    offset_idxs = (\n",
    "        np.arange(0, len(signal) - period2, period2) + offsets[:, np.newaxis]\n",
    "    ).astype(np.int_)\n",
    "    objective = signal[offset_idxs].sum(axis=1)\n",
    "    offset_idx = objective.argmax()\n",
    "    offset = offsets[offset_idx]\n",
    "    plt.figure()\n",
    "    plt.plot(offsets, objective)\n",
    "    plt.scatter([offset], [objective[offset_idx]], c=\"r\")\n",
    "    return period2, offset\n",
    "\n",
    "\n",
    "def detect_trench_anchors(img, t0, theta):\n",
    "    x_lim, y_lim = get_img_limits(img)\n",
    "    x1 = edge_point(t0, theta - np.pi / 2, x_lim, y_lim)\n",
    "    x2 = edge_point(t0, theta + np.pi / 2, x_lim, y_lim)\n",
    "    xs, ys = coords_along(x1, x2)\n",
    "    profile = img[ys, xs]\n",
    "    period, offset = detect_periodic_peaks(profile)\n",
    "    idxs = np.arange(offset, len(profile), period).astype(np.int_)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(profile)\n",
    "    plt.scatter(idxs, profile[idxs], c=\"r\")\n",
    "    return np.vstack((xs[idxs], ys[idxs])).T\n",
    "\n",
    "\n",
    "def _detect_trench_end(img, anchors, theta):\n",
    "    x_lim, y_lim = get_img_limits(img)\n",
    "    xss = []\n",
    "    yss = []\n",
    "    trench_profiles = []\n",
    "    for anchor in anchors:\n",
    "        x_end = edge_point(anchor, theta, x_lim, y_lim)\n",
    "        xs, ys = coords_along(anchor, x_end)\n",
    "        xss.append(xs)\n",
    "        yss.append(ys)\n",
    "        trench_profiles.append(img[ys, xs])\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for trench_profile in trench_profiles:\n",
    "        plt.plot(trench_profile)\n",
    "    stacked_profile = np.percentile(stack_jagged(trench_profiles), 80, axis=0)\n",
    "    # cum_profile = np.cumsum(stacked_profile)\n",
    "    # cum_profile /= cum_profile[-1]\n",
    "    # end = np.where(cum_profile > 0.8)[0][0]\n",
    "    stacked_profile_diff = holo_diff(1, stacked_profile)\n",
    "    end = stacked_profile_diff.argmin()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(stacked_profile)\n",
    "    plt.axvline(end, c=\"r\")\n",
    "    # ax2 = plt.gca().twinx()\n",
    "    # ax2.plot(stacked_profile_diff, color='g')\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(stacked_profile_diff, color=\"g\")\n",
    "    plt.axvline(end, c=\"r\")\n",
    "    end_points = []\n",
    "    for xs, ys in zip(xss, yss):\n",
    "        idx = end\n",
    "        if len(xs) <= end:\n",
    "            idx = -1\n",
    "        end_points.append((xs[idx], ys[idx]))\n",
    "    return np.array(end_points)\n",
    "\n",
    "\n",
    "def detect_trench_ends(img, bin_img, anchors, theta):\n",
    "    img_masked = np.where(\n",
    "        skimage.morphology.binary_dilation(bin_img), img, np.percentile(img, 5)\n",
    "    )\n",
    "    top_points = _detect_trench_end(img_masked, anchors, theta)\n",
    "    bottom_points = _detect_trench_end(img_masked, anchors, theta + np.pi)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(img_masked)\n",
    "    plt.scatter(*anchors.T, s=3, c=\"w\")\n",
    "    plt.scatter(*top_points.T, s=3, c=\"g\")\n",
    "    plt.scatter(*bottom_points.T, s=3, c=\"r\")\n",
    "    return top_points, bottom_points\n",
    "\n",
    "\n",
    "def detect_trenches(img, bin_img, theta):\n",
    "    t0 = detect_trench_region(bin_img, theta)\n",
    "    trench_anchors = detect_trench_anchors(img, t0, theta)\n",
    "    trench_points = detect_trench_ends(img, bin_img, trench_anchors, theta)\n",
    "    return trench_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_for_trenches(img_series):\n",
    "    img = img_series.max(axis=0)\n",
    "    # TODO: need rotation-invariant detrending\n",
    "    img = img - np.percentile(img, 3, axis=1)[:, np.newaxis]\n",
    "    img_thresh = img > skimage.filters.threshold_otsu(img)\n",
    "    img_labels = label_binary_image(img_thresh)\n",
    "    return img, img_labels\n",
    "\n",
    "\n",
    "def get_image_series_trenches(img_series, label):\n",
    "    img, img_labels = _label_for_trenches(img_series)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_labels)\n",
    "    theta, dists = detect_rotation(img_labels == label)\n",
    "    trench_points = detect_trenches(img, img_labels == label, theta)\n",
    "    return trench_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frame_series = frames_z[0, 0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_points = get_image_series_trenches(frame_series, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_eigenvalues(img):\n",
    "    I = skimage.filters.gaussian(img, 1.5)\n",
    "    I_x = skimage.filters.sobel_h(I)\n",
    "    I_y = skimage.filters.sobel_v(I)\n",
    "    I_xx = skimage.filters.sobel_h(I_x)\n",
    "    I_xy = skimage.filters.sobel_v(I_x)\n",
    "    I_yx = skimage.filters.sobel_h(I_y)\n",
    "    I_yy = skimage.filters.sobel_v(I_y)\n",
    "    kappa_1 = (I_xx + I_yy) / 2\n",
    "    kappa_2 = (np.sqrt((I_xx + I_yy) ** 2 - 4 * (I_xx * I_yy - I_xy * I_yx))) / 2\n",
    "    k1 = kappa_1 + kappa_2\n",
    "    k2 = kappa_1 - kappa_2\n",
    "    k1[np.isnan(k1)] = 0\n",
    "    k2[np.isnan(k2)] = 0\n",
    "    return k1, k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frame_series_k1 = np.zeros_like(frame_series, dtype=np.float32)  # TODO: fixed dtype\n",
    "for i in range(frame_series.shape[0]):\n",
    "    frame_series_k1[i, :, :] = hessian_eigenvalues(frame_series[i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kymograph(img_series, x0, x1):\n",
    "    num_timepoints = img_series.shape[0]\n",
    "    xs, ys = coords_along(x0, x1)\n",
    "    kymo = np.zeros((len(xs), num_timepoints))\n",
    "    for t in range(num_timepoints):\n",
    "        kymo[:, t] = img_series[t, ys, xs][::-1]\n",
    "    return kymo\n",
    "\n",
    "\n",
    "def get_image_series_segmentation(img_series, x0, x1):\n",
    "    pass  # list of trenches, for each trench, a list of cell masks\n",
    "\n",
    "\n",
    "def map_over_segmentation(img_series, cell_seg, func):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(points):\n",
    "    upper_left_x = min(point[0] for point in points)\n",
    "    upper_left_y = min(point[1] for point in points)\n",
    "    lower_right_x = max(point[0] for point in points)\n",
    "    lower_right_y = max(point[1] for point in points)\n",
    "    return (\n",
    "        np.array([upper_left_x, upper_left_y]),\n",
    "        np.array([lower_right_x, lower_right_y]),\n",
    "    )\n",
    "\n",
    "\n",
    "def crop_point(x, x_lim, y_lim):\n",
    "    return np.clip(x, *np.vstack([[x_lim], [y_lim]]).T)\n",
    "\n",
    "\n",
    "def get_trench_thumbnail(img, trench_points, trench_idx):\n",
    "    x_lim, y_lim = get_img_limits(img)\n",
    "    ul, lr = get_trench_bbox(trench_points, trench_idx, x_lim, y_lim)\n",
    "    return img[ul[1] : lr[1], ul[0] : lr[0]]\n",
    "\n",
    "\n",
    "def get_trench_bbox(trench_points, trench_idx, x_lim, y_lim):\n",
    "    # trench_points[0][trench_idx], trench_points[1][trench_idx]\n",
    "    num_trenches = min(len(trench_points[0]), len(trench_points[1]))\n",
    "    if not 0 <= trench_idx < num_trenches:\n",
    "        raise ValueError(\"trench index out of bounds\")\n",
    "    points = [trench_points[i][trench_idx] for i in (0, 1)]\n",
    "    if trench_idx == 0:\n",
    "        x0_prev = 2 * trench_points[0][0] - trench_points[0][1]\n",
    "        x1_prev = 2 * trench_points[1][0] - trench_points[1][1]\n",
    "        x0_prev = crop_point(x0_prev, x_lim, y_lim)\n",
    "        x1_prev = crop_point(x1_prev, x_lim, y_lim)\n",
    "        points += [x0_prev, x1_prev]\n",
    "    else:\n",
    "        points += [trench_points[i][trench_idx - 1] for i in (0, 1)]\n",
    "    if trench_idx + 1 == num_trenches:\n",
    "        x0_next = 2 * trench_points[0][-1] - trench_points[0][-2]\n",
    "        x1_next = 2 * trench_points[1][-1] - trench_points[1][-2]\n",
    "        x0_next = crop_point(x0_next, x_lim, y_lim)\n",
    "        x1_next = crop_point(x1_next, x_lim, y_lim)\n",
    "        points += [x0_next, x1_next]\n",
    "    else:\n",
    "        points += [trench_points[i][trench_idx + 1] for i in (0, 1)]\n",
    "    return bounding_box(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_sharpness(img):\n",
    "    # FROM: https://stackoverflow.com/questions/7765810/is-there-a-way-to-detect-if-an-image-is-blurry/7767755#7767755\n",
    "    img_blurred = skimage.filters.gaussian(img, 1)\n",
    "    img_lofg = skimage.filters.laplace(img_blurred)\n",
    "    return np.percentile(img_lofg, 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sharpness = np.array(\n",
    "    [image_sharpness(frame_series[i]) for i in range(frame_series.shape[0])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameStream = Stream.define(\"Frame\", t=0, v=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_browser(frames, frame_stream):\n",
    "    num_timepoints = len(frames_z.attrs[\"metadata\"][\"frames\"])\n",
    "    num_fovs = len(frames_z.attrs[\"metadata\"][\"fields_of_view\"])\n",
    "    t_slider = widgets.IntSlider(\n",
    "        label=\"t\", min=0, max=num_timepoints, step=1, value=0, continuous_update=False\n",
    "    )\n",
    "    v_slider = widgets.IntSlider(\n",
    "        min=0, max=num_fovs, step=1, value=0, continuous_update=False\n",
    "    )\n",
    "    slider_box = widgets.VBox([v_slider, t_slider])\n",
    "    t_slider.observe(lambda change: frame_stream.event(t=change[\"new\"]), names=\"value\")\n",
    "    v_slider.observe(lambda change: frame_stream.event(v=change[\"new\"]), names=\"value\")\n",
    "    return slider_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_viewer(frames, frame_stream):\n",
    "    channels = frames.attrs[\"metadata\"][\"channels\"]\n",
    "    n_channels = len(channels)\n",
    "    colors = [hex2color(channel_colors[channel]) for channel in channels]\n",
    "    channel_boxes = []\n",
    "    channel_widgets = []\n",
    "    for channel in channels:\n",
    "        solo_button = widgets.Button(\n",
    "            description=\"S\", layout=widgets.Layout(width=\"10%\")\n",
    "        )\n",
    "        enabled_button = widgets.ToggleButton(description=channel, value=True)\n",
    "        solo_button._button_to_enable = enabled_button\n",
    "        color_picker = widgets.ColorPicker(concise=True, value=channel_colors[channel])\n",
    "        channel_box = widgets.HBox([solo_button, enabled_button, color_picker])\n",
    "        channel_widgets.append([solo_button, enabled_button, color_picker, channel_box])\n",
    "    solo_buttons, enabled_buttons, color_pickers, channel_boxes = zip(*channel_widgets)\n",
    "    channels_box = widgets.VBox(channel_boxes)\n",
    "    max_val = 2**14  # TODO: infer\n",
    "    DisplaySettings = Stream.define(\n",
    "        \"DisplaySettings\", channel_enabled=np.array([True] * n_channels)\n",
    "    )\n",
    "    display_settings_stream = DisplaySettings()\n",
    "\n",
    "    def composite_image(t, v, channel_enabled):\n",
    "        channel_imgs = [frames[v, c, t, :, :] for c in range(n_channels)]\n",
    "        scaled_imgs = [\n",
    "            channel_imgs[i][:, :, np.newaxis] / np.percentile(channel_imgs[i], 99.9)\n",
    "            for i in range(n_channels)\n",
    "        ]\n",
    "        for scaled_img in scaled_imgs:\n",
    "            np.clip(scaled_img, 0, 1, scaled_img)  # clip in place\n",
    "        colored_imgs = [scaled_imgs[i] * np.array(colors[i]) for i in range(n_channels)]\n",
    "        imgs_to_combine = [\n",
    "            colored_imgs[i] for i in range(n_channels) if channel_enabled[i]\n",
    "        ]\n",
    "        if not len(imgs_to_combine):\n",
    "            imgs_to_combine = [np.ones(colored_imgs[0].shape)]  # white placeholder\n",
    "        img = imgs_to_combine[0]\n",
    "        for img2 in imgs_to_combine[1:]:\n",
    "            img = 1 - (1 - img) * (1 - img2)\n",
    "        return hv.RGB(\n",
    "            img, bounds=(-1, -1, 1, 1)\n",
    "        )  # .opts(plot={'size': 250}, tools=[''])\n",
    "\n",
    "    def update_enabled_channels(change):\n",
    "        channel_enabled = np.array([button.value for button in enabled_buttons])\n",
    "        display_settings_stream.event(channel_enabled=channel_enabled)\n",
    "\n",
    "    def update_solo(solo_button):\n",
    "        if (\n",
    "            solo_button._button_to_enable.value\n",
    "            and sum([b.value for b in enabled_buttons]) == 1\n",
    "        ):\n",
    "            for enabled_button in enabled_buttons:\n",
    "                enabled_button.value = True\n",
    "        else:\n",
    "            for enabled_button in enabled_buttons:\n",
    "                enabled_button.value = enabled_button == solo_button._button_to_enable\n",
    "        # update_enabled_channels(None)\n",
    "\n",
    "    for solo_button in solo_buttons:\n",
    "        solo_button.on_click(update_solo)\n",
    "\n",
    "    for enabled_button in enabled_buttons:\n",
    "        enabled_button.observe(update_enabled_channels, names=\"value\")\n",
    "    # for color_picker in color_pickers:\n",
    "    #    color_picker.observe(update_image, names='value')\n",
    "\n",
    "    # hv.DynamicMap(composite_image, kdims=['t', 'v', 'channel_enabled']).select(t=0,v=0,channel_enabled=np.array([True,False,False,False,False]))\n",
    "    image = hv.DynamicMap(\n",
    "        composite_image, streams=[frame_stream, display_settings_stream]\n",
    "    )\n",
    "    regridded_image = regrid(image)\n",
    "    return channels_box, regridded_image\n",
    "\n",
    "\n",
    "def big_image_viewer(frames):\n",
    "    frame_stream = FrameStream()\n",
    "    slider_box = frame_browser(frames, frame_stream)\n",
    "    channels_box, image = image_viewer(frames, frame_stream)\n",
    "    image = image.opts(plot={\"width\": 500, \"height\": 500})\n",
    "    output = widgets.Output()\n",
    "    box = widgets.VBox([widgets.HBox([channels_box, slider_box]), output])\n",
    "    display(box)\n",
    "    with output:\n",
    "        display(image)\n",
    "    return None  # box\n",
    "\n",
    "\n",
    "def mini_image_viewer(frames):\n",
    "    channels_box, image = image_viewer(frames)\n",
    "    output = widgets.Output()\n",
    "    box = widgets.VBox([channels_box, output])\n",
    "    display(box)\n",
    "    with output:\n",
    "        display(image)\n",
    "    return None  # box\n",
    "\n",
    "\n",
    "big_image_viewer(frames_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "%%output size=250\n",
    "mini_image_viewer(frames_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%output size=250\n",
    "mini_image_viewer(frames_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "%%opts Image (cmap='viridis')\n",
    "def scrubber_callback(t, y):\n",
    "    t = int(t)\n",
    "    v_line = hv.VLine(t + 0.5)(style={\"color\": \"white\", \"alpha\": 0.3, \"line_width\": 2})\n",
    "    t_label = hv.Text(t + kymo.shape[1] / 20, kymo.shape[0] / 20, \"t={:}\".format(t))(\n",
    "        style={\"color\": \"white\", \"text_alpha\": 0.8}\n",
    "    )\n",
    "    h_line = hv.HLine(y)(style={\"color\": \"white\", \"alpha\": 0.3, \"line_width\": 2})\n",
    "    return v_line * t_label * h_line\n",
    "\n",
    "\n",
    "def cross_section_callback(t, y):\n",
    "    y = int(y)\n",
    "    h_line = hv.HLine(y)(style={\"color\": \"white\", \"alpha\": 0.3, \"line_width\": 2})\n",
    "    return h_line\n",
    "\n",
    "\n",
    "def sharpness_scrubber_callback(t):\n",
    "    t = int(t)\n",
    "    v_line = hv.VLine(t + 0.5)(style={\"color\": \"black\", \"alpha\": 0.3, \"line_width\": 2})\n",
    "    return v_line\n",
    "\n",
    "\n",
    "def trench_thumbnail_callback(img_series, t):\n",
    "    t = int(t)\n",
    "    img = get_trench_thumbnail(img_series[t], trench_points, trench_idx)\n",
    "    # TODO: don't know why calling hv.Image.opts(plot={'invert_axes': True}) doesn't work\n",
    "    # also switched HLine to VLine in cross_section, above\n",
    "    thumb = hv.Image(\n",
    "        img[::-1], bounds=(0, 0, img.shape[1], img.shape[0]), kdims=[\"x2\", \"y\"]\n",
    "    )\n",
    "    # return thumb\n",
    "    # return thumb.opts(plot={'invert_axes': False, 'width': max_thumbnail_width})#, 'xaxis': None, 'yaxis': None})\n",
    "    return thumb.opts(\n",
    "        plot={\"invert_axes\": False, \"invert_yaxis\": True, \"width\": 200}\n",
    "    )  # , 'xaxis': None, 'yaxis': None})\n",
    "    # return thumb.opts(plot={'invert_axes': False, 'invert_yaxis': True})\n",
    "\n",
    "\n",
    "pointerx = hv.streams.PointerX(x=0).rename(x=\"t\")\n",
    "pointery = hv.streams.PointerY(y=0)\n",
    "\n",
    "# BOUNDS: (left, bottom, top, right)\n",
    "trench_idx = 15\n",
    "img_series = frame_series_k1\n",
    "bbox_ul, bbox_lr = get_trench_bbox(\n",
    "    trench_points, trench_idx, *get_img_limits(img_series[0])\n",
    ")\n",
    "kymo = extract_kymograph(\n",
    "    img_series, trench_points[0][trench_idx], trench_points[1][trench_idx]\n",
    ")\n",
    "kymo_img = hv.Image(kymo[::-1], bounds=(0, 0, kymo.shape[1], kymo.shape[0])).opts(\n",
    "    plot={\"width\": 700, \"height\": 300, \"invert_yaxis\": True}\n",
    ")\n",
    "# kymo_img = hv.Raster(kymo).opts(plot={'width': 700, 'height': 300, 'yaxis': 'left'})\n",
    "scrubber_line = hv.DynamicMap(scrubber_callback, streams=[pointerx, pointery])\n",
    "cross_section_line = hv.DynamicMap(cross_section_callback, streams=[pointerx, pointery])\n",
    "trench_thumbnail_img = hv.DynamicMap(\n",
    "    partial(trench_thumbnail_callback, img_series), streams=[pointerx]\n",
    ")  # .opts(plot={'invert_axes': True})\n",
    "sharpness_plot = hv.Curve(sharpness, kdims=[\"x\"], vdims=[\"s\"]).opts(\n",
    "    plot={\"width\": 700, \"height\": 100}\n",
    ")\n",
    "sharpness_scrubber = hv.DynamicMap(sharpness_scrubber_callback, streams=[pointerx])\n",
    "# pointery.source = kymo_img\n",
    "# SEE: http://holoviews.org/reference/elements/bokeh/Distribution.html\n",
    "# kymo_img * scrubber_line << (trench_thumbnail_img * cross_section_line) << (sharpness_plot * sharpness_scrubber)\n",
    "\n",
    "KymoOverlayStream = Stream.define(\"KymoOverlayStream\", overlay_enabled=True)\n",
    "kymo_overlay_stream = KymoOverlayStream()\n",
    "kymo_overlay_button = widgets.ToggleButton(description=\"Overlay\", value=True)\n",
    "\n",
    "\n",
    "def update_kymo_overlay(change):\n",
    "    kymo_overlay_stream.event(overlay_enabled=change[\"new\"])\n",
    "\n",
    "\n",
    "kymo_overlay_button.observe(update_kymo_overlay, names=\"value\")\n",
    "\n",
    "\n",
    "def get_thumbnail_overlay(t, overlay_enabled):\n",
    "    # because of https://github.com/ioam/holoviews/issues/1388, overlay_enabled must be True initially\n",
    "    x0 = trench_points[0][trench_idx]\n",
    "    x1 = trench_points[1][trench_idx]\n",
    "    trench_line = hv.Curve([x0 - bbox_ul, x1 - bbox_ul]).opts(\n",
    "        style={\"color\": \"white\", \"alpha\": 0.5, \"line_width\": 1.5},\n",
    "        plot={\"yaxis\": None, \"shared_axes\": True},\n",
    "    )\n",
    "    overlays = []\n",
    "    if overlay_enabled:\n",
    "        overlays.append(trench_line)\n",
    "    else:\n",
    "        pass  # overlays = []#line.opts(style={'alpha': 0.0, 'color': 'red'})\n",
    "    return hv.Overlay(overlays)\n",
    "\n",
    "\n",
    "thumbnail_overlay = hv.DynamicMap(\n",
    "    get_thumbnail_overlay, streams=[pointerx, kymo_overlay_stream]\n",
    ")  # .opts(plot={'invert_yaxis': True})\n",
    "\n",
    "display(kymo_overlay_button)\n",
    "(\n",
    "    kymo_img * scrubber_line\n",
    "    << (trench_thumbnail_img * thumbnail_overlay * cross_section_line)\n",
    "    << (sharpness_plot * sharpness_scrubber)\n",
    ")\n",
    "# kymo_img * scrubber_line << (trench_thumbnail_img * cross_section_line) << (sharpness_plot * sharpness_scrubber)\n",
    "# trench_thumbnail_img * thumbnail_overlay\n",
    "# trench_thumbnail_img * thumbnail_overlay * cross_section_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trench_peaks_callback(t, y):\n",
    "    t = int(t)\n",
    "    overlays = [hv.Curve(kymo[:, t]).opts(plot={\"width\": 500})]\n",
    "    overlays.extend(\n",
    "        [\n",
    "            hv.Points([(x, kymo[int(x), t])]).opts(style={\"size\": 6})\n",
    "            for x in kymo_endpoints[t]\n",
    "        ]\n",
    "    )\n",
    "    return hv.Overlay(overlays)\n",
    "\n",
    "\n",
    "hv.DynamicMap(trench_peaks_callback, streams=[pointerx, pointery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def find_kymograph_cell_endpoints(kymograph, thresh=0.2, min_dist=3):\n",
    "    endpoints = []\n",
    "    for t in range(kymograph.shape[1]):\n",
    "        idxs = peakutils.indexes(kymograph[:, t], thres=thresh, min_dist=min_dist)\n",
    "        xs = idxs\n",
    "        # xs = peakutils.interpolate(np.arange(kymograph.shape[0]), kymograph[:,t], ind=idxs)\n",
    "        endpoints.append(xs)\n",
    "    return endpoints\n",
    "\n",
    "\n",
    "kymo_endpoints = find_kymograph_cell_endpoints(kymo, thresh=0.2, min_dist=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(kymo.shape[1] // 5):\n",
    "    plt.plot(kymo[220:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%opts Image (cmap='viridis')\n",
    "img_series = frame_series\n",
    "kymo = extract_kymograph(\n",
    "    img_series, trench_points[0][trench_idx], trench_points[1][trench_idx]\n",
    ")\n",
    "kymo_img = hv.Image(kymo, bounds=(0, 0, kymo.shape[1], kymo.shape[0])).opts(\n",
    "    plot={\"width\": 700, \"height\": 300}\n",
    ")\n",
    "trench_thumbnail_img = hv.DynamicMap(\n",
    "    partial(trench_thumbnail, img_series), streams=[pointerx]\n",
    ")\n",
    "(\n",
    "    kymo_img.opts(plot={\"invert_axes\": True}) * scrubber_line\n",
    "    + trench_thumbnail_img * cross_section_line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle out-of-bounds/negative t/x values\n",
    "# focus quality score\n",
    "# show focus quality as a bar above kymograph\n",
    "# accurate horizontal line position on thumbnail\n",
    "# draw cross-section line through thumbnail\n",
    "# synchronize zoom/pan of multiple kymograph viewers\n",
    "# 3-up thumbnail viewer with crosshairs/endpoint correspondences (links) synchronized with kymograph viewer\n",
    "# clicking through a track advances 3-up by one frame\n",
    "# if you press END it finishes track and rewinds to the first unfinished track\n",
    "# automatic tracking: always identify bottom-most endpoint?\n",
    "# can we work out all other correspondences from this? (look at t=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
