{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import warnings\n",
    "from collections import defaultdict, namedtuple\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from operator import getitem\n",
    "\n",
    "import cachetools\n",
    "import dask\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import nd2reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import parambokeh\n",
    "import qgrid\n",
    "import scipy\n",
    "import skimage.morphology\n",
    "import streamz\n",
    "import zarr\n",
    "from bokeh.models.tools import HoverTool\n",
    "from cytoolz import *\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from holoviews.operation.datashader import regrid\n",
    "from holoviews.streams import Stream, param\n",
    "from tqdm import tnrange, tqdm, tqdm_notebook\n",
    "from traitlets import All\n",
    "\n",
    "IDX = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from processing import *\n",
    "# from trench_detection import *\n",
    "# from trench_segmentation import *\n",
    "# from trench_segmentation.watershed import *\n",
    "# from util import *\n",
    "# from ui import *\n",
    "import common\n",
    "import diagnostics\n",
    "import geometry\n",
    "import image\n",
    "import metadata\n",
    "import trench_detection\n",
    "import trench_detection.core\n",
    "import trench_detection.hough\n",
    "import trench_segmentation.watershed\n",
    "import ui\n",
    "import util\n",
    "import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "hv.extension(\"bokeh\")\n",
    "%matplotlib inline\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Restore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trench_points trench_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue=\"short\",\n",
    "    walltime=\"5:00:00\",\n",
    "    # job_extra=['-p transfer'],\n",
    "    # job_extra=['--cores-per-socket=8'],\n",
    "    # interface='ib0',\n",
    "    memory=\"16GB\",\n",
    "    local_directory=\"/tmp\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    "    # diagnostics_port=('127.0.0.1', 8787),\n",
    "    env_extra=['export PYTHONPATH=\"/home/jqs1/projects/matriarch\"'],\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster._widget().children[1].children[1].children[0].children[0].layout.width = \"200px\"\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.stop_workers(cluster.jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2']#, '/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "nd2_filenames = [\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2\",\n",
    "]\n",
    "# nd2_filenames = ['/home/jqs1/scratch/fidelity/180518_triplegrowthcurve/PHASE_GC001.nd2', '/home/jqs1/scratch/fidelity/180518_triplegrowthcurve/PHASE_GC002.nd2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames, metadata, parsed_metadata = workflow.get_nd2_frame_list(nd2_filenames)\n",
    "image_limits = workflow.get_filename_image_limits(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reload():\n",
    "    import trench_segmentation.watershed\n",
    "\n",
    "    # from importlib import reload\n",
    "    # import util, trench_detection, diagnostics, workflow, image\n",
    "    # reload(util)\n",
    "    # reload(trench_detection)\n",
    "    # reload(diagnostics)\n",
    "    # reload(workflow)\n",
    "    # reload(image)\n",
    "\n",
    "\n",
    "client.run(do_reload)\n",
    "do_reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Finding trenches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_process = all_frames.loc[IDX[:, :, [\"MCHERRY\"], :10], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Frame quality finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_psd2 = compose(image.radial_profile, image.psd2)\n",
    "frame_psd2s_futures = {\n",
    "    idx: client.submit(\n",
    "        radial_psd2, client.submit(workflow._get_nd2_frame, **idx._asdict())\n",
    "    )\n",
    "    for idx, row in util.iter_index(frames_to_process)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_psd2s = client.gather(frame_psd2s_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameStream = ui.DataframeStream.define(\n",
    "    \"FrameStream\", frames_to_process.index.to_frame(index=False)\n",
    ")\n",
    "frame_stream = FrameStream()\n",
    "\n",
    "box = ui.dataframe_browser(frame_stream)\n",
    "frame_stream.event()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Layout [shared_axes=False]\n",
    "dict_viewer(\n",
    "    frame_psd2s, frame_stream, wrapper=lambda k, v: hv.Curve(np.log(v))\n",
    ") + ui.image_viewer(frame_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Run trench finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally: get trench_points dict?? (how to organize? use dict proxy to index into it?)\n",
    "# where do I list all trenches, so that I can map over them?? e.g., compute per-timepoint focus\n",
    "# turn trench_points into df\n",
    "# locally: get diag df (by trench_set)\n",
    "# then dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trench_points_futures = {idx: client.submit(get_trenches,\n",
    "#                                            client.submit(workflow._get_nd2_frame, **idx._asdict())) for idx, row in util.iter_index(frames_to_process)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_trenches_diag = diagnostics.wrap_diagnostics(\n",
    "    trench_detection.hough.find_trenches, ignore_exceptions=True, pandas=True\n",
    ")\n",
    "trench_info_futures = {\n",
    "    idx: client.submit(\n",
    "        find_trenches_diag, client.submit(workflow.get_nd2_frame, **idx._asdict())\n",
    "    )\n",
    "    for idx, row in util.iter_index(frames_to_process)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(trench_info_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_info = util.apply_map_futures(\n",
    "    client.gather, trench_info_futures, predicate=lambda x: x.status == \"finished\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trench_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = {k: v[2] for k, v in trench_info.items() if v[2] is not None}\n",
    "errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_points, trench_diag, trench_err = workflow.unzip_trench_info(trench_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trench_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%store trench_points\n",
    "%store trench_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_angle = trench_diag[\"find_trench_lines.hough_2.angle\"].abs() > 2\n",
    "bad_angle.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pitch = (trench_diag[\"find_trench_lines.hough_2.pitch\"] - 24).abs() > 1\n",
    "bad_pitch.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = trench_diag[bad_pitch]  # trench_diag[bad_angle | bad_period]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_stream.event(_df=selected.index.to_frame(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_points_good = trench_points[~util.multi_join(trench_points.index, bad_pitch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(trench_points_good), len(trench_points_good) / len(trench_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trench_points_good_t0 = util.get_one(trench_points_good.groupby(\"t\"))[1]\n",
    "# trench_points_good_t0.index = trench_points_good_t0.index.droplevel('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_bbox_futures = []\n",
    "for _, trenches in trench_points_good_t0.groupby([\"filename\", \"position\"]):\n",
    "    trench_bbox_futures.append(\n",
    "        client.submit(workflow.get_trench_bboxes, trenches, image_limits)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_bbox_results = util.apply_map_futures(\n",
    "    client.gather, trench_bbox_futures, predicate=lambda x: x.status == \"finished\"\n",
    ")\n",
    "trench_bboxes = pd.concat(\n",
    "    [trench_points_good_t0, pd.concat(trench_bbox_results, axis=0)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%store trench_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trench_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Trench finding QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameStream = ui.DataframeStream.define(\n",
    "    \"FrameStream\", selected.index.to_frame(index=False)\n",
    ")\n",
    "frame_stream = FrameStream()\n",
    "\n",
    "box = ui.dataframe_browser(frame_stream)\n",
    "frame_stream.event()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.image_viewer(frame_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_frame_info(trench_diag, frame_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ui.show_grid(selected, stream=frame_stream)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frame = workflow.get_nd2_frame(**dict(frame_stream.get_param_values()))\n",
    "_, diag, _ = diagnostics.wrap_diagnostics(trench_detection.hough.find_trenches)(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_plot_browser(diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_trenches_segmarker = trench_bboxes.loc[IDX[:, :10, [\"MCHERRY\"], 0, :, :], :]\n",
    "selected_trenches_index = next(iter(selected_trenches_segmarker.groupby(\"t\")))[\n",
    "    1\n",
    "].index.droplevel(\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trench_bboxes) / len(selected_trenches_segmarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_trenches_reporter = selected_trenches_segmarker.rename(\n",
    "    index={\"MCHERRY\": \"YFP\"}, level=\"channel\", copy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_trenches_all = pd.concat(\n",
    "    [selected_trenches_segmarker, selected_trenches_reporter]\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "[all_frames_future] = client.scatter([all_frames], broadcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_stacks_futures = {}\n",
    "for frame_idx, trenches in util.iter_index(\n",
    "    selected_trenches_all.groupby([\"filename\", \"position\"])\n",
    "):\n",
    "    frame_stacks_futures[frame_idx] = client.submit(\n",
    "        workflow.get_trench_stacks, trenches, all_frames_future, image_limits\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(frame_stacks_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, frame_stacks_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_segment_trench(img_stack):\n",
    "    import trench_segmentation.watershed\n",
    "    from numcodecs import Blosc\n",
    "\n",
    "    seg = np.stack(\n",
    "        [trench_segmentation.watershed.segment_trench(img) for img in img_stack]\n",
    "    )\n",
    "    return zarr.array(\n",
    "        seg,\n",
    "        compressor=Blosc(cname=\"zstd\", clevel=5, shuffle=Blosc.NOSHUFFLE, blocksize=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_trenchwise(func, frame_stacks, trenches):\n",
    "#     results = {}\n",
    "#     for trench_idx, _ in util.iter_index(trenches):\n",
    "#         results[trench_idx] = func(frame_stacks[trench_idx])\n",
    "#     return results\n",
    "\n",
    "\n",
    "def map_trenchwise(func, frame_stacks, trenches, channels=None):\n",
    "    results = {}\n",
    "    for trench_idx, _ in util.iter_index(trenches):\n",
    "        if channels is None:\n",
    "            results[trench_idx] = func(frame_stacks[trench_idx])\n",
    "        else:\n",
    "            results[trench_idx] = func(\n",
    "                frame_stacks[trench_idx],\n",
    "                *[\n",
    "                    frame_stacks[trench_idx._replace(channel=channel)]\n",
    "                    for channel in channels\n",
    "                ],\n",
    "            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_segs_futures = {}\n",
    "for frame_idx, trenches in util.iter_index(\n",
    "    selected_trenches_index.to_series().groupby([\"filename\", \"position\"])\n",
    "):\n",
    "    frame_segs_futures[frame_idx] = client.submit(\n",
    "        partial(map_trenchwise, do_segment_trench),\n",
    "        frame_stacks_futures[frame_idx],\n",
    "        trenches,\n",
    "    )\n",
    "# frame_segs_futures = valmap(partial(client.submit, partial(valmap, do_segment_trench)), frame_stacks_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(frame_segs_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regionprops(seg_stack, reporter_stack):\n",
    "    import skimage.measure\n",
    "\n",
    "    results = []\n",
    "    for i in range(seg_stack.shape[0]):\n",
    "        results.append(skimage.measure.regionprops(seg_stack[i], reporter_stack[i]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "_frame_stacks = util.get_one(frame_stacks_futures).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "_frame_stacks2 = workflow.get_trench_stacks(\n",
    "    selected_trenches_all.xs(\n",
    "        (\"/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2\", 0), drop_level=False\n",
    "    ),\n",
    "    all_frames,\n",
    "    image_limits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key = util.get_one(_frame_stacks.keys())\n",
    "_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seg_masks = util.get_one(frame_segs_futures).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_one(_seg_masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rps = compute_regionprops(\n",
    "    _seg_masks[_key], _frame_stacks[_key]\n",
    ")  # , _frame_stacks[_key._replace(channel='YFP')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rp0 = _rps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rp0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial(map_trenchwise, compute_regionprops, channels=[\"YFP\"])(_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_regionprops_futures = {}\n",
    "for frame_idx, trenches in util.iter_index(\n",
    "    selected_trenches_index.to_series().groupby([\"filename\", \"position\"])\n",
    "):\n",
    "    frame_regionprops_futures[frame_idx] = client.submit(\n",
    "        partial(map_trenchwise, compute_regionprops, channels=[\"YFP\"]),\n",
    "        frame_stacks_futures[frame_idx],\n",
    "        trenches,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(frame_regionprops_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_regionprops = util.apply_map_futures(\n",
    "    client.gather, frame_regionprops_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_segs = util.apply_map_futures(\n",
    "    client.gather, frame_segs_futures, predicate=lambda x: x.status == \"finished\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysize.get_size(frame_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frame_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%store frame_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = util.get_one(util.get_one(frame_segs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysize.get_size(frame_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: old version\n",
    "# frame_segs_futures = valmap(partial(client.submit, partial(valmap, do_segment_trench)), frame_stacks_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED: how to add another channel to trench stacks\n",
    "# selected_trenches_yfp = selected_trenches.rename(index={'MCHERRY': 'YFP'}, level='channel', copy=False)\n",
    "# frame_stacks_yfp_futures = {}\n",
    "# for frame_idx, trenches in util.iter_index(selected_trenches_yfp.groupby(['filename', 'position'])):\n",
    "#     frame_stacks_yfp_futures[frame_idx] = client.submit(merge,\n",
    "#                                                        [frame_stacks_futures[frame_idx],\n",
    "#                                                         client.submit(workflow.get_trench_stacks,\n",
    "#                                                                       trenches,\n",
    "#                                                                       all_frames_future,\n",
    "#                                                                       image_limits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a dataframe of segmentation frames and a dataframe of readout frames, return regionprops objects (cheaper to serialize than dataframes?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(frame_segs_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(util.get_one(frame_segs_futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.array(\n",
    "    a, compressor=Blosc(cname=\"zstd\", clevel=5, shuffle=Blosc.NOSHUFFLE, blocksize=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame0_stacks = client.gather(util.get_one(trench_stack_futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_stack = util.get_one(frame0_stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.stack([trench_segmentation.watershed.segment_trench(img) for img in img_stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Blosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_bbox_results = util.apply_map_futures(\n",
    "    client.gather, trench_bbox_futures, predicate=lambda x: x.status == \"finished\"\n",
    ")\n",
    "trench_bboxes = pd.concat(\n",
    "    [trench_points_good, pd.concat(trench_bbox_results, axis=0)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_stacks = workflow.get_trench_stacks(\n",
    "    selected_trenches, all_frames, workflow.get_filename_image_limits(metadata)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "segs, seg_diags, seg_errs = zip(\n",
    "    *[\n",
    "        diagnostics.wrap_diagnostics(trench_segmentation.watershed.segment_trench)(img)\n",
    "        for img in img_stack\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_trenches(root_group[\"raw\"][str(pos)][1, 30], diagnostics=diag_pos[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = tree()\n",
    "_ = get_trenches(root_group[\"raw\"][str(pos)][0, 1], diagnostics=diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(img_stack):\n",
    "    ary = np.stack(\n",
    "        [\n",
    "            segment_trench(img_stack[t], diagnostics=None)\n",
    "            for t in range(img_stack.shape[0])\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    ary = zarr.array(ary, compressor=DEFAULT_FRAME_COMPRESSOR)\n",
    "    return ary\n",
    "\n",
    "\n",
    "trench_seg_masks = positionwise_trenchwise_map(\n",
    "    root_group[\"raw\"],\n",
    "    trench_points_pos,\n",
    "    f,\n",
    "    channel_slice=1,\n",
    "    preload=True,\n",
    "    time_slice=slice(None),\n",
    "    positions=range(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(img_stack):\n",
    "    return pd.Series(np.percentile(img_stack, 95, axis=(1, 2)))\n",
    "    # return pd.Series(np.max(img_stack, axis=(1,2)))\n",
    "\n",
    "\n",
    "trench_traces_all = positionwise_trenchwise_map(\n",
    "    root_group[\"raw\"],\n",
    "    trench_points_pos,\n",
    "    f,\n",
    "    channel_slice=2,\n",
    "    preload=True,\n",
    "    time_slice=slice(None),\n",
    "    positions=range(100),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
