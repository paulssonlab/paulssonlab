{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.feather as feather\n",
    "import zarr\n",
    "import dask\n",
    "from dask import delayed\n",
    "import distributed\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import streamz\n",
    "import streamz.dataframe as sdf\n",
    "import holoviews as hv\n",
    "from holoviews.streams import Stream, param, Selection1D\n",
    "from holoviews.operation.datashader import regrid\n",
    "from bokeh.models.tools import HoverTool, TapTool\n",
    "import matplotlib.pyplot as plt\n",
    "import qgrid\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tnrange, tqdm, tqdm_notebook\n",
    "import warnings\n",
    "from functools import partial\n",
    "from cytoolz import *\n",
    "from operator import getitem\n",
    "import nd2reader\n",
    "from importlib import reload\n",
    "import traceback\n",
    "import hvplot.pandas\n",
    "import param\n",
    "import parambokeh\n",
    "from traitlets import All\n",
    "import cachetools\n",
    "from collections import namedtuple, defaultdict\n",
    "from collections.abc import Mapping, Sequence\n",
    "from numbers import Number\n",
    "import skimage.morphology\n",
    "import scipy\n",
    "from glob import glob\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "IDX = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from processing import *\n",
    "# from trench_detection import *\n",
    "# from trench_segmentation import *\n",
    "# from trench_segmentation.watershed import *\n",
    "# from util import *\n",
    "# from ui import *\n",
    "import common, trench_detection, util\n",
    "import ui, diagnostics, metadata\n",
    "import workflow, image, geometry\n",
    "import trench_detection.hough, trench_detection.core\n",
    "import trench_segmentation.watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "hv.extension(\"bokeh\")\n",
    "%matplotlib inline\n",
    "tqdm.monitor_interval = 0\n",
    "asyncio.get_event_loop().set_debug(True)\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trench_points\n",
    "%store -r trench_diag\n",
    "%store -r trench_bboxes\n",
    "trench_bboxes_t0 = util.get_one(trench_bboxes.groupby(\"t\"))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrow copy with dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_arrow2(\n",
    "    in_filename, out_filename, length=None, batch_size=10000, process_func=None\n",
    "):\n",
    "    in_file = pa.memory_map(in_filename)\n",
    "    reader = pa.RecordBatchStreamReader(in_file)\n",
    "    out_file = pa.OSFile(out_filename, \"wb\")\n",
    "    table0 = pa.Table.from_batches([reader.read_next_batch()])\n",
    "    if process_func is not None:\n",
    "        table0 = process_func(table0)\n",
    "    writer = pa.RecordBatchStreamWriter(out_file, table0.schema)\n",
    "    writer.write_table(table0)\n",
    "    if length is not None:\n",
    "        reader = take(length, reader)\n",
    "    t0 = time.time()\n",
    "    for i, batches in enumerate(util.grouper(reader, batch_size)):\n",
    "        if True:  # i % 100 == 0:\n",
    "            t = time.time()\n",
    "            dt = t - t0\n",
    "            t0 = t\n",
    "            print(\"batch\", i, \"time {:.2f}\".format(dt))\n",
    "        table = pa.Table.from_batches(batches)  # .drop(columns_to_drop)\n",
    "        if process_func is not None:\n",
    "            table = process_func(table)\n",
    "        print(\"    rows per second\", len(table) / dt)\n",
    "        writer.write_table(table)\n",
    "\n",
    "\n",
    "def fix_filename(table):\n",
    "    for i in range(table.num_columns):\n",
    "        if table.column(i).name == \"filename\":\n",
    "            table = table.set_column(i, table.column(i).dictionary_encode())\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_arrow2(\n",
    "    \"/tmp/analysis_full_stream11_2.arrow\",\n",
    "    \"/tmp/analysis_full_stream11_2.cat.full.arrow\",\n",
    "    process_func=fix_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = pa.open_stream(\"/tmp/analysis_full_stream11_2.cat.arrow\").read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.schema.metadata[b\"pandas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t.replace_schema_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# tp = t2.to_pandas(zero_copy_only=True, categories=['filename'], use_threads=True)\n",
    "tp = t2.to_pandas(zero_copy_only=True, use_threads=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet with dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(source, columns=None, categories=None, length=None):\n",
    "    reader = pq.ParquetFile(source)\n",
    "    t0 = time.time()\n",
    "    tables = []\n",
    "    category_idxs = None\n",
    "    for i in range(length or reader.num_row_groups):\n",
    "        if True:  # i % 100 == 0:\n",
    "            t = time.time()\n",
    "            dt = t - t0\n",
    "            t0 = t\n",
    "            print(\n",
    "                \"batch {}/{}\".format(i, reader.num_row_groups), \"time {:.2f}\".format(dt)\n",
    "            )\n",
    "        table = reader.read_row_group(i, nthreads=4, columns=columns)\n",
    "        # TODO\n",
    "        # table = table.replace_schema_metadata()\n",
    "        if categories is not None:\n",
    "            if category_idxs is None:\n",
    "                category_idxs = [\n",
    "                    table.schema.get_field_index(column) for column in categories\n",
    "                ]\n",
    "            for idx in category_idxs:\n",
    "                table = table.set_column(idx, table.column(idx).dictionary_encode())\n",
    "            # category_table = pa.Table.from_arrays([table.column(idx).dictionary_encode() for idx in category_idxs])\n",
    "            # category_tables.append(category_table)\n",
    "            # table = table.drop(categories)\n",
    "        print(\"    rows per second\", len(table) / dt)\n",
    "        tables.append(table)\n",
    "    if categories is not None:\n",
    "        for idx in category_idxs:\n",
    "            category_columns = [table.column(idx) for table in tables]\n",
    "            new_category_columns = harmonize_dictionaries(category_columns)\n",
    "            for i, new_column in enumerate(new_category_columns):\n",
    "                tables[i] = tables[i].set_column(idx, new_column)\n",
    "    return pa.concat_tables(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['position', 'label', 'filename', 't', 'trench', 'trench_set', 'channel']\n",
    "# cols = [\"('YFP', 'labelwise', 'p0.9')\"]\n",
    "cols = [\n",
    "    \"filename\",\n",
    "    \"position\",\n",
    "    \"channel\",\n",
    "    \"t\",\n",
    "    \"trench_set\",\n",
    "    \"trench\",\n",
    "    \"label\",\n",
    "    \"('YFP', 'labelwise', 'p0.9')\",\n",
    "    \"('MCHERRY', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'regionprops', 'area')\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy_indexed as npi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imos = pa.BufferOutputStream()\n",
    "in_file = pa.OSFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\"\n",
    ")\n",
    "imos.upload(in_file)\n",
    "# tt = pq.read_pandas(imos.getvalue())#.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t4 = read_parquet(imos.getvalue(), columns=cols, categories=[\"filename\"], length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parquet_filename = '/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4'\n",
    "parquet_filename = (\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\"\n",
    ")\n",
    "# parquet_filename = '/tmp/analysis_full_stream11_2.parquet4'\n",
    "t3 = read_parquet(parquet_filename, columns=cols, categories=[\"filename\"], length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t3.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t3.replace_schema_metadata().to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun t3.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun t3.replace_schema_metadata().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = t3.replace_schema_metadata().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun k.set_index('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp = t3.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Categorical.from_codes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tc3[0].column(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.data.chunk(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.DictionaryArray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([c.to_pandas() for c in cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([c.to_pandas() for c in cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [tbl.column(0) for tbl in tc3]\n",
    "dicts = [c.data.chunk(0).dictionary for c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([0]) + set([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dictionary = list(set.union(*(set(d) for d in dicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_index = dict(map(reversed, enumerate(master_dictionary)))\n",
    "value_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = cols[0].data.chunk(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.DictionaryArray.from_arrays(ch.indices, pa.array([\"x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.ChunkedArray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.dictionary = [\"hoo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.indices.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_rule = {orig: value_to_index[value] for orig, value in enumerate(dicts[3])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = cols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.array([\"/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2\", \"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.array([\"x\", master_dictionary[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy_indexed as npi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npi.remap(q, [1, 2], [11, 22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_dictionaries(columns):\n",
    "    dictionaries = [\n",
    "        list(map(str, column.data.chunk(0).dictionary)) for column in columns\n",
    "    ]\n",
    "    master_dictionary = pa.array(set.union(*(set(d) for d in dictionaries)))\n",
    "    value_to_index = dict(map(reversed, enumerate(master_dictionary)))\n",
    "    new_columns = []\n",
    "    for column in columns:\n",
    "        column_name = column.name\n",
    "        rewrite_rules = {\n",
    "            orig: value_to_index[value]\n",
    "            for orig, value in enumerate(column.data.chunk(0).dictionary)\n",
    "        }\n",
    "        from_values = list(rewrite_rules.keys())\n",
    "        to_values = list(rewrite_rules.values())\n",
    "        new_chunks = []\n",
    "        for chunk_idx in range(column.data.num_chunks):\n",
    "            chunk = column.data.chunk(chunk_idx)\n",
    "            ary = chunk.indices.to_numpy()\n",
    "            npi.remap(ary, from_values, to_values, inplace=True)\n",
    "            new_chunk = pa.DictionaryArray.from_arrays(chunk.indices, master_dictionary)\n",
    "            new_chunks.append(new_chunk)\n",
    "        chunked_ary = pa.chunked_array(new_chunks)\n",
    "        new_column = pa.Column.from_array(column_name, chunked_ary)\n",
    "        new_columns.append(new_column)\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = harmonize_dictionaries(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = pa.concat_tables([pa.Table.from_arrays([c]) for c in z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tzp = tz.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(tp),\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.nofilename.parquet4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tt = pq.read_pandas(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.nofilename.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imos = pa.BufferOutputStream()\n",
    "in_file = pa.OSFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.nofilename.parquet4\"\n",
    ")\n",
    "imos.upload(in_file)\n",
    "tt = pq.read_pandas(imos.getvalue())  # .to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2 = tt.replace_schema_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp2 = tt2.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet redux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_to_dataset(\n",
    "    rg0, root_path=\"/tmp/parq_test\", partition_cols=[\"filename\", \"position\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(filename):\n",
    "    reader = pq.ParquetFile(filename)\n",
    "    table = pa.concat_tables(\n",
    "        [reader.read_row_group(i) for i in range(reader.num_row_groups)]\n",
    "    )\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pq.ParquetFile(\"/tmp/analysis_full_stream11_2.parquet4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg0 = reader.read_row_group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = rg0.schema.get_field_index(\"filename\")\n",
    "rg1 = rg0.set_column(idx, rg0.column(idx).dictionary_encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp = rg0.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = read_parquet(\"/tmp/analysis_full_stream11_2.parquet4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(rg1, \"/tmp/analysis_full_stream11_2.3cols.rg100000.rg1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg1roundtrip = pq.read_table(\n",
    "    \"/tmp/analysis_full_stream11_2.3cols.rg100000.rg1.parquet\",\n",
    "    use_pandas_metadata=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg1roundtrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = read_parquet(\"/tmp/analysis_full_stream11_2.3cols.rg10000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t.drop([\"position\", \"t\", \"trench_set\", \"trench\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp = t.to_pandas(zero_copy_only=True, use_threads=True, categories=[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(t3, \"/tmp/analysis_full_stream11_2.3cols.scienceonly.nomd.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_filename = (\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset1000.arrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"position\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = list(set(c.name for c in table0.columns) - set(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = table0.drop(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pa.open_stream(arrow_filename)\n",
    "table0 = pa.Table.from_batches([reader.read_next_batch()])\n",
    "imos = pa.BufferOutputStream()\n",
    "buffer = imos.getvalue()\n",
    "writer = pa.RecordBatchStreamWriter(buffer, table0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arrow(arrow_filename, columns, categorical_columns=None, batch_size=1000):\n",
    "    reader = pa.open_stream(arrow_filename)\n",
    "    table0 = pa.Table.from_batches([reader.read_next_batch()])\n",
    "    columns_to_drop = list(set(c.name for c in table0.columns) - set(columns))\n",
    "    table1 = table0.drop(columns_to_drop)\n",
    "    imos = pa.BufferOutputStream()\n",
    "    writer = pa.RecordBatchStreamWriter(imos, table1.schema)\n",
    "    t0 = time.time()\n",
    "    for i, batches in enumerate(util.grouper(reader, batch_size)):\n",
    "        if True:  # i % 100 == 0:\n",
    "            t = time.time()\n",
    "            dt = t - t0\n",
    "            t0 = t\n",
    "            print(\"batch\", i, \"time {:.2f}\".format(dt))\n",
    "        table = pa.Table.from_batches(batches).drop(columns_to_drop)\n",
    "        for i in range(table.num_columns):\n",
    "            if table.column(i).name == \"filename\":\n",
    "                table = table.set_column(i, table.column(i).dictionary_encode())\n",
    "        print(\"    rows per second\", len(table) / dt)\n",
    "        writer.write_table(table)\n",
    "    # with pq.ParquetWriter(parquet_filename, table0.schema) as writer:\n",
    "    #     writer.write_table(table0)\n",
    "    #    for batches in util.grouper(reader, batch_size):\n",
    "    #         table = pa.Table.from_batches(batches)\n",
    "    #         writer.write_table(table)\n",
    "    output_reader = pa.open_stream(imos.getvalue())\n",
    "    return output_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrow_filename = '/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset1000.arrow'\n",
    "# arrow_filename = '/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset15000.arrow'\n",
    "# arrow_filename = '/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset15000.arrow'\n",
    "arrow_filename = \"/tmp/analysis_full_stream11_2.arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cols = ['position', 'label', 'filename', 't', 'trench', 'trench_set', 'channel']\n",
    "# cols = [\"('YFP', 'labelwise', 'p0.9')\"]\n",
    "cols = [\n",
    "    \"filename\",\n",
    "    \"position\",\n",
    "    \"channel\",\n",
    "    \"t\",\n",
    "    \"trench_set\",\n",
    "    \"trench\",\n",
    "    \"label\",\n",
    "    \"('YFP', 'labelwise', 'p0.9')\",\n",
    "    \"('MCHERRY', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'regionprops', 'area')\",\n",
    "]\n",
    "b = read_arrow(arrow_filename, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time table = b.read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = table.set_column(3, table.column(3).dictionary_encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_file = pa.OSFile(\"/tmp/analysis_full_stream11_2.3cols.dict3.arrow\", \"wb\")\n",
    "writer = pa.RecordBatchStreamWriter(arrow_file, table3.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "writer.write_table(table3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3 = table2.drop([\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp = table.to_pandas(zero_copy_only=True, strings_to_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp.info()  # memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    table, \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    table,\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.rg1000.parquet\",\n",
    "    row_group_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    table,\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.rg1000000.parquet\",\n",
    "    row_group_size=1000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(table3, \"/tmp/analysis_full_stream11_2.3cols.nostr.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    table,\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.rg100000.parquet\",\n",
    "    row_group_size=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(tp),\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.roundtrip.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pt = pq.read_table(\"/tmp/analysis_full_stream11_2.3cols.nostr.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = [\n",
    "    \"('MCHERRY', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'regionprops', 'area')\",\n",
    "    \"position\",\n",
    "    \"t\",\n",
    "    \"trench_set\",\n",
    "    \"trench\",\n",
    "    \"label\",\n",
    "]\n",
    "ptf = fastparquet.ParquetFile(\n",
    "    \"/tmp/analysis_full_stream11_2.3cols.nostr.parquet\"\n",
    ").to_pandas(columns=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf.to_pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pt.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp = pq.read_pandas(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.rg100000.parquet\"\n",
    ")  # .to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_filename = '/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.rg100000.parquet'\n",
    "parquet_filename = \"/tmp/analysis_full_stream11_2.3cols.rg100000.parquet\"\n",
    "f = pq.ParquetFile(parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f.read_row_group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = f.read_row_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t0 = time.time()\n",
    "for i in range(f.num_row_groups):\n",
    "    row_group = f.read_row_group(i)\n",
    "    if i % 100 == 0:\n",
    "        t = time.time()\n",
    "        dt = t - t0\n",
    "        t0 = t\n",
    "        print(\"batch\", i, \"time {:.2f}\".format(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tpq = pa.concat_tables([f.read_row_group(i) for i in range(f.num_row_groups)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tpq.column(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpq2 = tpq.set_column(3, tpq.column(3).dictionary_encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    tpq2,\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.rg10000.dict.parquet\",\n",
    "    row_group_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c.dictionary_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tpqs = tpq.to_pandas(strings_to_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pa.array(list(\"1\" * 2**30))\n",
    "\n",
    "demo = \"demo.parquet\"\n",
    "\n",
    "\n",
    "def scenario():\n",
    "    t = pa.Table.from_arrays([x], [\"x\"])\n",
    "    writer = pq.ParquetWriter(demo, t.schema)\n",
    "    for i in range(2):\n",
    "        writer.write_table(t)\n",
    "    writer.close()\n",
    "\n",
    "    pf = pq.ParquetFile(demo)\n",
    "\n",
    "    # pyarrow.lib.ArrowIOError: Arrow error: Invalid: BinaryArray cannot contain more than 2147483646 bytes, have 2147483647\n",
    "    t2 = pf.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f.read().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.read_row_group(10000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fastparquet.write(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.3cols.fastparquet\",\n",
    "    tp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquetify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquetify(arrow_filename, parquet_filename=None, batch_size=1000):\n",
    "    if parquet_filename is None:\n",
    "        parquet_filename = arrow_filename.replace(\".arrow\", \".parquet\")\n",
    "    reader = pa.open_stream(arrow_filename)\n",
    "    table0 = pa.Table.from_batches([reader.read_next_batch()])\n",
    "    with pq.ParquetWriter(parquet_filename, table0.schema) as writer:\n",
    "        writer.write_table(table0)\n",
    "        for batches in util.grouper(reader, batch_size):\n",
    "            table = pa.Table.from_batches(batches)\n",
    "            writer.write_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.arrow\",\n",
    "    parquet_filename=\"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.test.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pq.ParquetFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.read_table(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.test.parquet\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify(\"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify(\"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "open(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.parquet\", \"rb\"\n",
    ").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.read_table(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.parquet\", nthreads=4\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Parquet from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = pa.BufferOutputStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = pa.OSFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.arrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "buf.upload(in_file, buffer_size=2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufr = pa.BufferReader(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufr.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = pa.open_stream(bufr).read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.MemoryMappedFile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file2 = pa.MemoryMappedFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.arrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file2 = pa.memory_map(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.arrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file2.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file2.read(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tt = pa.open_stream(in_file2).read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pq.read_table(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.parquet\", nthreads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    t,\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.test.parquet\",\n",
    "    compression=\"zstd\",\n",
    "    row_group_size=10000,\n",
    "    version=\"2.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.read_table(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.test.parquet\",\n",
    "    nthreads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun pq.read_table('/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.parquet', nthreads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun pq.ParquetFile(bufr).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.ParquetFile(bufr).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquetify2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_arrow(in_filename, out_filename, length=None, batch_size=1000):\n",
    "    in_file = pa.memory_map(in_filename)\n",
    "    reader = pa.RecordBatchStreamReader(in_file)\n",
    "    out_file = pa.OSFile(out_filename, \"wb\")\n",
    "    table0 = pa.Table.from_batches([reader.read_next_batch()])\n",
    "    writer = pa.RecordBatchStreamWriter(out_file, table0.schema)\n",
    "    writer.write_table(table0)\n",
    "    if length is not None:\n",
    "        reader = take(length, reader)\n",
    "    for batches in util.grouper(reader, batch_size):\n",
    "        table = pa.Table.from_batches(batches)\n",
    "        writer.write_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_arrow(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.arrow\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.arrow\",\n",
    "    batch_size=1000,\n",
    "    length=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_arrow(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.arrow\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.rebatched1000.arrow\",\n",
    "    batch_size=1000,\n",
    "    length=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_arrow(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.arrow\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.rebatched10000.arrow\",\n",
    "    batch_size=10000,\n",
    "    length=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_arrow(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.arrow\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.rebatched100.arrow\",\n",
    "    batch_size=100,\n",
    "    length=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify2(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.arrow\",\n",
    "    batch_size=1,\n",
    "    length=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify2(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.rebatched100.arrow\",\n",
    "    batch_size=1,\n",
    "    length=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify2(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.rebatched1000.arrow\",\n",
    "    batch_size=1,\n",
    "    length=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify2(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.rebatched10000.arrow\",\n",
    "    batch_size=1,\n",
    "    length=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = pa.OSFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset.arrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pa.RecordBatchStreamReader(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = reader.read_next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(take(10, reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_batches(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches2 = table.to_batches(chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = table.column(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.Table.from_batches([batches2[0]]).to_pandas().info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquetify2(arrow_filename, parquet_filename=None, batch_size=1000, length=None):\n",
    "    if parquet_filename is None:\n",
    "        parquet_filename = arrow_filename.replace(\".arrow\", \".parquet4\")\n",
    "    arrow_file = pa.OSFile(arrow_filename)\n",
    "    # arrow_file = pa.memory_map(arrow_filename)\n",
    "    # parquet_mmap = pa.memory_map(parquet_filename, 'wb')\n",
    "    reader = pa.open_stream(arrow_file)\n",
    "    table0 = pa.Table.from_batches([reader.read_next_batch()])\n",
    "    if length is not None:\n",
    "        reader = take(length, reader)\n",
    "    with pq.ParquetWriter(parquet_filename, table0.schema) as writer:\n",
    "        # with pq.ParquetWriter(parquet_mmap, table0.schema) as writer:\n",
    "        writer.write_table(table0)\n",
    "        for batches in util.grouper(reader, batch_size):\n",
    "            table = pa.Table.from_batches(batches)\n",
    "            writer.write_table(table)\n",
    "    # arrow_file.close()\n",
    "    # parquet_mmap.flush()\n",
    "    # parquet_mmap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_arrow(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.arrow\",\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset15000.arrow\",\n",
    "    length=15000,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parquetify2(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.arrow\",\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"('YFP', 'labelwise', 'p0.9')\",\n",
    "    \"('MCHERRY', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'regionprops', 'mean')\",\n",
    "    \"('YFP', 'regionprops', 'area')\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [\"('YFP', 'labelwise', 'p0.9')\"]\n",
    "# cols = ['position']\n",
    "# cols = [\"('YFP', 'labelwise', 'p0.9')\", 'filename', 'position']\n",
    "# cols = [\"('YFP', 'labelwise', 'p0.9')\", 'position', 'trench', 'label']\n",
    "cols = [\"('YFP', 'labelwise', 'p0.9')\", \"position\", \"trench\", \"label\", \"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# reader = pq.ParquetFile('/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4')\n",
    "reader = pq.ParquetFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.onecol.rg10000.parquet4\"\n",
    ")\n",
    "# for i in range(0,reader.num_row_groups):\n",
    "#     print('row group', i)\n",
    "#     reader.read_row_group(i, use_pandas_metadata=True)\n",
    "tables = [\n",
    "    reader.read_row_group(i, columns=None, nthreads=1, use_pandas_metadata=False)\n",
    "    for i in range(reader.num_row_groups)\n",
    "]\n",
    "table = pa.concat_tables(tables)\n",
    "# t = table.to_pandas(use_threads=True, strings_to_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf = fastparquet.ParquetFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(tf.iter_row_groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.row_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.read_row_group(0, [(\"YFP\", \"labelwise\", \"p0.9\")], {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf = fastparquet.ParquetFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\"\n",
    ")  # .to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = tf.to_pandas(columns=[\"position\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf = fastparquet.ParquetFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.onecol.rg10000.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reader = pq.ParquetFile(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\"\n",
    ")\n",
    "tables = [\n",
    "    reader.read_row_group(i, columns=cols, nthreads=4, use_pandas_metadata=False)\n",
    "    for i in range(reader.num_row_groups)\n",
    "]\n",
    "table = pa.concat_tables(tables)\n",
    "# t = table.to_pandas(use_threads=True, strings_to_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = table.to_pandas(use_threads=True, strings_to_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(t),\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.onecol.rg10000.parquet4\",\n",
    "    row_group_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsel = pq.read_pandas(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.onecol.rg10000.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t[(\"YFP\", \"labelwise\", \"p0.9\")].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t2 = t.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t2.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = pq.read_pandas(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\",\n",
    "    nthreads=0,\n",
    "    columns=cols,\n",
    ")  # .to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp = t.to_pandas(use_threads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pq.ParquetFile(\n",
    "    pa.OSFile(\"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.parquet4\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reader.read_row_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tt = (\n",
    "    pa.open_stream(\n",
    "        pa.memory_map(\n",
    "            \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset1000.arrow\"\n",
    "        )\n",
    "    )\n",
    "    .read_all()\n",
    "    .to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ttt = (\n",
    "    pa.open_stream(\n",
    "        pa.OSFile(\n",
    "            \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.subset1000.arrow\"\n",
    "        )\n",
    "    )\n",
    "    .read_all()\n",
    "    .to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tt.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.to_pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun parquetify2('/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.arrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "framewise_df = pa.open_stream(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.arrow\"\n",
    ").read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pa.open_stream(\"/n/scratch2/jqs1/fidelity/all/output/analysis50_stream_0.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in take(2, s):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.read_next_batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun pa.open_stream('/n/scratch2/jqs1/fidelity/all/output/analysis50_stream_0.arrow').read_all()#.read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun pa.open_stream('/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.arrow').read_all()#.read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trenchwise_df = pa.open_stream(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.arrow\"\n",
    ").read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchwise_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labelwise_df = pq.read_pandas(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis50_full_2.parquet\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelwise_df.index.names = ['filename', 'position', 't', 'trench_set', 'trench', 'label']\n",
    "# labelwise_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelwise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchwise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
