{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from functools import partial, reduce\n",
    "from pathlib import Path\n",
    "\n",
    "import dask\n",
    "import distributed\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import nd2reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import skimage.measure\n",
    "import zarr\n",
    "from dask import delayed\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IDX = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.image_analysis.new as new\n",
    "from paulssonlab.image_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext pyinstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue=\"short\",\n",
    "    walltime=\"06:00:00\",\n",
    "    memory=\"2.5GB\",\n",
    "    local_directory=\"/tmp\",\n",
    "    log_directory=\"/home/jqs1/log\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(maximum=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_correction_channel = \"Phase-Fluor\"\n",
    "segmentation_channel = \"RFP-PENTA\"\n",
    "trench_detection_channel = segmentation_channel  # channel for trench detection, almost always same as segmentation_channel\n",
    "measure_channels = [\"RFP-PENTA\", \"GFP-PENTA\"]\n",
    "fish_channels = [\"RFP-Penta\", \"Cy5-PENTA\", \"Cy7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, output_dir):\n",
    "        self.logger = logging.getLogger(\"Pipeline\")\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.state = {}\n",
    "        self.array = {}\n",
    "        self.table = {}\n",
    "\n",
    "    def delayed(self, func, *args, **kwargs):\n",
    "        # TODO:\n",
    "        # log exceptions\n",
    "        # log warnings (deduplicated, count instances)\n",
    "        # optionally retry with diag if func takes \"diagnostics\" argument\n",
    "        # log benchmarking/profiling? or collect stats, only log outliers (+ call arguments)\n",
    "        return dask.delayed(func, *args, **kwargs)\n",
    "\n",
    "\n",
    "def crop_trenches(img, trenches):\n",
    "    crops = {}\n",
    "    # TODO: the islice is just for testing (we only deal with three trenches for FOV), otherwise every dask task takes a long time\n",
    "    # for i, crop in it.islice(new.image.iter_crops(img, trenches), 3):\n",
    "    for i, crop in new.image.iter_crops(img, trenches):\n",
    "        crops[i] = crop\n",
    "    return crops\n",
    "\n",
    "\n",
    "def segment_trenches(crops):\n",
    "    masks = {}\n",
    "    for i, crop in crops.items():\n",
    "        try:\n",
    "            masks[i] = trench_segmentation.segment(crop)\n",
    "        except:\n",
    "            pass\n",
    "    return masks\n",
    "\n",
    "\n",
    "# TODO: this is really boilerplatey, also we want finer task granularity than doing a whole FOV at once\n",
    "def measure_crops(label_images, intensity_images):\n",
    "    keys = label_images.keys() & intensity_images.keys()\n",
    "    return {k: measure_crop(label_images[k], intensity_images[k]) for k in keys}\n",
    "\n",
    "\n",
    "def measure_crop(label_image, intensity_image):\n",
    "    return pd.DataFrame(\n",
    "        skimage.measure.regionprops_table(\n",
    "            label_image,\n",
    "            intensity_image,\n",
    "            properties=(\n",
    "                \"label\",\n",
    "                \"intensity_mean\",\n",
    "            ),\n",
    "        )\n",
    "    ).set_index(\"label\")\n",
    "\n",
    "\n",
    "def measure_mask_crops(label_images):\n",
    "    return {k: measure_mask_crop(v) for k, v in label_images.items()}\n",
    "\n",
    "\n",
    "def measure_mask_crop(label_image):\n",
    "    return pd.DataFrame(\n",
    "        skimage.measure.regionprops_table(\n",
    "            label_image,\n",
    "            properties=(\n",
    "                \"label\",\n",
    "                \"area\",\n",
    "                \"axis_major_length\",\n",
    "                \"axis_minor_length\",\n",
    "                \"orientation\",\n",
    "                \"centroid\",\n",
    "            ),\n",
    "        )\n",
    "    ).set_index(\"label\")\n",
    "\n",
    "\n",
    "# TODO: use a namedtuple (or typing.NamedTuple, or dataclass) for keys so that fields are named\n",
    "def handle_image(pipeline, msg):\n",
    "    image = msg[\"image\"]\n",
    "    metadata = msg[\"metadata\"]\n",
    "    fov_num = metadata[\"fov_num\"]\n",
    "    t = metadata[\"t\"]\n",
    "    channel = metadata[\"channel\"]\n",
    "    raw_key = (\"raw\", fov_num, t, channel)\n",
    "    # store raw image (in production, we won't do this, we will only store crops as we do below)\n",
    "    pipeline.array[raw_key] = image\n",
    "    # TODO: we need a way to store per-frame metadata and write it to disk\n",
    "    trenches_key = (\n",
    "        \"trenches\",\n",
    "        fov_num,\n",
    "    )\n",
    "    trenches = pipeline.table.get(trenches_key)\n",
    "    # check if we have done trench detection for this FOV\n",
    "    if trenches is None and channel == trench_detection_channel:\n",
    "        # if not, find trenches and save the resulting table\n",
    "        trenches = pipeline.delayed(new.image.find_trench_bboxes)(\n",
    "            image, peak_func=trench_detection.peaks.find_peaks\n",
    "        )\n",
    "        pipeline.table[trenches_key] = trenches\n",
    "    # this list keeps track of all the raw frames that need to be cropped\n",
    "    # frames for multiple channels will accumulate in this list until we get a frame for trench_detection_channel\n",
    "    # if we have already processed such a frame, then keys_to_crop will contain only the current frame (raw_key)\n",
    "    keys_to_crop = pipeline.state.setdefault((\"keys_to_crop\", fov_num), [])\n",
    "    keys_to_crop.append(raw_key)\n",
    "    # we only can do further processing if we have already detected trenches for this FOV\n",
    "    if trenches is not None:\n",
    "        for raw_to_crop in keys_to_crop:\n",
    "            crop_key = (\"crops\", *raw_to_crop[1:])\n",
    "            # save trench crops for every frame in keys_to_crop\n",
    "            pipeline.array[crop_key] = pipeline.delayed(crop_trenches)(\n",
    "                pipeline.array[raw_to_crop], trenches\n",
    "            )\n",
    "            segmentation_key = (\"segmentation\", fov_num, t, segmentation_channel)\n",
    "            segmentation = pipeline.array.get(segmentation_key)\n",
    "            if segmentation is not None:\n",
    "                if crop_key[-1] in measure_channels:\n",
    "                    # if we have segmentation masks for this frame, we can immediately segment only this frame\n",
    "                    keys_to_measure = [crop_key]\n",
    "                else:\n",
    "                    keys_to_measure = []\n",
    "            else:\n",
    "                # we don't have a segmentation mask yet, so we need to add to the keys_to_measure list\n",
    "                keys_to_measure = pipeline.state.setdefault(\n",
    "                    (\"keys_to_measure\", fov_num, t), []\n",
    "                )\n",
    "                if crop_key[-1] in measure_channels:\n",
    "                    # we want to measure this frame\n",
    "                    keys_to_measure.append(crop_key)\n",
    "                if crop_key[-1] == segmentation_channel:\n",
    "                    # if this frame's channel is the segmentation channel, run segmentation\n",
    "                    segmentation = pipeline.delayed(segment_trenches)(\n",
    "                        pipeline.array[crop_key]\n",
    "                    )\n",
    "                    pipeline.array[segmentation_key] = segmentation\n",
    "                    # once we have the segmentation mask, get measurements for the mask\n",
    "                    pipeline.table[\n",
    "                        (\n",
    "                            \"mask_measurements\",\n",
    "                            *crop_key[1:],\n",
    "                        )\n",
    "                    ] = pipeline.delayed(measure_mask_crops)(segmentation)\n",
    "            segmentation = pipeline.array.get(segmentation_key)\n",
    "            # if we now have the segmentation mask, try measuring all frames in the keys_to_measure list\n",
    "            if segmentation is not None:\n",
    "                for crop_to_measure in keys_to_measure:\n",
    "                    measurements_key = (\"measurements\", *crop_to_measure[1:])\n",
    "                    pipeline.table[measurements_key] = pipeline.delayed(measure_crops)(\n",
    "                        segmentation, pipeline.array[crop_to_measure]\n",
    "                    )\n",
    "                pipeline.state.pop((\"keys_to_measure\", fov_num, t), None)\n",
    "        pipeline.state.pop((\"keys_to_crop\", fov_num), None)\n",
    "\n",
    "\n",
    "def handle_fish_barcode(pipeline, msg):\n",
    "    pass  # TODO\n",
    "\n",
    "\n",
    "# we should pick a name that's better/more intuitive than handle_message\n",
    "def handle_message(pipeline, msg):\n",
    "    match msg:\n",
    "        case {\"type\": \"image\", **info}:\n",
    "            match info:\n",
    "                case {\"image_type\": \"fish_barcode\"}:\n",
    "                    handle_fish_barcode(pipeline, msg)\n",
    "                case other:\n",
    "                    handle_image(pipeline, msg)\n",
    "        case {\"type\": \"nd2_metadata\"}:\n",
    "            print(\"got metadata\")  # TODO\n",
    "        case {\"type\": \"event\", **info}:\n",
    "            print(\"event\", info)\n",
    "        case {\"type\": \"done\"}:\n",
    "            print(\"DONE\")\n",
    "        case _:\n",
    "            # this exception should be caught, we don't want malformed messages to crash the pipeline\n",
    "            raise ValueError(\"cannot handle message\", msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# filename = \"/home/jqs1/scratch/jqs1/microscopy/210511/RBS_ramp.nd2\"\n",
    "filename = \"/home/jqs1/scratch/jqs1/microscopy/220718/RBS_DEG_library_20x.nd2\"\n",
    "pipeline = Pipeline(\"/home/jqs1/scratch/jqs1/microscopy/220718/new_architecture/test1\")\n",
    "for msg in new.readers.send_nd2(\n",
    "    filename,\n",
    "    # slices=dict(v=[30], t=slice(40,None)),\n",
    "    slices=dict(v=[30], t=slice(None)),\n",
    "):\n",
    "    handle_message(pipeline, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "futures = util.apply_map_futures(client.compute, (pipeline.table, pipeline.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "table, array = client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save outputs to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = \"/home/jqs1/group/221108rbsdeglibrary_1.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(pickle_filename, \"wb\") as f:\n",
    "    pickle.dump((table, array), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs \"/home/jqs1/group/221108rbsdeglibrary_1.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load outputs from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_filename = \"/home/jqs1/group/221108rbsdeglibrary_1.pickle\"\n",
    "pickle_filename = \"/home/jqs1/group/221108rbsdeglibrary_1_table.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(pickle_filename, \"rb\") as f:\n",
    "    table, array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -y ibis-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_table(\n",
    "    table, prefix, flatten_column_names=False, truncate_column_names=False\n",
    "):\n",
    "    if not isinstance(prefix, tuple):\n",
    "        prefix = (prefix,)\n",
    "    keys = sorted([k for k in table.keys() if k[: len(prefix)] == prefix])\n",
    "    if not keys:\n",
    "        return None\n",
    "    df = pd.concat(\n",
    "        {k[len(prefix) :]: pd.concat(table[k], names=[\"roi\"]) for k in keys},\n",
    "        names=[\"fov\", \"t\", \"channel\"],\n",
    "    )\n",
    "    df = df.unstack(\"channel\")\n",
    "    if flatten_column_names and truncate_column_names:\n",
    "        raise ValueError(\n",
    "            \"flatten_column_names and truncate_column_names cannot both be True\"\n",
    "        )\n",
    "    if flatten_column_names:\n",
    "        # replace MultiIndex with Index of slash-separated names like \"GFP-PENTA/mean_intensity\"\n",
    "        df.columns = [\"/\".join(col[::-1]) for col in df.columns.values]\n",
    "        #df.columns = [re.sub(r\"^(\\w+)-[^/]*/intensity_mean\", r\"\\1\", col) for col in df.columns.values]\n",
    "    elif truncate_column_names:\n",
    "        # replace MultiIndex with Index of slash-separated names with only the last component,\n",
    "        # e.g., \"mean_intensity\" instead of (\"RFP-Penta\", \"mean_intensity\")\n",
    "        df.columns = [col[0] for col in df.columns.values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_crops(array, prefix, fov, channel):\n",
    "    keys = sorted(\n",
    "        [\n",
    "            k\n",
    "            for k in array.keys()\n",
    "            if len(k) == 4 and k[:2] == (prefix, fov) and k[3] == channel\n",
    "        ]\n",
    "    )\n",
    "    trenches = reduce(operator.and_, [array[k].keys() for k in keys])\n",
    "    crops = {}\n",
    "    for trench in list(trenches):\n",
    "        crops[trench] = np.stack([array[k][trench] for k in keys])\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack(ary):\n",
    "    return np.swapaxes(ary, 0, 1).reshape(ary.shape[1], -1)\n",
    "\n",
    "\n",
    "def pad_and_stack(arys, fill_value=0):\n",
    "    shape = np.max([ary.shape for ary in arys], axis=0)\n",
    "    return np.stack(\n",
    "        [\n",
    "            np.pad(\n",
    "                ary,\n",
    "                ((shape[0] - ary.shape[0], 0), (shape[1] - ary.shape[1], 0)),\n",
    "                constant_values=fill_value,\n",
    "            )\n",
    "            for ary in arys\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def pad_unstack(arys):\n",
    "    return unstack(pad_and_stack(arys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.streamz\n",
    "from streamz.dataframe import PeriodicDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "\n",
    "\n",
    "def poll_table(last, now, **kwargs):\n",
    "    counter = state.setdefault(\"counter\", 0) + 1\n",
    "    state[\"counter\"] = counter\n",
    "    table_subset = {k: v for k, v in table.items() if len(k) == 4 and k[2] == counter}\n",
    "    measurements = reformat_table(\n",
    "        table_subset, \"measurements\", flatten_column_names=True\n",
    "    )\n",
    "    mask_measurements = reformat_table(\n",
    "        table_subset, \"mask_measurements\", truncate_column_names=True\n",
    "    )\n",
    "    if measurements is not None and mask_measurements is not None:\n",
    "        all_measurements = pd.concat((measurements, mask_measurements), axis=1)\n",
    "        if state.get(\"df\") is not None:\n",
    "            state[\"df\"] = pd.concat((state[\"df\"], all_measurements))\n",
    "        else:\n",
    "            state[\"df\"] = all_measurements\n",
    "    # freq = kwargs.get(\"freq\", pd.Timedelta(\"50ms\"))\n",
    "    # index = pd.date_range(start=last + freq, end=now, freq=freq)\n",
    "    # return pd.DataFrame({'x': np.random.random(len(index))}, index=index)\n",
    "    return state[\"df\"]\n",
    "\n",
    "\n",
    "measurements_stream = PeriodicDataFrame(poll_table, interval=\"300ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_plots(data, singles, pairs):\n",
    "    #return pn.Column(pn.pane.HoloViews(hv.Layout([data.hvplot.kde(\"area\", yaxis=\"bare\"), data.hvplot.kde(\"axis_major_length\", yaxis=\"bare\")])))\n",
    "    # sel = link_selections.instance()\n",
    "    return pn.Column(\n",
    "        # hv.Layout(\n",
    "        #    [hv.Distribution(data, k).opts(height=250, width=200, yaxis=\"bare\") for idx, k in enumerate(singles)]\n",
    "        # ),\n",
    "        pn.pane.HoloViews(\n",
    "            #link_selections(\n",
    "                hv.Layout(\n",
    "                    [\n",
    "                        # hv.Distribution(data, k).opts(height=200, width=200, yaxis=\"bare\")\n",
    "                        #data.hvplot.kde(k, height=200, responsive=True, yaxis=\"bare\")\n",
    "                        data.hvplot.kde(k, height=200, width=200, yaxis=\"bare\", backlog=100)\n",
    "                        for idx, k in enumerate(singles)\n",
    "                    ]\n",
    "                ).cols(6),\n",
    "            #),\n",
    "            #sizing_mode=\"stretch_width\",\n",
    "        ),\n",
    "        #hv.Layout([hv.Scatter(data, *k) for k in pairs]),\n",
    "        pn.pane.HoloViews(\n",
    "            # hv.Layout([hv.Scatter(data, *k) for k in pairs]), sizing_mode=\"stretch_width\"\n",
    "            # link_selections(\n",
    "                hv.Layout(\n",
    "                    [data.hvplot.scatter(*k, height=300, width=300, hover=False, size=2, backlog=100) for k in pairs]\n",
    "                ),\n",
    "            # ),\n",
    "            sizing_mode=\"stretch_width\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "p = filter_plots(\n",
    "    measurements_stream,\n",
    "    [\n",
    "       \"RFP-Penta/intensity_mean\",\n",
    "       \"YFP-DUAL/intensity_mean\",\n",
    "       \"area\",\n",
    "       \"axis_minor_length\",\n",
    "       \"axis_major_length\",\n",
    "    ],\n",
    "    [\n",
    "       (\"RFP-Penta/intensity_mean\", \"YFP-DUAL/intensity_mean\"),\n",
    "       (\"axis_minor_length\", \"axis_major_length\"),\n",
    "       (\"area\", \"RFP-Penta/intensity_mean\"),\n",
    "    ],\n",
    ")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_stream[\"area\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_stream[\"area\"].hvplot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_stream.hvplot.scatter(\n",
    "    \"area\", \"axis_major_length\", backlog=100000\n",
    ").redim.range(area=(0, 300), axis_major_length=(0, 45)).opts(size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_stream.hvplot.bivariate(\"area\", \"axis_major_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_stream.hvplot.bivariate(\"area\", \"axis_major_length\").redim.range(\n",
    "    area=(0, 300), axis_major_length=(0, 45)\n",
    ").opts(filled=True, bandwidth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tabular visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "measurements = reformat_table(table, \"measurements\", flatten_column_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mask_measurements = reformat_table(\n",
    "    table, \"mask_measurements\", truncate_column_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measurements = pd.concat((measurements, mask_measurements), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measurements_subset = all_measurements[:1000]\n",
    "all_measurements_subset = all_measurements_subset[all_measurements_subset[\"RFP\"] < 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median+MAD (median absolute deviation) plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "measurements_subset = all_measurements.reset_index()[\n",
    "    [\n",
    "        \"t\",\n",
    "        \"RFP-Penta/intensity_mean\",\n",
    "        \"YFP-DUAL/intensity_mean\",\n",
    "        \"area\",\n",
    "        \"axis_major_length\",\n",
    "        \"axis_minor_length\",\n",
    "    ]\n",
    "]\n",
    "medians = measurements_subset.groupby([\"t\"]).agg(\n",
    "    [\"median\", astropy.stats.median_absolute_deviation]\n",
    ")\n",
    "\n",
    "\n",
    "def get_limits(x):\n",
    "    x = x.droplevel(0, axis=1)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"lower\": x[\"median\"] - x[\"median_absolute_deviation\"],\n",
    "            \"upper\": x[\"median\"] + x[\"median_absolute_deviation\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "limits = medians.groupby(level=0, axis=1).apply(get_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_mad(observable, medians, limits):\n",
    "    medians2 = medians[observable].reset_index()\n",
    "    limits2 = limits[observable].reset_index()\n",
    "    mean_plot = medians2.hvplot.line(\"t\", \"median\", logy=True)\n",
    "    noise_plot = limits2.hvplot.area(\n",
    "        x=\"t\", y=\"lower\", y2=\"upper\", stacked=False, alpha=0.2, logy=True\n",
    "    )\n",
    "    return (mean_plot * noise_plot).opts(width=800, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_mad(\"YFP-DUAL/intensity_mean\", medians, limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_mad(\"RFP-Penta/intensity_mean\", medians, limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    plot_median_mad(\"RFP-Penta/intensity_mean\", medians, limits)\n",
    "    * plot_median_mad(\"YFP-DUAL/intensity_mean\", medians, limits)\n",
    "    * plot_median_mad(\"area\", medians, limits)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "channel = \"YFP-DUAL/intensity_mean\"\n",
    "measurements_subset = all_measurements.reset_index()[\n",
    "    [\n",
    "        \"t\",\n",
    "        \"RFP-Penta/intensity_mean\",\n",
    "        \"YFP-DUAL/intensity_mean\",\n",
    "        \"area\",\n",
    "        \"axis_major_length\",\n",
    "        \"axis_minor_length\",\n",
    "    ]\n",
    "]\n",
    "bins = np.geomspace(\n",
    "    measurements_subset[channel].min(), measurements_subset[channel].max(), 100\n",
    ")\n",
    "heatmap = measurements_subset.groupby([\"t\"]).apply(\n",
    "    lambda x: pd.Series(np.histogram(x[channel], bins=bins)[0], index=bins[:-1])\n",
    ")\n",
    "heatmap.columns.name = channel\n",
    "heatmap = xr.DataArray(heatmap.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.hvplot.quadmesh(\n",
    "    cmap=\"blues\",\n",
    "    logy=True,\n",
    "    logz=True,\n",
    "    clim=(1, 1e4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Interactive selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.selection import link_selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for weirdness with responsive=True in holoviews/hvplot\n",
    "# SEE: https://github.com/holoviz/panel/issues/1394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_plots(data, singles, pairs):\n",
    "    # sel = link_selections.instance()\n",
    "    return pn.Column(\n",
    "        # hv.Layout(\n",
    "        #    [hv.Distribution(data, k).opts(height=250, width=200, yaxis=\"bare\") for idx, k in enumerate(singles)]\n",
    "        # ),\n",
    "        pn.pane.HoloViews(\n",
    "            #link_selections(\n",
    "                hv.Layout(\n",
    "                    [\n",
    "                        # hv.Distribution(data, k).opts(height=200, width=200, yaxis=\"bare\")\n",
    "                        data.hvplot.kde(k, height=200, responsive=True, yaxis=\"bare\")\n",
    "                        for idx, k in enumerate(singles)\n",
    "                    ]\n",
    "                ).cols(6),\n",
    "            #),\n",
    "            sizing_mode=\"stretch_width\",\n",
    "        ),\n",
    "        # hv.Layout([hv.Scatter(data, *k) for k in pairs]),\n",
    "        pn.pane.HoloViews(\n",
    "            # hv.Layout([hv.Scatter(data, *k) for k in pairs]), sizing_mode=\"stretch_width\"\n",
    "            link_selections(\n",
    "                hv.Layout(\n",
    "                    [data.hvplot.scatter(*k, height=300, width=300, hover=False) for k in pairs]\n",
    "                ),\n",
    "            ),\n",
    "            sizing_mode=\"stretch_width\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "p = filter_plots(\n",
    "    all_measurements_subset,\n",
    "    [\n",
    "        \"RFP-Penta/intensity_mean\",\n",
    "        \"YFP-DUAL/intensity_mean\",\n",
    "        \"area\",\n",
    "        \"axis_minor_length\",\n",
    "        \"axis_major_length\",\n",
    "    ],\n",
    "    [\n",
    "        (\"RFP-Penta/intensity_mean\", \"YFP-DUAL/intensity_mean\"),\n",
    "        (\"axis_minor_length\", \"axis_major_length\"),\n",
    "        (\"area\", \"RFP-Penta/intensity_mean\"),\n",
    "    ],\n",
    ")\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfp_stacks = stack_crops(array, \"crops\", 30, \"RFP-Penta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yfp_stacks = stack_crops(array, \"crops\", 30, \"YFP-DUAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kymographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rfp_stacks[200]\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(np.swapaxes(a, 0, 1).reshape(a.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = yfp_stacks[300]\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(np.swapaxes(a, 0, 1).reshape(a.shape[1], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-trenches viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(pad_unstack([d2[i][93] for i in range(330, 370)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
