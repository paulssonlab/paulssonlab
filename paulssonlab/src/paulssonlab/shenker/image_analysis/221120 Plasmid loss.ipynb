{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from functools import partial, reduce\n",
    "from pathlib import Path\n",
    "\n",
    "import dask\n",
    "import distributed\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import nd2reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import skimage.measure\n",
    "import zarr\n",
    "from dask import delayed\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IDX = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.image_analysis.new as new\n",
    "from paulssonlab.image_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pyinstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue=\"short\",\n",
    "    walltime=\"06:00:00\",\n",
    "    memory=\"2.5GB\",\n",
    "    local_directory=\"/tmp\",\n",
    "    log_directory=\"/home/jqs1/log\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(maximum=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_correction_channel = \"Phase-Fluor\"\n",
    "segmentation_channel = \"RFP-PENTA\"\n",
    "trench_detection_channel = segmentation_channel  # channel for trench detection, almost always same as segmentation_channel\n",
    "measure_channels = [\"RFP-PENTA\", \"GFP-PENTA\"]\n",
    "fish_channels = [\"RFP-Penta\", \"Cy5-PENTA\", \"Cy7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, output_dir):\n",
    "        self.logger = logging.getLogger(\"Pipeline\")\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.state = {}\n",
    "        self.array = {}\n",
    "        self.table = {}\n",
    "\n",
    "    def delayed(self, func, *args, **kwargs):\n",
    "        # TODO:\n",
    "        # log exceptions\n",
    "        # log warnings (deduplicated, count instances)\n",
    "        # optionally retry with diag if func takes \"diagnostics\" argument\n",
    "        # log benchmarking/profiling? or collect stats, only log outliers (+ call arguments)\n",
    "        return dask.delayed(func, *args, **kwargs)\n",
    "\n",
    "\n",
    "def crop_trenches(img, trenches):\n",
    "    crops = {}\n",
    "    # TODO: the islice is just for testing (we only deal with three trenches for FOV), otherwise every dask task takes a long time\n",
    "    # for i, crop in it.islice(new.image.iter_crops(img, trenches), 3):\n",
    "    for i, crop in new.image.iter_crops(img, trenches):\n",
    "        crops[i] = crop\n",
    "    return crops\n",
    "\n",
    "\n",
    "def segment_trenches(crops):\n",
    "    masks = {}\n",
    "    for i, crop in crops.items():\n",
    "        try:\n",
    "            masks[i] = trench_segmentation.segment(crop)\n",
    "        except:\n",
    "            pass\n",
    "    return masks\n",
    "\n",
    "\n",
    "# TODO: this is really boilerplatey, also we want finer task granularity than doing a whole FOV at once\n",
    "def measure_crops(label_images, intensity_images):\n",
    "    keys = label_images.keys() & intensity_images.keys()\n",
    "    return {k: measure_crop(label_images[k], intensity_images[k]) for k in keys}\n",
    "\n",
    "\n",
    "def measure_crop(label_image, intensity_image):\n",
    "    return pd.DataFrame(\n",
    "        skimage.measure.regionprops_table(\n",
    "            label_image,\n",
    "            intensity_image,\n",
    "            properties=(\n",
    "                \"label\",\n",
    "                \"intensity_mean\",\n",
    "            ),\n",
    "        )\n",
    "    ).set_index(\"label\")\n",
    "\n",
    "\n",
    "def measure_mask_crops(label_images):\n",
    "    return {k: measure_mask_crop(v) for k, v in label_images.items()}\n",
    "\n",
    "\n",
    "def measure_mask_crop(label_image):\n",
    "    return pd.DataFrame(\n",
    "        skimage.measure.regionprops_table(\n",
    "            label_image,\n",
    "            properties=(\n",
    "                \"label\",\n",
    "                \"area\",\n",
    "                \"axis_major_length\",\n",
    "                \"axis_minor_length\",\n",
    "                \"orientation\",\n",
    "                \"centroid\",\n",
    "            ),\n",
    "        )\n",
    "    ).set_index(\"label\")\n",
    "\n",
    "\n",
    "# TODO: use a namedtuple (or typing.NamedTuple, or dataclass) for keys so that fields are named\n",
    "def handle_image(pipeline, msg):\n",
    "    image = msg[\"image\"]\n",
    "    metadata = msg[\"metadata\"]\n",
    "    fov_num = metadata[\"fov_num\"]\n",
    "    t = metadata[\"t\"]\n",
    "    channel = metadata[\"channel\"]\n",
    "    raw_key = (\"raw\", fov_num, t, channel)\n",
    "    # store raw image (in production, we won't do this, we will only store crops as we do below)\n",
    "    pipeline.array[raw_key] = image\n",
    "    # TODO: we need a way to store per-frame metadata and write it to disk\n",
    "    trenches_key = (\n",
    "        \"trenches\",\n",
    "        fov_num,\n",
    "    )\n",
    "    trenches = pipeline.table.get(trenches_key)\n",
    "    # check if we have done trench detection for this FOV\n",
    "    if trenches is None and channel == trench_detection_channel:\n",
    "        # if not, find trenches and save the resulting table\n",
    "        trenches = pipeline.delayed(new.image.find_trench_bboxes)(\n",
    "            image, peak_func=trench_detection.peaks.find_peaks\n",
    "        )\n",
    "        pipeline.table[trenches_key] = trenches\n",
    "    # this list keeps track of all the raw frames that need to be cropped\n",
    "    # frames for multiple channels will accumulate in this list until we get a frame for trench_detection_channel\n",
    "    # if we have already processed such a frame, then keys_to_crop will contain only the current frame (raw_key)\n",
    "    keys_to_crop = pipeline.state.setdefault((\"keys_to_crop\", fov_num), [])\n",
    "    keys_to_crop.append(raw_key)\n",
    "    # we only can do further processing if we have already detected trenches for this FOV\n",
    "    if trenches is not None:\n",
    "        for raw_to_crop in keys_to_crop:\n",
    "            crop_key = (\"crops\", *raw_to_crop[1:])\n",
    "            # save trench crops for every frame in keys_to_crop\n",
    "            pipeline.array[crop_key] = pipeline.delayed(crop_trenches)(\n",
    "                pipeline.array[raw_to_crop], trenches\n",
    "            )\n",
    "            segmentation_key = (\"segmentation\", fov_num, t, segmentation_channel)\n",
    "            segmentation = pipeline.array.get(segmentation_key)\n",
    "            if segmentation is not None:\n",
    "                if crop_key[-1] in measure_channels:\n",
    "                    # if we have segmentation masks for this frame, we can immediately segment only this frame\n",
    "                    keys_to_measure = [crop_key]\n",
    "                else:\n",
    "                    keys_to_measure = []\n",
    "            else:\n",
    "                # we don't have a segmentation mask yet, so we need to add to the keys_to_measure list\n",
    "                keys_to_measure = pipeline.state.setdefault(\n",
    "                    (\"keys_to_measure\", fov_num, t), []\n",
    "                )\n",
    "                if crop_key[-1] in measure_channels:\n",
    "                    # we want to measure this frame\n",
    "                    keys_to_measure.append(crop_key)\n",
    "                if crop_key[-1] == segmentation_channel:\n",
    "                    # if this frame's channel is the segmentation channel, run segmentation\n",
    "                    segmentation = pipeline.delayed(segment_trenches)(\n",
    "                        pipeline.array[crop_key]\n",
    "                    )\n",
    "                    pipeline.array[segmentation_key] = segmentation\n",
    "                    # once we have the segmentation mask, get measurements for the mask\n",
    "                    pipeline.table[\n",
    "                        (\n",
    "                            \"mask_measurements\",\n",
    "                            *crop_key[1:],\n",
    "                        )\n",
    "                    ] = pipeline.delayed(measure_mask_crops)(segmentation)\n",
    "            segmentation = pipeline.array.get(segmentation_key)\n",
    "            # if we now have the segmentation mask, try measuring all frames in the keys_to_measure list\n",
    "            if segmentation is not None:\n",
    "                for crop_to_measure in keys_to_measure:\n",
    "                    measurements_key = (\"measurements\", *crop_to_measure[1:])\n",
    "                    pipeline.table[measurements_key] = pipeline.delayed(measure_crops)(\n",
    "                        segmentation, pipeline.array[crop_to_measure]\n",
    "                    )\n",
    "                pipeline.state.pop((\"keys_to_measure\", fov_num, t), None)\n",
    "        pipeline.state.pop((\"keys_to_crop\", fov_num), None)\n",
    "\n",
    "\n",
    "def handle_fish_barcode(pipeline, msg):\n",
    "    pass  # TODO\n",
    "\n",
    "\n",
    "# we should pick a name that's better/more intuitive than handle_message\n",
    "def handle_message(pipeline, msg):\n",
    "    match msg:\n",
    "        case {\"type\": \"image\", **info}:\n",
    "            match info:\n",
    "                case {\"image_type\": \"fish_barcode\"}:\n",
    "                    handle_fish_barcode(pipeline, msg)\n",
    "                case other:\n",
    "                    handle_image(pipeline, msg)\n",
    "        case {\"type\": \"nd2_metadata\"}:\n",
    "            print(\"got metadata\")  # TODO\n",
    "        case {\"type\": \"event\", **info}:\n",
    "            print(\"event\", info)\n",
    "        case {\"type\": \"done\"}:\n",
    "            print(\"DONE\")\n",
    "        case _:\n",
    "            # this exception should be caught, we don't want malformed messages to crash the pipeline\n",
    "            raise ValueError(\"cannot handle message\", msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename = \"/home/jqs1/scratch/jqs1/microscopy/210511/RBS_ramp.nd2\"\n",
    "pipeline = Pipeline(\"/home/jqs1/scratch/jqs1/microscopy/220718/new_architecture/test1\")\n",
    "for msg in new.readers.send_nd2(\n",
    "    filename,\n",
    "    slices=dict(v=[30], t=slice(40, None)),\n",
    "):\n",
    "    handle_message(pipeline, msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Drift correction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.registration import phase_cross_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd2_filename = \"/home/jqs1/scratch/jqs1/microscopy/210511/RBS_ramp.nd2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd2 = nd2reader.ND2Reader(nd2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd2.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov = 20\n",
    "delta_lag1_dask = {\n",
    "    nd2.metadata[\"channels\"][channel_idx]: dask.delayed(np.stack)(\n",
    "        [\n",
    "            dask.delayed(phase_cross_correlation)(\n",
    "                dask.delayed(new.readers.get_nd2_frame, pure=True)(\n",
    "                    nd2_filename, fov, channel_idx, t\n",
    "                ),\n",
    "                dask.delayed(new.readers.get_nd2_frame, pure=True)(\n",
    "                    nd2_filename, fov, channel_idx, t - 1\n",
    "                ),\n",
    "                return_error=False,\n",
    "                upsample_factor=16,\n",
    "                overlap_ratio=0.8,\n",
    "                normalization=None,\n",
    "            )\n",
    "            for t in range(1, nd2.sizes[\"t\"])\n",
    "        ]\n",
    "    )\n",
    "    for channel_idx in range(nd2.sizes[\"c\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_lag1 = client.compute(delta_lag1_dask, sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Curve(delta_lag1[\"Phase-Fluor\"][:, 0]) * hv.Curve(delta_lag1[\"RFP-PENTA\"][:, 0])\n",
    ").opts(height=200, responsive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Curve(delta_lag1[\"Phase-Fluor\"][:, 0]) * hv.Curve(delta_lag1[\"GFP-PENTA\"][:, 0])\n",
    ").opts(height=200, responsive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Curve(delta_lag1[\"GFP-PENTA\"][:, 0]) * hv.Curve(delta_lag1[\"RFP-PENTA\"][:, 0])\n",
    ").opts(height=200, responsive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_lag1[\"RFP-PENTA\"].argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c = 2\n",
    "\n",
    "\n",
    "def adjust(img):\n",
    "    return img\n",
    "    # soft_min, soft_max = np.percentile(img, (2, 98))\n",
    "    # return skimage.exposure.rescale_intensity(img, in_range=(soft_min, soft_max))\n",
    "\n",
    "\n",
    "f1 = adjust(nd2.get_frame_2D(v=30, t=170, c=c))\n",
    "f2 = adjust(nd2.get_frame_2D(v=30, t=171, c=c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale_intensity\n",
    "# find_trenches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust(img):\n",
    "    # return img\n",
    "    soft_min, soft_max = np.percentile(img, (2, 98))\n",
    "    return skimage.exposure.rescale_intensity(img, in_range=(soft_min, soft_max))\n",
    "    # return skimage.exposure.equalize_hist(img)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(adjust(f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "?phase_cross_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "phase_cross_correlation(f1, f2, upsample_factor=16, normalization=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "phase_cross_correlation(\n",
    "    f1, f2, upsample_factor=1, overlap_ratio=0.1, return_error=True, normalization=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diag = util.tree()\n",
    "trenches = new.image.find_trench_bboxes(\n",
    "    f1,\n",
    "    peak_func=trench_detection.peaks.find_peaks,\n",
    "    diagnostics=diag,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag[\"bboxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag[\"find_trenches\"][\"labeling\"][\"binarize_trench_image\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_plot_browser(diag[\"find_trenches\"][\"labeling\"][\"binarize_trench_image\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(f1 - f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(f1 / f1.max() - f2 / f2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(f2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
