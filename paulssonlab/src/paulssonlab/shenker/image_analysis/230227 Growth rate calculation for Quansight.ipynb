{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import dask\n",
    "import distributed\n",
    "import h5py\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import nd2reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import skimage.measure\n",
    "import zarr\n",
    "from dask import delayed\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IDX = pd.IndexSlice\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paulssonlab.image_analysis.new as new\n",
    "from paulssonlab.image_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pyinstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This renames columns with a slightly different convention than my previous `reformat_table` function (mask_measurement column names look like `centroid-0` whereas measurement column names look like `GFP-PENTA/mean_intensity`). It also joins `mask_measurements` together with `measurements`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rename_column(col):\n",
    "    if col[0] == \"mask_measurements\":\n",
    "        return col[1]\n",
    "    elif col[0] == \"measurements\":\n",
    "        return \"/\".join(col[1:])\n",
    "    else:\n",
    "        return \"/\".join(col)\n",
    "\n",
    "\n",
    "def reformat_table(table, flatten_column_names=False):\n",
    "    prefixes = sorted(set(k[0] for k in table.keys()))\n",
    "    df = pd.concat(\n",
    "        {\n",
    "            prefix: pd.concat(\n",
    "                {\n",
    "                    k[1:]: pd.concat(table[k], names=[\"roi\"])\n",
    "                    for k in table.keys()\n",
    "                    if k[0] == prefix\n",
    "                },\n",
    "                names=[\"fov\", \"t\", \"channel\"],\n",
    "            ).unstack(\"channel\")\n",
    "            for prefix in prefixes\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    if flatten_column_names:\n",
    "        # replace MultiIndex with Index of slash-separated names like \"GFP-PENTA/mean_intensity\"\n",
    "        df.columns = [_rename_column(col) for col in df.columns.values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix ROI orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to adjust the `label` index. This step depends on the `trenches` dataframe. We look up each `(fov, roi)` key in the `trenches` dataframe, see what `trench_set` the roi belongs to, and reverses the ordering of the labels for odd-numbered `trench_sets` (e.g., `labels=[1,2,3]` -> `labels=[3,2,1]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# labeling for odd trench_sets need to be inverted\n",
    "def fix_label_order(df):\n",
    "    if trenches[\"trench_set\"].loc[df.index[0][:2]] % 2 == 0:\n",
    "        return df\n",
    "    else:\n",
    "        df[\"label\"] = df[\"label\"].max() - df[\"label\"] + df[\"label\"].min()\n",
    "        return df.sort_values(\"label\")\n",
    "\n",
    "\n",
    "all_measurements_reordered = (\n",
    "    all_measurements.reset_index([\"label\"])\n",
    "    .groupby([\"fov\", \"roi\", \"t\"], group_keys=False)\n",
    "    .progress_transform(fix_label_order)\n",
    ").set_index(\"label\", append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play around with real data, you can load. This pickle dataset already has the above steps (`reformat_table` and `fix_label_order`) applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle_filename = \"/home/jqs1/group/221108rbsdeglibrary_1_table_reformatted2.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(pickle_filename, \"rb\") as f:\n",
    "    all_measurements, trenches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growth rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking associates segmentation mask labels at one timepoint `t` with labels at `t+1`. For each label at `t`, it corresponds one label at `t+1` (representing the same cell at a later time), with two labels at `t+1` (representing its two daughter cells arising from a cell division event), or it is marked as a cell that died/went out of frame. In production we use a more sophisticated linear-programming-based tracking algorithm; I am working on rewriting it and will integrate it with our codebase soon. For testing we will use this simple mock tracking algorithm that assigns `cell_id=1` to the mother cell (the cell with `label=1`) at `t=0`; every time the mother cell shrinks, it increments the mother cell `cell_id` by one. It assigns `cell_id=0` to all non-mother cells. `cell_id=0` is used as a sentinel for untracked cells and they are filtered out for downstream processing. The reason we needed `fix_label_order` above is so that all segmentation labeling is standardized so that the mother cell (at the dead end of each trench) for each `roi` and timepoint `t` gets `label=1`.\n",
    "\n",
    "The output of any tracking algorithm, including `track_mother_cell`, is to add a `cell_id` column to the measurements dataframe where each unique non-zero positive integer represents the same cell identity across time.\n",
    "\n",
    "**A note about uniqueness of keys:** ROI numbers are only unique within an fov (so rois are keyed by `(fov, roi)`), and cell_ids are only unique within an roi (so cell_ids are keyed by `(fov, roi, cell_id)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def track_mother_cell(df):\n",
    "    # groupby rois\n",
    "    mothers = df.xs(IDX[:, :, :, 1], drop_level=False)\n",
    "    lengths = mothers[\"axis_major_length\"].values\n",
    "    # first cell ID is 1, (0 is used as marker of non-tracked cell segment)\n",
    "    mother_cell_ids = np.concatenate(([1], 1 + np.cumsum(lengths[1:] < lengths[:-1])))\n",
    "    cell_ids = np.zeros(len(df), dtype=np.uint64)\n",
    "    cell_ids[df.index.get_locs(IDX[:, :, :, 1])] = mother_cell_ids\n",
    "    return df.assign(cell_id=cell_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_tracked = all_measurements.groupby([\"fov\", \"roi\"], group_keys=False).progress_apply(\n",
    "    track_mother_cell\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot cell lengths (`axis_major_length`) colored by `cell_id`. You can see that each time the cell shrinks, it means the cell has divided and so is assigned a new `cell_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tracked.xs(IDX[:, :, 3000, 1]).hvplot.scatter(\n",
    "    \"t\", \"axis_major_length\", by=\"cell_id\", cmap=\"Category20\", legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each colored sequence of points looks approximately linear. (Given that cells grow exponentially, we actually fit a line to `log(axis_major_length)`.) We thus fit a separate line segment (using ordinary least squares) to each `cell_id`, resulting in a dataframe with y-intercept (`alpha`), slope/growth rate parameter (`beta`), and fit quality (`r2`).\n",
    "\n",
    "In this particular case of growth rate estimation using OLS, this could almost certainly be sped up a lot by batching and vectorizing OLS fits and/or using `numpy.linalg.lstsq` (which calls LAPACK). I keep the slow implementation here because this is representative of the kind of custom computations we want to be able to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ols(x, y):\n",
    "    num_obs = len(x)\n",
    "    x_bar = x.sum() / num_obs\n",
    "    y_bar = y.sum() / num_obs\n",
    "    beta = (num_obs * (x * y).sum() - x.sum() * y.sum()) / (\n",
    "        num_obs * (x**2).sum() - x.sum()**2\n",
    "    )\n",
    "    alpha = y_bar - beta * x_bar\n",
    "    y_hat = alpha + beta * x\n",
    "    r2 = np.sum((y_hat - y_bar) ** 2) / np.sum((y - y_bar) ** 2)\n",
    "    return alpha, beta, r2\n",
    "\n",
    "\n",
    "def lineage_growth_rate(df, min_obs=3):\n",
    "    if len(df) < min_obs:\n",
    "        return\n",
    "    ts = df.index.get_level_values(\"t\").values\n",
    "    # TODO: not necessary, but makes alpha (y-intercept) comparable between lineages\n",
    "    ts -= ts.min()\n",
    "    log_length = np.log(df[\"axis_major_length\"].values)\n",
    "    new_df = pd.DataFrame(\n",
    "        np.repeat([ols(ts, log_length)], len(df), axis=0),\n",
    "        columns=[\"alpha\", \"beta\", \"r2\"],\n",
    "        index=df.index,\n",
    "    )\n",
    "    return new_df.assign(cell_id=df[\"cell_id\"]).set_index(\"cell_id\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_growth_rates = all_tracked[all_tracked[\"cell_id\"] != 0].groupby(\n",
    "    [\"fov\", \"roi\", \"cell_id\"], group_keys=False\n",
    ").progress_apply(lineage_growth_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that our $R^2$ values are close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_growth_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_growth_rates[\"r2\"].hvplot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "observable = \"beta\"\n",
    "num_bins = 100\n",
    "#measurements_subset = all_growth_rates[all_growth_rates[\"r2\"] > 0.9]\n",
    "measurements_subset = measurements_subset.reset_index()[[\"t\", \"alpha\", \"beta\", \"r2\"]]\n",
    "bins = np.linspace(\n",
    "    measurements_subset[observable].min(),\n",
    "    measurements_subset[observable].max(),\n",
    "    num_bins,\n",
    ")\n",
    "heatmap = measurements_subset.groupby([\"t\"]).apply(\n",
    "    lambda x: pd.Series(np.histogram(x[observable], bins=bins)[0], index=bins[:-1])\n",
    ")\n",
    "heatmap.columns.name = observable\n",
    "heatmap = xr.DataArray(heatmap.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the exact same 2D heatmap code you've seen before, we can plot. This is the kind of plot we want to see update in real-time as new timepoints roll in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap.hvplot.quadmesh(\n",
    "    cmap=\"blues\",\n",
    "    # logy=True,\n",
    "    logz=True,\n",
    "    # clim=(1, 1e4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
