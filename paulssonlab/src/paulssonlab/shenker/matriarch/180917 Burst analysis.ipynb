{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.feather as feather\n",
    "import zarr\n",
    "import dask\n",
    "from dask import delayed\n",
    "import distributed\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import streamz\n",
    "import streamz.dataframe as sdf\n",
    "import holoviews as hv\n",
    "from holoviews.streams import Stream, param, Selection1D\n",
    "from holoviews.operation.datashader import regrid\n",
    "from bokeh.models.tools import HoverTool, TapTool\n",
    "import matplotlib.pyplot as plt\n",
    "import qgrid\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tnrange, tqdm, tqdm_notebook\n",
    "import warnings\n",
    "from functools import partial\n",
    "from cytoolz import *\n",
    "from operator import getitem\n",
    "import nd2reader\n",
    "from importlib import reload\n",
    "import traceback\n",
    "import hvplot.pandas\n",
    "import param\n",
    "import parambokeh\n",
    "from traitlets import All\n",
    "import cachetools\n",
    "from collections import namedtuple, defaultdict\n",
    "from collections.abc import Mapping, Sequence\n",
    "from numbers import Number\n",
    "import skimage.morphology\n",
    "import scipy\n",
    "from glob import glob\n",
    "import asyncio\n",
    "\n",
    "IDX = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from processing import *\n",
    "# from trench_detection import *\n",
    "# from trench_segmentation import *\n",
    "# from trench_segmentation.watershed import *\n",
    "# from util import *\n",
    "# from ui import *\n",
    "import common, trench_detection, util, data_io\n",
    "import ui, diagnostics, metadata\n",
    "import workflow, image, geometry\n",
    "import trench_detection.hough, trench_detection.core\n",
    "import trench_segmentation.watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext snakeviz\n",
    "hv.extension(\"bokeh\")\n",
    "%matplotlib inline\n",
    "tqdm.monitor_interval = 0\n",
    "asyncio.get_event_loop().set_debug(True)\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trench_points\n",
    "%store -r trench_diag\n",
    "%store -r trench_bboxes\n",
    "trench_bboxes_t0 = util.get_one(trench_bboxes.groupby(\"t\"))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(ip=\"0.0.0.0\", n_workers=1, threads_per_worker=1, processes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.config[\"distributed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.config[\"distributed\"][\"worker\"][\"profile\"] = {\n",
    "    \"interval\": \"10s\",\n",
    "    \"cycle\": \"10s\",\n",
    "}\n",
    "# {'interval': '10ms', 'cycle': '1000ms'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue=\"short\",\n",
    "    walltime=\"09:00:00\",\n",
    "    # job_extra=['-p transfer'],\n",
    "    # job_extra=['--cores-per-socket=8'],\n",
    "    # job_extra=['--exclude=compute-e-16-181,compute-e-16-186'],\n",
    "    # interface='ib0',\n",
    "    memory=\"1GB\",\n",
    "    local_directory=\"/tmp\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    "    # diagnostics_port=('127.0.0.1', 8787),\n",
    "    env_extra=[\n",
    "        'export PYTHONPATH=\"/home/jqs1/projects/matriarch\"',\n",
    "        #'export PYTHONTRACEMALLOC=25',\n",
    "        #'export MALLOC_CONF=prof:true,prof_leak:true,lg_prof_interval:31,prof_final:true',\n",
    "        'export LD_PRELOAD=\"/home/jqs1/lib/libjemalloc.so.2\"',\n",
    "    ],\n",
    ")\n",
    "client = Client(cluster)  # , direct_to_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster._widget().children[1].children[1].children[0].children[0].layout.width = \"200px\"\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(n_workers=1)\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.adapt(minimum=10,maximum=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.stop_jobs(cluster.running_jobs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scheduler.stop_services()\n",
    "cluster.scheduler.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(distributed)\n",
    "from distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(ip=\"0.0.0.0\", n_workers=1, threads_per_worker=1, processes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2']#, '/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2',\n",
    "#                 '/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2', '/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/home/jqs1/scratch/fidelity/180518_triplegrowthcurve/PHASE_GC001.nd2', '/home/jqs1/scratch/fidelity/180518_triplegrowthcurve/PHASE_GC002.nd2']\n",
    "nd2_filenames = glob(\"/n/scratch2/jqs1/fidelity/all/180405_*.nd2\") + glob(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/TrErr*.nd2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames, metadata, parsed_metadata = workflow.get_nd2_frame_list(nd2_filenames)\n",
    "image_limits = workflow.get_filename_image_limits(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check():\n",
    "    import os\n",
    "\n",
    "    return os.path.exists(\n",
    "        \"/n/scratch2/jqs1/fidelity/all/180405_txnerr_loweronly_fast.nd2\"\n",
    "    )\n",
    "\n",
    "\n",
    "[k for k, v in client.run(_check).items() if v is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reload():\n",
    "    from importlib import reload\n",
    "    import util, trench_detection, diagnostics, workflow, image\n",
    "\n",
    "    # reload(util)\n",
    "    # reload(trench_detection.hough)\n",
    "    # reload(diagnostics)\n",
    "    reload(workflow)\n",
    "    # reload(image)\n",
    "\n",
    "\n",
    "client.run(do_reload)\n",
    "do_reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding trenches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_process = all_frames.loc[IDX[:, :, [\"MCHERRY\"], 0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run trench finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "find_trenches_diag = diagnostics.wrap_diagnostics(\n",
    "    trench_detection.hough.find_trenches, ignore_exceptions=True, pandas=True\n",
    ")\n",
    "trench_info_futures = {\n",
    "    idx: client.submit(\n",
    "        find_trenches_diag, client.submit(workflow.get_nd2_frame, **idx._asdict())\n",
    "    )\n",
    "    for idx, row in util.iter_index(frames_to_process)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(trench_info_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(trench_info_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_completed(obj, with_results=True):\n",
    "    if isinstance(obj, Mapping):\n",
    "        futures = obj.values()\n",
    "        dask_to_keys = {future.key: k for k, future in obj.items()}\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    for res in distributed.as_completed(futures, with_results=with_results):\n",
    "        if with_results:\n",
    "            future, result = res\n",
    "            yield dask_to_keys[future.key], future, result\n",
    "        else:\n",
    "            future = res\n",
    "            yield dask_to_keys[future.key], future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trench_info = {}\n",
    "for key, fut, res in as_completed(trench_info_futures):\n",
    "    trench_info[key] = res\n",
    "    client.cancel(fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_info = util.apply_map_futures(\n",
    "    client.gather, trench_info_futures, predicate=lambda x: x.status == \"finished\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_points, trench_diag, trench_err = workflow.unzip_trench_info(trench_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trench_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%store trench_points\n",
    "%store trench_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_angle = trench_diag[\"find_trench_lines.hough_2.angle\"].abs() > 2\n",
    "bad_angle.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pitch = (trench_diag[\"find_trench_lines.hough_2.peak_func.pitch\"] - 24).abs() > 1\n",
    "bad_pitch.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = trench_diag[bad_pitch]  # trench_diag[bad_angle | bad_period]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_stream.event(_df=selected.index.to_frame(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_points_good = trench_points[~util.multi_join(trench_points.index, bad_pitch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(trench_points_good), len(trench_points_good) / len(trench_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_bbox_futures = []\n",
    "for _, trenches in trench_points_good.groupby([\"filename\", \"position\", \"t\"]):\n",
    "    trench_bbox_futures.append(\n",
    "        client.submit(workflow.get_trench_bboxes, trenches, image_limits)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_bbox_results = util.apply_map_futures(\n",
    "    client.gather, trench_bbox_futures, predicate=lambda x: x.status == \"finished\"\n",
    ")\n",
    "trench_bboxes = pd.concat(\n",
    "    [trench_points_good, pd.concat(trench_bbox_results, axis=0)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%store trench_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trench_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trench_bboxes_t0 = util.get_one(trench_bboxes.groupby(\"t\"))[1]\n",
    "# trench_bboxes_t0.index = trench_points_good_t0.index.droplevel('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trench finding QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameStream = ui.DataFrameStream.define(\n",
    "    \"FrameStream\", selected.index.to_frame(index=False)\n",
    ")\n",
    "frame_stream = FrameStream()\n",
    "\n",
    "box = ui.dataframe_browser(frame_stream)\n",
    "frame_stream.event()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.image_viewer(frame_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_frame_info(trench_diag, frame_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ui.show_grid(selected, stream=frame_stream)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = workflow.get_nd2_frame_anyargs(**dict(frame_stream.get_param_values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, diag, _ = diagnostics.wrap_diagnostics(\n",
    "    trench_detection.hough.find_trenches, ignore_exceptions=False\n",
    ")(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_plot_browser(diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_trenches_segmentation = trench_bboxes_t0[\n",
    "    trench_bboxes_t0[(\"info\", \"hough_value\")] > 90\n",
    "].loc[IDX[:, :, [\"MCHERRY\"], 0, :, :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(trench_bboxes_t0), len(selected_trenches_segmentation) / len(trench_bboxes_t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_to_analyze = all_frames.loc[IDX[:,:1,['MCHERRY','YFP'],1:5],:]\n",
    "frames_to_analyze = all_frames.loc[IDX[:, :, [\"MCHERRY\", \"YFP\"], :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    len(frames_to_analyze),\n",
    "    len(all_frames.loc[IDX[:, :, [\"MCHERRY\", \"YFP\"], :], :]) / len(frames_to_analyze),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_funcs = {\n",
    "    \"mean\": np.mean,\n",
    "    \"min\": np.min,\n",
    "    \"max\": np.max,\n",
    "    (\"p0.3\", \"p0.5\", \"p0.7\", \"p0.9\", \"p0.95\"): partial(\n",
    "        np.percentile, q=(30, 50, 70, 90, 95)\n",
    "    ),\n",
    "}\n",
    "trenchwise_funcs = {\"sharpness\": image.sharpness, **labelwise_funcs}\n",
    "framewise_funcs = {\"sharpness\": image.sharpness, **labelwise_funcs}\n",
    "\n",
    "analyze_trench_func = partial(\n",
    "    workflow.analyze_trenches,\n",
    "    framewise_funcs=framewise_funcs,\n",
    "    trenchwise_funcs=trenchwise_funcs,\n",
    "    labelwise_funcs=labelwise_funcs,\n",
    "    regionprops=True,\n",
    "    segment_func=trench_segmentation.watershed.segment_trench,\n",
    ")\n",
    "analyze_trench_func = compose(\n",
    "    list, partial(map, pa.RecordBatch.from_pandas), analyze_trench_func\n",
    ")\n",
    "\n",
    "# analyze_trench_func = compose(lambda x: None, analyze_trench_func)\n",
    "\n",
    "analyze_trench_func = partial(client.submit, analyze_trench_func, retries=2)\n",
    "\n",
    "analysis_futures_iter = workflow.analyze_frames_and_trenches_iter(\n",
    "    selected_trenches_segmentation, frames_to_analyze, analyze_trench_func\n",
    ")\n",
    "\n",
    "# analysis_futures = workflow.analyze_frames_and_trenches(selected_trenches_segmentation,\n",
    "#                                            frames_to_analyze,\n",
    "#                                            analyze_trench_func)\n",
    "\n",
    "# display(trenchwise_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ac = distributed.as_completed([], with_results=False, loop=client.loop)\n",
    "\n",
    "new_futures_stream = streamz.Stream()\n",
    "finished_futures_stream = streamz.Stream(asynchronous=True, loop=client.loop)\n",
    "\n",
    "stream_sinks = {}\n",
    "stream_writers = {}\n",
    "output_filename = \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_{}.arrow\"\n",
    "\n",
    "new_futures_stream.sink(lambda x: ac.add(x))\n",
    "\n",
    "errored_futures = set()\n",
    "finished_futures_stream.filter(lambda x: x.status == \"error\").sink(\n",
    "    lambda x: errored_futures.add(x)\n",
    ")\n",
    "\n",
    "\n",
    "def timeout_func(futures):\n",
    "    ac.update(futures)\n",
    "\n",
    "\n",
    "successful_futures_stream = finished_futures_stream.filter(\n",
    "    lambda x: x.status == \"finished\"\n",
    ")\n",
    "# batched_futures_stream = successful_futures_stream.rate_limit(0.0004).timed_window(1)\n",
    "# gathered_futures_stream = streamz.buffer(batched_futures_stream, 10).gather_and_cancel(client=client, cancel=True)\n",
    "# batched_futures_stream = successful_futures_stream.timed_window(1)\n",
    "batched_futures_stream = successful_futures_stream.rate_limit(0.01).timed_window(5)\n",
    "buffered_futures_stream = (\n",
    "    batched_futures_stream  # streamz.buffer(batched_futures_stream, 10)\n",
    ")\n",
    "\n",
    "cancelled_futures = set()\n",
    "gathered_futures_stream = buffered_futures_stream.gather_and_cancel(\n",
    "    client=client,\n",
    "    gather=True,\n",
    "    cancel=True,\n",
    "    timeout=4,\n",
    "    timeout_func=timeout_func,\n",
    "    success_func=cancelled_futures.update,\n",
    ")\n",
    "# gathered_futures_stream.flatten().sink(partial(workflow.sink_to_arrow, sinks=stream_sinks, writers=stream_writers, output_func=lambda i: pa.OSFile(output_filename.format(i), 'w')))\n",
    "write_failures = []\n",
    "flattened_futures_stream = gathered_futures_stream.flatten()\n",
    "writer_stream = (\n",
    "    flattened_futures_stream  # .timed_window(10).map(lambda x: list(zip(*x)))\n",
    ")\n",
    "sink_func = partial(\n",
    "    workflow.sink_to_arrow,\n",
    "    sinks=stream_sinks,\n",
    "    writers=stream_writers,\n",
    "    output_func=lambda i: pa.OSFile(output_filename.format(i), \"w\"),\n",
    ")\n",
    "# sink_func = partial(client.loop.run_in_executor, None, sink_func)\n",
    "# writer_stream.with_timeout(timeout=3, retries=2, failure_func=write_failures.append).sink(sink_func)\n",
    "writer_stream.sink(sink_func)\n",
    "# stored_data = writer_stream.sink_to_list()\n",
    "\n",
    "# finished_futures_stream.sink(excepts(StopIteration, lambda x: new_futures_stream.emit(next(analysis_futures_iter)) if should_add_task()))\n",
    "# new_futures_stream.sink_to_list()\n",
    "all_futures = set()\n",
    "finished_futures = set()\n",
    "new_futures_stream.sink(lambda x: all_futures.add(x))\n",
    "successful_futures_stream.sink(lambda x: finished_futures.add(x))\n",
    "\n",
    "TASK_BUFFER_SIZE = 10000\n",
    "\n",
    "# def should_add_task():\n",
    "#     #return len([f for f in all_futures if f.status == 'pending'])\n",
    "#     #return TASK_BUFFER_SIZE > len(all_futures - finished_futures)\n",
    "#     return TASK_BUFFER_SIZE > len(all_futures - cancelled_futures)\n",
    "#     #print('>',len(all_futures - finished_futures))\n",
    "#     #return True\n",
    "\n",
    "# def readd_task(x):\n",
    "#     if should_add_task():\n",
    "#         return new_futures_stream.emit(next(analysis_futures_iter))\n",
    "\n",
    "\n",
    "def readd_task(x):\n",
    "    num_tasks_needed = TASK_BUFFER_SIZE - len(\n",
    "        all_futures - cancelled_futures - errored_futures\n",
    "    )\n",
    "    if num_tasks_needed > 0:\n",
    "        for future in take(num_tasks_needed, analysis_futures_iter):\n",
    "            new_futures_stream.emit(future)\n",
    "\n",
    "\n",
    "finished_futures_stream.sink(excepts(StopIteration, readd_task))\n",
    "\n",
    "# ac.update(take(3000, analysis_futures_iter))\n",
    "for future in take(TASK_BUFFER_SIZE, analysis_futures_iter):\n",
    "    new_futures_stream.emit(future)\n",
    "\n",
    "gather_func = workflow.gather_stream(finished_futures_stream, ac)\n",
    "gather_task = client.loop.asyncio_loop.create_task(gather_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in take(5000, analysis_futures_iter):\n",
    "    new_futures_stream.emit(future)\n",
    "gather_func = workflow.gather_stream(finished_futures_stream, ac)\n",
    "gather_task = client.loop.asyncio_loop.create_task(gather_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_task.cancel()\n",
    "client.cancel(all_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errored_futures[-1].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pa.open_stream(stream_sinks[2].r()).read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, analysis_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "framewise_df = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.sorted.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trenchwise_df = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.sorted.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchwise_df.columns = [\"/\".join(col).strip() for col in trenchwise_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"filename\",\n",
    "    \"position\",\n",
    "    \"channel\",\n",
    "    \"t\",\n",
    "    \"trench_set\",\n",
    "    \"trench\",\n",
    "    \"label\",\n",
    "    \"('YFP', 'labelwise', 'p0.9')\",\n",
    "    \"('MCHERRY', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'regionprops', 'area')\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labelwise_df = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.sorted3.parquet4\",\n",
    "    columns=cols,\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: otherwise computing is_unique is costly when we want to get_loc with full key\n",
    "labelwise_df.index.__dict__[\"_cache\"] = {\"lexsort_depth\": 6, \"is_unique\": True}\n",
    "# labelwise_df.index.lexsort_depth # prime the cache\n",
    "# if '_cache' not in labelwise_df.index.__dict__:\n",
    "#     labelwise_df.index.__dict__['_cache'] = {}\n",
    "# labelwise_df.index._cache['is_unique'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_df.columns = [\"/\".join(col).strip() for col in labelwise_df.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfp = \"YFP/labelwise/p0.9\"\n",
    "mcherry = \"MCHERRY/labelwise/p0.9\"\n",
    "area = \"YFP/regionprops/area\"\n",
    "trench_key = [\"filename\", \"position\", \"trench_set\", \"trench\"]\n",
    "trench_t_key = [\"filename\", \"position\", \"trench_set\", \"trench\", \"t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# labelwise_selected = labelwise_df.loc[IDX['/n/scratch2/jqs1/fidelity/all/TrErr002_noBF.nd2',:],:]\n",
    "labelwise_selected = labelwise_df.loc[\n",
    "    IDX[\"/n/scratch2/jqs1/fidelity/all/180405_txnerr_loweronly_fast.nd2\", :], :\n",
    "]\n",
    "# labelwise_selected = labelwise_df.loc[IDX['/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2',:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelwise_df) / len(labelwise_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = 'YFP/p0.3'\n",
    "# trenchwise_yfp_bg = trenchwise_df[col].rename(col+'_trenchwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "background = labelwise_selected.loc[IDX[:, :, :, :, :, 0], [yfp, mcherry]]\n",
    "background.index = background.index.droplevel(\"label\")\n",
    "background.columns = [c + \"_bg\" for c in background.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cell_sized = labelwise_selected[labelwise_selected[area].between(100, 200)].loc[\n",
    "    IDX[:, :, :, :, :, 1:], :\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_t_median = cell_sized.groupby(trench_t_key).median()\n",
    "trench_t_median.columns = [c + \"_trench_t_median\" for c in trench_t_median.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_median = cell_sized.groupby(trench_key).median()\n",
    "trench_median.columns = [c + \"_trench_median\" for c in trench_median.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with_bg = util.multi_join(cell_sized, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with_bg = util.multi_join(with_bg, trench_t_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with_bg = util.multi_join(with_bg, trench_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bright_ts_median_t = pd.DataFrame(\n",
    "    {\n",
    "        \"bright_ts_median_t_{}\".format(thresh): (\n",
    "            (with_bg[yfp] - with_bg[yfp + \"_trench_t_median\"]) >= thresh\n",
    "        )\n",
    "        .groupby(trench_key)\n",
    "        .sum()\n",
    "        for thresh in (\n",
    "            5,\n",
    "            8,\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            50,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bright_ts_median = pd.DataFrame(\n",
    "    {\n",
    "        \"bright_ts_median_{}\".format(thresh): (\n",
    "            (with_bg[yfp] - with_bg[yfp + \"_trench_median\"]) >= thresh\n",
    "        )\n",
    "        .groupby(trench_key)\n",
    "        .sum()\n",
    "        for thresh in (\n",
    "            5,\n",
    "            8,\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            50,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bright_ts_bg = pd.DataFrame(\n",
    "    {\n",
    "        \"bright_ts_bg_{}\".format(thresh): (\n",
    "            (with_bg[yfp] - with_bg[yfp + \"_bg\"]) >= thresh\n",
    "        )\n",
    "        .groupby(trench_key)\n",
    "        .sum()\n",
    "        for thresh in (\n",
    "            5,\n",
    "            8,\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            50,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bright_ts_median_t[bright_ts_median_t[\"bright_ts_median_t_50\"] > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_ts_median_t[bright_ts_median_t[\"bright_ts_median_t_5\"] > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median_bg = background.groupby(trench_key).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_ts_all = util.multi_join(\n",
    "    util.multi_join(\n",
    "        util.multi_join(bright_ts_median_t, bright_ts_median), bright_ts_bg\n",
    "    ),\n",
    "    median_bg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cell_sized_with_ts = util.multi_join(cell_sized, bright_ts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "detected_bursts = cell_sized_with_ts[cell_sized_with_ts[\"bright_ts_median_t_20\"] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(detected_bursts.groupby(trench_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "detected_bursts2 = cell_sized_with_ts[cell_sized_with_ts[\"bright_ts_bg_20\"] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(detected_bursts2.groupby(trench_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_sized_with_ts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cell_sized_with_ts[yfp + \"_bg\"]\n",
    "x[x < 180].hist(bins=50, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# detected_bursts3 = cell_sized_with_ts[(cell_sized_with_ts[yfp+'_bg'] <= 200) & (cell_sized_with_ts['bright_ts_bg_20'] >= 2)]\n",
    "# detected_bursts3 = cell_sized_with_ts[(cell_sized_with_ts[yfp+'_bg'].between(120, 130)) & (cell_sized_with_ts['bright_ts_bg_20'] >= 1)]\n",
    "detected_bursts3 = cell_sized_with_ts[\n",
    "    (cell_sized_with_ts[\"bright_ts_median_t_50\"] >= 100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detected_bursts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(detected_bursts3.groupby(trench_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelStream = ui.MultiIndexStream.define(\"LabelStream\", labelwise_df.index)\n",
    "label_stream = LabelStream()\n",
    "box = ui.dataframe_browser(label_stream)\n",
    "label_stream.event()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Layout [normalize=False]\n",
    "%%output size=100\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        (\"(x,y)\", \"(@x{0[.]0}, @y{0[.]0})\"),\n",
    "        (\"value\", \"@z\"),\n",
    "    ]\n",
    ")\n",
    "# cb = compose(partial(ui.hover_image, hover), ui._trench_img, workflow.get_trench_image)\n",
    "cb = lambda v_max: compose(\n",
    "    partial(ui.hover_image, hover),\n",
    "    lambda x: x.redim.range(z=(0, v_max)),\n",
    "    ui._trench_img,\n",
    "    workflow.get_trench_image,\n",
    ")\n",
    "# cb = workflow.get_trench_image\n",
    "(\n",
    "    ui.trench_viewer(\n",
    "        trench_bboxes, label_stream, channel=\"MCHERRY\", image_callback=cb(5000)\n",
    "    )\n",
    "    + ui.trench_viewer(\n",
    "        trench_bboxes, label_stream, channel=\"YFP\", image_callback=cb(400)\n",
    "    )\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = detected_bursts3.groupby(trench_key)\n",
    "group_set_keys = list(util.grouper(groups.groups.keys(), 5))\n",
    "group_index = pd.MultiIndex.from_tuples([(i,) for i in range(len(group_set_keys))])\n",
    "group_index.names = [\"group_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupStream = ui.MultiIndexStream.define(\"GroupStream\", group_index)\n",
    "group_stream = GroupStream()\n",
    "group_box = ui.dataframe_browser(group_stream)\n",
    "group_stream.event()\n",
    "group_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "sel = Selection1D()\n",
    "\n",
    "\n",
    "def callback(group_set):\n",
    "    df = pd.concat([groups.get_group(key) for key in group_set_keys[group_set]])\n",
    "    plot = hv.Scatter(\n",
    "        df,\n",
    "        kdims=[\"t\"],\n",
    "        vdims=[\n",
    "            \"YFP/labelwise/p0.9\",\n",
    "            \"filename\",\n",
    "            \"position\",\n",
    "            \"trench_set\",\n",
    "            \"trench\",\n",
    "            \"label\",\n",
    "        ],\n",
    "    )\n",
    "    tooltips = [\n",
    "        (\"t\", \"@t{0[.]0}\"),\n",
    "        # ('filename', '@filename'),\n",
    "        (\"trench\", \"@position.@trench_set.@trench\"),\n",
    "        (\"label\", \"@label\"),\n",
    "        (\"YFP\", \"@{YFP/labelwise/p0.9}{0[.]0}\"),\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=tooltips)\n",
    "    tap = TapTool()\n",
    "    plot = plot.options(\n",
    "        \"Scatter\",\n",
    "        size=3,\n",
    "        color_index=\"trench\",\n",
    "        nonselection_alpha=0.3,\n",
    "        cmap=\"Category20\",\n",
    "        tools=[hover, tap],\n",
    "        show_legend=True,\n",
    "    )\n",
    "    # ui.selection_to_stream(plot, label_stream)\n",
    "    sel.clear()\n",
    "    sel.add_subscriber(\n",
    "        partial(\n",
    "            ui._selection_to_stream_callback,\n",
    "            data=plot.data,\n",
    "            keys=df.index.names,\n",
    "            stream=label_stream,\n",
    "        )\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "p = hv.DynamicMap(callback, streams=[group_stream])\n",
    "sel.source = p\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "def cb(**kwargs):\n",
    "    df = workflow.select_dataframe(\n",
    "        labelwise_df, kwargs, t=slice(None), label=slice(None)\n",
    "    )\n",
    "    # df = workflow.select_dataframe(labelwise_df, kwargs, label=slice(None))\n",
    "    plot = hv.Scatter(\n",
    "        df,\n",
    "        kdims=[\"t\"],\n",
    "        vdims=[\n",
    "            \"YFP/labelwise/p0.9\",\n",
    "            \"filename\",\n",
    "            \"position\",\n",
    "            \"trench_set\",\n",
    "            \"trench\",\n",
    "            \"label\",\n",
    "        ],\n",
    "    )\n",
    "    tooltips = [\n",
    "        (\"t\", \"@t{0[.]0}\"),\n",
    "        # ('filename', '@filename'),\n",
    "        (\"trench\", \"@position.@trench_set.@trench\"),\n",
    "        (\"label\", \"@label\"),\n",
    "        (\"YFP\", \"@{YFP/labelwise/p0.9}{0[.]0}\"),\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=tooltips)\n",
    "    plot = plot.options(\n",
    "        \"Scatter\",\n",
    "        size=3,\n",
    "        color_index=\"label\",\n",
    "        nonselection_alpha=0.3,\n",
    "        cmap=\"Category20\",\n",
    "        tools=[hover, \"tap\"],\n",
    "        show_legend=True,\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "ui.viewer(cb, label_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.select_dataframe(trenchwise_df, label_stream.contents, t=slice(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_sized_with_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.select_dataframe(\n",
    "    cell_sized_with_ts, label_stream.contents, t=0, label=slice(None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ui.dataframe_viewer(partial(ui.select_dataframe, cell_sized_with_ts), label_stream, label=slice(None), t=slice(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ui.dataframe_viewer(partial(ui.select_dataframe, background), label_stream, label=slice(None), t=slice(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trench_movie(trench_bboxes, key, channel, ts):\n",
    "    get_trench_frame = lambda t: ui._trench_img(\n",
    "        workflow.get_trench_image(\n",
    "            trench_bboxes, key[0], key[1], channel, t, key[2], key[3]\n",
    "        )\n",
    "    )\n",
    "    movie = hv.HoloMap({t: get_trench_frame(t) for t in tqdm_notebook(ts)})\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Layout [normalize=False]\n",
    "%%output size=100\n",
    "key = tuple(getattr(label_stream, attr) for attr in trench_key)\n",
    "index = detected_bursts.groupby(trench_key).get_group(key).index\n",
    "ts = index._get_level_values(index._get_level_number(\"t\"), unique=True)\n",
    "(\n",
    "    trench_movie(trench_bboxes, key, \"MCHERRY\", ts)\n",
    "    + trench_movie(trench_bboxes, key, \"YFP\", ts)\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median_by_t = cell_sized_with_ts.groupby(\"t\").median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_by_t[yfp].hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_by_t = cell_sized_with_ts.groupby(\"t\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_t[yfp].hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "# p = hv.Scatter(detected_bursts, kdims=['t'], vdims=['YFP/labelwise/p0.9', 'filename', 'position', 'trench_set', 'trench'])\n",
    "d = {\n",
    "    i: hv.Scatter(\n",
    "        pd.concat([g[1] for g in groups]),\n",
    "        kdims=[\"t\"],\n",
    "        vdims=[\"YFP/labelwise/p0.9\", \"filename\", \"position\", \"trench_set\", \"trench\"],\n",
    "    )\n",
    "    for i, groups in enumerate(util.grouper(detected_bursts.groupby(trench_key), 5))\n",
    "}\n",
    "p = hv.HoloMap(d)\n",
    "tooltips = [\n",
    "    (\"t\", \"@t{0[.]0}\"),\n",
    "    # ('filename', '@filename'),\n",
    "    (\"trench\", \"@position.@trench_set.@trench\"),\n",
    "    (\"YFP\", \"@{YFP/labelwise/p0.9}{0[.]0}\"),\n",
    "]\n",
    "hover = HoverTool(tooltips=tooltips)\n",
    "tap = TapTool()\n",
    "p = p.options(\n",
    "    \"Scatter\",\n",
    "    size=3,\n",
    "    color_index=\"trench\",\n",
    "    nonselection_alpha=0.3,\n",
    "    cmap=\"Category20\",\n",
    "    tools=[hover, tap],\n",
    ")\n",
    "# pr = hv.renderer('bokeh').get_plot(p).state\n",
    "# renderer = pr.renderers[-1]\n",
    "# renderer.nonselection_glyph = renderer.glyph\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "hv.Scatter(\n",
    "    detected_bursts[:10_000], kdims=[\"t\"], vdims=[\"YFP/labelwise/p0.9\", \"trench\"]\n",
    ").options(size=3, color_index=\"trench\", cmap=\"Category20\", tools=[\"hover\", \"tap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchwise_df.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:.2n}\".format(1.12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.dataframe_viewer(\n",
    "    partial(ui.select_dataframe, labelwise_df), label_stream, label=slice(None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "p = hv.Scatter(\n",
    "    detected_bursts[10_000:20_000], kdims=[\"t\"], vdims=[\"YFP/labelwise/p0.9\", \"trench\"]\n",
    ").options(\n",
    "    size=3,\n",
    "    color_index=\"trench\",\n",
    "    nonselection_alpha=0.3,\n",
    "    tools=[\"hover\", \"tap\"],\n",
    "    cmap=\"Category20\",\n",
    ")\n",
    "ui.selection_to_stream(p, label_stream)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb(**kwargs):\n",
    "    dat = labelwise_df.loc[\n",
    "        workflow.stream_slice(\n",
    "            labelwise_df.index.names, kwargs, t=slice(None), label=slice(None)\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "    plot = dat.hvplot.scatter(\"t\", \"YFP_labelwise_p0.95\").options(\n",
    "        tools=[\"tap\", \"hover\"]\n",
    "    )\n",
    "    # ui.selection_to_stream(plot, label_stream)\n",
    "    return plot\n",
    "\n",
    "\n",
    "ui.viewer(cb, label_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped trench scatterplot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = detected_bursts.groupby(trench_key)\n",
    "group_set_keys = list(util.grouper(groups.groups.keys(), 10))\n",
    "group_index = pd.MultiIndex.from_tuples([(i,) for i in range(len(group_set_keys))])\n",
    "group_index.names = [\"group_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupStream = ui.MultiIndexStream.define(\"GroupStream\", group_index)\n",
    "group_stream = GroupStream()\n",
    "group_box = ui.dataframe_browser(group_stream)\n",
    "group_stream.event()\n",
    "group_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "sel = Selection1D()\n",
    "\n",
    "\n",
    "def callback(group_set):\n",
    "    df = pd.concat([groups.get_group(key) for key in group_set_keys[group_set]])\n",
    "    plot = hv.Scatter(\n",
    "        df,\n",
    "        kdims=[\"t\"],\n",
    "        vdims=[\"YFP/labelwise/p0.9\", \"filename\", \"position\", \"trench_set\", \"trench\"],\n",
    "    )\n",
    "    tooltips = [\n",
    "        (\"t\", \"@t{0[.]0}\"),\n",
    "        # ('filename', '@filename'),\n",
    "        (\"trench\", \"@position.@trench_set.@trench\"),\n",
    "        (\"YFP\", \"@{YFP/labelwise/p0.9}{0[.]0}\"),\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=tooltips)\n",
    "    tap = TapTool()\n",
    "    plot = plot.options(\n",
    "        \"Scatter\",\n",
    "        size=3,\n",
    "        color_index=\"trench\",\n",
    "        nonselection_alpha=0.3,\n",
    "        cmap=\"Category20\",\n",
    "        tools=[hover, tap],\n",
    "        show_legend=True,\n",
    "    )\n",
    "    # ui.selection_to_stream(plot, label_stream)\n",
    "    sel.clear()\n",
    "    sel.add_subscriber(\n",
    "        partial(\n",
    "            ui._selection_to_stream_callback,\n",
    "            data=plot.data,\n",
    "            keys=df.index.names,\n",
    "            stream=label_stream,\n",
    "        )\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "p = hv.DynamicMap(callback, streams=[group_stream])\n",
    "sel.source = p\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun group_stream.event(group_set=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped trench movie UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "# sel = Selection1D()\n",
    "def callback(group_set):\n",
    "    #     df = pd.concat([groups.get_group(key) for key in group_set_keys[group_set]])\n",
    "    #     plot = hv.Scatter(df,\n",
    "    #                       kdims=['t'],\n",
    "    #                       vdims=['YFP/labelwise/p0.9', 'filename',\n",
    "    #                              'position', 'trench_set', 'trench'])\n",
    "    #     tooltips = [('t', '@t{0[.]0}'),\n",
    "    #             #('filename', '@filename'),\n",
    "    #             ('trench', '@position.@trench_set.@trench'),\n",
    "    #             ('YFP', '@{YFP/labelwise/p0.9}{0[.]0}')]\n",
    "    #     hover = HoverTool(tooltips=tooltips)\n",
    "    #     tap = TapTool()\n",
    "    #     plot = plot.options('Scatter',\n",
    "    #                         size=3,\n",
    "    #                         color_index='trench',\n",
    "    #                         nonselection_alpha=0.3,\n",
    "    #                         cmap='Category20',\n",
    "    #                         tools=[hover, tap],\n",
    "    #                         show_legend=True)\n",
    "    # ui.selection_to_stream(plot, label_stream)\n",
    "    #     sel.clear()\n",
    "    #     sel.add_subscriber(partial(ui._selection_to_stream_callback,\n",
    "    #                                data=plot.data,\n",
    "    #                                keys=df.index.names,\n",
    "    #                                stream=label_stream))\n",
    "    movies = []\n",
    "    for key in group_set_keys[group_set]:\n",
    "        ts = groups.get_group(key).index.get_level_values(\"t\")\n",
    "        movie = hv.HoloMap(\n",
    "            {\n",
    "                t: ui._trench_img(\n",
    "                    workflow.get_trench_image(\n",
    "                        trench_bboxes, key[0], key[1], \"YFP\", t, key[2], key[3]\n",
    "                    )\n",
    "                )\n",
    "                for t in ts\n",
    "            }\n",
    "        )\n",
    "        movies.append(movie)\n",
    "    plot = hv.Layout(movies).cols(1)\n",
    "    return plot\n",
    "\n",
    "\n",
    "p = hv.DynamicMap(callback, streams=[group_stream])\n",
    "# sel.source = p\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
