{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.feather as feather\n",
    "import zarr\n",
    "import dask\n",
    "from dask import delayed\n",
    "import distributed\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import streamz\n",
    "import streamz.dataframe as sdf\n",
    "import holoviews as hv\n",
    "from holoviews.streams import Stream, param, Selection1D\n",
    "from holoviews.operation.datashader import regrid\n",
    "from bokeh.models.tools import HoverTool, TapTool\n",
    "import matplotlib.pyplot as plt\n",
    "import qgrid\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tnrange, tqdm, tqdm_notebook\n",
    "import warnings\n",
    "from functools import partial\n",
    "from cytoolz import *\n",
    "from operator import getitem\n",
    "import nd2reader\n",
    "from importlib import reload\n",
    "import traceback\n",
    "import hvplot.pandas\n",
    "import param\n",
    "import parambokeh\n",
    "from traitlets import All\n",
    "import cachetools\n",
    "from collections import namedtuple, defaultdict\n",
    "from collections.abc import Mapping, Sequence\n",
    "from numbers import Number\n",
    "import skimage.morphology\n",
    "import scipy\n",
    "from glob import glob\n",
    "import os\n",
    "import asyncio\n",
    "from deepmerge import merge_or_raise\n",
    "from IPython.display import Video\n",
    "\n",
    "IDX = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from processing import *\n",
    "# from trench_detection import *\n",
    "# from trench_segmentation import *\n",
    "# from trench_segmentation.watershed import *\n",
    "# from util import *\n",
    "# from ui import *\n",
    "import common, trench_detection, util, data_io, processing\n",
    "import ui, diagnostics, metadata\n",
    "import workflow, image, geometry\n",
    "import trench_detection.hough, trench_detection.core\n",
    "import trench_segmentation.watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext snakeviz\n",
    "hv.extension(\"bokeh\", \"matplotlib\")\n",
    "%matplotlib inline\n",
    "tqdm.monitor_interval = 0\n",
    "asyncio.get_event_loop().set_debug(False)\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2']#, '/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2',\n",
    "#                 '/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2', '/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "# nd2_filenames = ['/home/jqs1/scratch/fidelity/180518_triplegrowthcurve/PHASE_GC001.nd2', '/home/jqs1/scratch/fidelity/180518_triplegrowthcurve/PHASE_GC002.nd2']\n",
    "# nd2_filenames = glob('/n/scratch2/jqs1/fidelity/all/180405*.nd2') + glob('/n/scratch2/jqs1/fidelity/all/TrErr*.nd2')\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr001.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr_loweronly.nd2', '/n/scratch2/jqs1/fidelity/all/180405_txnerr_loweronly_fast.nd2'] + glob('/n/scratch2/jqs1/fidelity/all/TrErr*.nd2')\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/TrErr.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.nd2']\n",
    "\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.stripe8.16m.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.stripe1.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.stripe8.64m.nd2']\n",
    "\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.stripe32.64m.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/TrErr.stripe-1.256m.nd2']\n",
    "# nd2_filenames = ['/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2']\n",
    "nd2_filenames = [\"/n/scratch2/jqs1/fidelity/all/181010_rpos_bigsnake.stripe-1.64m.nd2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames, metadata, parsed_metadata = workflow.get_nd2_frame_list(nd2_filenames)\n",
    "image_limits = workflow.get_filename_image_limits(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.get(\"distributed.worker.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.config[\"distributed\"][\"worker\"][\"memory\"] = {\n",
    "    \"target\": 0.9,\n",
    "    \"spill\": None,\n",
    "    \"pause\": None,\n",
    "    \"terminate\": 0.95,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask.config.config['distributed']['worker']['profile'] = {'interval': '10s', 'cycle': '10s'}\n",
    "# {'interval': '10ms', 'cycle': '1000ms'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue=\"short\",\n",
    "    walltime=\"04:00:00\",\n",
    "    # job_extra=['-p transfer'],\n",
    "    # job_extra=['--cores-per-socket=8'],\n",
    "    # job_extra=['--exclude=compute-e-16-181,compute-e-16-186'],\n",
    "    # interface='ib0',\n",
    "    memory=\"10GB\",  # TODO!!!\n",
    "    local_directory=\"/tmp\",\n",
    "    log_directory=\"/home/jqs1/projects/matriarch/log\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    "    # diagnostics_port=('127.0.0.1', 8787),\n",
    "    env_extra=[\n",
    "        'export PYTHONPATH=\"/home/jqs1/projects/matriarch\"',\n",
    "        #'export PYTHONTRACEMALLOC=25',\n",
    "        #'export MALLOC_CONF=prof:true,prof_leak:true,lg_prof_interval:31,prof_final:true',\n",
    "        'export LD_PRELOAD=\"/home/jqs1/lib/libjemalloc.so.2\"',\n",
    "    ],\n",
    ")\n",
    "client = Client(cluster)  # , direct_to_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=0, maximum=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster._widget().children[1].children[1].children[0].children[0].layout.width = \"200px\"\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.stop_jobs(cluster.running_jobs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scheduler.stop_services()\n",
    "cluster.scheduler.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reload():\n",
    "    from importlib import reload\n",
    "    import util, trench_detection, diagnostics, workflow, image\n",
    "\n",
    "    # reload(util)\n",
    "    # reload(trench_detection.hough)\n",
    "    # reload(diagnostics)\n",
    "    reload(workflow)\n",
    "    # reload(image)\n",
    "\n",
    "\n",
    "client.run(do_reload)\n",
    "do_reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trench detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameStream = ui.MultiIndexStream.define(\"FrameStream\", all_frames.index)\n",
    "frame_stream = FrameStream()\n",
    "box = ui.dataframe_browser(frame_stream)\n",
    "frame_stream.event()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# key = frame_stream.contents\n",
    "key = (\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    "    35,\n",
    "    \"MCHERRY\",\n",
    "    0,\n",
    ")\n",
    "frame = workflow.get_nd2_frame(*key)\n",
    "find_trenches_diag = diagnostics.wrap_diagnostics(\n",
    "    trench_detection.find_trenches, ignore_exceptions=False, pandas=False\n",
    ")\n",
    "trench_points, trench_diag, trench_err = find_trenches_diag(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_plot_browser(trench_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames = all_frames.loc[IDX[:, :, :, :], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = selected_frames.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = ('/n/scratch2/jqs1/fidelity/all/180405_txnerr002.stripe1.nd2', 271, 'MCHERRY', 0)\n",
    "key = (\n",
    "    \"/n/scratch2/jqs1/fidelity/all/181010_rpos_bigsnake.stripe-1.64m.nd2\",\n",
    "    0,\n",
    "    \"MCHERRY\",\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = workflow.get_nd2_frame(*key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,20))\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trenches = do_find_trenches(*key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diag = util.tree()\n",
    "trenches = trench_detection.find_trenches(frame, diagnostics=diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.show_plot_browser(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = measure(trenches, selected_frames, measurement_func=_measurement_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_one(res).keys()  # ['images'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_one(res)[\"measurements\"][\"trenchwise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_frames = all_frames.loc[IDX[:,:1,:,:2],:]\n",
    "selected_frames = all_frames.loc[IDX[:, :, :, :], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New trench detection+segmentation+analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trenches(trenches):\n",
    "    pitch = 20.9\n",
    "    # pitch = 24\n",
    "    if trenches is None:\n",
    "        return None\n",
    "    #     good_trenches = trenches[((trenches[('diag', 'find_trench_lines.hough_2.peak_func.pitch')] - 24).abs() <= 1)\n",
    "    #                               & (trenches[('info','hough_value')] > 90)]\n",
    "    # TODO: we shouldn't be filtering based on hough_value at all!!\n",
    "    good_trenches = trenches[\n",
    "        (\n",
    "            (\n",
    "                trenches[(\"diag\", \"find_trench_lines.hough_2.peak_func.pitch\")] - pitch\n",
    "            ).abs()\n",
    "            <= 1\n",
    "        )\n",
    "        & (~trenches[(\"upper_left\", \"x\")].isnull())\n",
    "    ]\n",
    "    # TODO: filter based on minimum trench length\n",
    "    return good_trenches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_funcs = {\n",
    "    \"mean\": np.mean,\n",
    "    \"min\": np.min,\n",
    "    \"max\": np.max,\n",
    "    (\"p0.3\", \"p0.5\", \"p0.7\", \"p0.9\", \"p0.95\"): partial(\n",
    "        np.percentile, q=(30, 50, 70, 90, 95)\n",
    "    ),\n",
    "}\n",
    "trenchwise_funcs = {\"sharpness\": image.sharpness, **pixelwise_funcs}\n",
    "\n",
    "\n",
    "def _measurement_func(label_image, intensity_image):\n",
    "    if label_image is not None:\n",
    "        eroded_label_image = (\n",
    "            util.repeat_apply(skimage.morphology.binary_erosion, 2)(label_image != 0)\n",
    "            * label_image\n",
    "        )\n",
    "    if intensity_image is None:\n",
    "        if label_image is None:\n",
    "            return None  # can't measure anything\n",
    "        minlength = label_image.max() + 1\n",
    "        mask_labelwise_df = pd.DataFrame(\n",
    "            {\n",
    "                (\"noerode\", \"size\"): np.bincount(label_image.flat, minlength=minlength),\n",
    "                (\"erode2\", \"size\"): np.bincount(\n",
    "                    eroded_label_image.flat, minlength=minlength\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        mask_labelwise_df.index.name = \"label\"\n",
    "        return dict(mask_labelwise=mask_labelwise_df)\n",
    "    trenchwise_df = workflow.map_frame(trenchwise_funcs, intensity_image)\n",
    "    res = dict(trenchwise=trenchwise_df)\n",
    "    if label_image is None:\n",
    "        return res  # only measure trenchwise\n",
    "    labelwise = {\n",
    "        \"noerode\": workflow.map_frame_over_labels(\n",
    "            pixelwise_funcs, label_image, intensity_image\n",
    "        ),\n",
    "        \"erode2\": workflow.map_frame_over_labels(\n",
    "            pixelwise_funcs, eroded_label_image, intensity_image\n",
    "        ),\n",
    "    }\n",
    "    labelwise_df = pd.concat(labelwise, axis=1)\n",
    "    res[\"labelwise\"] = labelwise_df\n",
    "    return res  # measure trenchwise and labelwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _measure(\n",
    "    trenches,\n",
    "    frames,\n",
    "    measurement_func,\n",
    "    segmentation_channel=\"MCHERRY\",\n",
    "    measure_channels=None,\n",
    "    segmentation_func=trench_segmentation.watershed.segment_trench,\n",
    "    include_frame=True,\n",
    "    frame_bits=8,\n",
    "    frame_downsample=4,\n",
    "    filename=None,\n",
    "    position=None,\n",
    "):\n",
    "    frame_transformation = compose(\n",
    "        processing.zarrify,\n",
    "        partial(image.quantize, bits=frame_bits),\n",
    "        partial(image.downsample, factor=frame_downsample),\n",
    "    )\n",
    "    trench_crops = processing._get_trench_crops(\n",
    "        trenches,\n",
    "        frames,\n",
    "        include_frame=include_frame,\n",
    "        frame_transformation=frame_transformation,\n",
    "        filename=filename,\n",
    "        position=position,\n",
    "    )\n",
    "    # flattened_crops = {(*k[0], *k[1:]): v for k, v in util.flatten_dict(trench_crops).items() if k[0] != '_frame'}\n",
    "    # flattened_crops = {k: v for k, v in util.flatten_dict(trench_crops).items() if k[0] != '_frame'}\n",
    "    # print('trenches>',list(trench_crops[1][92]['YFP'].keys()))\n",
    "    # print('trenches>',list(trench_crops[(1,92)]['MCHERRY'].keys()))\n",
    "    # print('trenches only>',list(flattened_crops.keys()))\n",
    "    # 0/0\n",
    "    res = {}\n",
    "    segmentation_masks = {}\n",
    "    measurements = {}\n",
    "    # segment\n",
    "    for trench_set, crops_trench_channel_t in trench_crops.items():\n",
    "        if trench_set == \"_frame\":\n",
    "            continue\n",
    "        for trench_idx, crops_channel_t in crops_trench_channel_t.items():\n",
    "            for channel, crops_t in crops_channel_t.items():\n",
    "                for t, crop in crops_t.items():\n",
    "                    if measure_channels is not None and channel not in measure_channels:\n",
    "                        continue\n",
    "                    segmentation_key = (trench_set, trench_idx, segmentation_channel, t)\n",
    "                    segmentation_mask = segmentation_masks.get(segmentation_key, None)\n",
    "                    if segmentation_mask is None and segmentation_func is not None:\n",
    "                        segmentation_mask = segmentation_func(\n",
    "                            trench_crops[trench_set][trench_idx][segmentation_channel][\n",
    "                                t\n",
    "                            ]\n",
    "                        )\n",
    "                        segmentation_masks[segmentation_key] = segmentation_mask\n",
    "                        # measure mask\n",
    "                        if measurement_func is not None:\n",
    "                            measurements[\n",
    "                                (\"mask\", (trench_set, trench_idx, t))\n",
    "                            ] = measurement_func(segmentation_mask, None)\n",
    "                    # measure\n",
    "                    if measurement_func is not None:\n",
    "                        measurements[\n",
    "                            (channel, (trench_set, trench_idx, t))\n",
    "                        ] = measurement_func(segmentation_mask, crop)\n",
    "    if measurement_func is not None:\n",
    "        measurement_dfs = util.map_dict_levels(\n",
    "            lambda k: (k[1], k[0], *k[2:]), measurements\n",
    "        )\n",
    "        for name, dfs in measurement_dfs.items():\n",
    "            dfs = util.unflatten_dict(dfs)\n",
    "            if isinstance(util.get_one(dfs, level=2), pd.Series):\n",
    "                df = pd.concat(\n",
    "                    {\n",
    "                        channel: pd.concat(channel_dfs, axis=1).T\n",
    "                        for channel, channel_dfs in dfs.items()\n",
    "                    },\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                df = pd.concat(\n",
    "                    {\n",
    "                        channel: pd.concat(channel_dfs, axis=0)\n",
    "                        for channel, channel_dfs in dfs.items()\n",
    "                    },\n",
    "                    axis=1,\n",
    "                )\n",
    "            df.index.names = [\"trench_set\", \"trench\", \"t\", *df.index.names[3:]]\n",
    "            measurement_dfs[name] = df\n",
    "        res[\"measurements\"] = measurement_dfs\n",
    "    images = dict(raw=trench_crops)\n",
    "    if segmentation_func is not None:\n",
    "        images[\"segmentation\"] = util.unflatten_dict(segmentation_masks)\n",
    "    res[\"images\"] = images\n",
    "    return res\n",
    "\n",
    "\n",
    "measure = processing.iterate_over_groupby([\"filename\", \"position\"])(_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_func(extension=None, kind=None, name=None, filename=None, position=None):\n",
    "    components = [s for s in (\"\", name, extension) if s is not None]\n",
    "    if position is None:\n",
    "        path = [f\"{filename}.{kind}\" + \".\".join(components)]\n",
    "    else:\n",
    "        path = [f\"{filename}.{kind}\", \"pos{:d}\".format(position) + \".\".join(components)]\n",
    "    return os.path.join(*path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trench_diag_to_dataframe(trench_diag, sep=\".\"):\n",
    "    df = trench_diag.to_frame().T\n",
    "    expanded_df = diagnostics.expand_diagnostics_by_label(df)\n",
    "    expanded_df.index = expanded_df.index.droplevel(0)\n",
    "    expanded_df.index.names = [*expanded_df.index.names[:-1], \"trench_set\"]\n",
    "    return expanded_df\n",
    "\n",
    "\n",
    "#     if len(expanded_df):\n",
    "#         expanded_df.index = expanded_df.index.droplevel(0)\n",
    "#         expanded_df.index.names = [*expanded_df.index.names[:-1], 'trench_set']\n",
    "#     else:\n",
    "#         expanded_df = pd.concat([df], keys=[-1], names=['trench_set'])\n",
    "#     return expanded_df\n",
    "\n",
    "\n",
    "def _trench_info_to_dataframe(trench_info):\n",
    "    trench_points, trench_diag, trench_err = trench_info\n",
    "    if trench_err is not None:\n",
    "        # TODO: write trench_err\n",
    "        return None\n",
    "    trench_diag = _trench_diag_to_dataframe(trench_info[1])\n",
    "    # FROM: https://stackoverflow.com/questions/14744068/prepend-a-level-to-a-pandas-multiindex\n",
    "    trench_diag = pd.concat([trench_diag], axis=1, keys=[\"diag\"])\n",
    "    trenches = pd.concat(\n",
    "        [trench_points, util.multi_join(trench_info[0].index, trench_diag)], axis=1\n",
    "    )\n",
    "    return trenches\n",
    "\n",
    "\n",
    "def _trenches_to_bboxes(trenches, image_limits):\n",
    "    trench_bboxes = workflow.get_trench_bboxes(trenches, image_limits)\n",
    "    if trench_bboxes is not None:\n",
    "        trenches = pd.concat([trenches, trench_bboxes], axis=1)\n",
    "    return trenches\n",
    "\n",
    "\n",
    "find_trenches_diag = diagnostics.wrap_diagnostics(\n",
    "    trench_detection.find_trenches, ignore_exceptions=True, pandas=True\n",
    ")\n",
    "\n",
    "\n",
    "def do_find_trenches(*key):\n",
    "    frame = workflow.get_nd2_frame(*key)\n",
    "    trench_info = find_trenches_diag(frame)\n",
    "    return trench_info\n",
    "\n",
    "\n",
    "def do_trenches_to_bboxes(trench_info, key=None, index_names=(\"filename\", \"position\")):\n",
    "    trenches = _trench_info_to_dataframe(trench_info)\n",
    "    if trenches is None:\n",
    "        return None\n",
    "    if key is not None:\n",
    "        trenches = pd.concat([trenches], names=index_names, keys=[key])\n",
    "    trenches = _trenches_to_bboxes(trenches, image_limits=image_limits)\n",
    "    return trenches\n",
    "\n",
    "\n",
    "def do_get_trench_err(trench_info):\n",
    "    trench_points, trench_diag, trench_err = trench_info\n",
    "    if trench_err is None:\n",
    "        return None\n",
    "    if trench_points is not None:\n",
    "        raise ValueError(\"expecting trench_points to be None\")\n",
    "    return trench_info\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def do_serialize_to_disk(\n",
    "    data, filename, overwrite=True, skip_nones=True, format=\"pickle\"\n",
    "):\n",
    "    if skip_nones:\n",
    "        data = {k: v for k, v in data.items() if v is not None}\n",
    "    if not overwrite and os.path.exists(filename):\n",
    "        raise FileExistsError\n",
    "    with open(filename, \"wb\") as f:\n",
    "        if format == \"arrow\":\n",
    "            buf = pa.serialize(data).to_buffer()\n",
    "            f.write(buf)\n",
    "        elif format == \"pickle\":\n",
    "            pickle.dump(data, f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def do_save_trenches(trenches, filename, overwrite=True):\n",
    "    trenches = pd.concat(trenches)\n",
    "    processing.write_dataframe_to_parquet(\n",
    "        filename, trenches, merge=False, overwrite=overwrite\n",
    "    )\n",
    "    return trenches\n",
    "\n",
    "\n",
    "def do_measure_and_write(trenches, frames, return_none=True, write=True, **kwargs):\n",
    "    if trenches is None:\n",
    "        return None\n",
    "    trenches = filter_trenches(trenches)\n",
    "    res = measure(trenches, frames, **kwargs)\n",
    "    if write:\n",
    "        processing.write_images_and_measurements(\n",
    "            res,\n",
    "            filename_func=filename_func,\n",
    "            dataframe_format=\"parquet\",\n",
    "            write_images=True,\n",
    "            write_measurements=True,\n",
    "        )\n",
    "    if return_none:\n",
    "        return None\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_trench_err_futures = {}\n",
    "all_analysis_futures = {}\n",
    "save_trenches_futures = {}\n",
    "save_trench_err_futures = {}\n",
    "\n",
    "all_trench_bboxes_futures = {}  # TODO: just for debugging\n",
    "\n",
    "for filename, filename_frames in selected_frames.groupby(\"filename\"):\n",
    "    # analysis_futures = {}\n",
    "    trench_bboxes_futures = {}\n",
    "    trench_err_futures = {}\n",
    "    for position, frames in filename_frames.groupby(\"position\"):\n",
    "        key = (filename, position)\n",
    "        frame_to_segment = frames.loc[\n",
    "            IDX[:, :, [\"MCHERRY\"], 0], :\n",
    "        ]  # TODO: make pluggable\n",
    "        trenches_future = client.submit(\n",
    "            do_find_trenches, *frame_to_segment.index[0], priority=10\n",
    "        )\n",
    "        trench_err_futures[key] = client.submit(do_get_trench_err, trenches_future)\n",
    "        trench_bboxes_future = client.submit(\n",
    "            do_trenches_to_bboxes, trenches_future, (filename, position), priority=10\n",
    "        )\n",
    "        trench_bboxes_futures[key] = trench_bboxes_future\n",
    "        all_trench_bboxes_futures[key] = trench_bboxes_future\n",
    "        analysis_future = client.submit(\n",
    "            do_measure_and_write,\n",
    "            trench_bboxes_future,\n",
    "            frames,\n",
    "            measurement_func=_measurement_func,\n",
    "            # measurement_func=None,\n",
    "            # segmentation_func=None,\n",
    "            return_none=True,\n",
    "            write=True,\n",
    "            priority=-10,\n",
    "        )\n",
    "        # analysis_futures[key] = analysis_future\n",
    "        all_analysis_futures[key] = analysis_future\n",
    "    # save trenches\n",
    "    trenches_filename = filename_func(\n",
    "        kind=\"trenches\", extension=\"parquet\", filename=filename\n",
    "    )\n",
    "    save_trenches_futures[filename] = client.submit(\n",
    "        do_save_trenches,\n",
    "        list(dict(sorted(trench_bboxes_futures.items())).values()),\n",
    "        trenches_filename,\n",
    "        priority=100,\n",
    "    )\n",
    "    trench_errs_filename = filename_func(\n",
    "        kind=\"trench_errs\", extension=\"pickle\", filename=filename\n",
    "    )\n",
    "    save_trench_err_futures[filename] = client.submit(\n",
    "        do_serialize_to_disk, trench_err_futures, trench_errs_filename, priority=100\n",
    "    )\n",
    "# OPTIONALLY: stream analysis to master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, trench_bboxes_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = do_find_trenches(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    "    35,\n",
    "    \"MCHERRY\",\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1][\"label_1.find_trench_lines.hough_2.peak_func.pitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tt = do_trenches_to_bboxes(\n",
    "    t, (\"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\", 35)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(save_trenches_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "do_find_trenches(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    "    1,\n",
    "    \"MCHERRY\",\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc[\n",
    "    IDX[(\"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\", 1)],\n",
    "    :,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(util.get_one(all_analysis_futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in all_analysis_futures.items() if v.status != \"pending\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analysis_futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, all_analysis_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(all_analysis_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = util.get_one(client.gather(save_trenches_futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = a[(\"diag\", \"find_trench_lines.hough_2.peak_func.pitch\")]\n",
    "z[z < 50].plot.hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = a[(\"info\", \"hough_value\")]\n",
    "z.plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(save_trench_err_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory profiling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshot():\n",
    "    import tracemalloc\n",
    "\n",
    "    return tracemalloc.take_snapshot()\n",
    "\n",
    "\n",
    "snaps = client.run(get_snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = util.get_one(snaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot2 = list(reversed(sorted(snapshot.statistics(\"lineno\"), key=lambda x: x.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat in snapshot2[:10]:\n",
    "    print(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _measure_and_write(trenches, all_frames.iloc[:1], return_none=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    a[(\"/n/scratch2/jqs1/fidelity/all/TrErr.nd2\", 0)][\"images\"][\"segmentation\"][2][9][\n",
    "        \"MCHERRY\"\n",
    "    ][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f _measure_and_write -f processing.write_images_and_measurements -f processing._get_trench_crops -f trench_segmentation.watershed.segment_trench -f processing.write_images_to_zarr _measure_and_write(trenches, all_frames.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mprun -f _get_trench_bboxes -f trenches_to_bboxes -f trench_info_to_dataframe -f trench_diag_to_dataframe -f workflow.get_nd2_frame -f trench_detection.find_trenches\n",
    "trenches = _get_trench_bboxes(*all_frames.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches = _get_trench_bboxes(*all_frames.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trenches_futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, all_trenches_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_one(all_trenches_futures).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analysis_futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_one(all_analysis_futures).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    list, all_analysis_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, all_analysis_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trenches = util.get_one(save_trenches_futures).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trenches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.apply_map_futures(\n",
    "    client.gather, all_analysis_futures, predicate=lambda x: x.status == \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ac = distributed.as_completed([], with_results=False, loop=client.loop)\n",
    "\n",
    "new_futures_stream = streamz.Stream()\n",
    "finished_futures_stream = streamz.Stream(asynchronous=True, loop=client.loop)\n",
    "\n",
    "stream_sinks = {}\n",
    "stream_writers = {}\n",
    "output_filename = \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_{}.arrow\"\n",
    "\n",
    "new_futures_stream.sink(lambda x: ac.add(x))\n",
    "\n",
    "errored_futures = set()\n",
    "finished_futures_stream.filter(lambda x: x.status == \"error\").sink(\n",
    "    lambda x: errored_futures.add(x)\n",
    ")\n",
    "\n",
    "\n",
    "def timeout_func(futures):\n",
    "    ac.update(futures)\n",
    "\n",
    "\n",
    "successful_futures_stream = finished_futures_stream.filter(\n",
    "    lambda x: x.status == \"finished\"\n",
    ")\n",
    "# batched_futures_stream = successful_futures_stream.rate_limit(0.0004).timed_window(1)\n",
    "# gathered_futures_stream = streamz.buffer(batched_futures_stream, 10).gather_and_cancel(client=client, cancel=True)\n",
    "# batched_futures_stream = successful_futures_stream.timed_window(1)\n",
    "batched_futures_stream = successful_futures_stream.rate_limit(0.01).timed_window(5)\n",
    "buffered_futures_stream = (\n",
    "    batched_futures_stream  # streamz.buffer(batched_futures_stream, 10)\n",
    ")\n",
    "\n",
    "cancelled_futures = set()\n",
    "gathered_futures_stream = buffered_futures_stream.gather_and_cancel(\n",
    "    client=client,\n",
    "    gather=True,\n",
    "    cancel=True,\n",
    "    timeout=4,\n",
    "    timeout_func=timeout_func,\n",
    "    success_func=cancelled_futures.update,\n",
    ")\n",
    "# gathered_futures_stream.flatten().sink(partial(workflow.sink_to_arrow, sinks=stream_sinks, writers=stream_writers, output_func=lambda i: pa.OSFile(output_filename.format(i), 'w')))\n",
    "write_failures = []\n",
    "flattened_futures_stream = gathered_futures_stream.flatten()\n",
    "writer_stream = (\n",
    "    flattened_futures_stream  # .timed_window(10).map(lambda x: list(zip(*x)))\n",
    ")\n",
    "sink_func = partial(\n",
    "    workflow.sink_to_arrow,\n",
    "    sinks=stream_sinks,\n",
    "    writers=stream_writers,\n",
    "    output_func=lambda i: pa.OSFile(output_filename.format(i), \"w\"),\n",
    ")\n",
    "# sink_func = partial(client.loop.run_in_executor, None, sink_func)\n",
    "# writer_stream.with_timeout(timeout=3, retries=2, failure_func=write_failures.append).sink(sink_func)\n",
    "writer_stream.sink(sink_func)\n",
    "# stored_data = writer_stream.sink_to_list()\n",
    "\n",
    "# finished_futures_stream.sink(excepts(StopIteration, lambda x: new_futures_stream.emit(next(analysis_futures_iter)) if should_add_task()))\n",
    "# new_futures_stream.sink_to_list()\n",
    "all_futures = set()\n",
    "finished_futures = set()\n",
    "new_futures_stream.sink(lambda x: all_futures.add(x))\n",
    "successful_futures_stream.sink(lambda x: finished_futures.add(x))\n",
    "\n",
    "TASK_BUFFER_SIZE = 10000\n",
    "\n",
    "# def should_add_task():\n",
    "#     #return len([f for f in all_futures if f.status == 'pending'])\n",
    "#     #return TASK_BUFFER_SIZE > len(all_futures - finished_futures)\n",
    "#     return TASK_BUFFER_SIZE > len(all_futures - cancelled_futures)\n",
    "#     #print('>',len(all_futures - finished_futures))\n",
    "#     #return True\n",
    "\n",
    "# def readd_task(x):\n",
    "#     if should_add_task():\n",
    "#         return new_futures_stream.emit(next(analysis_futures_iter))\n",
    "\n",
    "\n",
    "def readd_task(x):\n",
    "    num_tasks_needed = TASK_BUFFER_SIZE - len(\n",
    "        all_futures - cancelled_futures - errored_futures\n",
    "    )\n",
    "    if num_tasks_needed > 0:\n",
    "        for future in take(num_tasks_needed, analysis_futures_iter):\n",
    "            new_futures_stream.emit(future)\n",
    "\n",
    "\n",
    "finished_futures_stream.sink(excepts(StopIteration, readd_task))\n",
    "\n",
    "# ac.update(take(3000, analysis_futures_iter))\n",
    "for future in take(TASK_BUFFER_SIZE, analysis_futures_iter):\n",
    "    new_futures_stream.emit(future)\n",
    "\n",
    "gather_func = workflow.gather_stream(finished_futures_stream, ac)\n",
    "gather_task = client.loop.asyncio_loop.create_task(gather_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad trenches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.trenches.parquet\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.sqrt(\n",
    "    (trenches[(\"top\", \"x\")] - trenches[(\"bottom\", \"x\")]) ** 2\n",
    "    + (trenches[(\"top\", \"y\")] - trenches[(\"bottom\", \"y\")]) ** 2\n",
    ")\n",
    "pixels = (trenches[(\"upper_left\", \"x\")] - trenches[(\"lower_right\", \"x\")]) * (\n",
    "    trenches[(\"upper_left\", \"y\")] - trenches[(\"lower_right\", \"y\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length.plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels.plot.hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = IDX[\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\", 2, 1, 92:\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trench_subset = trenches.loc[slc, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = processing._get_trench_crops(\n",
    "    trench_subset,\n",
    "    all_frames.loc[IDX[:, 2, :, :2], :],\n",
    "    include_frame=True,\n",
    "    frame_transformation=None,\n",
    "    filename=\"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    "    position=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.flatten_dict(util.unflatten_dict(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_trench_bboxes(trench_subset, image_limits)  # .loc[slc,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_trench_bboxes(trenches.loc[IDX[:, 2], :], image_limits).loc[slc, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches.loc[\n",
    "    IDX[\n",
    "        \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    "        2,\n",
    "        1,\n",
    "        92:,\n",
    "    ],\n",
    "    :,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenches.loc[\n",
    "    IDX[\n",
    "        \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    "        2,\n",
    "        1,\n",
    "        96,\n",
    "    ],\n",
    "    :,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ims = zarr.open_group(store=zarr.LMDBStore('/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.images/pos1157.zarr'))\n",
    "# ims = zarr.open_group(store=zarr.LMDBStore('/n/scratch2/jqs1/fidelity/all/TrErr.stripe-1.256m.nd2.images/pos0.zarr'))\n",
    "ims = zarr.open_group(\n",
    "    store=zarr.LMDBStore(\n",
    "        \"/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2.images/pos110.zarr/\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims[\"raw/1\"].tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ims[\"raw\"][\"_frame\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = np.asarray(ims[\"raw\"][1][60][\"MCHERRY\"])[:, :, :30]\n",
    "plt.figure(figsize=(40, 5))\n",
    "plt.imshow(np.hstack([ary[:, :, t] for t in range(ary.shape[2])]), cmap=\"OrRd_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = np.asarray(ims[\"raw\"][1][58][\"YFP\"])[:, :, :30]\n",
    "plt.figure(figsize=(40, 5))\n",
    "plt.imshow(np.hstack([ary[:, :, t] for t in range(ary.shape[2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = np.asarray(ims[\"raw\"][1][58][\"BF\"])[:, :, :30]\n",
    "plt.figure(figsize=(40, 5))\n",
    "plt.imshow(np.hstack([ary[:, :, t] for t in range(ary.shape[2])]), cmap=\"gist_yarg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 40))\n",
    "# frame = workflow.get_nd2_frame('/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.nd2', 15, 'YFP', 37)\n",
    "# frame = workflow.get_nd2_frame('/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.nd2', 1130, 'YFP', 47)\n",
    "frame = workflow.get_nd2_frame(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/TrErr002_Exp.nd2\", 100, \"MCHERRY\", 47\n",
    ")\n",
    "# frame = workflow.get_nd2_frame('/n/scratch2/jqs1/fidelity/all/', 1130, 'MCHERRY', 47)\n",
    "plt.imshow(frame, cmap=\"OrRd_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[\"raw\"][1][40][\"BF\"][:, :, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[\"raw\"][\"_frame\"][\"MCHERRY\"][:, :, -10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ims[\"raw\"][2][60].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[\"raw\"][2][60][\"MCHERRY\"][:, :, -10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[\"raw\"][(1, 96)][\"MCHERRY\"][:, :, -10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nbytes = 0\n",
    "total_nbytes_stored = 0\n",
    "total_pixels = 0\n",
    "all_pixels = []\n",
    "\n",
    "\n",
    "def func(name, x):\n",
    "    if name.startswith(\"_frame\"):\n",
    "        return\n",
    "    global total_nbytes, total_nbytes_stored, total_pixels\n",
    "    if isinstance(x, zarr.Array):\n",
    "        total_nbytes += x.nbytes\n",
    "        total_nbytes_stored += x.nbytes_stored\n",
    "        pixels = reduce(operator.mul, x.shape, 1)\n",
    "        total_pixels += pixels\n",
    "        all_pixels.append(pixels)\n",
    "        if pixels > 1e7:\n",
    "            print(\">>\", name)\n",
    "\n",
    "\n",
    "dict_root[\"raw\"].visititems(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    total_nbytes,\n",
    "    total_nbytes_stored,\n",
    "    total_nbytes / total_nbytes_stored,\n",
    "    total_pixels * 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_pixels, bins=100, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels / ((2048**2) * 66 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((2048**2) * 1404 * 66 * 2 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dict_root[\"raw/(1, 0)/MCHERRY\"][:, :, 0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dict_root[\"raw/(1, 1)/MCHERRY\"][:, :, 0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_root[\"raw/(1, 0)/MCHERRY\"].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.copy_store?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get(\"raw/(1, 0)/MCHERRY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements/pos50.labelwise.parquet\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements/pos50.mask_labelwise.parquet\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[(\"YFP\", \"noerode\", \"mean\")].groupby([\"trench_set\", \"trench\", \"t\"]).count().plot.hist(\n",
    "    bins=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[b.index.get_level_values(\"label\") != 0][(\"mask\", \"noerode\", \"size\")].plot.hist(\n",
    "    bins=100, log=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labelwise_df = data_io.read_parquet(\n",
    "    \"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements/pos1050.labelwise.parquet\"\n",
    ")\n",
    "mask_labelwise_df = data_io.read_parquet(\n",
    "    \"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements/pos0.mask_labelwise.parquet\"\n",
    ")\n",
    "trenchwise_df = data_io.read_parquet(\n",
    "    \"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements/pos0.trenchwise.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent import futures\n",
    "from util import tqdm_auto\n",
    "\n",
    "\n",
    "def _load_measurements(base_filename):\n",
    "    labelwise_filename = base_filename + \".labelwise.parquet\"\n",
    "    if not os.path.exists(labelwise_filename):\n",
    "        return\n",
    "    # labelwise_df = data_io.read_parquet(labelwise_filename, progress_bar=None)\n",
    "    # open(labelwise_filename, 'rb').read()\n",
    "    with pa.OSFile(labelwise_filename, \"rb\") as f:\n",
    "        # res = f.read()\n",
    "        # buf = f.read()\n",
    "        # buf = f.read_buffer()\n",
    "        res = pq.ParquetFile(f).read(use_pandas_metadata=True, use_threads=False)\n",
    "        # res = res.to_pandas()\n",
    "        # res = data_io.read_parquet(buf, progress_bar=None)\n",
    "        # res = pq.read_pandas(buf)\n",
    "        # res = data_io.read_parquet(f.read_buffer(), progress_bar=None)\n",
    "    return\n",
    "    # mask_labelwise_df = data_io.read_parquet(base_filename+'.mask_labelwise.parquet')\n",
    "    # trenchwise_df = data_io.read_parquet(base_filename+'.trenchwise.parquet')\n",
    "    # return labelwise_df#.to_pandas()\n",
    "\n",
    "\n",
    "def load_measurements(filename, nthreads=False, progress_bar=tqdm_auto):\n",
    "    tasks = []\n",
    "    for i in range(100):\n",
    "        base_filename = os.path.join(filename, \"pos{:d}\".format(i))\n",
    "        tasks.append((_load_measurements, base_filename))\n",
    "    if nthreads:\n",
    "        ex = futures.ThreadPoolExecutor(max_workers=nthreads)\n",
    "        completed_tasks = futures.as_completed([ex.submit(*t) for t in tasks])\n",
    "        if progress_bar is not None:\n",
    "            completed_tasks = progress_bar(completed_tasks, total=len(tasks))\n",
    "        for future in completed_tasks:\n",
    "            res = future.result()\n",
    "            # print('>',res.iloc[0])\n",
    "    else:\n",
    "        if progress_bar is not None:\n",
    "            tasks = progress_bar(tasks, total=len(tasks))\n",
    "        for t in tasks:\n",
    "            res = t[0](*t[1:])\n",
    "    return\n",
    "\n",
    "\n",
    "load_measurements(\n",
    "    \"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent import futures\n",
    "from util import tqdm_auto\n",
    "\n",
    "\n",
    "def _load_measurements(base_filename, kind):\n",
    "    labelwise_filename = base_filename + \".{}.parquet\".format(kind)\n",
    "    if not os.path.exists(labelwise_filename):\n",
    "        return\n",
    "    with pa.OSFile(labelwise_filename, \"rb\") as f:\n",
    "        res = pq.ParquetFile(f).read(use_pandas_metadata=True, use_threads=False)\n",
    "    res = res.to_pandas()\n",
    "    return res\n",
    "    # labelwise_df = data_io.read_parquet(labelwise_filename, progress_bar=None)\n",
    "    # mask_labelwise_df = data_io.read_parquet(base_filename+'.mask_labelwise.parquet')\n",
    "    # trenchwise_df = data_io.read_parquet(base_filename+'.trenchwise.parquet')\n",
    "    # return labelwise_df#.to_pandas()\n",
    "\n",
    "\n",
    "def load_measurements(\n",
    "    parquet_filename, kind, nthreads=False, progress_bar=tqdm_auto, filename=None\n",
    "):\n",
    "    positions = range(100)\n",
    "    if progress_bar is not None:\n",
    "        positions = progress_bar(positions, total=len(positions))\n",
    "    res = {}\n",
    "    for pos in positions:\n",
    "        key = (filename, pos)\n",
    "        base_filename = os.path.join(parquet_filename, \"pos{:d}\".format(pos))\n",
    "        res[key] = _load_measurements(base_filename, kind)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_dfs = load_measurements(\n",
    "    \"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements\",\n",
    "    \"labelwise\",\n",
    "    filename=\"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    ")\n",
    "mask_labelwise_dfs = load_measurements(\n",
    "    \"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2.measurements\",\n",
    "    \"mask_labelwise\",\n",
    "    filename=\"/home/jqs1/scratch/fidelity/all/180928_txnerr_bigsnake.stripe-1.256m.nd2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labelwise_df = pd.concat(labelwise_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mask_labelwise_df = pd.concat(mask_labelwise_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_labelwise_df.index = labelwise_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labelwise_df = pd.concat([labelwise_df, mask_labelwise_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "framewise_df = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_0.sorted.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trenchwise_df = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_1.sorted.parquet4\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenchwise_df.columns = [\"/\".join(col).strip() for col in trenchwise_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"filename\",\n",
    "    \"position\",\n",
    "    \"channel\",\n",
    "    \"t\",\n",
    "    \"trench_set\",\n",
    "    \"trench\",\n",
    "    \"label\",\n",
    "    \"('YFP', 'labelwise', 'p0.9')\",\n",
    "    \"('MCHERRY', 'labelwise', 'p0.9')\",\n",
    "    \"('YFP', 'regionprops', 'area')\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labelwise_df = data_io.read_parquet(\n",
    "    \"/n/scratch2/jqs1/fidelity/all/output/analysis_full_stream11_2.sorted3.parquet4\",\n",
    "    columns=cols,\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: otherwise computing is_unique is costly when we want to get_loc with full key\n",
    "labelwise_df.index.__dict__[\"_cache\"] = {\"lexsort_depth\": 6, \"is_unique\": True}\n",
    "# labelwise_df.index.lexsort_depth # prime the cache\n",
    "# if '_cache' not in labelwise_df.index.__dict__:\n",
    "#     labelwise_df.index.__dict__['_cache'] = {}\n",
    "# labelwise_df.index._cache['is_unique'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_df.columns = [\"/\".join(col).strip() for col in labelwise_df.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfp = \"YFP/labelwise/p0.9\"\n",
    "mcherry = \"MCHERRY/labelwise/p0.9\"\n",
    "area = \"YFP/regionprops/area\"\n",
    "trench_key = [\"filename\", \"position\", \"trench_set\", \"trench\"]\n",
    "trench_t_key = [\"filename\", \"position\", \"trench_set\", \"trench\", \"t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# labelwise_selected = labelwise_df.loc[IDX['/n/scratch2/jqs1/fidelity/all/TrErr002_noBF.nd2',:],:]\n",
    "# labelwise_selected = labelwise_df.loc[IDX['/n/scratch2/jqs1/fidelity/all/180405_txnerr_loweronly_fast.nd2',:],:]\n",
    "# labelwise_selected = labelwise_df.loc[IDX['/n/scratch2/jqs1/fidelity/all/180405_txnerr002.nd2.nd2',:],:]\n",
    "labelwise_selected = labelwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelwise_df) / len(labelwise_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = 'YFP/p0.3'\n",
    "# trenchwise_yfp_bg = trenchwise_df[col].rename(col+'_trenchwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelwise_selected.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "background = labelwise_selected.loc[IDX[:, :, :, :, :, 0], [yfp, mcherry]]\n",
    "background.index = background.index.droplevel(\"label\")\n",
    "background.columns = [c + \"_bg\" for c in background.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cell_sized = labelwise_selected[labelwise_selected[area].between(100, 200)].loc[\n",
    "    IDX[:, :, :, :, :, 1:], :\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_t_median = cell_sized.groupby(trench_t_key).median()\n",
    "trench_t_median.columns = [c + \"_trench_t_median\" for c in trench_t_median.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trench_median = cell_sized.groupby(trench_key).median()\n",
    "trench_median.columns = [c + \"_trench_median\" for c in trench_median.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with_bg = util.multi_join(cell_sized, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with_bg = util.multi_join(with_bg, trench_t_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with_bg = util.multi_join(with_bg, trench_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bright_ts_median_t = pd.DataFrame(\n",
    "    {\n",
    "        \"bright_ts_median_t_{}\".format(thresh): (\n",
    "            (with_bg[yfp] - with_bg[yfp + \"_trench_t_median\"]) >= thresh\n",
    "        )\n",
    "        .groupby(trench_key)\n",
    "        .sum()\n",
    "        for thresh in (\n",
    "            5,\n",
    "            8,\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            50,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bright_ts_median = pd.DataFrame(\n",
    "    {\n",
    "        \"bright_ts_median_{}\".format(thresh): (\n",
    "            (with_bg[yfp] - with_bg[yfp + \"_trench_median\"]) >= thresh\n",
    "        )\n",
    "        .groupby(trench_key)\n",
    "        .sum()\n",
    "        for thresh in (\n",
    "            5,\n",
    "            8,\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            50,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bright_ts_bg = pd.DataFrame(\n",
    "    {\n",
    "        \"bright_ts_bg_{}\".format(thresh): (\n",
    "            (with_bg[yfp] - with_bg[yfp + \"_bg\"]) >= thresh\n",
    "        )\n",
    "        .groupby(trench_key)\n",
    "        .sum()\n",
    "        for thresh in (\n",
    "            5,\n",
    "            8,\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            50,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bright_ts_median_t[bright_ts_median_t[\"bright_ts_median_t_50\"] > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_ts_median_t[bright_ts_median_t[\"bright_ts_median_t_5\"] > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median_bg = background.groupby(trench_key).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_ts_all = util.multi_join(\n",
    "    util.multi_join(\n",
    "        util.multi_join(bright_ts_median_t, bright_ts_median), bright_ts_bg\n",
    "    ),\n",
    "    median_bg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cell_sized_with_ts = util.multi_join(cell_sized, bright_ts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "detected_bursts = cell_sized_with_ts[cell_sized_with_ts[\"bright_ts_median_t_20\"] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(detected_bursts.groupby(trench_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "detected_bursts2 = cell_sized_with_ts[cell_sized_with_ts[\"bright_ts_bg_20\"] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(detected_bursts2.groupby(trench_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_sized_with_ts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cell_sized_with_ts[yfp + \"_bg\"]\n",
    "x[x < 180].hist(bins=50, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# detected_bursts3 = cell_sized_with_ts[(cell_sized_with_ts[yfp+'_bg'] <= 200) & (cell_sized_with_ts['bright_ts_bg_20'] >= 2)]\n",
    "# detected_bursts3 = cell_sized_with_ts[(cell_sized_with_ts[yfp+'_bg'].between(120, 130)) & (cell_sized_with_ts['bright_ts_bg_20'] >= 1)]\n",
    "detected_bursts3 = cell_sized_with_ts[\n",
    "    (cell_sized_with_ts[\"bright_ts_median_t_50\"] >= 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detected_bursts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(detected_bursts3.groupby(trench_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelStream = ui.MultiIndexStream.define(\"LabelStream\", labelwise_df.index)\n",
    "label_stream = LabelStream()\n",
    "box = ui.dataframe_browser(label_stream)\n",
    "label_stream.event()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "%%opts Layout [normalize=False]\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        (\"(x,y)\", \"(@x{0[.]0}, @y{0[.]0})\"),\n",
    "        (\"value\", \"@z\"),\n",
    "    ]\n",
    ")\n",
    "# cb = compose(partial(ui.hover_image, hover), ui._trench_img, workflow.get_trench_image)\n",
    "cb = lambda v_max: compose(\n",
    "    partial(ui.hover_image, hover),\n",
    "    lambda x: x.redim.range(z=(0, v_max)),\n",
    "    ui._trench_img,\n",
    "    workflow.get_trench_image,\n",
    ")\n",
    "# cb = workflow.get_trench_image\n",
    "(\n",
    "    ui.trench_viewer(\n",
    "        trench_bboxes, label_stream, channel=\"MCHERRY\", image_callback=cb(5000)\n",
    "    )\n",
    "    + ui.trench_viewer(\n",
    "        trench_bboxes, label_stream, channel=\"YFP\", image_callback=cb(400)\n",
    "    )\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = detected_bursts3.groupby(trench_key)\n",
    "group_set_keys = list(util.grouper(groups.groups.keys(), 5))\n",
    "group_index = pd.MultiIndex.from_tuples([(i,) for i in range(len(group_set_keys))])\n",
    "group_index.names = [\"group_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupStream = ui.MultiIndexStream.define(\"GroupStream\", group_index)\n",
    "group_stream = GroupStream()\n",
    "group_box = ui.dataframe_browser(group_stream)\n",
    "group_stream.event()\n",
    "group_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "sel = Selection1D()\n",
    "\n",
    "\n",
    "def callback(group_set):\n",
    "    df = pd.concat([groups.get_group(key) for key in group_set_keys[group_set]])\n",
    "    plot = hv.Scatter(\n",
    "        df,\n",
    "        kdims=[\"t\"],\n",
    "        vdims=[\n",
    "            \"YFP/labelwise/p0.9\",\n",
    "            \"filename\",\n",
    "            \"position\",\n",
    "            \"trench_set\",\n",
    "            \"trench\",\n",
    "            \"label\",\n",
    "        ],\n",
    "    )\n",
    "    tooltips = [\n",
    "        (\"t\", \"@t{0[.]0}\"),\n",
    "        # ('filename', '@filename'),\n",
    "        (\"trench\", \"@position.@trench_set.@trench\"),\n",
    "        (\"label\", \"@label\"),\n",
    "        (\"YFP\", \"@{YFP/labelwise/p0.9}{0[.]0}\"),\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=tooltips)\n",
    "    tap = TapTool()\n",
    "    plot = plot.options(\n",
    "        \"Scatter\",\n",
    "        size=3,\n",
    "        color_index=\"trench\",\n",
    "        nonselection_alpha=0.3,\n",
    "        cmap=\"Category20\",\n",
    "        tools=[hover, tap],\n",
    "        show_legend=True,\n",
    "    )\n",
    "    # ui.selection_to_stream(plot, label_stream)\n",
    "    sel.clear()\n",
    "    sel.add_subscriber(\n",
    "        partial(\n",
    "            ui._selection_to_stream_callback,\n",
    "            data=plot.data,\n",
    "            keys=df.index.names,\n",
    "            stream=label_stream,\n",
    "        )\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "p = hv.DynamicMap(callback, streams=[group_stream])\n",
    "sel.source = p\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "#%%opts Layout [normalize=False fig_inches=2 vspace=0 aspect_weight=1 sublabel_format='' tight=True title_format=\"{filename:}\\npos: {position:} trench: {trench_set:}.{trench:} t: {t:}\".format(**label_stream.contents) fontsize=20]\n",
    "#%%opts Scatter [aspect=6]\n",
    "key = tuple(getattr(label_stream, attr) for attr in trench_key)\n",
    "index = detected_bursts.groupby(trench_key).get_group(key).index\n",
    "ts = index._get_level_values(index._get_level_number(\"t\"), unique=True)\n",
    "# ts = list(range(3))\n",
    "\n",
    "movie = (\n",
    "    trench_movie(trench_bboxes, key, \"MCHERRY\", ts)\n",
    "    + trench_movie(trench_bboxes, key, \"YFP\", ts)\n",
    "    + scatter_movie(labelwise_df, label_stream.contents, ts)\n",
    "    * hv.HoloMap(\n",
    "        {t: hv.VLine(t).options(color=\"red\", backend=\"matplotlib\") for t in ts}\n",
    "    )\n",
    ").cols(1)\n",
    "movie2 = movie.options(\n",
    "    {\n",
    "        \"Layout\": dict(\n",
    "            normalize=False,\n",
    "            framewise=True,\n",
    "            fig_inches=7,\n",
    "            vspace=0,\n",
    "            aspect_weight=1,\n",
    "            sublabel_format=\"\",\n",
    "            tight=False,\n",
    "            fontsize=15,\n",
    "            title_format=\"{filename:}\\npos: {position:} trench: {trench_set:}.{trench:} t: {t:}\".format(\n",
    "                **label_stream.contents\n",
    "            ),\n",
    "        ),\n",
    "        \"Scatter\": dict(aspect=6, s=20),\n",
    "    },\n",
    "    backend=\"matplotlib\",\n",
    ")\n",
    "m = holomap_to_video(movie2, out=\"/tmp/jqsmovie.mp4\", size=100, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"/tmp/jqsmovie.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "%%opts Layout [normalize=False fig_inches=2 vspace=0 aspect_weight=1 sublabel_format='' tight=True title_format=\"{filename:}\\npos: {position:} trench: {trench_set:}.{trench:} t: {t:}\".format(**label_stream.contents) fontsize=20]\n",
    "%%opts Scatter [aspect=6]\n",
    "key = tuple(getattr(label_stream, attr) for attr in trench_key)\n",
    "index = detected_bursts.groupby(trench_key).get_group(key).index\n",
    "# ts = index._get_level_values(index._get_level_number('t'), unique=True)\n",
    "ts = list(range(3))\n",
    "\n",
    "movie = (\n",
    "    trench_movie(trench_bboxes, key, \"MCHERRY\", ts)\n",
    "    + trench_movie(trench_bboxes, key, \"YFP\", ts)\n",
    "    + scatter_movie(labelwise_df, label_stream.contents, ts)\n",
    "    * hv.HoloMap(\n",
    "        {t: hv.VLine(t).options(color=\"red\", backend=\"matplotlib\") for t in ts}\n",
    "    )\n",
    ").cols(\n",
    "    1\n",
    ")  # .options('Layout', normalize=False)\n",
    "m = holomap_to_video(movie, out=\"/tmp/jqsmovie.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=180\n",
    "def cb(**kwargs):\n",
    "    df = workflow.select_dataframe(\n",
    "        labelwise_df, kwargs, t=slice(None), label=slice(None)\n",
    "    )\n",
    "    # df = workflow.select_dataframe(labelwise_df, kwargs, label=slice(None))\n",
    "    plot = hv.Scatter(\n",
    "        df,\n",
    "        kdims=[\"t\"],\n",
    "        vdims=[\n",
    "            \"YFP/labelwise/p0.9\",\n",
    "            \"filename\",\n",
    "            \"position\",\n",
    "            \"trench_set\",\n",
    "            \"trench\",\n",
    "            \"label\",\n",
    "        ],\n",
    "    )\n",
    "    tooltips = [\n",
    "        (\"t\", \"@t{0[.]0}\"),\n",
    "        # ('filename', '@filename'),\n",
    "        (\"trench\", \"@position.@trench_set.@trench\"),\n",
    "        (\"label\", \"@label\"),\n",
    "        (\"YFP\", \"@{YFP/labelwise/p0.9}{0[.]0}\"),\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=tooltips)\n",
    "    plot = plot.options(\n",
    "        \"Scatter\",\n",
    "        size=3,\n",
    "        color_index=\"label\",\n",
    "        nonselection_alpha=0.3,\n",
    "        cmap=\"Category20\",\n",
    "        tools=[hover, \"tap\"],\n",
    "        show_legend=True,\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "ui.viewer(cb, label_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
