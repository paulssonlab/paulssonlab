{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import regrid\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "import param\n",
    "import pickle\n",
    "import os\n",
    "from scipy.integrate import simps\n",
    "import segmentation\n",
    "from matriarch_stub import RevImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select file\n",
    "# checkboxes: positions\n",
    "# select: var estimator\n",
    "# select: mean/median/p.095\n",
    "# double-slider: size filtering (on histogram?)\n",
    "# slider: bin size\n",
    "# slider: threshold\n",
    "# plot: downsampled log traces (checkbox: normalized to t0)\n",
    "# plot: var as a function of p\n",
    "# plot: integral as a function of p\n",
    "# plot: spatial (select: color dimension: initial intensity, etc.)\n",
    "# plot: counts per bin\n",
    "# plot: regridded image+seg mask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = \"/n/groups/paulsson/jqs1/molecule-counting\"\n",
    "filename = \"190304photobleaching.pickle\"\n",
    "# filename = '190328photobleaching_flatcorr.pickle'\n",
    "with open(os.path.join(base_filename, filename), \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "# buf = pa.serialize(data).to_buffer()\n",
    "# with pa.OSFile(os.path.join(base_filename, '190328photobleaching_flatcorr.arrow'), 'wb') as f:\n",
    "#    f.write(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fluctuation scatter plot/merge fluct+int plots\n",
    "\n",
    "# colors: initial fluorescence, initial fluorescence bin, area, fluorescence bin gain\n",
    "# plot update dependencies/cache computations\n",
    "\n",
    "# fast simpson integration/benchmark\n",
    "# midweight_bivariance\n",
    "\n",
    "# check integral values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trace_plot(traces, downsample=10, normalize=True, colors=None):\n",
    "    if colors is None:\n",
    "        colors = np.random.permutation(traces.shape[0])\n",
    "        cmap = \"Category20\"\n",
    "    else:\n",
    "        cmap = \"inferno\"\n",
    "    y = traces\n",
    "    if normalize:\n",
    "        y = y / y[:, 0, np.newaxis]\n",
    "    curves = [\n",
    "        {\"x\": np.arange(traces.shape[1]), \"y\": y[i], \"i\": colors[i]}\n",
    "        for i in range(traces.shape[0] // downsample)\n",
    "    ]\n",
    "    plot = hv.Contours(curves, vdims=[\"i\"]).options(\n",
    "        color_index=\"i\", cmap=cmap, logy=True\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "def _fluctuation_plots(pbar, y, qs, nu_qs, colors=None):\n",
    "    if colors is None:\n",
    "        colors = np.random.permutation(y.shape[0])\n",
    "        cmap = \"Category20\"\n",
    "    else:\n",
    "        cmap = \"inferno\"\n",
    "    nus = nu_qs.iloc[:, -1]\n",
    "    fluctuation_curves = [\n",
    "        {\"x\": 1 - pbar.values[i], \"y\": y.values[i], \"i\": colors[i]}\n",
    "        for i in range(y.shape[0])\n",
    "    ]\n",
    "    fluctuation_plot = hv.Contours(fluctuation_curves, vdims=[\"i\"]).options(\n",
    "        color_index=\"i\", cmap=cmap\n",
    "    )\n",
    "    integral_curves = [\n",
    "        {\"x\": qs, \"y\": nu_qs.values[i], \"i\": colors[i]} for i in range(nu_qs.shape[0])\n",
    "    ]\n",
    "    integral_plot = (\n",
    "        hv.Contours(integral_curves, vdims=[\"i\"])\n",
    "        .redim(y=\"integral\")\n",
    "        .options(color_index=\"i\", cmap=cmap)\n",
    "    )\n",
    "    plots = fluctuation_plot, integral_plot\n",
    "    return plots\n",
    "\n",
    "\n",
    "def _spatial_plots(regionprops, labels, img, colors=None):\n",
    "    if colors is None:\n",
    "        colors = np.random.permutation(regionprops.shape[0])\n",
    "        cmap = \"Category20\"\n",
    "    else:\n",
    "        cmap = \"inferno\"\n",
    "    regionprops[\"x\"] = regionprops[\"centroid_x\"]\n",
    "    regionprops[\"y\"] = regionprops[\"centroid_y\"]\n",
    "    regionprops[\"color\"] = colors\n",
    "    xy_plot = hv.Points(regionprops, kdims=[\"x\", \"y\"], vdims=[\"color\"]).options(\n",
    "        color=hv.dim(\"color\"), cmap=cmap\n",
    "    )\n",
    "    image_plot = regrid(RevImage(img))\n",
    "    labels_plot = regrid(\n",
    "        RevImage(segmentation.permute_labels(labels)), aggregator=\"max\"\n",
    "    ).redim(z=\"label\")\n",
    "    spatial_plots = xy_plot + image_plot + labels_plot\n",
    "    return spatial_plots\n",
    "\n",
    "\n",
    "class PhotobleachingViewer(param.Parameterized):\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        self._data = data\n",
    "        filenames = list(data.keys())\n",
    "        self.param[\"filename\"].objects = filenames\n",
    "        # self.param['filename'].default = filenames[0]\n",
    "        self.filename = filenames[0]\n",
    "        self.measurement = list(data[self.filename][0].keys())[0]\n",
    "        self._update_measurements()\n",
    "        self._process_traces()\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    filename = param.ObjectSelector()\n",
    "    measurement = param.ObjectSelector()\n",
    "    estimator = param.ObjectSelector(\n",
    "        objects=[\"variance\", \"mad\", \"biweight_midvariance\"],\n",
    "        default=\"biweight_midvariance\",\n",
    "    )\n",
    "    fluctuation_colormap = param.ObjectSelector(\n",
    "        objects=[\"random\", \"bin\", \"gain\"], default=\"bin\"\n",
    "    )\n",
    "    xy_colormap = param.ObjectSelector(\n",
    "        objects=[\"random\", \"fluorescence\", \"bin\", \"area\", \"gain\"], default=\"bin\"\n",
    "    )\n",
    "    traces_colormap = param.ObjectSelector(\n",
    "        objects=[\"random\", \"fluorescence\", \"bin\", \"area\", \"gain\"], default=\"bin\"\n",
    "    )\n",
    "    downsample = param.ObjectSelector(objects=[1000, 100, 10, 1], default=100)\n",
    "    normalize_traces = param.Boolean(True)\n",
    "\n",
    "    @param.depends(\"filename\", watch=True)\n",
    "    def _update_measurements(self):\n",
    "        old_measurement = self.measurement\n",
    "        measurements = list(self._data[self.filename][0].keys())\n",
    "        self.param[\"measurement\"].objects = measurements\n",
    "        if old_measurement not in measurements:\n",
    "            self.measurement = measurements[0]\n",
    "        traces, regionprops, labels, img = self._data[self.filename]\n",
    "        self._traces = traces[self.measurement]\n",
    "        self._regionprops = regionprops\n",
    "        self._labels = labels\n",
    "        self._img = img\n",
    "\n",
    "    @param.depends(\"filename\", \"measurement\", watch=True)\n",
    "    def _process_traces(self):\n",
    "        thresh = 30  # TODO: parameterize\n",
    "        num_qs = 10\n",
    "        traces = self._traces\n",
    "        # bins = np.arange(data[0].min()-1,data[0].max()+1,50)\n",
    "        # FROM: numpy.histogram\n",
    "        traces0 = traces[:, 0]\n",
    "        bin_edges = np.histogram_bin_edges(traces0, bins=\"auto\")\n",
    "        bins = pd.cut(traces0, bin_edges)\n",
    "        traces_df = pd.DataFrame(traces)\n",
    "        bin_count = pd.Series(bins).groupby(bins).size()\n",
    "        bin_count.name = \"bin_count\"\n",
    "        # p.join(bin_count, on='bins')\n",
    "        pbar = (\n",
    "            traces_df.div(traces_df.iloc[:, 0], axis=0)\n",
    "            .groupby(bins)\n",
    "            .mean()[bin_count > thresh]\n",
    "        )\n",
    "        mu = traces_df.groupby(bins).mean()[bin_count > thresh]\n",
    "        sigma2 = traces_df.groupby(bins).var(ddof=0)[bin_count > thresh]\n",
    "        y = sigma2.div(mu[0], axis=\"rows\")\n",
    "        qs = np.linspace(0.1, 1, num_qs)  # TODO: make start point adjustable?\n",
    "        nu_qs = pd.DataFrame(\n",
    "            np.array(\n",
    "                [\n",
    "                    -1\n",
    "                    / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "                    * simps(y[pbar >= 1 - q].fillna(0), pbar, axis=1)\n",
    "                    for q in qs\n",
    "                ]\n",
    "            ).T,\n",
    "            index=y.index,\n",
    "            columns=qs,\n",
    "        )\n",
    "        self._pbar = pbar\n",
    "        self._y = y\n",
    "        self._qs = qs\n",
    "        self._nu_qs = nu_qs\n",
    "\n",
    "    @param.depends(\n",
    "        \"filename\",\n",
    "        \"measurement\",\n",
    "        \"fluctuation_colormap\",\n",
    "        \"xy_colormap\",\n",
    "        \"traces_colormap\",\n",
    "        \"downsample\",\n",
    "        \"estimator\",\n",
    "        \"normalize_traces\",\n",
    "    )\n",
    "    def view(self):\n",
    "        traces, regionprops, labels, img = self._data[self.filename]\n",
    "        traces = traces[self.measurement]\n",
    "        gs = pn.GridSpec(sizing_mode=\"stretch_both\")\n",
    "        if self.traces_colormap == \"random\":\n",
    "            trace_colors = None\n",
    "        elif self.traces_colormap == \"fluorescence\":\n",
    "            trace_colors = np.log(traces[:, 0])\n",
    "        elif self.traces_colormap == \"bin\":\n",
    "            trace_colors = None  # TODO\n",
    "        elif self.traces_colormap == \"area\":\n",
    "            colors = regionprops[\"area\"]\n",
    "        elif self.traces_colormap == \"gain\":\n",
    "            trace_colors = None  # TODO\n",
    "        else:\n",
    "            raise ValueError\n",
    "        fluctuation_colors = None\n",
    "        spatial_colors = None\n",
    "        gs[0:1, 0:2] = _trace_plot(\n",
    "            traces,\n",
    "            downsample=self.downsample,\n",
    "            normalize=self.normalize_traces,\n",
    "            colors=trace_colors,\n",
    "        )\n",
    "        fluct_plots = _fluctuation_plots(\n",
    "            self._pbar, self._y, self._qs, self._nu_qs, colors=fluctuation_colors\n",
    "        )\n",
    "        gs[1:2, 0:1] = fluct_plots[0]\n",
    "        gs[1:2, 1:2] = fluct_plots[1]\n",
    "        gs[2:3, 0:2] = _spatial_plots(regionprops, labels, img, colors=spatial_colors)\n",
    "        return gs\n",
    "        # return pn.Row(hv.Curve(np.random.random(10)), hv.Points(np.random.random((10,2))))\n",
    "        # return pn.Column(trace_plots[self.filename], overlay_plots[self.filename])\n",
    "\n",
    "\n",
    "viewer = PhotobleachingViewer(data, name=\"Photobleaching\")\n",
    "pn.Column(viewer.param, viewer.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces, regionprops, _, _ = data[\n",
    "    \"/n/scratch2/jqs1/fidelity/190301/jqs_photobleach_100ms_de32_mkate2_0002.nd2\"\n",
    "]\n",
    "traces = traces[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.arange(data[0].min()-1,data[0].max()+1,50)\n",
    "# FROM: numpy.histogram\n",
    "traces0 = traces[:, 0]\n",
    "bin_edges = np.histogram_bin_edges(traces0, bins=\"auto\")\n",
    "bins = pd.cut(traces0, bin_edges)\n",
    "# traces_normed = traces / traces0[:,np.newaxis]\n",
    "traces_df = pd.DataFrame(traces)\n",
    "bin_count = pd.Series(bins).groupby(bins).size()\n",
    "bin_count.name = \"bin_count\"\n",
    "# p.join(bin_count, on='bins')\n",
    "thresh = 30\n",
    "pbar = (\n",
    "    traces_df.div(traces_df.iloc[:, 0], axis=0).groupby(bins).mean()[bin_count > thresh]\n",
    ")\n",
    "mu = traces_df.groupby(bins).mean()[bin_count > thresh]\n",
    "sigma2 = traces_df.groupby(bins).var(ddof=0)[bin_count > thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.5\n",
    "cq = -1 / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "y = sigma2.div(mu[0], axis=\"rows\")\n",
    "q_mask = pbar >= 1 - q\n",
    "nus = pd.Series(cq * simps(y[q_mask].fillna(0), pbar, axis=1), index=y.index, name=\"nu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(0.1, 1, 10)\n",
    "nu_qs = pd.DataFrame(\n",
    "    np.array(\n",
    "        [\n",
    "            -1\n",
    "            / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "            * simps(y[pbar >= 1 - q].fillna(0), pbar, axis=1)\n",
    "            for q in qs\n",
    "        ]\n",
    "    ).T,\n",
    "    index=y.index,\n",
    "    columns=qs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nus = nu_qs.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.Series(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.name = \"bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(b, nus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(pd.Series(bins, name=\"bin\"), nus, left_on=\"bin\", right_index=True, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_qs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Points(regionprops, kdims=[\"centroid_x\", \"centroid_y\"], vdims=[\"area\"]).options(\n",
    "    color=hv.dim(\"area\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionprops[\"centroid_x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs = np.arange(0, 1, 10)\n",
    "q = 0.5\n",
    "cq = -1 / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "q_mask = pbar >= 1 - q\n",
    "# nu_qs = pd.Series(cq*simps(y[q_mask].fillna(0),pbar,axis=1), index=y.index, name='nu')\n",
    "idxs = np.random.permutation(y.shape[0])\n",
    "curves = [{\"x\": qs, \"y\": nu_qs.values[i], \"i\": idxs[i]} for i in range(nu_qs.shape[0])]\n",
    "integral_plot = hv.Contours(curves, vdims=[\"i\"]).options(\n",
    "    color_index=\"i\", cmap=\"Category20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(1-pbar.loc[name].values,y.loc[name].values, color = cmap(c),label = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.permutation(y.shape[0])\n",
    "curves = [\n",
    "    {\"x\": 1 - pbar.values[i], \"y\": y.values[i], \"i\": idxs[i]} for i in range(y.shape[0])\n",
    "]\n",
    "plot = hv.Contours(curves, vdims=[\"i\"]).options(color_index=\"i\", cmap=\"Category20\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluct_plot(pbar, mu, sigma2, thresh):\n",
    "    hist_df = df.groupby(df[\"bin\"]).size()\n",
    "    hist_df = hist_df[hist_df.values > thresh]\n",
    "    print(hist_df)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    cmap = cm.get_cmap(\"coolwarm\")\n",
    "    y = sigma2.div(mu[0].values, axis=\"rows\")\n",
    "    imax = pbar.index[-1].left\n",
    "    imin = pbar.index[0].left\n",
    "    q = 1 / 2\n",
    "    cq = -1 / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "    # plt.vlines(q,0,3)\n",
    "\n",
    "    for name, group in pbar.groupby(\"bin\"):\n",
    "        dp = pbar.loc[name].values\n",
    "        dp = dp[pbar.loc[name].values > 1 - q]\n",
    "        f = y.loc[name].values[pbar.loc[name].values > 1 - q]\n",
    "        c = (name.left - imin) / (imax - imin)\n",
    "        plt.scatter(\n",
    "            1 - pbar.loc[name].values, y.loc[name].values, color=cmap(c), label=name\n",
    "        )\n",
    "        plt.legend()\n",
    "\n",
    "    plt.title(\n",
    "        r\"$\\nu = \"\n",
    "        + str(np.round(-cq))\n",
    "        + r\"\\cdot \\int\\frac{\\hat{\\sigma}^2}{f_{max}}dp$ =\"\n",
    "        + str(np.round(nu_df.mean(), 2)),\n",
    "        fontsize=20,\n",
    "        pad=20,\n",
    "    )\n",
    "    plt.xlabel(r\"$(1-\\hat{p})$\", fontsize=20)\n",
    "    plt.ylabel(r\"$\\frac{\\hat{\\sigma}^2}{f_{max}}$\", fontsize=20)\n",
    "\n",
    "\n",
    "thresh = 40\n",
    "\n",
    "pbar, mu, sigma2 = get_stats(df, thresh)\n",
    "\n",
    "\n",
    "nu_df = nu_int(pbar, mu, sigma2)\n",
    "print(nu_df)\n",
    "\n",
    "fluct_plot(pbar, mu, sigma2, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements, regionprops, labels, img = d[\n",
    "    \"/n/scratch2/jqs1/fidelity/190311/190311_mGFPmut2_100ms_laser100pct_006.nd2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regionprops.reset_index(inplace=True)\n",
    "regionprops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=150\n",
    "#%%opts Image {+axiswise}\n",
    "hv.Image(img / img.max()).options(cmap=\"gray\") + hv.Image(labels != 0).options(\n",
    "    cmap=\"blues\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(measurements.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "rp_list = []\n",
    "rp_df = pd.DataFrame()\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    measurements, regionprops, labels, img = d[\n",
    "        \"/n/scratch2/jqs1/fidelity/190311/190311_mGFPmut2_100ms_laser100pct_00\"\n",
    "        + str(i)\n",
    "        + \".nd2\"\n",
    "    ]\n",
    "    traces.append(measurements[\"mean\"][1:])\n",
    "    rp_list.append(regionprops)\n",
    "    print(measurements[\"mean\"][1:].shape)\n",
    "traces = np.concatenate(traces)\n",
    "rp_df = pd.concat(rp_list, sort=False)\n",
    "rp_df.reset_index(inplace=True)\n",
    "print(traces.shape)\n",
    "data = pd.DataFrame(traces)  # + np.random.normal(0,1,Gsamp.T.shape))\n",
    "\n",
    "bins = np.arange(data[0].min() - 1, data[0].max() + 1, 50)\n",
    "data[\"bin\"] = pd.cut(data[0], bins=bins)\n",
    "data = pd.concat([data, rp_df], axis=1, sort=False)\n",
    "# rp_df['initial_intensity'] = df[0]\n",
    "t_end = traces.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=250\n",
    "# traces = measurements['mean']\n",
    "idxs = np.random.permutation(traces.shape[0])\n",
    "downsample = (\n",
    "    10  # set to 1 to show all traces (instead of 10%); this will make your browser slow\n",
    ")\n",
    "curves = [\n",
    "    {\"x\": np.arange(traces.shape[1]), \"y\": traces[i], \"i\": idxs[i]}\n",
    "    for i in range(traces.shape[0] // downsample)\n",
    "]\n",
    "hv.Contours(curves, vdims=[\"i\"]).options(color_index=\"i\", cmap=\"Category20\", logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from scipy.integrate import simps\n",
    "\n",
    "\n",
    "def filter_df(df, prop_dict):\n",
    "    processed_df = df.copy()\n",
    "    for prop in prop_dict:\n",
    "        processed_df = processed_df[\n",
    "            (processed_df[prop] > prop_dict[prop][0])\n",
    "            & (processed_df[prop] < prop_dict[prop][1])\n",
    "        ]\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def get_stats(df, thresh=30):\n",
    "    p = df.iloc[:, :t_end].apply(lambda x: x / x[0], axis=1)\n",
    "    p[\"bin\"] = df[\"bin\"]\n",
    "    pbar = p.groupby(p[\"bin\"]).mean()[p.groupby(p[\"bin\"]).size() > thresh]\n",
    "    mu = (\n",
    "        df.iloc[:, :t_end]\n",
    "        .groupby(df[\"bin\"])\n",
    "        .mean()[df.groupby(df[\"bin\"]).size() > thresh]\n",
    "    )\n",
    "    sigma2 = (\n",
    "        df.iloc[:, :t_end]\n",
    "        .groupby(df[\"bin\"])\n",
    "        .var(ddof=0)[df.groupby(df[\"bin\"]).size() > thresh]\n",
    "    )\n",
    "\n",
    "    return pbar, mu, sigma2\n",
    "\n",
    "\n",
    "def nu_int(pbar, mu, sigma, q=1):\n",
    "    nu_dict = {}  # pd.Series()\n",
    "    cq = -1 / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "    y = sigma2.div(mu[0].values, axis=\"rows\")\n",
    "    for name, group in pbar.groupby(\"bin\"):\n",
    "        dp = pbar.loc[name].values\n",
    "        dp = dp[pbar.loc[name].values > 1 - q]\n",
    "        f = y.loc[name].values[pbar.loc[name].values > 1 - q]\n",
    "        nu_dict[name] = cq * simps(f, dp)\n",
    "\n",
    "    return pd.Series(nu_dict)\n",
    "\n",
    "\n",
    "def fluct_plot(pbar, mu, sigma2, thresh):\n",
    "    hist_df = df.groupby(df[\"bin\"]).size()\n",
    "    hist_df = hist_df[hist_df.values > thresh]\n",
    "    print(hist_df)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    cmap = cm.get_cmap(\"coolwarm\")\n",
    "\n",
    "    y = sigma2.div(mu[0].values, axis=\"rows\")\n",
    "    imax = pbar.index[-1].left\n",
    "    imin = pbar.index[0].left\n",
    "    q = 1 / 2\n",
    "    cq = -1 / (1 / 2 * q**2 - 1 / 3 * q**3)\n",
    "    # plt.vlines(q,0,3)\n",
    "\n",
    "    for name, group in pbar.groupby(\"bin\"):\n",
    "        dp = pbar.loc[name].values\n",
    "        dp = dp[pbar.loc[name].values > 1 - q]\n",
    "        f = y.loc[name].values[pbar.loc[name].values > 1 - q]\n",
    "        c = (name.left - imin) / (imax - imin)\n",
    "        plt.scatter(\n",
    "            1 - pbar.loc[name].values, y.loc[name].values, color=cmap(c), label=name\n",
    "        )\n",
    "        plt.legend()\n",
    "\n",
    "    plt.title(\n",
    "        r\"$\\nu = \"\n",
    "        + str(np.round(-cq))\n",
    "        + r\"\\cdot \\int\\frac{\\hat{\\sigma}^2}{f_{max}}dp$ =\"\n",
    "        + str(np.round(nu_df.mean(), 2)),\n",
    "        fontsize=20,\n",
    "        pad=20,\n",
    "    )\n",
    "    plt.xlabel(r\"$(1-\\hat{p})$\", fontsize=20)\n",
    "    plt.ylabel(r\"$\\frac{\\hat{\\sigma}^2}{f_{max}}$\", fontsize=20)\n",
    "\n",
    "\n",
    "thresh = 40\n",
    "\n",
    "prop_dict = {\n",
    "    \"centroid_x\": [300, 1000],\n",
    "    \"centroid_y\": [750, 1500],\n",
    "    \"area\": [30, 150],\n",
    "    0: [2000, 10000],\n",
    "}\n",
    "# prop_dict = {'area': [30,150],\n",
    "#              0: [2000, 10000]}\n",
    "\n",
    "df = filter_df(data, prop_dict)\n",
    "pbar, mu, sigma2 = get_stats(df, thresh)\n",
    "\n",
    "\n",
    "nu_df = nu_int(pbar, mu, sigma2)\n",
    "print(nu_df)\n",
    "\n",
    "fluct_plot(pbar, mu, sigma2, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=250\n",
    "# traces = measurements['mean']\n",
    "idxs = np.random.permutation(traces.shape[0])\n",
    "downsample = (\n",
    "    10  # set to 1 to show all traces (instead of 10%); this will make your browser slow\n",
    ")\n",
    "curves = [\n",
    "    {\"x\": np.arange(traces.shape[1]), \"y\": df[i], \"i\": idxs[i]}\n",
    "    for i in range(traces.shape[0] // downsample)\n",
    "]\n",
    "hv.Contours(curves, vdims=[\"i\"]).options(color_index=\"i\", cmap=\"Category20\", logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I0 = traces[:, 0]\n",
    "plt.hist(I0, bins=30)\n",
    "print(np.mean(I0), np.var(I0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "cmap = cm.get_cmap(\"coolwarm\")\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "imax = pbar.index[-1].left\n",
    "imin = pbar.index[0].left\n",
    "for name, group in df.groupby(\"bin\"):\n",
    "    if group.shape[0] > thresh:\n",
    "        c = (name.left - imin) / (imax - imin)\n",
    "        #         c = (nu_dict[name] - min(nu_int))/(max(nu_int) - min(nu_int))\n",
    "        z = name.left * np.ones(group.centroid_x.shape)\n",
    "        plt.scatter(\n",
    "            group.centroid_x, group.centroid_y, color=cmap(c), label=nu_df[name]\n",
    "        )\n",
    "        plt.xlim(data.centroid_x.min(), data.centroid_x.max())\n",
    "        plt.ylim(data.centroid_y.min(), data.centroid_y.max())\n",
    "    else:\n",
    "        plt.scatter(group.centroid_x, group.centroid_y, color=\"k\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
