{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nd2reader\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import regrid\n",
    "import skimage.filters\n",
    "import skimage.feature\n",
    "import scipy.ndimage\n",
    "import peakutils\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import dask\n",
    "import dask.array as da\n",
    "import distributed\n",
    "from distributed import Client, LocalCluster, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from cytoolz import partial, compose, juxt\n",
    "from itertools import repeat\n",
    "from glob import glob\n",
    "import cachetools\n",
    "import numpy_indexed\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import warnings\n",
    "import os\n",
    "from numbers import Integral\n",
    "from dask.delayed import Delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segmentation import *\n",
    "# from util import *\n",
    "# from matriarch_stub import *\n",
    "import segmentation\n",
    "import matriarch_stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask.config.config['distributed']['scheduler']['allowed-failures'] = 20\n",
    "# dask.config.config['distributed']['worker']['memory'] = {'target': 0.4,\n",
    "#                                                         'spill': 0.5,\n",
    "#                                                         'pause': 0.9,\n",
    "#                                                         'terminate': 0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue=\"short\",\n",
    "    walltime=\"03:00:00\",\n",
    "    memory=\"4GB\",\n",
    "    local_directory=\"/tmp\",\n",
    "    log_directory=\"/home/jqs1/projects/molecule-counting/log\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    ")\n",
    "# diagnostics_port=('127.0.0.1', 8787),\n",
    "# env_extra=['export PYTHONPATH=\\\"/home/jqs1/projects/matriarch\\\"'])\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 0) make benchmarking notebook\n",
    "# 1) benchmark manual reduceat vs. npi reduction vs. npi non-reduction vs np.unique/bincount for single frame, image stack\n",
    "# 2) numba gufunc to make it work on numpy image stacks\n",
    "# 3) dask gufunc to make it work on dask arrays\n",
    "# 4) dry run without regionprops\n",
    "# 5) arbitrary sequence of traces using initial segmentation (replace sandwich)\n",
    "# 6) named_funcs_as_juxt: decorator to turn {'func1': func1, ('q0.5', 'q0.7'): partial(np.percentile, q=(0.5,0.7))} into a multiple-valued func\n",
    "# 7) BENCHMARK: try readahead buffering/chunk size\n",
    "# 8) fix regionprops memory usage\n",
    "# 9) zarrification of labels\n",
    "\n",
    "# better group_by, only argsort once per labels img, handle multiple funcs as a juxt, also funcs with multiple return values; uses chunks\n",
    "# use optimized mean? BENCHMARK\n",
    "# convert dask arrays to delayed before calling short_circuit_none (otherwise we wait until all frames are in RAM)\n",
    "# don't process FOV if too many labels\n",
    "# readahead buffering/benchmark buffer size vs chunk size; also VS non-dask array\n",
    "# dask array correction\n",
    "# sandwich -> arbitrary sequence of traces (using same segmentation)\n",
    "# regionprops (use measure func??)\n",
    "# TODO: if we zarrify labels, we need to turn back into ndarray before map_over_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = segmentation.nd2_to_dask(\n",
    "    \"/n/scratch2/jqs1/190922/190922_photobleaching_greens/GFP_photobleaching_100pct_100ms_0001.nd2\",\n",
    "    0,\n",
    "    \"GFP-PENTA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_img = ary[0].compute()\n",
    "labels = segmentation.segment(seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(func, labels, img_stack, skip0=True):\n",
    "    keys = labels.ravel()\n",
    "    sorter = np.argsort(keys, kind=\"mergesort\")\n",
    "    sorted_ = keys[sorter]\n",
    "    flag = sorted_[:-1] != sorted_[1:]\n",
    "    slices = np.concatenate(([0], np.flatnonzero(flag) + 1, [keys.size]))\n",
    "    unique = sorted_[slices[:-1]]\n",
    "    values = img_stack.reshape((img_stack.shape[0], -1))[:, sorter]\n",
    "    groups = np.split(values, slices[1:-1], axis=1)\n",
    "    return {\n",
    "        key: func(group, axis=1)\n",
    "        for key, group in zip(unique, groups)\n",
    "        if key != 0 or not skip0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dask(func, labels, img_stack, skip0=False):\n",
    "    keys = labels.ravel()\n",
    "    sorter = np.argsort(keys, kind=\"mergesort\")\n",
    "    sorted_ = keys[sorter]\n",
    "    flag = sorted_[:-1] != sorted_[1:]\n",
    "    slices = np.concatenate(([0], np.flatnonzero(flag) + 1, [keys.size]))\n",
    "    unique = sorted_[slices[:-1]]\n",
    "    values = img_stack.reshape((img_stack.shape[0], -1))[:, sorter]\n",
    "\n",
    "    def f(x):\n",
    "        groups = np.split(x, slices[1:-1], axis=1)\n",
    "        # TODO: why is this (commented) so much slower??\n",
    "        # reductions = [np.mean(x, axis=1) for x in groups]\n",
    "        # return np.array(reductions).T\n",
    "        reductions = [func(x, axis=1)[:, np.newaxis] for x in groups]\n",
    "        return np.hstack(reductions)\n",
    "\n",
    "    if isinstance(img_stack, dask.array.Array):\n",
    "        groups = values.map_blocks(\n",
    "            f, drop_axis=1, new_axis=1, chunks=(values.chunks[0], unique.shape[0])\n",
    "        )\n",
    "    else:\n",
    "        groups = [func(x, axis=1) for x in np.split(values, slices[1:-1], axis=1)]\n",
    "    return {key: group for key, group in zip(unique, groups) if key != 0 or not skip0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiaggregate_dask(func, labels, img_stack, skip0=False):\n",
    "    keys = labels.ravel()\n",
    "    sorter = np.argsort(keys, kind=\"mergesort\")\n",
    "    sorted_ = keys[sorter]\n",
    "    flag = sorted_[:-1] != sorted_[1:]\n",
    "    slices = np.concatenate(([0], np.flatnonzero(flag) + 1, [keys.size]))\n",
    "    unique = sorted_[slices[:-1]]\n",
    "    values = img_stack.reshape((img_stack.shape[0], -1))[:, sorter]\n",
    "    ret = func(np.ones((1,) * values.ndim))\n",
    "\n",
    "    def f(x):\n",
    "        groups = np.split(x, slices[1:-1], axis=1)\n",
    "        reductions = [func(x) for x in groups]\n",
    "        stack = np.stack(reductions, axis=0)\n",
    "        return stack\n",
    "\n",
    "    if isinstance(img_stack, dask.array.Array):\n",
    "        chunks = (unique.shape[0], *ret.shape[:-1], values.chunks[0])\n",
    "        new_axis = tuple(range(len(chunks) - 1))\n",
    "        groups = values.map_blocks(f, drop_axis=1, new_axis=new_axis, chunks=chunks)\n",
    "    else:\n",
    "        groups = [func(x) for x in np.split(values, slices[1:-1], axis=1)]\n",
    "    return {key: group for key, group in zip(unique, groups) if key != 0 or not skip0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    # return x.mean(axis=1)\n",
    "    return np.array([x.mean(axis=1), x.sum(axis=1), np.median(x, axis=1)])\n",
    "\n",
    "\n",
    "z = multiaggregate_dask(g, labels, ary[:10])\n",
    "z[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = labels.ravel()\n",
    "sorter = np.argsort(keys, kind=\"mergesort\")\n",
    "sorted_ = keys[sorter]\n",
    "flag = sorted_[:-1] != sorted_[1:]\n",
    "slices = np.concatenate(([0], np.flatnonzero(flag) + 1, [keys.size]))\n",
    "unique = sorted_[slices[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ary.reshape((ary.shape[0], -1))[:, sorter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = values[:100]  # .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    groups = np.split(x, slices[1:-1], axis=1)\n",
    "    # TODO: why is this so much slower??\n",
    "    # reductions = [np.mean(x, axis=1) for x in groups]\n",
    "    # return np.array(reductions).T\n",
    "    # reductions = [np.mean(x, axis=1)[:,np.newaxis] for x in groups]\n",
    "    reductions = [\n",
    "        np.array([np.mean(x, axis=1), np.mean(x, axis=1), np.mean(x, axis=1)])\n",
    "        for x in groups\n",
    "    ]\n",
    "    print(\">>\", reductions[0].shape)\n",
    "    # val = np.hstack(reductions)\n",
    "    val = np.stack(reductions, axis=-1)\n",
    "    return val\n",
    "\n",
    "\n",
    "# z = v.map_blocks(f, drop_axis=1, new_axis=(0,2), chunks=(v.chunks[0], unique.shape[0]))\n",
    "# z\n",
    "f(v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    groups = np.split(x, slices[1:-1], axis=1)\n",
    "    # TODO: why is this so much slower??\n",
    "    # reductions = [np.mean(x, axis=1) for x in groups]\n",
    "    # return np.array(reductions).T\n",
    "    # reductions = [np.mean(x, axis=1)[:,np.newaxis] for x in groups]\n",
    "    reductions = [\n",
    "        np.array([np.mean(x, axis=1), np.mean(x, axis=1), np.mean(x, axis=1)])\n",
    "        for x in groups\n",
    "    ]\n",
    "    print(\">>\", reductions[0].shape)\n",
    "    # val = np.hstack(reductions)\n",
    "    val = np.stack(reductions, axis=-1)\n",
    "    return val\n",
    "\n",
    "\n",
    "z = v.map_blocks(\n",
    "    f, drop_axis=1, new_axis=(0, 2), chunks=(3, v.chunks[0], unique.shape[0])\n",
    ")\n",
    "z\n",
    "# f(v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(zz[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dask(func, labels, img_stack, skip0=True):\n",
    "    keys = labels.ravel()\n",
    "    sorter = np.argsort(keys, kind=\"mergesort\")\n",
    "    sorted_ = keys[sorter]\n",
    "    flag = sorted_[:-1] != sorted_[1:]\n",
    "    slices = np.concatenate(([0], np.flatnonzero(flag) + 1, [keys.size]))\n",
    "    unique = sorted_[slices[:-1]]\n",
    "    _\n",
    "    values = img_stack.reshape((img_stack.shape[0], -1))[:, sorter]\n",
    "    groups = np.split(values, slices[1:-1], axis=1)\n",
    "    return {\n",
    "        key: func(group, axis=1)\n",
    "        for key, group in zip(unique, groups)\n",
    "        if key != 0 or not skip0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(np.mean, labels, ary[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = {\"mean\": np.mean}  # ,\n",
    "#'median': np.median}\n",
    "#'p0.05': partial(np.percentile, q=5),\n",
    "#'p0.20': partial(np.percentile, q=20),\n",
    "#'p0.70': partial(np.percentile, q=70),\n",
    "#'p0.95': partial(np.percentile, q=95)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = \"/n/scratch2/jqs1\"\n",
    "# fluorescence_filenames = (glob(os.path.join(base_filename, '190411/Noah_Runs/*.nd2')) +\n",
    "#                           glob(os.path.join(base_filename, '190507/*.nd2')) +\n",
    "#                           glob(os.path.join(base_filename, '190508/*.nd2')) +\n",
    "#                           glob(os.path.join(base_filename, '190514/*.nd2')) +\n",
    "#                           glob(os.path.join(base_filename, '190515/*.nd2')) +\n",
    "#                           glob(os.path.join(base_filename, '190516/*.nd2')))\n",
    "# fluorescence_filenames = glob(os.path.join(base_filename, '190523/*ti5*.nd2'))\n",
    "# fluorescence_filenames = glob(os.path.join(base_filename, '190401/*.nd2')) + glob(os.path.join(base_filename, '190411/Noah_Runs/*.nd2'))\n",
    "fluorescence_filenames = glob(\n",
    "    os.path.join(base_filename, \"190922/*/*photobleaching*.nd2\")\n",
    ")\n",
    "phase_filenames = (\n",
    "    []\n",
    ")  # glob(os.path.join(base13_filename, 'phase/*_0001.nd2')) + glob('/n/scratch2/jqs1/fidelity/190325/phase/*/*_0001.nd2')\n",
    "sandwich_filenames = []  # glob(os.path.join(base13_filename, 'sandwich/*_0001.nd2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_filenames = fluorescence_filenames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dark_frames = segmentation.nd2_to_dask(os.path.join(base13_filename, 'calibration/dark_100ms.nd2'), 0, 0)\n",
    "# dark_frame = dark_frames.mean(axis=0)\n",
    "# # TODO: hack\n",
    "# #dark_frame = dark_frame.compute()\n",
    "# #dark_frame = client.persist(dark_frame)\n",
    "# #dark_frame = client.scatter(dark_frame, broadcast=True)\n",
    "# dark_frame = dark_frame.to_delayed()[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_fields = {}\n",
    "# for filename in glob(os.path.join(base13_filename, 'calibration/*flat*100ms*.nd2')):\n",
    "#     channel = segmentation.get_nd2_reader(filename).metadata['channels'][0]\n",
    "#     flat_field = segmentation.nd2_to_dask(filename, 0, 0).mean(axis=0)\n",
    "#     # TODO: hack\n",
    "#     #flat_field = flat_field.compute()\n",
    "#     #flat_field = client.scatter(flat_field, broadcast=True)\n",
    "#     #flat_field = client.persist(flat_field)\n",
    "#     flat_field = flat_field.to_delayed()[0,0]\n",
    "#     flat_fields[channel] = flat_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_frame = None\n",
    "flat_fields = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_graph = {}\n",
    "for photobleaching_filename in fluorescence_filenames[:]:\n",
    "    data_graph[photobleaching_filename] = segmentation.process_file(\n",
    "        funcs, photobleaching_filename, dark_frame=dark_frame, flat_fields=flat_fields\n",
    "    )\n",
    "\n",
    "for photobleaching_filename in phase_filenames[:]:\n",
    "    segmentation_filename = photobleaching_filename.replace(\"_0001.nd2\", \".nd2\")\n",
    "    data_graph[segmentation_filename] = segmentation.process_file(\n",
    "        funcs,\n",
    "        photobleaching_filename,\n",
    "        segmentation_filename=segmentation_filename,\n",
    "        dark_frame=dark_frame,\n",
    "        flat_fields=flat_fields,\n",
    "    )\n",
    "\n",
    "for initial_filename in sandwich_filenames[:]:\n",
    "    segmentation_filename = initial_filename.replace(\"_0001.nd2\", \".nd2\")\n",
    "    photobleaching_filename = initial_filename.replace(\"_0001.nd2\", \"_0002.nd2\")\n",
    "    final_filename = initial_filename.replace(\"_0001.nd2\", \"_0003.nd2\")\n",
    "    data_graph[segmentation_filename] = segmentation.process_file(\n",
    "        funcs,\n",
    "        photobleaching_filename,\n",
    "        segmentation_filename=segmentation_filename,\n",
    "        initial_filename=initial_filename,\n",
    "        final_filename=final_filename,\n",
    "        dark_frame=dark_frame,\n",
    "        flat_fields=flat_fields,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up computes so we can gather results from multiple workers\n",
    "# (otherwise the single worker assembling the dict will run out of memory)\n",
    "# TODO: use recursive_map(..., levels=?)\n",
    "data_futures = {\n",
    "    k: {k2: client.compute(v2) for k2, v2 in v.items()} for k, v in data_graph.items()\n",
    "}\n",
    "data_futures[\"_calibration\"] = client.compute(\n",
    "    {\"dark_frame\": dark_frame, \"flat_fields\": flat_fields}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = client.gather(data_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/n/groups/paulsson/jqs1/molecule-counting/191221photobleaching.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    k: {pos: np.asarray(d[\"labels\"]).max() for pos, d in v.items()}\n",
    "    for k, v in data.items()\n",
    "    if k[0] != \"_\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
