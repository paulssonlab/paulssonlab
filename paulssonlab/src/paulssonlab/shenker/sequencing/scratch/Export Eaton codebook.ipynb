{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bd070a-098c-4199-816c-62cf8ed3766d",
   "metadata": {},
   "source": [
    "This notebook loads in the pipeline output from the extract_segments step and exports an Eaton-formatted codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c57f7d-eb34-40e9-b810-10d4c44d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import gfapy\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import paulssonlab.sequencing.processing as processing\n",
    "from paulssonlab.util.sequence import reverse_complement\n",
    "\n",
    "hv.extension(\"matplotlib\")\n",
    "# this is important, the variants_path and grouping_path columns may appear corrupted without this\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5f417-e951-4a21-a5e9-b8c4182a17e3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec49ac-7ef6-41d8-aed9-9182a4f5d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_glob(filename):\n",
    "    return pl.concat([pl.scan_ipc(f) for f in glob(filename)], how=\"diagonal\")\n",
    "\n",
    "\n",
    "def load_sequencing(filename, filter=True):\n",
    "    df = concat_glob(filename)\n",
    "    if \"is_primary_alignment\" not in df.collect_schema().names():\n",
    "        df = df.with_columns(is_primary_alignment=pl.col(\"name\").is_first_distinct())\n",
    "    df = df.with_columns(\n",
    "        dup=pl.col(\"name\").is_duplicated(),\n",
    "        e2e=pl.col(\"variants_path\")\n",
    "        .list.set_intersection([\"<UNS9\", \">UNS9\", \"<UNS3\", \">UNS3\"])\n",
    "        .list.len()\n",
    "        == 2,\n",
    "        bc_e2e=pl.col(\"variants_path\")\n",
    "        .list.set_intersection(\n",
    "            [\"<BC:T7_prom\", \">BC:T7_prom\", \"<BC:spacer2\", \">BC:spacer2\"]\n",
    "        )\n",
    "        .list.len()\n",
    "        == 2,\n",
    "    )\n",
    "    if filter:\n",
    "        df = df.filter(pl.col(\"is_primary_alignment\"), pl.col(\"e2e\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def path_to_barcode_string(path_col, bits=list(range(30))):\n",
    "    if isinstance(path_col, str):\n",
    "        path_col = pl.col(path_col)\n",
    "    return pl.concat_str(\n",
    "        [\n",
    "            pl.when(\n",
    "                path_col.list.contains(f\">BC:bit{bit}=1\").or_(\n",
    "                    path_col.list.contains(f\"<BC:bit{bit}=1\")\n",
    "                )\n",
    "            )\n",
    "            .then(pl.lit(\"1\"))\n",
    "            .otherwise(pl.lit(\"0\"))\n",
    "            for bit in bits\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34545ca-a1aa-430d-bf3e-d4d27e7ac7cc",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262ab52-16dc-4efd-a868-095863327a19",
   "metadata": {},
   "source": [
    "Load the appropriate variants GFA and extract_segments Arrow output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898348f-4dc7-4949-b350-dc7045896ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfa = gfapy.Gfa.from_file(\n",
    "    \"/home/jqs1/scratch/sequencing/sequencing_references/pLIB502-503.gfa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4cecd-a265-47b9-a6d7-ab1aed575ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = load_sequencing(\n",
    "    \"/home/jqs1/scratch/sequencing/241007_pLIB502-503/output/max_divergence=0.3/extract_segments/*.arrow\"\n",
    ")\n",
    "df = processing.compute_divergences(\n",
    "    df,\n",
    "    list(dict.fromkeys(([s.split(\"=\")[0] for s in gfa.segment_names]))),\n",
    "    struct_name=\"variants_segments\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ed955-46f5-45e6-9b8b-6ac0cc82feb0",
   "metadata": {},
   "source": [
    "Examine the columns:\n",
    "- `e2e`: Whether the alignment path covers the variants GFA end-to-end.\n",
    "- `bc_e2e`: Whether the alignment path covers the grouping GFA end-to-end.\n",
    "- `is_primary_alignment`: GraphAligner may output more than one alignment for each consensus sequence aligned against the variants GFA, you typically only want the primary (best) alignment.\n",
    "- `dup`: Flags all consensus sequences that have more than one alignment. Secondary alignments are not unusual nor a sign that the primary alignment is poor, so it usually does not make sense to filter out based on this column.\n",
    "  `name`: A unique ID for each consensus. Note that these IDs may appear multiple times if the same consensus has multiple alignments; it should be unique after filtering for `is_primary_aligmnent`.\n",
    "- `consensus_depth`: The number of sequences that were used to compute this consensus.\n",
    "- `grouping_depth`: The number of sequences that were grouped together during the `PREPARE_CONSENSUS` step. Typically this is the same as `consensus_depth`, but may be different depending on the filtering arguments passed to `consensus.py` during the `PREPARE_CONSENSUS` and `CONSENSUS_PREPARED` steps.\n",
    "- `consensus_seq`: The raw consensus sequence.\n",
    "- `grouping_path`: The alignment path used for grouping reads during the `PREPARE_CONSENSUS` step.\n",
    "- `variants_path`: The alignment path produced by GraphAligner aligning `consensus_seq` against the variants GFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fed30-04e4-4599-8a3d-00835c494db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff7a91-8c3b-4ac1-aeb9-8ce0cd3c2c84",
   "metadata": {},
   "source": [
    "Additionally, there is a `variants_segments` struct column containing hundreds of fields (subcolumns). For each segment `seg`, it has fields for:\n",
    "- `seg|seq`: The slice of the consensus sequence that aligns to segment `seg`.\n",
    "- `seg|cigar`: The slice of the CIGAR string that corresponds to the alignment across segment `seg`.\n",
    "- `seg|variant` (if applicable): If there are mutually exclusive segments `seg=variant1`, `seg=variant2`, and so forth, this specifies which of those variants (`variant1`, `variant2`, etc.) appeared in the alignment.\n",
    "- `seg|matches`: The number of matches in the CIGAR string for this segment.\n",
    "- `seg|mismatches`: The number of mismatches in the CIGAR string for this segment.\n",
    "- `seg|insertions`: The number of insertions in the CIGAR string for this segment.\n",
    "- `seg|deletions`: The number of deletions in the CIGAR string for this segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8f9d9-f8f3-44f3-9274-ed90e4bdbb49",
   "metadata": {},
   "source": [
    "Examine the list of `variants_segments` field names and select which segments you want to load in by setting `segment_columns`. It is recommended not to load in all columns because a typical extract_segments output with all columns is tens of GB and is less convenient to work with (e.g., you need to request a lot of memory for your jupyterlab job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13b0d7-49e4-463a-bfb1-4c26930ae2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f.name for f in df.schema[\"variants_segments\"].fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33fd487-5fc7-4377-a409-222a4a7daae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_columns = [\n",
    "    \"sigma:promoter|variant\",\n",
    "    \"sigma:promoter|divergence\",\n",
    "    \"antisigma:promoter|variant\",\n",
    "    \"antisigma:promoter|divergence\",\n",
    "    \"reporter:promoter|variant\",\n",
    "    \"reporter:promoter|divergence\",\n",
    "    \"sigma:RBS:RiboJ|divergence\",\n",
    "    \"sigma:RBS:BCD_leader|divergence\",\n",
    "    \"antisigma:RBS:RiboJ|divergence\",\n",
    "    \"antisigma:RBS:BCD_leader|divergence\",\n",
    "    \"reporter:RBS:RiboJ|divergence\",\n",
    "    \"reporter:RBS:BCD_leader|divergence\",\n",
    "    \"sigma:RBS|seq\",\n",
    "    \"antisigma:RBS|seq\",\n",
    "    \"reporter:RBS|seq\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa802f1b-944a-4830-bd07-cad77503620e",
   "metadata": {},
   "source": [
    "We've been working with a polars LazyFrame up until this point. Once we select only the columns we want to load in, we call `df.collect()` to execute the query and load the dataframe into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9702512-85de-4c24-9367-cc2bebbdcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.select(\n",
    "    pl.col(\n",
    "        \"grouping_path\",\n",
    "        # uncomment if you want to export full consensus sequences, this increases memory usage/file size\n",
    "        # if so, you probably want 64GB of memory\n",
    "        # \"consensus_seq\",\n",
    "        \"name\",\n",
    "        \"grouping_path_hash\",\n",
    "        \"grouping_depth\",\n",
    "        \"consensus_depth\",\n",
    "        \"strand\",\n",
    "        \"variants_path\",\n",
    "        \"is_primary_alignment\",\n",
    "        \"dup\",\n",
    "        \"e2e\",\n",
    "        \"bc_e2e\",\n",
    "    ),\n",
    "    *[pl.col(\"variants_segments\").struct[f] for f in segment_columns]\n",
    ")\n",
    "df = df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea2646-b65c-49a3-a499-f165043cd721",
   "metadata": {},
   "source": [
    "Check the size of the resulting dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9d708-98cd-4832-881c-be3d16d00791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.estimated_size(\"gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ffd5a-1a41-4841-9a64-2936d019115e",
   "metadata": {},
   "source": [
    "And that it has the columns you expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8eccff-3097-467d-ae55-522ebb828bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83976c42-8840-4715-95ae-75888400d772",
   "metadata": {},
   "source": [
    "# Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db333567-4512-4f65-8419-c4d710d0aba8",
   "metadata": {},
   "source": [
    "## Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684d645-c64f-4e79-871a-bf393738c85c",
   "metadata": {},
   "source": [
    "Here we plot grouping depth in descending order (barcode index on x-axis). Our informal heuristic is that a properly sampled sequencing run will show a steep cliff on the right-hand side of this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8a4fe-b094-4607-b16b-2d96cff543a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"grouping_depth\"].sort(descending=True).to_pandas().hvplot.step(\n",
    "    logy=True, height=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ef770-7f97-480c-b28c-bbe785934b45",
   "metadata": {},
   "source": [
    "Here we plot cumulative fractions of barcodes and reads (y-axis) for the subset of the dataset with at most a particular grouping depth (x-axis). The steep part of each curve indicates the depth at which we're spending most of our sequencing capacity on (the steep part of the curve should be roughly centered on our target depth). The left and right extremes of both curves show how much barcode space/sequencing capacity we're “wasting” on low-depth (low accuracy) or high-depth (diminishing returns) barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebe655-1c16-461a-b0fb-ff0824bea034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(\"grouping_depth\").select(\n",
    "    pl.col(\"grouping_depth\"),\n",
    "    frac_barcodes=pl.int_range(1, pl.len() + 1, dtype=pl.UInt32) / pl.len(),\n",
    "    frac_reads=pl.col(\"grouping_depth\").cum_sum() / pl.col(\"grouping_depth\").sum(),\n",
    ").to_pandas().hvplot.step(\"grouping_depth\", logx=True, logy=False, where=\"pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe275ba-2b09-411d-8ac0-ec30ddb98aa9",
   "metadata": {},
   "source": [
    "## Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9a94c-2e15-45c1-ae12-89f872476939",
   "metadata": {},
   "source": [
    "Here we can check for balance among our promoter variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b90d10-9d26-4bb5-98d8-58c57cb27d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sigma:promoter|variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb290986-0f11-4b55-ad2a-c3144239ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"antisigma:promoter|variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8d028-d09c-42c8-87aa-5ad11d963654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reporter:promoter|variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7355f8-daa2-41cc-a07f-5aa78bae8081",
   "metadata": {},
   "source": [
    "And their pairwise/three-way frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc836faf-12a5-4f83-bcc8-69507deb1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_by(pl.col(\"sigma:promoter|variant\", \"antisigma:promoter|variant\")).agg(\n",
    "    pl.len()\n",
    ").sort(\"len\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfc998-a42d-4863-9808-e5ca2a1f8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_by(\n",
    "    pl.col(\n",
    "        \"sigma:promoter|variant\",\n",
    "        \"antisigma:promoter|variant\",\n",
    "        \"reporter:promoter|variant\",\n",
    "    )\n",
    ").agg(pl.len()).sort(\"len\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041b799-6636-4bb2-bd62-d7118f433ae6",
   "metadata": {},
   "source": [
    "Here we plot the frequency distribution of six-tuples, including all promoter variants and RBS sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d212ac-b167-468e-989b-307342be1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.select(\n",
    "    pl.struct(\n",
    "        \"sigma:promoter|variant\",\n",
    "        \"antisigma:promoter|variant\",\n",
    "        \"sigma:RBS|seq\",\n",
    "        \"antisigma:RBS|seq\",\n",
    "        \"reporter:promoter|variant\",\n",
    "        \"reporter:RBS|seq\",\n",
    "    ).alias(\"foo\")\n",
    ")[\"foo\"].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510e6dc-1cbc-4bb9-9866-e64233ca9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[\"count\"].to_pandas().hvplot.step(logy=True, logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc252c6-0841-426d-a2d5-133ab32fa4da",
   "metadata": {},
   "source": [
    "# Export to Eaton format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cb769-9ca8-4a24-9a97-6909ec4956c6",
   "metadata": {},
   "source": [
    "Now we convert the barcode into the Eaton-style “0100110...” string format and add some dummy columns that Eaton's pipeline expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80216d8b-7396-4ffa-918c-8619e018dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_eaton = df.with_columns(\n",
    "    barcode=path_to_barcode_string(\"variants_path\"),\n",
    "    reference=pl.lit(\"\"),\n",
    "    alignmentstart=1,\n",
    "    cigar=pl.lit(\"\"),\n",
    "    subsample=pl.lit(\"\"),\n",
    ")\n",
    "if \"consesus_seq\" not in df_eaton.columns:\n",
    "    # if not including consensus seq\n",
    "    df_eaton = df_eaton.with_columns(consensus_seq=pl.lit(\"\"))\n",
    "df_eaton = (\n",
    "    df_eaton.rename({\"consensus_seq\": \"consensus\"})\n",
    "    .sort(\"barcode\")\n",
    "    .with_row_index(name=\"barcodeid\")\n",
    "    .with_row_index(name=\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba8282-8b37-4d99-8e41-eebaaaccb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eaton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14ab2e-49ae-4498-8085-27f5184ca346",
   "metadata": {},
   "source": [
    "We then write this to Parquet (which results in much smaller file sizes than CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a07d1-83f7-4979-8de5-25e432724e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eaton.write_parquet(\"eaton_export.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}